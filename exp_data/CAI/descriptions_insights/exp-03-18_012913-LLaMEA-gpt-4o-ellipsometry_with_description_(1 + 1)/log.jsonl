{"id": "53295112-db3a-4502-881d-9656759f25c2", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        \n        # Evolution loop\n        while self.budget > 0:\n            # Select parents based on fitness proportional selection\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size, \n                p=fitness / fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb)\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n        \n        # Return the best found solution\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "DynamicPopulationEvolution", "description": "The algorithm leverages a dynamic population-based search with adaptive mutation and crossover strategies inspired by biological evolution to efficiently explore and exploit the search space for optimal solutions.", "configspace": "", "generation": 0, "fitness": 0.2438456795121671, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.021. And the mean value of best solutions found was 0.099 (0. is the best) with standard deviation 0.046.", "error": "", "parent_id": null, "metadata": {"aucs": [0.21816921349052365, 0.24431891605614198, 0.2690489089898357], "final_y": [0.1532246301597944, 0.10244050118474861, 0.04055519839219992]}, "mutation_prompt": null}
{"id": "eb40be6b-e8f5-44fc-babf-349e22eb25f8", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        \n        # Evolution loop\n        while self.budget > 0:\n            # Select parents based on fitness proportional selection\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size, \n                p=fitness / fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0.3, 0.7, self.dim)  # refined blend range\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.05 * (func.bounds.ub - func.bounds.lb) # reduced mutation strength\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            elitism_idx = np.argmin(combined_fitness) # Elite strategy\n            population = combined_population[best_individuals_idx]\n            population[0] = combined_population[elitism_idx] # Add elite\n            fitness = combined_fitness[best_individuals_idx]\n        \n        # Return the best found solution\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "DynamicPopulationEvolution", "description": "The algorithm enhances the exploration-exploitation balance by implementing an elite strategy and refined mutation, boosting convergence efficiency.", "configspace": "", "generation": 1, "fitness": 0.23334797218017458, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.233 with standard deviation 0.010. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.048.", "error": "", "parent_id": "53295112-db3a-4502-881d-9656759f25c2", "metadata": {"aucs": [0.24458023696109743, 0.22114101768219852, 0.23432266189722784], "final_y": [0.1317912153005378, 0.17368849229760278, 0.05681988065064785]}, "mutation_prompt": null}
{"id": "9a669157-7c5a-4076-b5b1-a9c2b381339f", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        \n        # Evolution loop\n        while self.budget > 0:\n            # Select parents based on fitness proportional selection\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size, \n                p=fitness / fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (Lévy flight mutation)\n                levy_flight = np.random.standard_cauchy(self.dim) * 0.1\n                child += levy_flight * (func.bounds.ub - func.bounds.lb)\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n        \n        # Return the best found solution\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "DynamicPopulationEvolution", "description": "The algorithm improves exploration by incorporating Lévy flights for mutation, enhancing the search efficiency in complex landscapes.", "configspace": "", "generation": 2, "fitness": 0.21331015465055372, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.213 with standard deviation 0.011. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.047.", "error": "", "parent_id": "53295112-db3a-4502-881d-9656759f25c2", "metadata": {"aucs": [0.21490767304750746, 0.19887872573060927, 0.22614406517354446], "final_y": [0.15504188232716584, 0.24216468408023165, 0.13295869535821916]}, "mutation_prompt": null}
{"id": "cb4a3ce8-b061-45ce-a330-d8ca797a6f9e", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Select parents based on fitness proportional selection\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size, \n                p=fitness / fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb)\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce elitism by always preserving the best solution found so far, ensuring that the top candidate is never lost during population updates.", "configspace": "", "generation": 3, "fitness": 0.24701358946899302, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.247 with standard deviation 0.019. And the mean value of best solutions found was 0.077 (0. is the best) with standard deviation 0.044.", "error": "", "parent_id": "53295112-db3a-4502-881d-9656759f25c2", "metadata": {"aucs": [0.24255183167067107, 0.2724225106499747, 0.22606642608633332], "final_y": [0.12177558632167293, 0.01772135932308937, 0.0903189080723321]}, "mutation_prompt": null}
{"id": "a4fa13b1-c8fd-40dd-956e-d3ae32727054", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        population_size = min(20, self.budget // 2)\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        \n        no_improvement_count = 0  # Counter to track convergence\n\n        while self.budget > 0:\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size, \n                p=(1 / (fitness + 1e-9)) / (1 / (fitness + 1e-9)).sum()\n            )\n            parents = population[parents_idx]\n            \n            offspring = []\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * np.std(fitness)\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                child += mutation\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            combined_population = np.vstack((population, offspring, [best_solution]))\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            new_best_idx = np.argmin(fitness)\n            if fitness[new_best_idx] < fitness[best_idx]:\n                best_idx = new_best_idx\n                best_solution = population[best_idx]\n                no_improvement_count = 0  # Reset convergence counter\n            else:\n                no_improvement_count += 1\n\n            if no_improvement_count > 5:  # Restart if no improvement for 5 iterations\n                population = np.random.uniform(\n                    low=func.bounds.lb, \n                    high=func.bounds.ub, \n                    size=(population_size, self.dim)\n                )\n                fitness = np.array([func(ind) for ind in population])\n                self.budget -= population_size\n                no_improvement_count = 0\n\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhance search efficiency by introducing diversity via a restart mechanism when convergence stalls, and adjust mutation strength dynamically based on fitness variance.", "configspace": "", "generation": 4, "fitness": 0.19034896063130582, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.190 with standard deviation 0.022. And the mean value of best solutions found was 0.438 (0. is the best) with standard deviation 0.162.", "error": "", "parent_id": "cb4a3ce8-b061-45ce-a330-d8ca797a6f9e", "metadata": {"aucs": [0.18223006288253873, 0.22018747651931025, 0.1686293424920685], "final_y": [0.5174477332373543, 0.21192945620447948, 0.584254686760048]}, "mutation_prompt": null}
{"id": "1435a9ad-5e2d-4e6d-873d-2e25ee67b93c", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Select parents based on fitness proportional selection\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size, \n                p=fitness / fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce a dynamic mutation strength based on the best solution's fitness relative to the population to enhance exploration and convergence.", "configspace": "", "generation": 5, "fitness": 0.2683079435757411, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.268 with standard deviation 0.011. And the mean value of best solutions found was 0.041 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "cb4a3ce8-b061-45ce-a330-d8ca797a6f9e", "metadata": {"aucs": [0.2758665883780784, 0.2532069289095866, 0.27585031343955824], "final_y": [0.06112787011361208, 0.04494891236423311, 0.017567005402042413]}, "mutation_prompt": null}
{"id": "04928b9d-c56c-419a-aeed-0792bf554f86", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        population_size = min(20, self.budget // 2)\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n\n        while self.budget > 0:\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size, \n                p=fitness / fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            offspring = []\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                \n                if np.std(fitness) > 0.05:  # Switch mutation strategy based on fitness variance\n                    mutation = np.random.standard_cauchy(self.dim) * mutation_strength\n                else:\n                    mutation = np.random.normal(0, mutation_strength, self.dim)\n                child += mutation\n                \n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            combined_population = np.vstack((population, offspring, [best_solution]))\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Implement a multi-strategy mutation mechanism that alternates between Gaussian and Cauchy mutations based on fitness variance to improve exploration and robustness.", "configspace": "", "generation": 6, "fitness": 0.2672084385465224, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.267 with standard deviation 0.043. And the mean value of best solutions found was 0.098 (0. is the best) with standard deviation 0.110.", "error": "", "parent_id": "1435a9ad-5e2d-4e6d-873d-2e25ee67b93c", "metadata": {"aucs": [0.2969827483367229, 0.20653603966516842, 0.29810652763767576], "final_y": [0.036220115308696434, 0.2527224380963262, 0.004354772655302739]}, "mutation_prompt": null}
{"id": "96e8532f-8983-49f9-8fcd-46dbb3f7006b", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhance selection by incorporating a rank-based fitness scaling to improve diversity and maintain exploration.", "configspace": "", "generation": 7, "fitness": 0.272084140043332, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.272 with standard deviation 0.015. And the mean value of best solutions found was 0.067 (0. is the best) with standard deviation 0.028.", "error": "", "parent_id": "1435a9ad-5e2d-4e6d-873d-2e25ee67b93c", "metadata": {"aucs": [0.28539091581927345, 0.25074832400012903, 0.2801131803105935], "final_y": [0.044059701488059454, 0.10683482846498608, 0.05052713042325002]}, "mutation_prompt": null}
{"id": "e98dcfb8-8cb7-4f69-ad62-1080d1e93176", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation_strength *= np.random.uniform(0.9, 1.1)  # Adaptive mutation scaling\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce elitism and adaptive mutation scaling to enhance exploitation and achieve better convergence.", "configspace": "", "generation": 8, "fitness": 0.25133620969033194, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.251 with standard deviation 0.013. And the mean value of best solutions found was 0.113 (0. is the best) with standard deviation 0.032.", "error": "", "parent_id": "96e8532f-8983-49f9-8fcd-46dbb3f7006b", "metadata": {"aucs": [0.24670741971772425, 0.23866652457223658, 0.268634684781035], "final_y": [0.13614526796292004, 0.13408332721514155, 0.06785271885413777]}, "mutation_prompt": null}
{"id": "201c3260-ba4a-4307-9637-ff9308f30de1", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean()) * (self.budget / (self.budget + population_size))\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce a more dynamic mutation strength by incorporating the current budget percentage to balance exploration and exploitation dynamically.", "configspace": "", "generation": 9, "fitness": 0.2686239750166652, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.269 with standard deviation 0.027. And the mean value of best solutions found was 0.090 (0. is the best) with standard deviation 0.050.", "error": "", "parent_id": "96e8532f-8983-49f9-8fcd-46dbb3f7006b", "metadata": {"aucs": [0.25247905318310004, 0.3063878806002347, 0.24700499126666076], "final_y": [0.13173201549856497, 0.019265611051632065, 0.1196742141959678]}, "mutation_prompt": null}
{"id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduces an adaptive mutation probability in offspring generation to enhance exploration and convergence speed.", "configspace": "", "generation": 10, "fitness": 0.34244446605676654, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.342 with standard deviation 0.032. And the mean value of best solutions found was 0.007 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "96e8532f-8983-49f9-8fcd-46dbb3f7006b", "metadata": {"aucs": [0.37844372083969935, 0.30005826159664006, 0.34883141573396026], "final_y": [0.0019814248757511334, 0.01571056834596914, 0.003829127220022387]}, "mutation_prompt": null}
{"id": "55fc9074-2140-4b23-b88d-9dc433b39994", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness.std() / fitness.mean())  # Changed line\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduces a dynamic mutation strength based on the fitness variance to enhance exploration and convergence.", "configspace": "", "generation": 11, "fitness": 0.33927807559094, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.339 with standard deviation 0.011. And the mean value of best solutions found was 0.001 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.35374677151206124, 0.3269005625480156, 0.33718689271274316], "final_y": [0.0005014602625626267, 0.0003639658006434595, 0.002046504848259306]}, "mutation_prompt": null}
{"id": "b46d405d-ecab-4731-ae53-3e519f53675e", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size - 1]  # Preserve top two solutions\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduces elitism by always preserving the top two solutions for improved solution quality and convergence reliability.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {}, "mutation_prompt": null}
{"id": "2ccad727-5250-4135-ba31-34b300e3c2ec", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        population_size = min(20, self.budget // 2)\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n\n        while self.budget > 0:\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                levy_flight = np.random.standard_cauchy(self.dim)  # Levy flight modification\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation + levy_flight  # Incorporate Levy flight\n                \n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            combined_population = np.vstack((population, offspring, [best_solution]))\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhances exploration by introducing a random shift using Levy flights in offspring mutation.", "configspace": "", "generation": 13, "fitness": 0.2684856279246937, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.268 with standard deviation 0.086. And the mean value of best solutions found was 0.203 (0. is the best) with standard deviation 0.163.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.20448298906530205, 0.3899723388850169, 0.21100155582376212], "final_y": [0.40337537317320843, 0.004339444887254424, 0.20061091943226061]}, "mutation_prompt": null}
{"id": "91e782c7-84ad-40c8-a750-93358a4e3f84", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        population_size = min(20, self.budget // 2)\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        while self.budget > 0:\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            offspring = []\n            adaptive_mutation_prob = 0.15 + 0.35 * (fitness[best_idx] / fitness.mean())  # Adjusted adaptive probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                # Diversity-enhancing mutation\n                diversity_strength = 0.05 * (func.bounds.ub - func.bounds.lb)\n                diversity_mutation = np.random.uniform(-diversity_strength, diversity_strength, self.dim)\n                if np.random.rand() < 0.1:\n                    child += diversity_mutation\n\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            combined_population = np.vstack((population, offspring, [best_solution]))\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhances convergence by introducing a diversity-enhancing mutation and elitist selection strategy.", "configspace": "", "generation": 14, "fitness": 0.27183440729711233, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.272 with standard deviation 0.005. And the mean value of best solutions found was 0.045 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.2657661971666003, 0.27806666538667024, 0.2716703593380664], "final_y": [0.03632555516094848, 0.06287616348470793, 0.035942680029765396]}, "mutation_prompt": null}
{"id": "10e9f0e0-1a4c-4307-9e9e-8b2b39c7fc18", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.12 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())  # Increased mutation strength\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Implements a small modification in the mutation strategy to improve solution diversity by slightly increasing mutation strength.", "configspace": "", "generation": 15, "fitness": 0.2274398496419486, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.227 with standard deviation 0.022. And the mean value of best solutions found was 0.150 (0. is the best) with standard deviation 0.070.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.2580134828391084, 0.2084701478465696, 0.2158359182401678], "final_y": [0.12212492125080855, 0.08181327478836019, 0.24720350188604312]}, "mutation_prompt": null}
{"id": "5050ab5c-e93f-4e44-b75f-118389e8abf3", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * np.std(fitness)  # Adjusted line\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Adjusts mutation strength dynamically based on the current generation's fitness variability to better explore the search space.", "configspace": "", "generation": 16, "fitness": 0.25934616116233355, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.259 with standard deviation 0.063. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.150.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.34190621894463336, 0.18800969211780705, 0.24812257242456026], "final_y": [0.004998960598385526, 0.359274603682764, 0.09430272908872105]}, "mutation_prompt": null}
{"id": "669f8ea2-7910-401e-b13d-9bf9efe155c7", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0.3, 0.7, self.dim)  # Adjusted blend factor\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Adjust crossover mechanism by modifying the blend factor for improved exploration.", "configspace": "", "generation": 17, "fitness": 0.26255652558380416, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.263 with standard deviation 0.046. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.093.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.23829196506902728, 0.2224724901671461, 0.3269051215152391], "final_y": [0.13628881464489423, 0.2344545904790562, 0.006718134935442481]}, "mutation_prompt": null}
{"id": "eb9d4682-cc3c-478f-8fcb-11bc13536475", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define initial population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Adjust population size based on fitness diversity\n            population_size = max(10, min(30, int(np.std(fitness) * 100)))\n\n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduces a dynamic population size based on fitness diversity to further improve exploration and convergence.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {}, "mutation_prompt": null}
{"id": "7946baa7-cee6-4d46-b578-dd5846b45a09", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define initial population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Dynamic population size adjustment based on fitness variance\n            variance = np.var(fitness)\n            population_size = max(10, int(variance * 10))  # Adjust population size\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduces a dynamic population size adjustment based on current fitness variance to improve exploration and exploitation balance.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {}, "mutation_prompt": null}
{"id": "6a9a4bf0-75a5-4bf6-ba69-e846afcdc393", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define initial population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n\n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n\n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n\n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n\n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n\n            # Elitist selection and adaptive population sizing\n            elite_size = max(1, population_size // 5)\n            population_size = max(20, int(population_size * 0.9))  # Reduce population size adaptively\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n\n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhances convergence by incorporating elitist selection and adaptive population sizing to better balance exploration and exploitation.", "configspace": "", "generation": 20, "fitness": 0.33438929216646657, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.334 with standard deviation 0.036. And the mean value of best solutions found was 0.008 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.28448958843679895, 0.34803724736353225, 0.37064104069906834], "final_y": [0.01001351265407998, 0.012255091042178484, 0.0029313604447687544]}, "mutation_prompt": null}
{"id": "7bf4a370-cb10-4663-8b08-2872b4264802", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) * (self.budget / (self.budget + population_size))  # Linear decay in mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduces a linear decay in mutation probability over generations to balance exploration and exploitation.", "configspace": "", "generation": 21, "fitness": 0.29535758409125434, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.295 with standard deviation 0.050. And the mean value of best solutions found was 0.074 (0. is the best) with standard deviation 0.090.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.22612909589399444, 0.32035325168566786, 0.33959040469410073], "final_y": [0.2014302893234827, 0.00909296899961662, 0.011664718841909507]}, "mutation_prompt": null}
{"id": "edd2b376-ba53-4bd0-a8b3-f8f4e43d093b", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n        success_rate = 0.2  # Initial success rate for adaptive mutation\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * success_rate  # Self-adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Calculate success rate\n            success_rate = np.mean(offspring_fitness < fitness.min())\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduces a self-adaptive mutation probability based on historical success rate, enhancing exploration and convergence in varying landscapes.", "configspace": "", "generation": 22, "fitness": 0.3406762977784726, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.341 with standard deviation 0.223. And the mean value of best solutions found was 0.283 (0. is the best) with standard deviation 0.399.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.1711730419169064, 0.1948450817946601, 0.6560107696238514], "final_y": [0.8477327727917028, 0.002232996209082399, 1.4941388834383535e-08]}, "mutation_prompt": null}
{"id": "81d0cdc7-c408-4ee5-8d16-e9c0fa43a0da", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            diversity = np.std(population, axis=0).mean()  # Calculate diversity\n            dynamic_crossover_rate = 0.5 + 0.5 * (1 - diversity / (func.bounds.ub - func.bounds.lb).mean())  # Dynamic crossover rate\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, dynamic_crossover_rate, self.dim)  # Use dynamic crossover rate\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduces a dynamic crossover rate based on the diversity of the population to improve solution exploration and convergence.", "configspace": "", "generation": 23, "fitness": 0.2864796099119962, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.286 with standard deviation 0.014. And the mean value of best solutions found was 0.026 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.2665653900762187, 0.29348606371488273, 0.2993873759448872], "final_y": [0.03380422903638361, 0.00413737282119331, 0.03935211073400164]}, "mutation_prompt": null}
{"id": "cfd0ec21-d879-4681-a918-10c54137eec4", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduces elitism by preserving a fraction of the best solutions in each generation to improve convergence.", "configspace": "", "generation": 24, "fitness": 0.2749724587474791, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.275 with standard deviation 0.047. And the mean value of best solutions found was 0.090 (0. is the best) with standard deviation 0.077.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.22717766974344822, 0.2581493017614882, 0.3395904047375009], "final_y": [0.19446040532052541, 0.06331293704336795, 0.011664718848688641]}, "mutation_prompt": null}
{"id": "558a8ece-ff23-455c-89a6-4b76aa9a9746", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                fitness_variance = np.var(fitness)  # Adjust mutation strength based on fitness variance\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness_variance / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhances exploration by dynamically adjusting mutation strength based on fitness variance across the population.", "configspace": "", "generation": 25, "fitness": 0.202847850020639, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.203 with standard deviation 0.080. And the mean value of best solutions found was 1.148 (0. is the best) with standard deviation 1.081.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.17287289022966645, 0.1233061643355694, 0.3123644954966811], "final_y": [0.8093582587154406, 2.608676076576166, 0.026442850235092092]}, "mutation_prompt": null}
{"id": "78afe484-43a5-44f0-ad50-bb177646ece6", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                diversity_factor = np.std(population, axis=0)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean()) + 0.05 * diversity_factor\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhances exploration by introducing a diversity-enhancing factor in the mutation strength calculation.", "configspace": "", "generation": 26, "fitness": 0.33268877786242895, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.333 with standard deviation 0.034. And the mean value of best solutions found was 0.003 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.29451629741678065, 0.32666034307209313, 0.37688969309841314], "final_y": [0.0032384373083262034, 0.0010040890384663932, 0.0041129909204828275]}, "mutation_prompt": null}
{"id": "58dc6ff3-c8cc-40ac-b16f-a6a67f779a4e", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Incorporate rank-based offspring selection to enhance diversity and convergence speed.", "configspace": "", "generation": 27, "fitness": 0.2749724587474791, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.275 with standard deviation 0.047. And the mean value of best solutions found was 0.090 (0. is the best) with standard deviation 0.077.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.22717766974344822, 0.2581493017614882, 0.3395904047375009], "final_y": [0.19446040532052541, 0.06331293704336795, 0.011664718848688641]}, "mutation_prompt": null}
{"id": "5e070d0f-a5d5-4e76-b206-3453bc824b0a", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (np.std(population) / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduces a more dynamic scaling of mutation strength based on current population diversity to enhance exploration.", "configspace": "", "generation": 28, "fitness": 0.21032963692178042, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.039. And the mean value of best solutions found was 0.460 (0. is the best) with standard deviation 0.308.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.17092948843081102, 0.19694861103092276, 0.2631108113036075], "final_y": [0.8458159095179077, 0.44283398221344095, 0.09096987295226365]}, "mutation_prompt": null}
{"id": "40443c57-c508-4440-8fb8-4862ec0c6e3a", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhances selection by introducing elitist selection to preserve top solutions, thereby improving convergence reliability.", "configspace": "", "generation": 29, "fitness": 0.2749724587474791, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.275 with standard deviation 0.047. And the mean value of best solutions found was 0.090 (0. is the best) with standard deviation 0.077.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.22717766974344822, 0.2581493017614882, 0.3395904047375009], "final_y": [0.19446040532052541, 0.06331293704336795, 0.011664718848688641]}, "mutation_prompt": null}
{"id": "e3ca19c1-0f07-4ff8-9dd7-73cfd98080d2", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * ((fitness.max() - fitness[best_idx]) / (fitness.max() - fitness.min()))  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhances mutation strategy by adapting mutation probability based on normalized fitness deviation for better exploration and convergence.", "configspace": "", "generation": 30, "fitness": 0.2515201320395976, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.252 with standard deviation 0.041. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.125.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.2041804270096721, 0.30514772126059897, 0.2452322478485217], "final_y": [0.3227840833473652, 0.021698189548307024, 0.12978668455086895]}, "mutation_prompt": null}
{"id": "fc52af44-5306-439c-a01f-641c88229e55", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation * (best_solution - child) * 0.01  # Introduce slight bias towards best solution\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhances adaptive mutation by introducing a slight bias towards the best solution for improved convergence. ", "configspace": "", "generation": 31, "fitness": 0.19709373679753017, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.197 with standard deviation 0.077. And the mean value of best solutions found was 1.264 (0. is the best) with standard deviation 1.196.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.17038946807441546, 0.11917903675748975, 0.3017127055606853], "final_y": [0.8741387302759209, 2.8851605890846543, 0.033550376938392176]}, "mutation_prompt": null}
{"id": "40ff81c6-1085-4be7-bb99-1c705343dee4", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]\n                \n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2\n                \n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduces an adaptive population size based on convergence progress to enhance exploration and exploitation balance.", "configspace": "", "generation": 32, "fitness": 0.2749724587474791, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.275 with standard deviation 0.047. And the mean value of best solutions found was 0.090 (0. is the best) with standard deviation 0.077.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.22717766974344822, 0.2581493017614882, 0.3395904047375009], "final_y": [0.19446040532052541, 0.06331293704336795, 0.011664718848688641]}, "mutation_prompt": null}
{"id": "7f864cdd-f577-4ec5-a3fe-838cca0c6749", "solution": "# Code for DynamicPopulationEvolution class\nimport numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * (0.5 * (parent2 + parent3))  # Modified crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduces ensemble crossover to enhance exploration by combining multiple parent solutions.", "configspace": "", "generation": 33, "fitness": 0.34613637674242126, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.346 with standard deviation 0.054. And the mean value of best solutions found was 0.014 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "71251e46-8fdd-4c8a-818e-b10e3989cac8", "metadata": {"aucs": [0.35062066473591613, 0.2783963988419439, 0.40939206664940375], "final_y": [0.010144449651944757, 0.029550917350292347, 0.0021886062510881746]}, "mutation_prompt": null}
{"id": "ac23a60e-d70e-4bfb-acb7-cecdd1ca7e9d", "solution": "# Code for DynamicPopulationEvolution class\nimport numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * (0.5 * (parent2 + parent3))  # Modified crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                population_diversity = np.std(population, axis=0)  # Calculate population diversity\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (1 + population_diversity.mean())  # Updated mutation strength\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Incorporates an adaptive mutation strength scaling based on the population diversity to enhance exploration.", "configspace": "", "generation": 34, "fitness": 0.23566516334654897, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.236 with standard deviation 0.011. And the mean value of best solutions found was 0.180 (0. is the best) with standard deviation 0.046.", "error": "", "parent_id": "7f864cdd-f577-4ec5-a3fe-838cca0c6749", "metadata": {"aucs": [0.22649480721785153, 0.22964242336321417, 0.2508582594585812], "final_y": [0.22461850651433887, 0.1987801043122033, 0.11595144186761044]}, "mutation_prompt": null}
{"id": "579d01ad-89f4-4255-a602-636d8a66c431", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  \n\n                # Crossover (blend crossover)\n                alpha, beta = np.random.uniform(0, 1, self.dim), np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * (beta * parent2 + (1 - beta) * parent3)  # Modified crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhanced exploration by introducing a third parent in crossover with a modified ensemble crossover strategy.", "configspace": "", "generation": 35, "fitness": 0.27143991834163084, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.271 with standard deviation 0.013. And the mean value of best solutions found was 0.037 (0. is the best) with standard deviation 0.019.", "error": "", "parent_id": "7f864cdd-f577-4ec5-a3fe-838cca0c6749", "metadata": {"aucs": [0.25302970511778955, 0.28389242545395266, 0.2773976244531503], "final_y": [0.03116114861583052, 0.0170498739215076, 0.06213944129612212]}, "mutation_prompt": null}
{"id": "3dc07ddb-cdc2-4500-b01d-8ae4b0c2a354", "solution": "# Code for DynamicPopulationEvolution class\nimport numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhances local exploitation by incorporating a weighted centroid mechanism during crossover to improve convergence.", "configspace": "", "generation": 36, "fitness": 0.3463971354647286, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.346 with standard deviation 0.043. And the mean value of best solutions found was 0.008 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "7f864cdd-f577-4ec5-a3fe-838cca0c6749", "metadata": {"aucs": [0.3024447361854038, 0.33286023859205616, 0.4038864316167259], "final_y": [0.011426708763101943, 0.013775802641338016, 5.747304673302325e-06]}, "mutation_prompt": null}
{"id": "28894137-dfa4-4c36-909c-e0c9e1ddcbba", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.15 + 0.4 * (fitness[best_idx] / fitness.mean())  # Increased mutation probability\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhances local exploitation by incorporating a weighted centroid mechanism during crossover, with an increased mutation probability to improve convergence.", "configspace": "", "generation": 37, "fitness": 0.22782281172487853, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.228 with standard deviation 0.021. And the mean value of best solutions found was 0.200 (0. is the best) with standard deviation 0.086.", "error": "", "parent_id": "3dc07ddb-cdc2-4500-b01d-8ae4b0c2a354", "metadata": {"aucs": [0.25792817141864877, 0.21187050144976194, 0.21366976230622492], "final_y": [0.08475747460842711, 0.22493324728704492, 0.2911143257112858]}, "mutation_prompt": null}
{"id": "52327742-61d0-4904-97ca-b7473bf9cec4", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        population_size = min(20, self.budget // 2)\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n\n        iteration = 0\n        while self.budget > 0:\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())\n            for i in range(population_size):\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]\n\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3\n                child = alpha * parent1 + (1 - alpha) * centroid\n\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            combined_population = np.vstack((population, offspring, [best_solution]))\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            \n            iteration += 1\n            if iteration % 10 == 0:  # Population reduction every 10 iterations\n                penalty_factor = 0.9\n                population_size = max(2, int(population_size * penalty_factor))\n        \n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduces a dynamic population size scaling mechanism by adding population reduction every 10 iterations based on a population penalty factor.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "3dc07ddb-cdc2-4500-b01d-8ae4b0c2a354", "metadata": {}, "mutation_prompt": null}
{"id": "daf7d3a1-d649-4555-a15b-a93dcd164874", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        population_size = min(20, self.budget // 2)\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n\n        while self.budget > 0:\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            # Tournament selection\n            parents = np.array([self.tournament_selection(population, fitness, 3) for _ in range(population_size)])\n            \n            offspring = []\n            for i in range(population_size):\n                parent1, parent2 = parents[i], parents[(i + 1) % population_size]  # Removed third parent\n                alpha = np.random.uniform(0, 1, self.dim)\n                child = alpha * parent1 + (1 - alpha) * parent2  # Two-point crossover\n\n                # Adaptive mutation based on convergence history\n                history_factor = fitness[best_idx] / (fitness.mean() + 1e-9)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * history_factor\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < 0.3 + 0.5 * history_factor:  # Updated adaptive mutation probability\n                    child += mutation\n                \n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            combined_population = np.vstack((population, offspring, [best_solution]))\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        return best_solution\n\n    def tournament_selection(self, population, fitness, tournament_size):\n        \"\"\"Tournament selection process to enhance diversity.\"\"\"\n        indices = np.random.choice(len(population), tournament_size, replace=False)\n        selected_idx = indices[np.argmin(fitness[indices])]\n        return population[selected_idx]", "name": "DynamicPopulationEvolution", "description": "Introduces dynamic individual adaption based on convergence history and employs a tournament selection mechanism to enhance solution quality.", "configspace": "", "generation": 39, "fitness": 0.30815771227084565, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.308 with standard deviation 0.048. And the mean value of best solutions found was 0.045 (0. is the best) with standard deviation 0.030.", "error": "", "parent_id": "3dc07ddb-cdc2-4500-b01d-8ae4b0c2a354", "metadata": {"aucs": [0.27944401631342786, 0.3753616228922073, 0.2696674976069018], "final_y": [0.06681744212087319, 0.0029322220397548916, 0.06420964422829771]}, "mutation_prompt": null}
{"id": "3b233035-2766-4f72-b16a-e63cde7df3ee", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                # Use differential weighting strategy for mutation\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation * 0.5\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            # Use tournament selection for the next generation\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhances convergence by introducing tournament selection and a differential weighting strategy during mutation.", "configspace": "", "generation": 40, "fitness": 0.27083994436314046, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.271 with standard deviation 0.027. And the mean value of best solutions found was 0.057 (0. is the best) with standard deviation 0.038.", "error": "", "parent_id": "3dc07ddb-cdc2-4500-b01d-8ae4b0c2a354", "metadata": {"aucs": [0.2876745879468998, 0.2924054729334784, 0.23243977220904322], "final_y": [0.03998806446413786, 0.021270564440225688, 0.10990419109791211]}, "mutation_prompt": null}
{"id": "547c8b08-1549-4779-8ba9-06a6024eb4e7", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Tournament selection\n            parents_idx = [np.argmin(random.sample(list(np.arange(population_size)), k=3)) for _ in range(population_size)]\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.mean())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduces tournament selection to replace rank-based scaling for more diverse parent selection.", "configspace": "", "generation": 41, "fitness": 0.2961724032435221, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.296 with standard deviation 0.064. And the mean value of best solutions found was 0.058 (0. is the best) with standard deviation 0.043.", "error": "", "parent_id": "3dc07ddb-cdc2-4500-b01d-8ae4b0c2a354", "metadata": {"aucs": [0.380946176145211, 0.22771148799037755, 0.27985954559497783], "final_y": [0.004548499029184989, 0.10970538898694435, 0.05893483628205564]}, "mutation_prompt": null}
{"id": "79058899-bec7-4d3d-85c6-c896b1c0d000", "solution": "# Code for DynamicPopulationEvolution class\nimport numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Improves convergence by adjusting the mutation strength relative to the current best fitness.", "configspace": "", "generation": 42, "fitness": 0.41913689972477025, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.419 with standard deviation 0.220. And the mean value of best solutions found was 0.058 (0. is the best) with standard deviation 0.075.", "error": "", "parent_id": "3dc07ddb-cdc2-4500-b01d-8ae4b0c2a354", "metadata": {"aucs": [0.24517243820197154, 0.28210528439611005, 0.7301329765762292], "final_y": [0.16338662969791554, 0.010607705021737154, 6.590827161229862e-11]}, "mutation_prompt": null}
{"id": "12e693db-3f28-4ace-a95d-bdffd412a7ea", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                diversity_coefficient = np.std(fitness) / fitness.mean()  # Introducing diversity measure\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min()) * (1 + diversity_coefficient)  # Adjusted mutation strength\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Incorporates fitness diversity by calculating a diversity coefficient influencing mutation strength.", "configspace": "", "generation": 43, "fitness": 0.27814990857686317, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.278 with standard deviation 0.092. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.096.", "error": "", "parent_id": "79058899-bec7-4d3d-85c6-c896b1c0d000", "metadata": {"aucs": [0.21258320650880513, 0.2139409110931073, 0.40792560812867706], "final_y": [0.17974804655469862, 0.22425109816643932, 0.0027171839271196902]}, "mutation_prompt": null}
{"id": "dcef105f-341e-4e9e-b462-9fc271bfe714", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks**1.5)  # Non-linear rank-based scaling\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhances exploration by introducing a non-linear rank-based scaling factor for parent selection.", "configspace": "", "generation": 44, "fitness": 0.2643529929984017, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.264 with standard deviation 0.030. And the mean value of best solutions found was 0.090 (0. is the best) with standard deviation 0.078.", "error": "", "parent_id": "79058899-bec7-4d3d-85c6-c896b1c0d000", "metadata": {"aucs": [0.3049798271005748, 0.2562190464771439, 0.23186010541748636], "final_y": [0.009790726039142914, 0.06518906095448575, 0.1951007827504336]}, "mutation_prompt": null}
{"id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce a dynamic mutation strength that decreases over generations, enhancing exploitation as the search progresses.", "configspace": "", "generation": 45, "fitness": 0.4654706574532936, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.465 with standard deviation 0.135. And the mean value of best solutions found was 0.001 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "79058899-bec7-4d3d-85c6-c896b1c0d000", "metadata": {"aucs": [0.3710710146342928, 0.3689115912855849, 0.6564293664400029], "final_y": [0.0007173834974168456, 0.0032277807209470997, 7.299384644565142e-07]}, "mutation_prompt": null}
{"id": "d9c810f4-fa2c-4076-ac33-9e5b188e8389", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation_strength *= ranks[i] / (population_size - 1)  # New Change: Rank-based mutation strength\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce rank-based mutation strength to intensify exploration based on population diversity.", "configspace": "", "generation": 46, "fitness": 0.45801533268856365, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.458 with standard deviation 0.113. And the mean value of best solutions found was 0.002 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.4209561317383428, 0.3415012459917073, 0.6115886203356409], "final_y": [6.227088759660834e-06, 0.005191374974019123, 1.3335549556120988e-06]}, "mutation_prompt": null}
{"id": "dcaa1b95-6656-4c6e-8492-c446b1899df3", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced probabilistic multi-parent crossover\n                if np.random.rand() < 0.5:\n                    parent1, parent2, parent3 = random.sample(list(parents), 3)\n                else:\n                    parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce a probabilistic multi-parent crossover to improve diversity and exploration during the search process.", "configspace": "", "generation": 47, "fitness": 0.22123825238079356, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.221 with standard deviation 0.098. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.097.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.15674858260993663, 0.14749168390795198, 0.3594744906244921], "final_y": [0.219407192976603, 0.1890904627409439, 0.00010424469196469928]}, "mutation_prompt": null}
{"id": "7f6112b7-8b09-49f7-a43c-b8fcae773e8f", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize multiple swarms\n        num_swarms = 3  # New: Utilize multiple swarms\n        swarms = [np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        ) for _ in range(num_swarms)]\n        \n        fitness_swarms = [np.array([func(ind) for ind in swarm]) for swarm in swarms]\n        self.budget -= population_size * num_swarms\n        best_idx_swarms = [np.argmin(fitness) for fitness in fitness_swarms]\n        best_solution_swarms = [swarms[i][best_idx_swarms[i]] for i in range(num_swarms)]\n\n        # Evolution loop\n        while self.budget > 0:\n            for swarm_idx, (population, fitness, best_solution) in enumerate(zip(swarms, fitness_swarms, best_solution_swarms)):\n                # Rank-based fitness scaling\n                ranks = np.argsort(fitness)\n                scaled_fitness = 1.0 / (1.0 + ranks)\n                parents_idx = np.random.choice(\n                    population_size, \n                    size=population_size,\n                    p=scaled_fitness / scaled_fitness.sum()\n                )\n                parents = population[parents_idx]\n                \n                # Generate offspring through crossover and mutation\n                offspring = []\n                adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx_swarms[swarm_idx]] / fitness.mean())  # Adaptive mutation probability\n                mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n                for i in range(population_size):\n                    parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]\n\n                    # Crossover (blend crossover)\n                    alpha = np.random.uniform(0, 1, self.dim)\n                    centroid = (parent1 + parent2 + parent3) / 3\n                    child = alpha * parent1 + (1 - alpha) * centroid\n\n                    # Mutation (adaptive Gaussian mutation)\n                    mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx_swarms[swarm_idx]] / fitness.min())\n                    mutation = np.random.normal(0, mutation_strength, self.dim)\n                    if np.random.rand() < adaptive_mutation_prob:\n                        child += mutation\n                    \n                    # Ensure child is within bounds\n                    child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                    offspring.append(child)\n                \n                offspring = np.array(offspring)\n                offspring_fitness = np.array([func(ind) for ind in offspring])\n                self.budget -= population_size\n                \n                # Combine and select the next generation\n                combined_population = np.vstack((population, offspring, [best_solution]))\n                combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx_swarms[swarm_idx]]]))\n                best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n                swarms[swarm_idx] = combined_population[best_individuals_idx]\n                fitness_swarms[swarm_idx] = combined_fitness[best_individuals_idx]\n                \n                # Update best solution for swarm\n                best_idx_swarms[swarm_idx] = np.argmin(fitness_swarms[swarm_idx])\n                best_solution_swarms[swarm_idx] = swarms[swarm_idx][best_idx_swarms[swarm_idx]]\n        \n        # Return the overall best found solution\n        global_best_idx = np.argmin([func(best) for best in best_solution_swarms])\n        return best_solution_swarms[global_best_idx]", "name": "DynamicPopulationEvolution", "description": "Introduce a multi-swarm strategy to diversify search and improve global exploration.", "configspace": "", "generation": 48, "fitness": 0.174447619927133, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.174 with standard deviation 0.036. And the mean value of best solutions found was 0.526 (0. is the best) with standard deviation 0.595.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.21364533333590152, 0.12691902549785083, 0.1827785009476467], "final_y": [0.08696060398319166, 1.3671751072280613, 0.12331472956983919]}, "mutation_prompt": null}
{"id": "d9bc335d-9b32-4018-ab8b-61aaf99c71b0", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    # Modification: Incorporate ensemble averaging of best and worst solutions\n                    worst_solution = population[np.argmax(fitness)]\n                    mutation = (best_solution + worst_solution) / 2 + mutation  # Line changed\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce an ensemble approach by averaging the best and worst solutions in the dynamic mutation strategy.", "configspace": "", "generation": 49, "fitness": 0.1118755052619816, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.112 with standard deviation 0.013. And the mean value of best solutions found was 3.474 (0. is the best) with standard deviation 0.940.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.10565324691678346, 0.12977154011813452, 0.10020172875102684], "final_y": [3.810785998681296, 2.191695081134787, 4.419253079374653]}, "mutation_prompt": null}
{"id": "4d8b9d22-7e9e-48ec-9724-689ea3b6b0ae", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.random.choice(np.argsort(combined_fitness)[:population_size], size=population_size, replace=False)  # Change: Use tournament selection\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Optimize offspring selection by using tournament selection for enhanced diversity and exploration.", "configspace": "", "generation": 50, "fitness": 0.23965676708314484, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.240 with standard deviation 0.120. And the mean value of best solutions found was 0.204 (0. is the best) with standard deviation 0.147.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.16537970836754357, 0.14526221711217824, 0.4083283757697127], "final_y": [0.26837875653110943, 0.34343816699785445, 0.0006420211545307402]}, "mutation_prompt": null}
{"id": "2465488d-19bf-4c6d-9f63-30cb48b2d20d", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3.5  # Change: Use weighted average with more influence from parent1\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhance exploration by modifying crossover to include a weighted average of more parents.", "configspace": "", "generation": 51, "fitness": 0.1725747861457796, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.173 with standard deviation 0.042. And the mean value of best solutions found was 0.727 (0. is the best) with standard deviation 0.546.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.11668780825430058, 0.21686779040513604, 0.18416875977790215], "final_y": [1.497888724713552, 0.2947674038093798, 0.38818746659172404]}, "mutation_prompt": null}
{"id": "55e2cefe-9d14-4adb-bc6a-e114c6752eb3", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (np.var(population, axis=0).mean())  # Adjusted mutation strength based on diversity\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhance exploration by adjusting mutation strength based on diversity in the population, improving search space coverage.", "configspace": "", "generation": 52, "fitness": 0.1640544516281367, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.164 with standard deviation 0.056. And the mean value of best solutions found was 1.746 (0. is the best) with standard deviation 1.138.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.12794068189779906, 0.1211993465913852, 0.24302332639522584], "final_y": [2.3397007866600807, 2.7442918555173907, 0.15343372355935378]}, "mutation_prompt": null}
{"id": "597b838d-e1de-463b-9907-ce598c8faab2", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            # Diversity preservation by reintegrating random solutions\n            if self.budget % 10 == 0:  # Every 10 iterations\n                weakest_idx = np.argsort(fitness)[-population_size//2:]\n                random_solutions = np.random.uniform(\n                    low=func.bounds.lb, \n                    high=func.bounds.ub, \n                    size=(population_size//2, self.dim)\n                )\n                population[weakest_idx] = random_solutions\n                \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Add a diversity-preserving mechanism by merging the weakest individuals with random solutions periodically.", "configspace": "", "generation": 53, "fitness": 0.20013472367151786, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.200 with standard deviation 0.051. And the mean value of best solutions found was 0.094 (0. is the best) with standard deviation 0.112.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.1809504504627497, 0.1490158452893735, 0.27043787526243035], "final_y": [0.00785176199782479, 0.25235659250017634, 0.020983206723897675]}, "mutation_prompt": null}
{"id": "98e6dbad-66f8-4e96-8ef9-31e59c4a7a5a", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.2 + 0.3 * (fitness[best_idx] / fitness.mean())  # Adjusted adaptive mutation probability\n            mutation_decay_factor = 1 - ((self.budget / (self.budget + population_size)) * (fitness.min() / fitness.mean()))  # Adjusted dynamic mutation decay\n            for i in range(population_size):\n                # Multi-parent crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  \n                child = alpha * parent1 + (1 - alpha) * centroid\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhance exploration with a dynamic multi-parent blend crossover and adaptive mutation using a normalized fitness-based decay.", "configspace": "", "generation": 54, "fitness": 0.11776449492934112, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.118 with standard deviation 0.060. And the mean value of best solutions found was 2.309 (0. is the best) with standard deviation 1.767.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.08385910331236712, 0.06789032636301207, 0.20154405511264417], "final_y": [2.552608837541807, 4.3407985678332395, 0.032430402327925816]}, "mutation_prompt": null}
{"id": "d7d78029-89ed-4180-aa90-b3ca94af72a4", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx] * (1 + 0.1 * np.std(population, axis=0))  # Fitness sharing added\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce a fitness sharing mechanism to increase diversity and prevent premature convergence.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (20,) (2,) ').", "error": "ValueError('operands could not be broadcast together with shapes (20,) (2,) ')", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {}, "mutation_prompt": null}
{"id": "cc1d2a2e-a6b2-4bca-b43d-c7d9bbb036ba", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness.max() - fitness.min())  # Changed to fitness.max() - fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce a rank-based adaptive mutation strength that adjusts based on the range of fitness values, enhancing exploration.", "configspace": "", "generation": 56, "fitness": 0.19069849219630938, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.191 with standard deviation 0.048. And the mean value of best solutions found was 0.967 (0. is the best) with standard deviation 0.942.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.2001443328629091, 0.24471041425172146, 0.12724072947429754], "final_y": [0.4537391965664657, 0.15856267984336772, 2.288059537102404]}, "mutation_prompt": null}
{"id": "be4604f0-4515-46ba-a7db-83e9336e312a", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Introduce random immigrant strategy\n            if self.budget > population_size:\n                random_immigrants = np.random.uniform(low=func.bounds.lb, high=func.bounds.ub, size=(population_size//10, self.dim))\n                population[:population_size//10] = random_immigrants  # Replace some solutions\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhance diversity by introducing random immigrant strategy, adding new random solutions periodically.", "configspace": "", "generation": 57, "fitness": 0.21718815612424083, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.217 with standard deviation 0.010. And the mean value of best solutions found was 0.208 (0. is the best) with standard deviation 0.107.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.22838283031979323, 0.20336148155232825, 0.21982015650060105], "final_y": [0.23074713508299124, 0.32532287389751857, 0.06667520379028852]}, "mutation_prompt": null}
{"id": "ceef1ca9-bbfa-4911-b721-0a11fa808278", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Reinitialize 10% of the population for diversity\n            if np.random.rand() < 0.1:\n                reinit_count = max(1, int(0.1 * population_size))\n                for _ in range(reinit_count):\n                    new_individual = np.random.uniform(low=func.bounds.lb, high=func.bounds.ub, size=self.dim)\n                    population[random.randint(0, population_size-1)] = new_individual\n                    fitness[random.randint(0, population_size-1)] = func(new_individual)\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Incorporate diversity re-initialization to introduce fresh genes periodically, while preserving elite solutions.", "configspace": "", "generation": 58, "fitness": 0.2961075614601893, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.296 with standard deviation 0.116. And the mean value of best solutions found was 0.024 (0. is the best) with standard deviation 0.031.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.13249297588305042, 0.36869958353368104, 0.38713012496383636], "final_y": [0.06823873354201136, 0.0011991485162481082, 0.0034643968924836024]}, "mutation_prompt": null}
{"id": "8258277f-5320-47b0-b75a-dd61526b2df9", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            fitness_variability = fitness.std() / (fitness.mean() + 1e-6)  # Calculate fitness variability\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3\n                adaptive_crossover_rate = 0.5 + 0.5 * fitness_variability  # Adaptive crossover rate\n                child = alpha * parent1 + (1 - alpha) * centroid * adaptive_crossover_rate\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce adaptive crossover rate based on fitness variability to enhance solution diversity.", "configspace": "", "generation": 59, "fitness": 0.1342891244874385, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.134 with standard deviation 0.031. And the mean value of best solutions found was 2.208 (0. is the best) with standard deviation 1.059.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.11247579872646774, 0.11277113610377043, 0.17762043863207733], "final_y": [2.474207763126737, 3.351198345893875, 0.7994724617674452]}, "mutation_prompt": null}
{"id": "7d475244-baca-4e60-baf4-1a7b59d11732", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce a stochastic elitism strategy by always including the best solution in offspring to improve convergence stability.", "configspace": "", "generation": 60, "fitness": 0.4654706574532936, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.465 with standard deviation 0.135. And the mean value of best solutions found was 0.001 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.3710710146342928, 0.3689115912855849, 0.6564293664400029], "final_y": [0.0007173834974168456, 0.0032277807209470997, 7.299384644565142e-07]}, "mutation_prompt": null}
{"id": "edb926b9-4c93-437a-81b2-fb1816e2892c", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + population_size))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * ((fitness[best_idx] + fitness.min()) / (2 * fitness.min()))  # Adjust mutation strength\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Improved exploration by adjusting mutation strength based on both global and local fitness landscapes.", "configspace": "", "generation": 61, "fitness": 0.4654706574532936, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.465 with standard deviation 0.135. And the mean value of best solutions found was 0.001 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.3710710146342928, 0.3689115912855849, 0.6564293664400029], "final_y": [0.0007173834974168456, 0.0032277807209470997, 7.299384644565142e-07]}, "mutation_prompt": null}
{"id": "3884f004-a691-4884-921d-6eaceaded2fc", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce a more aggressive mutation strategy early on by adjusting the mutation decay factor formula.", "configspace": "", "generation": 62, "fitness": 0.46951768067750094, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.470 with standard deviation 0.037. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7c02dff7-1beb-41a9-a8ee-b9e2723ef015", "metadata": {"aucs": [0.47421527399417995, 0.4221796452141896, 0.5121581228241332], "final_y": [4.512912243071751e-06, 0.0003059363735907808, 7.055562668520584e-05]}, "mutation_prompt": null}
{"id": "89147327-cb01-45ff-8a78-c8689e8445f6", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.2 + 0.3 * (fitness[best_idx] / fitness.mean())  # Slightly increased base mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhance adaptive mutation probability to improve exploration and exploitation balance.", "configspace": "", "generation": 63, "fitness": 0.41325018542305775, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.413 with standard deviation 0.058. And the mean value of best solutions found was 0.001 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "3884f004-a691-4884-921d-6eaceaded2fc", "metadata": {"aucs": [0.3349935632280646, 0.4322676825360482, 0.4724893105050605], "final_y": [0.0032619300128870574, 0.0004789430947183543, 0.00013837258892483782]}, "mutation_prompt": null}
{"id": "9d659228-c759-48a9-903e-428e086bf2e0", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * np.log1p(fitness[best_idx] / fitness.min())  # Change: Altered mutation strength formula\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Improve offspring generation by modifying the mutation strength formula for better exploration.", "configspace": "", "generation": 64, "fitness": 0.3712933147158677, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.371 with standard deviation 0.037. And the mean value of best solutions found was 0.001 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3884f004-a691-4884-921d-6eaceaded2fc", "metadata": {"aucs": [0.3194743230660334, 0.40433143791847714, 0.3900741831630924], "final_y": [0.0006616018857341149, 0.0007755495829546487, 0.001501446170665165]}, "mutation_prompt": null}
{"id": "1b5a69c3-3a65-4c3f-91a9-e5985d509436", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Dynamic rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks + random.uniform(0, 0.1))  # Added random factor to scale\n            \n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce a dynamic selection pressure mechanism to improve exploration in early iterations.", "configspace": "", "generation": 65, "fitness": 0.3919786256652444, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.392 with standard deviation 0.143. And the mean value of best solutions found was 0.001 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "3884f004-a691-4884-921d-6eaceaded2fc", "metadata": {"aucs": [0.5054426659265968, 0.19046192272190254, 0.48003128834723374], "final_y": [0.00010617408615828381, 0.0037741821582892125, 0.0002569624021685645]}, "mutation_prompt": null}
{"id": "f3dcecbf-e441-4e56-9131-c15af1dccb2c", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) * (self.budget / (self.budget + random.randint(1, population_size)))  # Modified\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhance adaptive mutation probability by incorporating budget usage to balance exploration and exploitation.", "configspace": "", "generation": 66, "fitness": 0.40090524281274914, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.401 with standard deviation 0.083. And the mean value of best solutions found was 0.003 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "3884f004-a691-4884-921d-6eaceaded2fc", "metadata": {"aucs": [0.2870167077002663, 0.431109010637884, 0.48459001010009717], "final_y": [0.006813201391824503, 0.001326544356708541, 2.5728765597565957e-05]}, "mutation_prompt": null}
{"id": "11796b9c-95d6-4f3d-b514-4af2527717a1", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha, beta = np.random.uniform(0, 1, self.dim), np.random.uniform(0, 1, self.dim)  # Change: Added beta for second parent\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = (alpha * parent1 + beta * parent2 + (1 - alpha - beta) * parent3)  # Use linear combination\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Tweak crossover strategy to use a linear combination of parents for enhanced diversity.", "configspace": "", "generation": 67, "fitness": 0.31838340333482607, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.318 with standard deviation 0.130. And the mean value of best solutions found was 0.228 (0. is the best) with standard deviation 0.320.", "error": "", "parent_id": "3884f004-a691-4884-921d-6eaceaded2fc", "metadata": {"aucs": [0.38577077930018944, 0.13670674883966194, 0.4326726818646268], "final_y": [0.0022124192299511573, 0.6805854552022975, 0.00017036091793903957]}, "mutation_prompt": null}
{"id": "f048f04f-9509-4880-ac64-ee4d568b4ce3", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling with increased selective pressure\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks**1.5)  # Change: Increased selective pressure\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            adaptive_crossover_rate = 0.3 + 0.7 * (fitness.mean() / fitness[best_idx])  # Change: Adaptive crossover rate\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, adaptive_crossover_rate, self.dim)  # Change: Adaptive crossover rate\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Implemented adaptive crossover rates and selective pressure to improve convergence by enhancing exploration and exploitation balance.", "configspace": "", "generation": 68, "fitness": 0.34429265799109005, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.344 with standard deviation 0.112. And the mean value of best solutions found was 0.024 (0. is the best) with standard deviation 0.034.", "error": "", "parent_id": "3884f004-a691-4884-921d-6eaceaded2fc", "metadata": {"aucs": [0.41243554273106076, 0.43409775028149, 0.1863446809607192], "final_y": [0.0006121089315669816, 0.0008354651363382945, 0.07201142274882706]}, "mutation_prompt": null}
{"id": "dade0853-8fb7-450e-8814-baac2736aaec", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(25, self.budget // 2)  # Changed from 20 to 25\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Increased population size to allow a more diverse initial exploration.", "configspace": "", "generation": 69, "fitness": 0.2699189815609713, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.270 with standard deviation 0.175. And the mean value of best solutions found was 0.337 (0. is the best) with standard deviation 0.239.", "error": "", "parent_id": "3884f004-a691-4884-921d-6eaceaded2fc", "metadata": {"aucs": [0.1472656267725122, 0.1452120787955007, 0.5172792391149011], "final_y": [0.476087336932906, 0.5344534806981618, 1.91026241208867e-05]}, "mutation_prompt": null}
{"id": "0d9d5846-61d9-40c4-b449-b6c58e105e94", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            # Modified adaptive mutation probability formula\n            adaptive_mutation_prob = 0.1 + 0.5 * (fitness[best_idx] / fitness.mean())  # Increased from 0.4 to 0.5\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhance the adaptive mutation probability to improve exploration in challenging fitness landscapes.", "configspace": "", "generation": 70, "fitness": 0.2802179417214647, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.280 with standard deviation 0.021. And the mean value of best solutions found was 0.002 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "3884f004-a691-4884-921d-6eaceaded2fc", "metadata": {"aucs": [0.2866220757345441, 0.30171280542626433, 0.2523189440035857], "final_y": [6.485003510191044e-05, 0.00017909864938516875, 0.005932046648135544]}, "mutation_prompt": null}
{"id": "dec924ab-fc5c-40dd-87cf-7165f8622cc3", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.15 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adjusted mutation probability to 0.15\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Adjust the adaptive mutation strategy to better balance exploration and exploitation by slightly increasing mutation probability in early generations.", "configspace": "", "generation": 71, "fitness": 0.4115759010903853, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.412 with standard deviation 0.076. And the mean value of best solutions found was 0.001 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "3884f004-a691-4884-921d-6eaceaded2fc", "metadata": {"aucs": [0.31911425702978147, 0.4106842571166128, 0.5049291891247615], "final_y": [0.0026569405639838665, 0.0006812255485964847, 0.000149744757679479]}, "mutation_prompt": null}
{"id": "49a92bee-b8a2-40cd-a086-43ef93f25b8a", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + np.std(fitness)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Adjust the mutation decay factor to consider both the remaining budget and current generation's diversity for improved convergence.", "configspace": "", "generation": 72, "fitness": 0.27213409794523624, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.272 with standard deviation 0.038. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.101.", "error": "", "parent_id": "3884f004-a691-4884-921d-6eaceaded2fc", "metadata": {"aucs": [0.2199933756694642, 0.28715757409809495, 0.3092513440681496], "final_y": [0.25256603698094765, 0.0546338590411705, 0.025076699910417315]}, "mutation_prompt": null}
{"id": "5287ea23-90d4-4c76-be00-a159ce5fbbdc", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size))) * 0.5  # Dynamic mutation decay, change made here\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Adjust the mutation decay factor to incorporate a dynamic balance between exploration and exploitation.", "configspace": "", "generation": 73, "fitness": 0.29573907110563674, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.296 with standard deviation 0.013. And the mean value of best solutions found was 0.022 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "3884f004-a691-4884-921d-6eaceaded2fc", "metadata": {"aucs": [0.31403280521043475, 0.283788220212009, 0.2893961878944664], "final_y": [0.01295409496259757, 0.0008775122797046321, 0.05139177147036641]}, "mutation_prompt": null}
{"id": "524240d6-f09c-4fab-86a3-3da1bcf64842", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean())  # Adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min()) * (np.std(fitness) / fitness.mean())  # Change: Added variance component\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Refine mutation strength by incorporating variance of population fitness.", "configspace": "", "generation": 74, "fitness": 0.3654661255647897, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.365 with standard deviation 0.144. And the mean value of best solutions found was 0.091 (0. is the best) with standard deviation 0.110.", "error": "", "parent_id": "3884f004-a691-4884-921d-6eaceaded2fc", "metadata": {"aucs": [0.22109768231669247, 0.31282281552896707, 0.5624778788487095], "final_y": [0.24552006900569262, 0.026203192807529918, 3.3087305533016263e-06]}, "mutation_prompt": null}
{"id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhance the adaptive mutation probability by incorporating a fitness variance ratio to balance exploration and exploitation.", "configspace": "", "generation": 75, "fitness": 0.4817965546251555, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.482 with standard deviation 0.093. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3884f004-a691-4884-921d-6eaceaded2fc", "metadata": {"aucs": [0.3629509661378343, 0.5887149211216494, 0.493723776615983], "final_y": [0.0004606072514717529, 1.06575414653952e-06, 0.00020656162358999143]}, "mutation_prompt": null}
{"id": "09181a12-e05b-436c-89af-dfe867815ba7", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        generation_no_improvement = 0  # Track generations without improvement\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n\n            # Update best solution\n            new_best_idx = np.argmin(fitness)\n            if fitness[new_best_idx] < fitness[best_idx]:\n                best_idx = new_best_idx\n                best_solution = population[best_idx]\n                generation_no_improvement = 0\n            else:\n                generation_no_improvement += 1\n            \n            # Random restart mechanism if no improvement\n            if generation_no_improvement > 5:\n                population = np.random.uniform(\n                    low=func.bounds.lb, \n                    high=func.bounds.ub, \n                    size=(population_size, self.dim)\n                )\n                fitness = np.array([func(ind) for ind in population])\n                self.budget -= population_size\n                generation_no_improvement = 0\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Improve diversity by adding a random restart mechanism upon stagnation to enhance global exploration.", "configspace": "", "generation": 76, "fitness": 0.32437976651663125, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.324 with standard deviation 0.039. And the mean value of best solutions found was 0.029 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.3702284650761819, 0.3276606591061343, 0.27525017536757757], "final_y": [0.002749951642992199, 0.019733822332229495, 0.06532484769706877]}, "mutation_prompt": null}
{"id": "08687911-607d-4eb3-b30c-34c56224d967", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        population_size = min(20, self.budget // 2)\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n\n        while self.budget > 0:\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))\n\n            # Calculate population diversity\n            diversity = np.mean(np.std(population, axis=0)) / (func.bounds.ub - func.bounds.lb).mean()\n            adaptive_crossover_prob = 0.5 + 0.5 * diversity  # New: Adaptive crossover probability based on diversity\n\n            for i in range(population_size):\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]\n\n                # Conditionally apply crossover based on adaptive probability\n                if np.random.rand() < adaptive_crossover_prob:  # New: Conditional crossover\n                    alpha = np.random.uniform(0, 1, self.dim)\n                    centroid = (parent1 + parent2 + parent3) / 3\n                    child = alpha * parent1 + (1 - alpha) * centroid\n                else:\n                    child = parent1.copy()  # Default to one parent if crossover not applied\n\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            combined_population = np.vstack((population, offspring, [best_solution]))\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce adaptive crossover probability based on population diversity to improve exploration and convergence.", "configspace": "", "generation": 77, "fitness": 0.17429085590743884, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.174 with standard deviation 0.127. And the mean value of best solutions found was 1.414 (0. is the best) with standard deviation 1.373.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.07730955938081374, 0.09189788283793965, 0.3536651255035631], "final_y": [3.273033864504375, 0.9675783385612265, 0.0010605590277624915]}, "mutation_prompt": null}
{"id": "f54ea84b-3ea2-4657-aeea-0920bbaec618", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) ** 0.5 + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability with nonlinear scaling\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce a nonlinear scaling factor to the adaptive mutation probability to enhance exploration in diverse conditions.", "configspace": "", "generation": 78, "fitness": 0.25978794323458615, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.260 with standard deviation 0.135. And the mean value of best solutions found was 0.870 (0. is the best) with standard deviation 1.227.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.38512014109878867, 0.32258601633401696, 0.07165767227095288], "final_y": [0.002459833565382344, 0.0029704044824100926, 2.6052034022307513]}, "mutation_prompt": null}
{"id": "0f2dfa8f-e9aa-4753-8a0b-c8e3661e5120", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0.5, 1, self.dim)  # Dynamic crossover rate, modified line\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce a dynamic crossover rate that increases as the solution improves to refine exploration and convergence balance.", "configspace": "", "generation": 79, "fitness": 0.25592141665846385, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.256 with standard deviation 0.159. And the mean value of best solutions found was 0.335 (0. is the best) with standard deviation 0.280.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.15453363725589553, 0.13301929241907895, 0.480211320300417], "final_y": [0.32077043124722276, 0.6848611566572833, 3.628542925135669e-05]}, "mutation_prompt": null}
{"id": "6fc510b4-12b5-4050-aa32-2436043ba1a6", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            population_size = max(5, int(population_size * np.random.uniform(0.9, 1.1)))  # Introduced stochastic component to adjust population size\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce a stochastic component to dynamically adjust population size for improved exploration and exploitation balance.", "configspace": "", "generation": 80, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {}, "mutation_prompt": null}
{"id": "37415c21-1152-47d7-9aa9-d4b6cd7792a2", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduced competition-based elitism to preserve top performers, enhancing convergence speed and solution quality.", "configspace": "", "generation": 81, "fitness": 0.17167414691942118, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.172 with standard deviation 0.051. And the mean value of best solutions found was 0.821 (0. is the best) with standard deviation 1.112.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.09996465341342708, 0.1966138438627062, 0.21844394348213025], "final_y": [2.392705192771119, 0.016139130788855503, 0.053033642324961924]}, "mutation_prompt": null}
{"id": "a1922ab9-282c-4c8e-ae78-6d55980840e5", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                weights = np.random.uniform(0, 1, 3)\n                weights /= weights.sum()  # Normalize weights\n                centroid = weights[0] * parent1 + weights[1] * parent2 + weights[2] * parent3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Refine the ensemble crossover strategy to increase genetic diversity by using random weights for centroid calculation.", "configspace": "", "generation": 82, "fitness": 0.39534962907182547, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.395 with standard deviation 0.121. And the mean value of best solutions found was 0.007 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.22915862746256588, 0.5154606722785091, 0.4414295874744014], "final_y": [0.02183741934887299, 7.573808905773966e-05, 0.0004636578560471526]}, "mutation_prompt": null}
{"id": "b2005cea-03d6-4f36-bee5-604e9ae9bfd0", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced multi-parent dynamic ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[random.randint(0, population_size - 1)]\n\n                # Crossover (dynamic blend crossover)\n                alpha = np.random.beta(0.5, 0.5, self.dim)  # Use beta distribution\n                centroid = np.mean([parent1, parent2, parent3], axis=0)  # Dynamic centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation with local search)\n                mutation_strength = mutation_decay_factor * 0.05 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce the concept of multi-parent dynamic ensemble crossover with adaptive local search and elitism to enhance exploration and exploitation balance.", "configspace": "", "generation": 83, "fitness": 0.24919083659606198, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.249 with standard deviation 0.105. And the mean value of best solutions found was 1.176 (0. is the best) with standard deviation 1.659.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.3138453835382331, 0.332064170192079, 0.10166295605787379], "final_y": [0.000954480916529463, 0.003557402600417542, 3.522432552839312]}, "mutation_prompt": null}
{"id": "32db471f-a452-49e8-b1e6-621b0191eb45", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3 * 1.05) / 3  # Compute weighted centroid with dynamic adjustment\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Improved convergence by enhancing the ensemble crossover mechanism with a dynamic centroid adjustment.", "configspace": "", "generation": 84, "fitness": 0.12151891882682331, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.122 with standard deviation 0.017. And the mean value of best solutions found was 1.718 (0. is the best) with standard deviation 1.231.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.09796263552852869, 0.1353496800666707, 0.13124444088527054], "final_y": [3.4477745303969027, 1.023829865094292, 0.6816872107484363]}, "mutation_prompt": null}
{"id": "61b25ee2-0481-440b-b678-642a6f4f8fdb", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            offspring_size = min(self.budget, population_size)  # Dynamic offspring size based on remaining budget\n            for i in range(offspring_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= offspring_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introducing dynamic offspring size to balance exploration and exploitation based on budget utilization.", "configspace": "", "generation": 85, "fitness": 0.3990796295225228, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.399 with standard deviation 0.049. And the mean value of best solutions found was 0.001 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.3301652360960704, 0.43564079136967526, 0.4314328611018228], "final_y": [0.0034752312083791035, 0.00036798716702854373, 0.0003313879528844287]}, "mutation_prompt": null}
{"id": "3212cb30-daeb-44ea-b5e5-fc8b856ccefd", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                # Change: Dynamic selection of parents\n                parent1, parent2, parent3 = parents[i], parents[(i + random.randint(1, population_size-1)) % population_size], parents[(i + 2) % population_size]\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Utilize ensemble crossover with dynamic parent selection to enhance diversity.", "configspace": "", "generation": 86, "fitness": 0.32216787268527186, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.322 with standard deviation 0.146. And the mean value of best solutions found was 0.230 (0. is the best) with standard deviation 0.320.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.3545104364451127, 0.12971684793585259, 0.48227633367485034], "final_y": [0.007420092056516598, 0.6830550684264637, 0.00012543723608721624]}, "mutation_prompt": null}
{"id": "05367bb7-0c00-4348-9f7b-2bd2b8c3c4f1", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, population[best_idx:best_idx+2]))  # Increase elite retention\n            combined_fitness = np.hstack((fitness, offspring_fitness, fitness[best_idx:best_idx+2]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhance elite retention by increasing the number of preserved best solutions to maintain diversity.", "configspace": "", "generation": 87, "fitness": 0.4090646406381022, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.409 with standard deviation 0.019. And the mean value of best solutions found was 0.002 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.41667786293418385, 0.42775397368956136, 0.38276208529056144], "final_y": [8.195107778760958e-05, 7.618856843741691e-05, 0.004374586805068329]}, "mutation_prompt": null}
{"id": "41fb7eb5-3645-4ea9-a07a-659f26cfb42b", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness / (combined_fitness.std() + 1e-9))[:population_size]  # New: Incorporate fitness diversity\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce fitness diversity scaling to enhance population adaptability and exploration.", "configspace": "", "generation": 88, "fitness": 0.4344811397229506, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.434 with standard deviation 0.096. And the mean value of best solutions found was 0.003 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.3151656393324028, 0.4379153341293611, 0.5503624457070877], "final_y": [0.00646101429836258, 0.001028786924948329, 1.4192117095678693e-05]}, "mutation_prompt": null}
{"id": "5db73e5c-6fda-4984-b8be-593b33a38522", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size // 2]  # Change: Elite selection\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce elite selection to enhance convergence by focusing on top-performing individuals.", "configspace": "", "generation": 89, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {}, "mutation_prompt": null}
{"id": "90a65baf-c755-49d7-b546-161ade6be79e", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        population_size = min(20, self.budget // 2)\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n\n        while self.budget > 0:\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))\n            for i in range(population_size):\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]\n\n                alpha = np.random.uniform(0, 1, self.dim) * (1 - fitness_variance_ratio)  # Dynamic alpha based on variance\n                centroid = (parent1 + parent2 + parent3) / 3\n                child = alpha * parent1 + (1 - alpha) * centroid\n\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) / (1 + fitness_variance_ratio)  # Inverse variance\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            combined_population = np.vstack((population, offspring, [best_solution]))\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce a dynamic crossover mechanism by adjusting alpha based on fitness, and refactor mutation strength to be inversely proportional to combined fitness variance.", "configspace": "", "generation": 90, "fitness": 0.270185176188818, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.270 with standard deviation 0.108. And the mean value of best solutions found was 0.098 (0. is the best) with standard deviation 0.137.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.36919286359631764, 0.3216718872850163, 0.11969077768512004], "final_y": [0.0003749104182350378, 0.00219092249561206, 0.2923372325242864]}, "mutation_prompt": null}
{"id": "5caeadcc-70fc-48f0-ae88-e13c6c98f572", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = np.std(population, axis=0) * 0.1  # Change: Adjusted to consider diversity in each dimension\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Adjusted mutation strength calculation by considering the diversity in each dimension to enhance exploration.", "configspace": "", "generation": 91, "fitness": 0.17610570622661556, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.176 with standard deviation 0.046. And the mean value of best solutions found was 1.249 (0. is the best) with standard deviation 1.063.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.17196970219248986, 0.12200839469692892, 0.23433902179042787], "final_y": [0.8486475619755268, 2.7037162613030685, 0.1949598351220292]}, "mutation_prompt": null}
{"id": "072b803d-072c-44f1-b82f-f35a71ae9bbd", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3 + 0.1 * fitness_variance_ratio  # Adjust centroid using fitness variance ratio\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Enhance exploration by adjusting crossover using a dynamic centroid based on fitness variance ratio.", "configspace": "", "generation": 92, "fitness": 0.22008027108401929, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.220 with standard deviation 0.066. And the mean value of best solutions found was 0.092 (0. is the best) with standard deviation 0.085.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.3021998864090256, 0.1393517490526648, 0.21868917779036745], "final_y": [0.01878372848856381, 0.21162725514754022, 0.04701671949683585]}, "mutation_prompt": null}
{"id": "c086c35a-42f9-4dd0-b068-5dec91f08ed8", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (1 + population.std(axis=0).mean())  # Change: Adjusted mutation strength based on population diversity\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Add dynamic learning of mutation strength based on population diversity to enhance exploration capacity.", "configspace": "", "generation": 93, "fitness": 0.33844687408256363, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.338 with standard deviation 0.144. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.157.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.3872290154360437, 0.1431679314255745, 0.4849436753860725], "final_y": [0.000494413455369208, 0.333549793463556, 7.00637227959305e-05]}, "mutation_prompt": null}
{"id": "4e4fe750-2f65-4e4f-8c50-6aeeec150f1c", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n\n            # Introduce random immigrants to encourage diversity\n            num_immigrants = max(1, population_size // 10)  # Add 10% of the population as random immigrants\n            immigrants = np.random.uniform(\n                low=func.bounds.lb, \n                high=func.bounds.ub, \n                size=(num_immigrants, self.dim)\n            )\n            offspring.extend(immigrants)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Improve population diversity by introducing random immigrants every generation.", "configspace": "", "generation": 94, "fitness": 0.314831843493937, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.315 with standard deviation 0.104. And the mean value of best solutions found was 0.006 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.3533400738274146, 0.1722481378222499, 0.4189073188321465], "final_y": [0.0015889681061827291, 0.01570382613151222, 0.00032444954665206393]}, "mutation_prompt": null}
{"id": "1cb45361-efd3-48c8-a499-6ae6d01324d7", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        population_size = min(20, self.budget // 2)\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        \n        # New: Maintain a memory of best solutions\n        best_memory = [best_solution]\n\n        while self.budget > 0:\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()\n            adaptive_mutation_prob = 0.15 + 0.25 * (fitness[best_idx] / fitness.mean())  # Adjusted mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))\n            for i in range(population_size):\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]\n\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3\n                child = alpha * parent1 + (1 - alpha) * centroid\n\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                # New: Dual mutation strategy\n                if np.random.rand() < adaptive_mutation_prob:\n                    mutation = np.where(np.random.rand(self.dim) > 0.5, mutation, -mutation)  # Dual mutation strategy\n                    child += mutation\n                \n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            combined_population = np.vstack((population, offspring, [best_solution]))\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            best_memory.append(best_solution)  # Update memory\n        \n        # Return the best found solution from memory\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce diversity through dual mutation strategies and memory-based selection to improve convergence.", "configspace": "", "generation": 95, "fitness": 0.1428627694079023, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.143 with standard deviation 0.026. And the mean value of best solutions found was 0.981 (0. is the best) with standard deviation 0.922.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.17225547269683106, 0.14697870830756066, 0.10935412721931514], "final_y": [0.08890149175716838, 0.6043676857474857, 2.2508611187450933]}, "mutation_prompt": null}
{"id": "fe2bf718-86dd-41eb-aca8-7a0c95bde714", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, population[ranks[:2]]))  # Preserve top solutions\n            combined_fitness = np.hstack((fitness, offspring_fitness, fitness[ranks[:2]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce elitism by preserving top solutions from both the population and offspring to enhance exploration-exploitation balance.", "configspace": "", "generation": 96, "fitness": 0.24556413117115616, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.126. And the mean value of best solutions found was 0.085 (0. is the best) with standard deviation 0.094.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.1888665493203533, 0.41972567117223425, 0.12810017302088095], "final_y": [0.03863101505692778, 0.0009568208135849874, 0.21638473528082738]}, "mutation_prompt": null}
{"id": "0f3d730f-f712-43bf-8ed5-09591ab4b831", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Elitism: Ensure the best solution remains in the population\n        worst_idx = np.argmax(fitness)\n        if fitness[worst_idx] > fitness[best_idx]:\n            population[worst_idx] = best_solution\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Introduce elitism by replacing the worst solution with the best in each iteration to enhance convergence stability.", "configspace": "", "generation": 97, "fitness": 0.4131435455366794, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.413 with standard deviation 0.004. And the mean value of best solutions found was 0.001 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.41233179852844215, 0.40845942759682585, 0.41863941048477027], "final_y": [8.207462095787723e-05, 0.0013585694482899325, 0.0013703668143339378]}, "mutation_prompt": null}
{"id": "18979400-7ffe-4328-b596-78481b1ac821", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            fitness_improvement_ratio = (fitness.mean() - fitness.min()) / fitness.mean()  # New: Calculate fitness improvement ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio + 0.1 * fitness_improvement_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Improved adaptive mutation leverages a fitness-improvement ratio for more dynamic exploration and exploitation.", "configspace": "", "generation": 98, "fitness": 0.4405532656428141, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.441 with standard deviation 0.079. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.40140061344628797, 0.5510133327410354, 0.36924585074111904], "final_y": [0.00016946683196314763, 3.862478154077094e-06, 0.0010356704081540937]}, "mutation_prompt": null}
{"id": "37aee25b-b935-4535-baf7-8583609d8423", "solution": "import numpy as np\nimport random\n\nclass DynamicPopulationEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Define population size\n        population_size = min(20, self.budget // 2)\n        # Initialize population within bounds\n        population = np.random.uniform(\n            low=func.bounds.lb, \n            high=func.bounds.ub, \n            size=(population_size, self.dim)\n        )\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        best_idx = np.argmin(fitness)  # Track best solution\n        best_solution = population[best_idx]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Rank-based fitness scaling\n            ranks = np.argsort(fitness)\n            scaled_fitness = 1.0 / (1.0 + ranks)\n            parents_idx = np.random.choice(\n                population_size, \n                size=population_size,\n                p=scaled_fitness / scaled_fitness.sum()\n            )\n            parents = population[parents_idx]\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            fitness_variance_ratio = fitness.std() / fitness.mean()  # Calculate fitness variance ratio\n            adaptive_mutation_prob = 0.1 + 0.4 * (fitness[best_idx] / fitness.mean()) + 0.1 * fitness_variance_ratio  # Enhanced adaptive mutation probability\n            mutation_decay_factor = 1 - (self.budget / (self.budget + random.randint(1, population_size)))  # Dynamic mutation decay\n            for i in range(population_size):\n                # Introduced ensemble crossover\n                parent1, parent2, parent3 = parents[i], parents[(i + 1) % population_size], parents[(i + 2) % population_size]  # Added a third parent\n\n                # Crossover (blend crossover)\n                alpha = np.random.uniform(0, 1, self.dim)\n                centroid = (parent1 + parent2 + parent3) / 3  # Compute weighted centroid\n                child = alpha * parent1 + (1 - alpha) * centroid  # Use centroid in crossover\n\n                # Mutation (adaptive Gaussian mutation)\n                mutation_strength = mutation_decay_factor * 0.1 * (func.bounds.ub - func.bounds.lb) * (fitness[best_idx] / fitness.min())  # Change: Adjusted to use fitness.min()\n                mutation = np.random.normal(0, mutation_strength, self.dim)\n                if np.random.rand() < adaptive_mutation_prob:\n                    child += mutation\n                \n                # Ensure child is within bounds\n                child = np.clip(child, func.bounds.lb, func.bounds.ub)\n                offspring.append(child)\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Combine and select the next generation\n            combined_population = np.vstack((population, offspring, [best_solution]))  # Preserve best solution\n            combined_fitness = np.hstack((fitness, offspring_fitness, [fitness[best_idx]]))\n            best_individuals_idx = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_individuals_idx]\n            fitness = combined_fitness[best_individuals_idx]\n            \n            # Update best solution\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n        \n        # Return the best found solution\n        return best_solution", "name": "DynamicPopulationEvolution", "description": "Fine-tune crossover strategy by introducing elitist selection for better diversity and convergence.", "configspace": "", "generation": 99, "fitness": 0.3659376125330576, "feedback": "The algorithm DynamicPopulationEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.366 with standard deviation 0.027. And the mean value of best solutions found was 0.002 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c4283f28-3cf8-4c1d-ac8f-984117293793", "metadata": {"aucs": [0.33319061282414575, 0.3664434935992623, 0.39817873117576474], "final_y": [0.0017397530321434872, 0.0018386635135590416, 0.002205220303478759]}, "mutation_prompt": null}
