{"id": "bef9567e-722c-4c5a-a52b-de87078ace72", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=[bounds]*self.dim)\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm combining Differential Evolution for global exploration and Nelder-Mead for local refinement, with dynamic layer expansion to adaptively manage complexity and enhance optimization efficiency.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\n  File \"<string>\", line 50, in adaptive_layer_expansion\n  File \"<string>\", line 37, in local_refinement\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 670, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 1058, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 437, in old_bound_to_new\n    lb = np.array([float(_arr_to_scalar(x)) if x is not None else -np.inf\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 437, in <listcomp>\n    lb = np.array([float(_arr_to_scalar(x)) if x is not None else -np.inf\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 14, in _arr_to_scalar\n    return x.item() if isinstance(x, np.ndarray) else x\nValueError: can only convert an array of size 1 to a Python scalar\n.", "error": "ValueError('can only convert an array of size 1 to a Python scalar')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\n  File \"<string>\", line 50, in adaptive_layer_expansion\n  File \"<string>\", line 37, in local_refinement\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 670, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 1058, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 437, in old_bound_to_new\n    lb = np.array([float(_arr_to_scalar(x)) if x is not None else -np.inf\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 437, in <listcomp>\n    lb = np.array([float(_arr_to_scalar(x)) if x is not None else -np.inf\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 14, in _arr_to_scalar\n    return x.item() if isinstance(x, np.ndarray) else x\nValueError: can only convert an array of size 1 to a Python scalar\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "de1fa95e-424e-4b8b-8129-464d33da511b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improved hybrid metaheuristic with fixed variable bounds for local refinement using Nelder-Mead.", "configspace": "", "generation": 1, "fitness": 0.8219117636175989, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.008. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "bef9567e-722c-4c5a-a52b-de87078ace72", "metadata": {"aucs": [0.8170100861196863, 0.832969111864627, 0.8157560928684833], "final_y": [0.1291477349827168, 0.13070403601403202, 0.13101346643591183]}, "mutation_prompt": null}
{"id": "75a9585d-e4f7-410d-b869-12962e188026", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Refine the HybridMetaheuristic by introducing an adaptive mutation strategy for improved exploration.", "configspace": "", "generation": 2, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "de1fa95e-424e-4b8b-8129-464d33da511b", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "cb09ed89-af9f-4d46-9c36-0cd3d3a31b01", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Change to adjust crossover probability based on diversity\n                adaptive_CR = CR * np.var(pop, axis=0).mean() / np.var(func.bounds.ub - func.bounds.lb).mean()\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Incrementally enhance diversity by adjusting crossover probability dynamically in the differential evolution process.", "configspace": "", "generation": 3, "fitness": 0.8300835614792578, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.032. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.799475536463417, 0.8739556532014967, 0.8168194947728596], "final_y": [0.14805369110723987, 0.1229822247112593, 0.14475930159670336]}, "mutation_prompt": null}
{"id": "5e6937e9-1c58-4d06-b7c8-d0662e56f129", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n        \n        # Restart mechanism: Reset if no improvement\n        if self.eval_count < self.budget * 0.8 and best_value == np.inf:\n            self.eval_count = 0  # Reset evaluation count\n        \n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a restart mechanism in adaptive_layer_expansion for enhanced exploration.", "configspace": "", "generation": 4, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "fb72014e-21df-4abc-84dc-e602193485f6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Adjust CR based on population diversity\n                adaptive_CR = CR * (np.std(pop) / (np.abs(np.mean(pop)) + 1e-10))\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by adjusting the crossover rate (CR) based on the population diversity.", "configspace": "", "generation": 5, "fitness": 0.8465163279347815, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.847 with standard deviation 0.008. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8531021123908772, 0.8357779643513344, 0.8506689070621329], "final_y": [0.12381354453275617, 0.1210299809398866, 0.11946474109414562]}, "mutation_prompt": null}
{"id": "53a97b6b-cadd-4cdd-bc98-800fc09a4f6d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Powell', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance local refinement by utilizing a more robust optimization method, Powell, for improved convergence.", "configspace": "", "generation": 6, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "c7ca31b3-a625-4abd-b998-36c1521f0d7d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F and CR based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                adaptive_CR = CR * (1 - self.eval_count / (2 * self.budget))\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve convergence by refining the adaptive mutation strategy with a more dynamic adjustment of both F and CR based on the evaluation progress.", "configspace": "", "generation": 7, "fitness": 0.8440054443264099, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.016. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8370070762361449, 0.8663541319721152, 0.8286551247709694], "final_y": [0.12168719556936991, 0.12284095629765024, 0.12635153405780386]}, "mutation_prompt": null}
{"id": "26398fe2-af42-4aa1-994d-0ce92b7608a0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Adaptive Crossover Rate: Adjust CR based on convergence\n                adaptive_CR = CR * (1 - self.eval_count / self.budget)\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the HybridMetaheuristic by adjusting the crossover rate (CR) adaptively based on evaluation progress to improve solution diversity.", "configspace": "", "generation": 8, "fitness": 0.8398913567857504, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8285611855587466, 0.8514643459675532, 0.8396485388309516], "final_y": [0.12381285914782203, 0.12561512878847236, 0.1256811194671762]}, "mutation_prompt": null}
{"id": "e985cb47-9208-4581-9476-b7b7585456a2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                decay_factor = 0.95 ** (self.eval_count / self.budget)\n                adaptive_F = F * (1 - self.eval_count / self.budget) * decay_factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the adaptive mutation strategy by introducing a decay factor to improve convergence in later stages.", "configspace": "", "generation": 9, "fitness": 0.8602280528793363, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.022. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.829510926069264, 0.8758989326350033, 0.8752742999337421], "final_y": [0.1230535328955149, 0.11986263044657652, 0.1175319459044305]}, "mutation_prompt": null}
{"id": "96fe8be4-4c90-402d-a1e1-dca4b08504a9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        # Introduce a step size parameter \"options={'xatol': 1e-8}\" for precision\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds), options={'xatol': 1e-8})\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Refine the HybridMetaheuristic by introducing a step size parameter to the local refinement for better convergence precision.", "configspace": "", "generation": 10, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "1890b7f2-78c3-47f5-9b98-10861898fe07", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        # Dynamically adjust population size based on budget usage\n        pop_size = max(5, pop_size - int(self.eval_count / (self.budget / 20)))\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the HybridMetaheuristic by adjusting the population size dynamically based on budget usage for finer control.", "configspace": "", "generation": 11, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "91ab2676-0762-4654-9f99-2f8cd7bbbbc0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Refine the HybridMetaheuristic by enhancing the local refinement step with a more robust 'L-BFGS-B' method for better solution accuracy.", "configspace": "", "generation": 12, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "09f38561-3b0c-43a4-b0b8-ab36ef54407a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the local refinement process by switching to the \"L-BFGS-B\" method for improved handling of box constraints.", "configspace": "", "generation": 13, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "9ff3a49b-16cc-4334-9eca-76fee3e3ea5f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the local refinement step by switching to the 'L-BFGS-B' method for improved convergence efficiency.", "configspace": "", "generation": 14, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "0fd815ce-89d2-4323-84db-94c9ecd8f53e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Adjust CR based on population diversity\n                adaptive_CR = CR * (1 - np.std(fitness) / np.mean(fitness))\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by adjusting crossover rate adaptively based on the convergence of the population.", "configspace": "", "generation": 15, "fitness": 0.8562445769056864, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.009. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8443594665332843, 0.8580874477356438, 0.8662868164481308], "final_y": [0.12376103189931342, 0.11910005833271509, 0.119000213815041]}, "mutation_prompt": null}
{"id": "aa99f0d6-2008-458e-abc3-9a70afe6bae1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence in HybridMetaheuristic by fine-tuning DE parameters dynamically based on the eval_count.", "configspace": "", "generation": 16, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "1a0266af-eb4d-43df-932c-e9577b3d7ea6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=np.transpose(bounds))  # Changed method\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance local refinement by using 'L-BFGS-B' for improved convergence in optimization.", "configspace": "", "generation": 17, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "af576eef-731a-473a-beda-afbfa860eed5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for iter_count in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                dynamic_CR = CR * (1 - iter_count / max_iter)  # Changed line for dynamic CR\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover probability (CR) based on iteration progress to enhance exploration and exploitation balance.", "configspace": "", "generation": 18, "fitness": 0.8518103392093549, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.013. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.833307880460766, 0.8587698574749261, 0.8633532796923726], "final_y": [0.12374393565706188, 0.11785335247707096, 0.12415126039103297]}, "mutation_prompt": null}
{"id": "d9701904-5ae2-4ad0-bce2-af074ea68778", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                \n                # Dynamic Crossover Rate: Adjust CR based on population diversity\n                current_diversity = np.std(pop, axis=0).mean()\n                adaptive_CR = CR * (1 + 0.5 * (current_diversity / np.max(np.std(pop, axis=0))))\n                \n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Refine the HybridMetaheuristic by incorporating a dynamic crossover rate based on the population's diversity to enhance exploration and exploitation.", "configspace": "", "generation": 19, "fitness": 0.8300835614792578, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.032. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.799475536463417, 0.8739556532014967, 0.8168194947728596], "final_y": [0.14805369110723987, 0.1229822247112593, 0.14475930159670336]}, "mutation_prompt": null}
{"id": "ed4fa73c-f2b9-4817-8d1f-e5105d8e51c3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        # Stratified sampling for initial population\n        pop = np.vstack([np.random.uniform(bounds[0][d], bounds[1][d], pop_size) for d in range(self.dim)]).T\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve exploration by introducing diversity in the initial population through a stratified sampling approach.", "configspace": "", "generation": 20, "fitness": 0.8548226419998827, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.005. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8567280194001536, 0.8485892472619219, 0.8591506593375725], "final_y": [0.1203834711116063, 0.12115271657228344, 0.12062296183776289]}, "mutation_prompt": null}
{"id": "2e13b2d7-4441-42c0-b429-4f5857824b93", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        if np.random.rand() < 0.5:  # Probabilistic refinement\n            result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n            return result.x, result.fun\n        return x0, func(x0)\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a probabilistic local refinement step with an early stopping condition based on fitness improvement.", "configspace": "", "generation": 21, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "41ba69fc-5616-4fa2-a9d0-80084a7f38a8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Powell', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the local search strategy by switching to the 'Powell' method for efficient local refinement in complex landscapes.", "configspace": "", "generation": 22, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "b27811ae-cf87-40a5-9d87-ccf784f28408", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        # Modified line to introduce noise handling\n        noise_handling_func = lambda x: np.mean([func(x + np.random.normal(0, 0.01, size=x.shape)) for _ in range(5)])\n        result = minimize(noise_handling_func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a noise handling mechanism in the local refinement step.  ", "configspace": "", "generation": 23, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522843298052354, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "71215faf-5b8f-4d1e-8a95-a93a94782fbc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(int(pop_size * (1 + self.eval_count / self.budget))):  # Dynamic population size\n                indices = [idx for idx in range(pop_size) if idx != i % pop_size]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i % pop_size])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i % pop_size]:\n                    fitness[i % pop_size] = f_trial\n                    pop[i % pop_size] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic population size strategy in the differential evolution phase to improve exploration and exploitation balance.", "configspace": "", "generation": 24, "fitness": 0.8378781395249737, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.838 with standard deviation 0.010. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8243596621103995, 0.8406992354269962, 0.8485755210375252], "final_y": [0.12776487888025478, 0.1291612938481178, 0.12341743624370705]}, "mutation_prompt": null}
{"id": "8ab11941-bb57-4f13-a923-64c0bbde7ffb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                adaptive_CR = CR + 0.1 * (np.random.rand() - 0.5)  # Dynamic crossover rate\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=np.transpose(bounds))  # Use 'L-BFGS-B' for refinement\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the HybridMetaheuristic by introducing a dynamic crossover rate and improving local refinement for efficient exploration.", "configspace": "", "generation": 25, "fitness": 0.8655933888110233, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.019. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8418102722145453, 0.8657284746039523, 0.8892414196145724], "final_y": [0.12754728782846037, 0.11565164544286222, 0.11648049873363708]}, "mutation_prompt": null}
{"id": "cd2fa20b-440d-499f-8b89-35c80dc1c2a8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                adaptive_CR = CR * (1 - self.eval_count / self.budget)  # Adaptive Crossover Strategy\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5 if num_layers < 20 else 2  # Dynamic Layer Expansion\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Refine the HybridMetaheuristic by adding dynamic layer expansion and adaptive crossover strategy for enhanced optimization efficiency.", "configspace": "", "generation": 26, "fitness": 0.8398913567857504, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8285611855587466, 0.8514643459675532, 0.8396485388309516], "final_y": [0.12381285914782203, 0.12561512878847236, 0.1256811194671762]}, "mutation_prompt": null}
{"id": "78949ab6-9232-46ef-bcf0-40c478d8efc2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < (CR * (1 - self.eval_count / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover rate and fine-tune local refinement for enhanced local search.", "configspace": "", "generation": 27, "fitness": 0.8398913567857504, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8285611855587466, 0.8514643459675532, 0.8396485388309516], "final_y": [0.12381285914782203, 0.12561512878847236, 0.1256811194671762]}, "mutation_prompt": null}
{"id": "8dafa597-df02-45f2-9a9b-6f8251f15190", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Nonlinear Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * np.exp(-self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by introducing a nonlinear adaptive mutation decay in the differential evolution phase.", "configspace": "", "generation": 28, "fitness": 0.8449301665861579, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.026. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8199061718143739, 0.8804942636127848, 0.8343900643313152], "final_y": [0.1261708777733811, 0.11672189710473424, 0.12302594134142564]}, "mutation_prompt": null}
{"id": "fe412568-6fd8-40ef-98c7-90cd61db37e0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds), options={'initial_simplex': x0 + 0.01*np.random.rand(len(x0))})  # Incremental step size added here\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce an incremental step size for the local refinement phase to improve convergence towards optimal solutions.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('`initial_simplex` should be an array of shape (N+1,N)').", "error": "ValueError('`initial_simplex` should be an array of shape (N+1,N)')", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {}, "mutation_prompt": null}
{"id": "e9bff765-7a4d-4983-9f34-70ddae001562", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                CR_dynamic = CR * (0.5 + 0.5 * (1 - self.eval_count / self.budget))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n                # Handle uncertainty by a simple re-evaluation strategy\n                if self.eval_count < self.budget and np.random.rand() < 0.1:\n                    reevaluated_fitness = func(pop[i])\n                    self.eval_count += 1\n                    if reevaluated_fitness < fitness[i]:\n                        fitness[i] = reevaluated_fitness\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the HybridMetaheuristic with adaptive crossover probability and uncertainty handling to improve resilience against noise.", "configspace": "", "generation": 30, "fitness": 0.8474541391221426, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.847 with standard deviation 0.004. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8450179411131722, 0.8531846674020102, 0.8441598088512454], "final_y": [0.12121697612764792, 0.11788519390716334, 0.1304935296226586]}, "mutation_prompt": null}
{"id": "4b97b663-b026-415e-b505-9a147f06f59a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, initial_pop_size=10, F=0.8, CR=0.9, max_iter=100):\n        pop_size = initial_pop_size + int(10 * (self.eval_count / self.budget))\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n        prev_best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n            \n            # Improved convergence detection\n            if np.abs(prev_best_value - best_value) < 1e-6:\n                break\n            prev_best_value = best_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance HybridMetaheuristic by adopting a dynamic population size and improved convergence detection for superior optimization performance.", "configspace": "", "generation": 31, "fitness": 0.8564075174438095, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.025. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.830617701988932, 0.8489552555470494, 0.889649594795447], "final_y": [0.13695406499806706, 0.13001553716900438, 0.1168001168331555]}, "mutation_prompt": null}
{"id": "a10f3dfe-ef9c-4227-a534-2283d54a1036", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n        success_rate_threshold = 0.2  # New variable to decide if extra local search is needed\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            # Adjust dimension increment based on success rate\n            success_rate = sum(f < best_value for f in fitness) / len(fitness)\n            num_layers += 5 if success_rate > success_rate_threshold else 3  # New logic for adaptive increment\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce local search based on success rate and adjust dimension increment adaptively.", "configspace": "", "generation": 32, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "1cbd9d41-b9af-4de2-a484-0b51ed67c34d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence with added noise\n                adaptive_F = F * (1 - self.eval_count / self.budget) + np.random.normal(0, 0.1)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by adding random perturbations to the adaptive mutation strategy, injecting diversity into the population.", "configspace": "", "generation": 33, "fitness": 0.8639760778927245, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.012. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8540683679729306, 0.8803677920235271, 0.8574920736817159], "final_y": [0.1216545107007867, 0.11489468851253593, 0.11845050493361287]}, "mutation_prompt": null}
{"id": "00e902ae-f651-4101-9a7b-ee6d982385cd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Adjust CR dynamically based on evaluation progress\n                adaptive_CR = CR * (1 - self.eval_count / self.budget)\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve convergence by adjusting the crossover probability (CR) dynamically based on evaluation progress.", "configspace": "", "generation": 34, "fitness": 0.8398913567857504, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8285611855587466, 0.8514643459675532, 0.8396485388309516], "final_y": [0.12381285914782203, 0.12561512878847236, 0.1256811194671762]}, "mutation_prompt": null}
{"id": "5b2d79e1-9949-4f3c-a40e-7581cea4df3d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n            pop_size = int(20 + 10 * (1 - self.eval_count / self.budget))  # Dynamic population size\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance HybridMetaheuristic by implementing a dynamic adjustment of the population size based on budget utilization to improve exploration efficiency.", "configspace": "", "generation": 35, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 27 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 27 is out of bounds for axis 0 with size 20')", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {}, "mutation_prompt": null}
{"id": "1f318523-3346-4e03-889f-4f2a82765386", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                noise = np.random.normal(0, 0.1, self.dim)  # Gaussian noise addition\n                mutant = np.clip(a + adaptive_F * (b - c) + noise, bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by adding Gaussian noise to the mutation process in the adaptive mutation strategy.", "configspace": "", "generation": 36, "fitness": 0.8551889922716948, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.008. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8484236549660836, 0.8671752634036398, 0.849968058445361], "final_y": [0.11419058351833655, 0.11923388748475616, 0.12404925194916128]}, "mutation_prompt": null}
{"id": "354cb439-93c2-4343-86c1-9d3749cdff56", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        # Optimize choice of x0 by averaging top solutions found in the population\n        best_candidates = np.argsort([func(ind) for ind in x0])[:3]\n        x0_avg = np.mean(x0[np.array(best_candidates)], axis=0)\n        result = minimize(func, x0_avg, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance local refinement by optimizing the choice of the initial point for the Nelder-Mead method.", "configspace": "", "generation": 37, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, 108.59798096216329').", "error": "TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, 108.59798096216329')", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {}, "mutation_prompt": null}
{"id": "d3cf00b3-ade8-4cd8-a4c2-71ad44cd64be", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Introduce dynamic crossover rate adjustment\n                adaptive_CR = CR * (self.eval_count / self.budget)\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by introducing a dynamic crossover rate adjustment based on the budget.", "configspace": "", "generation": 38, "fitness": 0.8369479790631466, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.008. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8252483055588014, 0.8434071139117165, 0.8421885177189219], "final_y": [0.12185224367578773, 0.12275509009497398, 0.12518859543303784]}, "mutation_prompt": null}
{"id": "3f63b6b5-a690-4b63-93f1-a7b500ae0435", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence and layer count\n                adaptive_F = F * ((1 - self.eval_count / self.budget) * (1 + i / pop_size))\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce layer-based adaptive mutation and dynamic local refinement for enhanced exploration and optimization efficiency.", "configspace": "", "generation": 39, "fitness": 0.8341970631939336, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.834 with standard deviation 0.015. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8130010684045299, 0.8480974954213131, 0.8414926257559577], "final_y": [0.12925503770817193, 0.11678933418606474, 0.11746579925202805]}, "mutation_prompt": null}
{"id": "78ac394b-f2ce-4e2a-973d-319e793ac160", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Dynamically adjust CR based on convergence\n                adaptive_CR = CR * (1 - self.eval_count / self.budget)\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover probability to improve exploration-exploitation balance in the adaptive mutation strategy.", "configspace": "", "generation": 40, "fitness": 0.8398913567857504, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8285611855587466, 0.8514643459675532, 0.8396485388309516], "final_y": [0.12381285914782203, 0.12561512878847236, 0.1256811194671762]}, "mutation_prompt": null}
{"id": "7c1b42fb-a8c6-4520-bd0d-65ef2eaa5ca0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        # Dynamic population size adjustment\n        pop_size = int(pop_size * (1 + 0.5 * (1 - self.eval_count / self.budget)))\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce dynamic population size in differential evolution to balance exploration and exploitation.", "configspace": "", "generation": 41, "fitness": 0.8402996876805489, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.007. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8307082302730068, 0.8472571799007678, 0.8429336528678723], "final_y": [0.12506709701066265, 0.12031918924265317, 0.12386419819233252]}, "mutation_prompt": null}
{"id": "d322beb9-b074-4d75-a2ae-0750c881a975", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += np.random.randint(4, 7)  # Stochastic expansion\n            if self.eval_count < self.budget // 2:  # Dynamic refinement trigger\n                local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n                if local_value < best_value:\n                    best_solution = local_best\n                    best_value = local_value\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Refine the HybridMetaheuristic by introducing a stochastic layer expansion strategy and dynamic local refinement trigger for enhanced exploration-exploitation balance.", "configspace": "", "generation": 42, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "3728c0c4-0666-493c-8e86-9d6b07da6bc4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', options={'xatol': 1e-8, 'fatol': 1e-8}, bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced local refinement through dynamic adjustment of Nelder-Mead algorithm parameters.", "configspace": "", "generation": 43, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "98556b71-0d31-46b2-8fab-0df027acfd48", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Dynamic Crossover Rate\n                dynamic_CR = CR * (1 + np.random.uniform(-0.1, 0.1))\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the Differential Evolution step with a dynamic crossover rate to balance exploration and exploitation.", "configspace": "", "generation": 44, "fitness": 0.8533799716085224, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.853 with standard deviation 0.011. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8402742740223751, 0.8519483856097694, 0.8679172551934228], "final_y": [0.1258568027230429, 0.12294628193344237, 0.11525497112197769]}, "mutation_prompt": null}
{"id": "ed9e0eb0-153e-4b06-9cc7-dcb9f24c57e9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Stochastic component added to adaptive_F\n                adaptive_F = F * (1 - self.eval_count / self.budget) * np.random.uniform(0.8, 1.2)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by introducing a stochastic component to the adaptive mutation strategy.", "configspace": "", "generation": 45, "fitness": 0.8605940519675266, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.013. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8448858810113439, 0.8591417569245015, 0.8777545179667342], "final_y": [0.12568552747138684, 0.12362558119759925, 0.12359740088905569]}, "mutation_prompt": null}
{"id": "a1f07146-4698-4af3-9877-93fcf0cacea5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Adaptive Crossover Strategy: Adjust CR based on convergence\n                adaptive_CR = CR * (1 - self.eval_count / self.budget) + 0.1\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the HybridMetaheuristic by incorporating an adaptive crossover rate to complement the adaptive mutation strategy, thus improving exploration.", "configspace": "", "generation": 46, "fitness": 0.8536223550634962, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.005. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8462436070694405, 0.8554823062186959, 0.8591411519023521], "final_y": [0.1176130353878514, 0.12491315510879608, 0.12179856664817668]}, "mutation_prompt": null}
{"id": "b431c6d1-b6b8-45df-94b5-99cfdf3de5e9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance local refinement by switching to a more robust optimization method for better local convergence.", "configspace": "", "generation": 47, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "79300fc3-bc04-4251-bdb7-5f4b9de1515c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Adaptive Crossover Strategy: Adjust CR based on convergence\n                adaptive_CR = CR * (1 - self.eval_count / self.budget)\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce an adaptive crossover rate for balancing exploration and exploitation. ", "configspace": "", "generation": 48, "fitness": 0.8398913567857504, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8285611855587466, 0.8514643459675532, 0.8396485388309516], "final_y": [0.12381285914782203, 0.12561512878847236, 0.1256811194671762]}, "mutation_prompt": null}
{"id": "5e9f88bb-ab82-4f4f-90a5-ed5dfff44d4b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Adaptive Crossover Rate: Adjust CR based on convergence\n                adaptive_CR = CR * (self.eval_count / self.budget)\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce an adaptive crossover rate based on budget utilization to enhance convergence rate.", "configspace": "", "generation": 49, "fitness": 0.8369479790631466, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.008. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8252483055588014, 0.8434071139117165, 0.8421885177189219], "final_y": [0.12185224367578773, 0.12275509009497398, 0.12518859543303784]}, "mutation_prompt": null}
{"id": "16ba54ff-b033-4107-b381-f6cee706e871", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Dynamic Crossover Rate: Adjust CR based on diversity\n                adaptive_CR = CR * (np.std(fitness) / np.mean(fitness))\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        # Use gradient-based method for refinement\n        result = minimize(func, x0, method='L-BFGS-B', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover rate and a gradient-based local refinement to enhance exploration and exploitation.", "configspace": "", "generation": 50, "fitness": 0.8348904719640039, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.835 with standard deviation 0.005. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8281334815335577, 0.8358411969521724, 0.8406967374062814], "final_y": [0.12362514168881822, 0.13219351167072846, 0.12925530393283202]}, "mutation_prompt": null}
{"id": "f15ec96e-0186-4ac0-91da-0797b8928349", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            \n            adaptive_CR = CR * (1 - self.eval_count / self.budget) # Adaptive Crossover Strategy\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        options = {'maxiter': 100, 'adaptive': True} # Improve local refinement strategy\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds), options=options)\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the HybridMetaheuristic by introducing adaptive crossover rates and improved local search strategies for better optimization performance.", "configspace": "", "generation": 51, "fitness": 0.8426757845438081, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.008. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.833151088406116, 0.8428010635418977, 0.852075201683411], "final_y": [0.12437290337886431, 0.12417074700963449, 0.11949756557660929]}, "mutation_prompt": null}
{"id": "a6508675-0a02-4621-9977-e38385630abc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Dynamic Crossover Rate based on progress\n                adaptive_CR = CR * (1 - self.eval_count / self.budget)\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover rate based on the progress of optimization for enhanced exploration.", "configspace": "", "generation": 52, "fitness": 0.8398913567857504, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8285611855587466, 0.8514643459675532, 0.8396485388309516], "final_y": [0.12381285914782203, 0.12561512878847236, 0.1256811194671762]}, "mutation_prompt": null}
{"id": "b4bd68f4-0c19-4e18-b734-546ef5922249", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F using square root\n                adaptive_F = F * np.sqrt(1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the hybrid algorithm by modifying the adaptive mutation strategy to use the square root of the remaining budget percentage for more stable convergence.", "configspace": "", "generation": 53, "fitness": 0.8479825455812056, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.013. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8324016038016268, 0.8477827367696792, 0.8637632961723106], "final_y": [0.12186891716715165, 0.1270969178612661, 0.12225852354172762]}, "mutation_prompt": null}
{"id": "bfbada3f-cc34-4b73-89da-408f7f9c58b3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                # Dynamic adjustment of CR\n                adaptive_CR = CR * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the adaptive mutation strategy by dynamically adjusting the crossover rate (CR) based on the evaluation count to balance exploration and exploitation better.", "configspace": "", "generation": 54, "fitness": 0.8398913567857504, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8285611855587466, 0.8514643459675532, 0.8396485388309516], "final_y": [0.12381285914782203, 0.12561512878847236, 0.1256811194671762]}, "mutation_prompt": null}
{"id": "285b2fcb-d02e-4982-a714-f49c19be06e6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Dynamically adjust crossover rate\n                adaptive_CR = CR * (1 - self.eval_count / (2 * self.budget))\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by dynamically adjusting the crossover rate based on the budget usage.", "configspace": "", "generation": 55, "fitness": 0.8440054443264099, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.016. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8370070762361449, 0.8663541319721152, 0.8286551247709694], "final_y": [0.12168719556936991, 0.12284095629765024, 0.12635153405780386]}, "mutation_prompt": null}
{"id": "0ebae5a2-3132-43c9-8781-d999bc9921c5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F and CR based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                adaptive_CR = CR * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the adaptive mutation strategy by making both `F` and `CR` adaptive to improve exploration and exploitation balance.", "configspace": "", "generation": 56, "fitness": 0.8398913567857504, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8285611855587466, 0.8514643459675532, 0.8396485388309516], "final_y": [0.12381285914782203, 0.12561512878847236, 0.1256811194671762]}, "mutation_prompt": null}
{"id": "b8946533-8915-46cb-9811-5b87e192606c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        # Dynamic population size based on remaining budget\n        dynamic_pop_size = int(np.clip(pop_size * (1 + self.eval_count / self.budget), pop_size, 2 * pop_size))\n        pop = np.random.rand(dynamic_pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += dynamic_pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(dynamic_pop_size):\n                indices = [idx for idx in range(dynamic_pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve exploration by introducing dynamic population size in differential evolution.", "configspace": "", "generation": 57, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "729dd021-a9f6-457c-a836-11efc995f264", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Dynamic Crossover Rate: Adjust CR based on population diversity\n                diversity = np.std(pop, axis=0).mean()\n                dynamic_CR = CR * (1 + diversity / self.dim)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the HybridMetaheuristic by implementing a dynamic crossover rate based on diversity to boost exploration.", "configspace": "", "generation": 58, "fitness": 0.830417199272131, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.032. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.7998497881561595, 0.8745178118839889, 0.8168839977762443], "final_y": [0.14725615155249727, 0.12219177918777091, 0.14468125787914177]}, "mutation_prompt": null}
{"id": "a8ccb3a3-6b60-4888-93e8-0b322d333af8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for iter_num in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            dynamic_pop_size = max(5, pop_size - iter_num // 10)  # Dynamically reduce population size\n            for i in range(dynamic_pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduced a dynamic population size reduction strategy to improve exploitation in later generations.", "configspace": "", "generation": 59, "fitness": 0.8541778077052192, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.025. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8270156393198256, 0.8864505830857433, 0.8490672007100888], "final_y": [0.1219478605088884, 0.11481593486434216, 0.12248858491118675]}, "mutation_prompt": null}
{"id": "49219d92-0a00-4233-804f-b0a5980fe8eb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                adaptive_CR = CR * (1 - fitness[i] / np.mean(fitness))\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n            if np.std(fitness) < 1e-5:\n                break\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the HybridMetaheuristic by introducing adaptive crossover and early stopping based on convergence.", "configspace": "", "generation": 60, "fitness": 0.8321114286046255, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.832 with standard deviation 0.018. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8082847832318557, 0.8381577899333282, 0.8498917126486929], "final_y": [0.13756865375793015, 0.12874317293745907, 0.12152966191035863]}, "mutation_prompt": null}
{"id": "42651295-c37a-4ddc-acac-7cf8fd8bb38a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on fitness diversity\n                fitness_variance = np.var(fitness)\n                adaptive_F = F * (1 - fitness_variance / (fitness_variance + 1e-8))\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance HybridMetaheuristic by introducing a fitness diversity measure to balance exploration and exploitation.", "configspace": "", "generation": 61, "fitness": 0.828362955905544, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.018. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8035978751238169, 0.8393674524284536, 0.8421235401643619], "final_y": [0.1514412284885137, 0.1415641982368826, 0.13818584433851988]}, "mutation_prompt": null}
{"id": "a4bdc702-bc28-4a09-af83-00d14780bf43", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Time-varying Crossover Rate\n                adaptive_CR = CR * (1 - self.eval_count / self.budget)\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance mutation in adaptive layer expansion using a time-varying crossover rate.", "configspace": "", "generation": 62, "fitness": 0.8398913567857504, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8285611855587466, 0.8514643459675532, 0.8396485388309516], "final_y": [0.12381285914782203, 0.12561512878847236, 0.1256811194671762]}, "mutation_prompt": null}
{"id": "2c15e913-d7d0-4113-90a9-d95aec727e85", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='BFGS', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance local search by switching to the BFGS method for faster convergence in the refinement phase.", "configspace": "", "generation": 63, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "0bce4b2b-c0bb-4f3a-9d39-8b6222436c44", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                decay_factor = (1 - self.eval_count / (2 * self.budget))  # New line\n                adaptive_F = F * (1 - self.eval_count / self.budget) * decay_factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a decay factor to the adaptive mutation strategy for more dynamic exploration.", "configspace": "", "generation": 64, "fitness": 0.8578302574043292, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.858 with standard deviation 0.009. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8462669947734485, 0.8669080406547746, 0.8603157367847643], "final_y": [0.11883706473663636, 0.11757780451982469, 0.12587198458554738]}, "mutation_prompt": null}
{"id": "78b20358-5575-453b-8310-fe0325814071", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Dynamically adjust CR based on population diversity\n                adaptive_CR = CR * (1 - np.std(fitness) / np.mean(fitness))\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the mutation strategy by dynamically adjusting the crossover rate (CR) based on the diversity of the population to balance exploration and exploitation.", "configspace": "", "generation": 65, "fitness": 0.8562445769056864, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.009. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8443594665332843, 0.8580874477356438, 0.8662868164481308], "final_y": [0.12376103189931342, 0.11910005833271509, 0.119000213815041]}, "mutation_prompt": null}
{"id": "76c82643-693e-495f-98b6-425394a44a96", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop_size = max(5, int(20 * (self.budget - self.eval_count) / self.budget))  # Dynamic scaling\n            pop, fitness = self.differential_evolution(func, reduced_bounds, pop_size=pop_size)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce dynamic population size scaling based on the remaining budget to maintain exploration effectiveness.", "configspace": "", "generation": 66, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "6b766b91-1a31-4eaf-91a8-04450b4a154f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            # Dynamically adjust the population size based on remaining budget\n            pop_size = max(5, int((self.budget - self.eval_count) / 10))\n            \n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve convergence by dynamically adjusting the population size based on remaining budget.", "configspace": "", "generation": 67, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 46 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 46 is out of bounds for axis 0 with size 20')", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {}, "mutation_prompt": null}
{"id": "ba2c1f60-52e4-4f71-938e-18dc5746d262", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop_size = int(pop_size * (1 + 0.1 * np.sin(self.eval_count / self.budget * np.pi)))  # Dynamic population size\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by introducing dynamic population size scaling in the Differential Evolution strategy.", "configspace": "", "generation": 68, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "36fe6b6a-d328-4b7e-9eb4-6b81e901fa9f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Dynamic Crossover Strategy: Adjust CR for enhanced exploration\n                adaptive_CR = CR * (1 - self.eval_count / self.budget)\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the adaptive mutation strategy by introducing a dynamic crossover rate to improve exploration and convergence.", "configspace": "", "generation": 69, "fitness": 0.8398913567857504, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8285611855587466, 0.8514643459675532, 0.8396485388309516], "final_y": [0.12381285914782203, 0.12561512878847236, 0.1256811194671762]}, "mutation_prompt": null}
{"id": "4e9c10b8-a1cc-406a-99b5-fdc410b3229f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Self-adaptive crossover: Adjust CR based on convergence\n                adaptive_CR = CR * (1 - self.eval_count / self.budget)\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introducing self-adaptive crossover probability to enhance exploration and convergence efficiency.", "configspace": "", "generation": 70, "fitness": 0.8398913567857504, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8285611855587466, 0.8514643459675532, 0.8396485388309516], "final_y": [0.12381285914782203, 0.12561512878847236, 0.1256811194671762]}, "mutation_prompt": null}
{"id": "b789a78c-63be-4001-9b30-2c488bf30c9d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n\n            # Dynamic update frequency based on convergence progress\n            if self.eval_count < 0.5 * self.budget or best_value > np.min(fitness):\n                local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n                if local_value < best_value:\n                    best_solution = local_best\n                    best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic local refinement update frequency based on convergence progress to improve fine-tuning.", "configspace": "", "generation": 71, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "169a81f1-8277-44e8-9e67-4085d4297027", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for iteration in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Adjust CR based on iteration to balance exploration and exploitation\n                adaptive_CR = CR * (1 - iteration / max_iter)\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Refine the HybridMetaheuristic by adjusting the crossover probability CR based on iteration count for enhanced exploration and exploitation balance.", "configspace": "", "generation": 72, "fitness": 0.8518103392093549, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.013. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.833307880460766, 0.8587698574749261, 0.8633532796923726], "final_y": [0.12374393565706188, 0.11785335247707096, 0.12415126039103297]}, "mutation_prompt": null}
{"id": "8b2169de-2eca-45f9-b3ed-37279bc417c1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation Strategy: Adjust F based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop_size = max(5, int(self.budget / (2 * reduced_dim)))  # Dynamically adjust population size\n            pop, fitness = self.differential_evolution(func, reduced_bounds, pop_size=pop_size)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a simple mechanism to dynamically adjust the population size based on the evaluation budget to enhance exploration capabilities.", "configspace": "", "generation": 73, "fitness": 0.8184030083202694, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.010. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8117113683917463, 0.8328041730221603, 0.8106934835469016], "final_y": [0.12613712807460298, 0.1337488103370278, 0.13207941905935006]}, "mutation_prompt": null}
{"id": "68e3b53e-9c58-4e19-bb49-ba9e80834d15", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n        prev_best = np.min(fitness)\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n                # Dynamically adjust CR based on fitness improvement\n                if f_trial < prev_best:\n                    CR = min(1.0, CR + 0.05)\n                else:\n                    CR = max(0.1, CR - 0.05)\n                \n                prev_best = np.min(fitness)\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by adjusting mutation and crossover rates dynamically based on fitness improvements.", "configspace": "", "generation": 74, "fitness": 0.8483313551199038, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.006. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8393386244694789, 0.8511897429056711, 0.8544656979845616], "final_y": [0.1295998957632727, 0.13146279626320667, 0.12629107023809694]}, "mutation_prompt": null}
{"id": "f6724744-46ce-483c-a3ea-5d265473561e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Adaptive Mutation and Crossover Strategy: Adjust F and CR based on convergence\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                adaptive_CR = CR * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the adaptive differential evolution by adjusting both mutation factor and crossover rate for better convergence.", "configspace": "", "generation": 75, "fitness": 0.8398913567857504, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8285611855587466, 0.8514643459675532, 0.8396485388309516], "final_y": [0.12381285914782203, 0.12561512878847236, 0.1256811194671762]}, "mutation_prompt": null}
{"id": "a7ec6487-1c30-4c9f-9555-ec8a763912cd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                adaptive_CR = CR * (1 - np.exp(-self.eval_count / self.budget))  # Adaptive CR\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < adaptive_CR  # Use adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Incorporate an adaptive crossover rate in the differential evolution strategy for enhanced exploration and convergence control.", "configspace": "", "generation": 76, "fitness": 0.8327799797682637, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.833 with standard deviation 0.010. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8306251818767687, 0.8214656888449036, 0.8462490685831187], "final_y": [0.11974751514511772, 0.13154748044180298, 0.11981119232150306]}, "mutation_prompt": null}
{"id": "b973e67d-2681-4253-b62f-d59be96e19df", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n            \n            # Preserve modular structures by injecting small perturbations\n            if np.random.rand() < 0.2:\n                best_solution += np.random.normal(0, 0.01, size=best_solution.shape)\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Integrate a modular preservation mechanism to improve long-term solution quality and stability.", "configspace": "", "generation": 77, "fitness": 0.8716229169203444, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8617476472949197, 0.8813764853234916, 0.8717446181426222], "final_y": [0.1129697002907255, 0.11522947429047503, 0.11879019405047708]}, "mutation_prompt": null}
{"id": "ee44d6f7-abe2-4881-8504-e07f67423162", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Dynamically adjust CR based on current evaluation count\n                adaptive_CR = CR * (1 - np.sin(np.pi * self.eval_count / (2 * self.budget)))\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the adaptive mutation strategy by dynamically adjusting the crossover rate (CR) based on the current evaluation count to improve solution diversity.", "configspace": "", "generation": 78, "fitness": 0.8371492055579283, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.011. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8226352561561338, 0.8497477010091676, 0.8390646595084833], "final_y": [0.1303525773142048, 0.1246359207700013, 0.12647278627308955]}, "mutation_prompt": null}
{"id": "751ca31a-82ab-4064-a1b7-9f8a0ef31f2f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01, self.dim)  # Add stochastic noise\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the HybridMetaheuristic by introducing layer-wise stochastic noise to improve robustness against noisy evaluations.", "configspace": "", "generation": 79, "fitness": 0.8722155947475848, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.019. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "75a9585d-e4f7-410d-b869-12962e188026", "metadata": {"aucs": [0.8734072079676001, 0.8487289026401149, 0.8945106736350397], "final_y": [0.11571657420666959, 0.11795009027438008, 0.11266200587214292]}, "mutation_prompt": null}
{"id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)  # Adaptive noise scaling\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance layer-wise exploration by using adaptive noise scaling to improve robustness against noisy evaluations.", "configspace": "", "generation": 80, "fitness": 0.8722306329381659, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.019. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "751ca31a-82ab-4064-a1b7-9f8a0ef31f2f", "metadata": {"aucs": [0.8733942664596648, 0.8487221735625163, 0.8945754587923169], "final_y": [0.11569324860366947, 0.1179419245629324, 0.1124817321095074]}, "mutation_prompt": null}
{"id": "6272ece9-3dea-4f22-a6e9-dc276a676261", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)  # Adaptive noise scaling\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Powell', bounds=np.transpose(bounds))  # Changed from Nelder-Mead to Powell\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve robustness by enhancing local refinement with noise-resilient optimization.", "configspace": "", "generation": 81, "fitness": 0.8722306329381659, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.019. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.8733942664596648, 0.8487221735625163, 0.8945754587923169], "final_y": [0.11569324860366947, 0.1179419245629324, 0.1124817321095074]}, "mutation_prompt": null}
{"id": "eeb6c1a6-764f-42f5-a3bb-104e28bfeb31", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)  # Adaptive noise scaling\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', options={'xatol': 0.001}, bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance local refinement by adding a dynamic adjustment to the Nelder-Mead method for better convergence.", "configspace": "", "generation": 82, "fitness": 0.8722306329381659, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.019. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.8733942664596648, 0.8487221735625163, 0.8945754587923169], "final_y": [0.11569324860366947, 0.1179419245629324, 0.1124817321095074]}, "mutation_prompt": null}
{"id": "f2f8bd57-5737-47f1-a6d8-4a5b4d052fd5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n            pop_size = int(pop_size * (0.9 + 0.1 * (self.eval_count / self.budget)))  # Adjust population size dynamically\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve convergence by dynamically adjusting population size in the differential evolution stage.", "configspace": "", "generation": 83, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"Cannot take a larger sample than population when 'replace=False'\").", "error": "ValueError(\"Cannot take a larger sample than population when 'replace=False'\")", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {}, "mutation_prompt": null}
{"id": "54946bfa-7adc-4dec-b030-ec89e08d8a61", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def dynamic_population_differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            \n            dynamic_pop_size = max(5, int(pop_size * (1 - self.eval_count / self.budget)))\n            indices = np.arange(pop_size)\n            np.random.shuffle(indices)\n            \n            for i in range(dynamic_pop_size):\n                idx = indices[i % pop_size]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[idx])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[idx]:\n                    fitness[idx] = f_trial\n                    pop[idx] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.dynamic_population_differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Incorporate dynamic population size and adaptive mutation strategies to enhance exploration and convergence.", "configspace": "", "generation": 84, "fitness": 0.8600114336552921, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.017. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.8780420198515866, 0.8652846195860032, 0.8367076615282866], "final_y": [0.11413977626577243, 0.11019619046909279, 0.12073892577340117]}, "mutation_prompt": null}
{"id": "8532ec54-9ab2-4882-9b1c-f86b96158b83", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def adaptive_local_refinement(self, func, x0, bounds):\n        # Adaptive local refinement using bounded constraints\n        result = minimize(func, x0, method='L-BFGS-B', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def multilevel_layer_optimization(self, func, bounds):\n        num_layers = 8\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.adaptive_local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 4\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.multilevel_layer_optimization(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce multilevel layer optimization and adaptive local refinement to enhance robustness and solution quality for noisy black-box functions.", "configspace": "", "generation": 85, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (20,10) (8,) ').", "error": "ValueError('operands could not be broadcast together with shapes (20,10) (8,) ')", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {}, "mutation_prompt": null}
{"id": "a5dce7e4-ff6a-4610-b992-8fc19848ee62", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                adaptive_CR = CR * (0.5 + 0.5 * np.cos(np.pi * self.eval_count / self.budget))  # Adaptive crossover rate\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)  # Adaptive noise scaling\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover rate in differential evolution to enhance exploration and exploitation balance.", "configspace": "", "generation": 86, "fitness": 0.8684133602702161, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.020. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.8629441846422256, 0.8476325763748251, 0.8946633197935976], "final_y": [0.11846563309546898, 0.1206932632192318, 0.11745692232510019]}, "mutation_prompt": null}
{"id": "185a75a9-c54c-4ef8-968b-ccec2b113ce7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                layer_specific_F = adaptive_F * (1 + 0.1 * np.sin(2 * np.pi * np.arange(self.dim) / self.dim))  # Layer-specific mutation\n                mutant = np.clip(a + layer_specific_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)  # Adaptive noise scaling\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=np.transpose(bounds))  # Enhanced local search method\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce layer-specific mutation factors and enhance local search for improved modular optimization.", "configspace": "", "generation": 87, "fitness": 0.8720894628320814, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.008. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.867654872318586, 0.8647815621787245, 0.8838319539989338], "final_y": [0.11689158142534306, 0.11857266667827548, 0.12188011046333325]}, "mutation_prompt": null}
{"id": "0e40eec1-7357-4d53-816e-23ce63d987b4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                mutation_scaling = 0.01 * (1 - self.eval_count / self.budget)  # Line changed for dynamic mutation rate scaling\n                trial += np.random.normal(0, mutation_scaling, self.dim)\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce dynamic mutation rate scaling based on budget usage to improve exploration capabilities.", "configspace": "", "generation": 88, "fitness": 0.8722306329381659, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.019. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.8733942664596648, 0.8487221735625163, 0.8945754587923169], "final_y": [0.11569324860366947, 0.1179419245629324, 0.1124817321095074]}, "mutation_prompt": null}
{"id": "1967e308-5fb7-46b5-a5d1-e36c6c6a0f70", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                # Modified line for dynamic CR\n                cross_points = np.random.rand(self.dim) < (CR * (1 - self.eval_count / (2 * self.budget)))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)  # Adaptive noise scaling\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover rate to improve solution diversity during differential evolution.", "configspace": "", "generation": 89, "fitness": 0.8565325826047027, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.014. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.8717261466977394, 0.8387242607576618, 0.8591473403587065], "final_y": [0.11423367611661206, 0.11939988974552651, 0.12020531670965506]}, "mutation_prompt": null}
{"id": "c5287319-9de4-4d2c-8a2d-9e1f26af7dfa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)  # Adaptive noise scaling\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve solution refinement by switching from Nelder-Mead to L-BFGS-B for faster local convergence.", "configspace": "", "generation": 90, "fitness": 0.8722306329381659, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.019. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.8733942664596648, 0.8487221735625163, 0.8945754587923169], "final_y": [0.11569324860366947, 0.1179419245629324, 0.1124817321095074]}, "mutation_prompt": null}
{"id": "b93d78a7-8ad7-453f-8e40-8e56f86d719a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * np.std(fitness), self.dim)  # Adjust noise scaling based on fitness variance\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance layer-wise exploration by optimizing noise scaling based on fitness variance for robustness against noisy evaluations.", "configspace": "", "generation": 91, "fitness": 0.8695822731231546, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.015. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.875039771088225, 0.8487241943913563, 0.8849828538898825], "final_y": [0.11580084629531562, 0.11794469163666776, 0.11319323195727449]}, "mutation_prompt": null}
{"id": "200712b0-eab8-41db-a8c0-357a74fedcbb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)  # Adaptive noise scaling\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += int(5 * (1 - self.eval_count / self.budget))  # Adjust layer increase rate\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Refine layer expansion by adaptively adjusting the number of layers based on the evaluation budget to enhance exploration-exploitation balance.", "configspace": "", "generation": 92, "fitness": 0.8722306329381659, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.019. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.8733942664596648, 0.8487221735625163, 0.8945754587923169], "final_y": [0.11569324860366947, 0.1179419245629324, 0.1124817321095074]}, "mutation_prompt": null}
{"id": "8c910a08-2225-4836-b331-193a7b621313", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop_size = max(5, int(pop_size * (1 - self.eval_count / self.budget)))  # Dynamic population size\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)  # Adaptive noise scaling\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Utilize dynamic population size adjustment in DE to better allocate the budget over layers.", "configspace": "", "generation": 93, "fitness": 0.8722306329381659, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.019. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.8733942664596648, 0.8487221735625163, 0.8945754587923169], "final_y": [0.11569324860366947, 0.1179419245629324, 0.1124817321095074]}, "mutation_prompt": null}
{"id": "789b70b7-76b4-42ca-8c1f-c2a1db9f1f80", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n            # Increase pop_size gradually for diversity\n            if self.eval_count < self.budget // 2:\n                pop_size += 1\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by gradually increasing `pop_size` to maintain diversity in the population as evaluations progress.", "configspace": "", "generation": 94, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {}, "mutation_prompt": null}
{"id": "0fdd4fad-ca4a-46dd-87fd-576808f04d9b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)  # Adaptive noise scaling\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', jac='2-point', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve local search by hybridizing gradient-based refinement with noise-aware adjustments.", "configspace": "", "generation": 95, "fitness": 0.8722306329381659, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.019. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.8733942664596648, 0.8487221735625163, 0.8945754587923169], "final_y": [0.11569324860366947, 0.1179419245629324, 0.1124817321095074]}, "mutation_prompt": null}
{"id": "524abc56-3feb-4f04-bd2a-d5441c0507ac", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                adaptive_CR = CR * (1 - self.eval_count / self.budget)  # Adjust crossover rate\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)  # Adaptive noise scaling\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by adjusting crossover rate based on iteration progress to balance exploration and exploitation.", "configspace": "", "generation": 96, "fitness": 0.8566587008246812, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.019. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.8451881897889526, 0.8413392109899175, 0.8834487016951734], "final_y": [0.11959749878366333, 0.12161571179760255, 0.11862317770626907]}, "mutation_prompt": null}
{"id": "cc5166cf-e6b8-45c5-9223-278b453127a0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.005 * (1 - self.eval_count / self.budget), self.dim)  # Refined noise scaling\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        options = {'xatol': 1e-4, 'fatol': 1e-4}  # Added tolerance options for adaptability\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds), options=options)\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve balance between exploration and exploitation by refining noise scaling and increasing adaptability in local refinement.", "configspace": "", "generation": 97, "fitness": 0.872216609241356, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.019. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.8733442399231981, 0.8487233481863838, 0.8945822396144864], "final_y": [0.11576415978379373, 0.11794323375652394, 0.11252306583578242]}, "mutation_prompt": null}
{"id": "edc9a860-b0e4-4adf-a39c-509b62762a90", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                layer_factor = np.random.rand(self.dim)  # Layer-aware mutation\n                mutant = np.clip(a + adaptive_F * (b - c) * layer_factor, bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)  # Adaptive noise scaling\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Integrate layer-aware mutation strategy into Differential Evolution to improve layer-specific adaptations.", "configspace": "", "generation": 98, "fitness": 0.8669395567078478, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.867 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.853764291042153, 0.8747892897892866, 0.8722650892921039], "final_y": [0.13084283697199062, 0.12436344028606594, 0.11862085241076503]}, "mutation_prompt": null}
{"id": "704ef56e-48d7-4cf8-9ce4-f094f7a09eca", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=100):\n        pop = np.random.rand(pop_size, self.dim) * (bounds[1] - bounds[0]) + bounds[0]\n        fitness = np.array([func(ind) for ind in pop])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F * (1 - self.eval_count / self.budget)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds[0], bounds[1])\n                adaptive_CR = CR * np.exp(-self.eval_count / self.budget)  # Adaptive crossover rate scaling\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial += np.random.normal(0, 0.01 * (1 - self.eval_count / self.budget), self.dim)  # Adaptive noise scaling\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n\n        return pop, fitness\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=np.transpose(bounds))\n        return result.x, result.fun\n\n    def adaptive_layer_expansion(self, func, bounds):\n        num_layers = 10\n        best_solution = None\n        best_value = np.inf\n\n        while num_layers <= self.dim and self.eval_count < self.budget:\n            reduced_dim = min(num_layers, self.dim)\n            reduced_bounds = [bounds[0][:reduced_dim], bounds[1][:reduced_dim]]\n            pop, fitness = self.differential_evolution(func, reduced_bounds)\n            local_best_idx = np.argmin(fitness)\n            local_best, local_value = self.local_refinement(func, pop[local_best_idx], reduced_bounds)\n\n            if local_value < best_value:\n                best_solution = local_best\n                best_value = local_value\n\n            num_layers += 5\n\n        return np.concatenate([best_solution, np.zeros(self.dim - len(best_solution))])\n\n    def __call__(self, func):\n        bounds = (np.array(func.bounds.lb), np.array(func.bounds.ub))\n        best_solution = self.adaptive_layer_expansion(func, bounds)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover rate scaling in differential evolution to improve exploration and convergence.", "configspace": "", "generation": 99, "fitness": 0.8607632588890183, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.016. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "2b6d6594-ad46-4f6b-ba06-d80d6f1065bc", "metadata": {"aucs": [0.8573313585331437, 0.8435612388910148, 0.8813971792428965], "final_y": [0.12189557906307946, 0.11814051994032093, 0.11639853629496455]}, "mutation_prompt": null}
