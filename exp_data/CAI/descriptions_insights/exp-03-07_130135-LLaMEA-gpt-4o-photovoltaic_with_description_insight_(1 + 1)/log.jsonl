{"id": "3fb50e38-0acf-42ee-bf07-21f438662d9c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic combining Differential Evolution (DE) for global exploration and local search refinement for optimizing multilayered photonic structures, integrating robustness and modularity preservation in high-dimensional noisy environments.", "configspace": "", "generation": 0, "fitness": 0.8600313672607021, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.035. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8108624633352985, 0.8910005854088756, 0.8782310530379323], "final_y": [0.13953098700030486, 0.11488995128881518, 0.11765857660699108]}, "mutation_prompt": null}
{"id": "d8109f46-4af9-4710-b6b8-e5833b617253", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'gtol': 1e-6})\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhanced the local search strategy by incorporating a gradient-based refinement step to improve convergence speed and solution quality.", "configspace": "", "generation": 1, "fitness": 0.8593857470983028, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.035. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "3fb50e38-0acf-42ee-bf07-21f438662d9c", "metadata": {"aucs": [0.8108645041222597, 0.889049446326738, 0.8782432908459108], "final_y": [0.1395294866238057, 0.11488920426301774, 0.11765131058600908]}, "mutation_prompt": null}
{"id": "682d3b9d-ef72-47e4-8596-3115e730b9ee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'gtol': 1e-5})\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance the local search step by adjusting the stopping criterion to improve accuracy and convergence speed.", "configspace": "", "generation": 2, "fitness": 0.8600313672607021, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.035. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "3fb50e38-0acf-42ee-bf07-21f438662d9c", "metadata": {"aucs": [0.8108624633352985, 0.8910005854088756, 0.8782310530379323], "final_y": [0.13953098700030486, 0.11488995128881518, 0.11765857660699108]}, "mutation_prompt": null}
{"id": "a4c5fa44-ddb7-44a7-a64e-8efd0a12dfa1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = min(50, max(10, dim//2))  # Adapt population size based on dimensionality\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive population size for improved multilayered photonic structure optimization.", "configspace": "", "generation": 3, "fitness": 0.8324295855309508, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.832 with standard deviation 0.060. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "3fb50e38-0acf-42ee-bf07-21f438662d9c", "metadata": {"aucs": [0.7516637314211913, 0.8965518029416152, 0.8490732222300459], "final_y": [0.1676504758679509, 0.11538201255841207, 0.13316850984678974]}, "mutation_prompt": null}
{"id": "a0100e2b-ef69-40c5-b787-a9d4a03ed1cd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 100  # Increased population size\n        self.F = 0.8  # Differential weight\n        self.CR = 0.95  # Increased crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhanced HybridMetaheuristic with adaptive population size and increased crossover probability for improved exploration and exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.8269141049601995, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.034. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "3fb50e38-0acf-42ee-bf07-21f438662d9c", "metadata": {"aucs": [0.7953893565318333, 0.8741561150682993, 0.8111968432804655], "final_y": [0.14901224330855767, 0.11765536739219107, 0.12736766657382437]}, "mutation_prompt": null}
{"id": "54ac3b17-9f9d-4ce6-b1f4-e9f2768d9f32", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic combining Differential Evolution (DE) with adaptive crossover probability for improved exploration and local search refinement in high-dimensional noisy environments.", "configspace": "", "generation": 5, "fitness": 0.8777812226575182, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.046. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "3fb50e38-0acf-42ee-bf07-21f438662d9c", "metadata": {"aucs": [0.817544721376198, 0.8871743454321983, 0.928624601164158], "final_y": [0.13953098700030486, 0.11765536739219107, 0.11209614542010626]}, "mutation_prompt": null}
{"id": "6b2d8643-2562-455f-94fe-2243dea126bb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n        if self.evals > self.budget // 2:  # Adaptive population size strategy\n            self.population_size = max(10, int(self.population_size * 0.9))\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive population size strategy for better exploration-exploitation balance in high-dimensional optimization.", "configspace": "", "generation": 6, "fitness": 0.8777812226575182, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.046. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "54ac3b17-9f9d-4ce6-b1f4-e9f2768d9f32", "metadata": {"aucs": [0.817544721376198, 0.8871743454321983, 0.928624601164158], "final_y": [0.13953098700030486, 0.11765536739219107, 0.11209614542010626]}, "mutation_prompt": null}
{"id": "465db090-c83c-4f55-a39d-1d64c8357ec7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n\n        # Reduce population size adaptively if close to budget\n        if self.evals > 0.9 * self.budget:\n            self.population_size = max(10, int(0.5 * self.population_size))\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic combining Differential Evolution (DE) with adaptive crossover probability and local search, augmented by an adaptive population size for improved exploration and local search refinement in high-dimensional noisy environments.", "configspace": "", "generation": 7, "fitness": 0.8777812226575182, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.046. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "54ac3b17-9f9d-4ce6-b1f4-e9f2768d9f32", "metadata": {"aucs": [0.817544721376198, 0.8871743454321983, 0.928624601164158], "final_y": [0.13953098700030486, 0.11765536739219107, 0.11209614542010626]}, "mutation_prompt": null}
{"id": "4a8a08b2-31e1-44f8-a5a9-c0a47809cfa4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            # Change the mutation strategy by introducing adaptive scaling factor\n            adaptive_F = self.F * (1 + np.random.normal(0, 0.1))\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic combining Differential Evolution (DE) with adaptive mutation strategy for improved exploration and local search refinement in high-dimensional noisy environments.", "configspace": "", "generation": 8, "fitness": 0.8753857694166194, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.875 with standard deviation 0.042. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "54ac3b17-9f9d-4ce6-b1f4-e9f2768d9f32", "metadata": {"aucs": [0.8195062089111262, 0.8875008597413143, 0.9191502395974175], "final_y": [0.1395404374238125, 0.11765536739219107, 0.11176256956844777]}, "mutation_prompt": null}
{"id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Incorporate adaptive mutation strategy in DE to enhance diversity and convergence in noisy environments.", "configspace": "", "generation": 9, "fitness": 0.8824630757155237, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.035. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "54ac3b17-9f9d-4ce6-b1f4-e9f2768d9f32", "metadata": {"aucs": [0.8368042550542054, 0.8872262735067333, 0.9233586985856326], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "a9d3fb2e-709b-4462-9205-58e8e366f2d0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score / abs(self.best_score + 0.0001))) # Line modified\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance DE with adaptive crossover probability based on current best solution to improve convergence.", "configspace": "", "generation": 10, "fitness": 0.8820610949613287, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.036. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8355755147075862, 0.8872262735067333, 0.9233814966696663], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "dcdb7323-5b65-461a-bbca-108a49a09a8f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            adaptive_CR = np.random.rand() * self.CR  # Adaptive crossover probability\n            cross_points = np.random.rand(self.dim) < (adaptive_CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover rate to further enhance diversity and convergence in noisy environments.", "configspace": "", "generation": 11, "fitness": 0.8737505722768221, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.048. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8089298533237346, 0.8872469353739572, 0.9250749281327741], "final_y": [0.13953056200830205, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "3659ed41-f1d8-4088-8c3c-5d4ac664c504", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            # Modified line: Introduce dynamic CR based on score\n            dynamic_CR = self.CR * (self.best_score + 1) / (self.best_score + np.abs(func(self.population[i])) + 1)\n            cross_points = np.random.rand(self.dim) < dynamic_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce dynamic crossover probability in DE to improve exploration-exploitation balance in noisy environments.", "configspace": "", "generation": 12, "fitness": 0.8707521165478221, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.039. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8197197872100337, 0.8795766429699985, 0.9129599194634342], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "6b3ff7e2-7066-4965-a8de-11eaa8a21608", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR + np.dot(self.best_solution, mutant) / (np.linalg.norm(self.best_solution) * np.linalg.norm(mutant)))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce cosine similarity-based adaptive crossover strategy in DE to enhance exploration and convergence efficiency.", "configspace": "", "generation": 13, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for *: 'NoneType' and 'float'\").", "error": "TypeError(\"unsupported operand type(s) for *: 'NoneType' and 'float'\")", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {}, "mutation_prompt": null}
{"id": "7c6906ab-910b-4427-9f28-7d0928cb8ae6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            if self.evals % 100 == 0:  # Inject randomness every 100 evaluations\n                self.initialize_population(bounds)\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration capabilities of DE by injecting random solutions periodically.", "configspace": "", "generation": 14, "fitness": 0.8031059588855562, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.059. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8114409336937555, 0.8702438648146057, 0.7276330781483076], "final_y": [0.12866075835605828, 0.12513925375862112, 0.17794364304429788]}, "mutation_prompt": null}
{"id": "8d8e22ad-199f-4db6-bada-4c40df363307", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        diversity = np.std(self.population, axis=0).mean()  # Compute population diversity\n        dynamic_CR = self.CR * (1 - diversity)  # Dynamic adjustment of CR based on diversity\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (dynamic_CR * (self.best_score + 1))  # Use dynamic_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance the exploration capability by dynamically adjusting the crossover probability based on population diversity.", "configspace": "", "generation": 15, "fitness": 0.8381829413510785, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.838 with standard deviation 0.056. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.7607467798811612, 0.888429841658218, 0.8653722025138562], "final_y": [0.1601872954627901, 0.11765536739219107, 0.1188241453937885]}, "mutation_prompt": null}
{"id": "ea0e70dc-efce-4698-a130-c434a97b1edb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        self.local_search_frequency = 1  # Adaptive local search frequency\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget and self.evals % self.local_search_frequency == 0: # Adaptive local search condition\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Integrate an adaptive local search frequency to balance exploration and exploitation effectively.", "configspace": "", "generation": 16, "fitness": 0.8824630757155237, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.035. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8368042550542054, 0.8872262735067333, 0.9233586985856326], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "54e6ba9c-1993-4c13-a9b2-497cc5f69e6e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            adaptive_CR = self.CR * (1 - (self.best_score + 1))  # Adaptive crossover strategy\n            cross_points = np.random.rand(self.dim) < max(adaptive_CR, 0.1)\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Incorporate an adaptive crossover strategy in DE to enhance solution quality and convergence in noisy environments.", "configspace": "", "generation": 17, "fitness": 0.8370371763417693, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.058. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.756103508208769, 0.8876379505448659, 0.8673700702716731], "final_y": [0.1602240486771621, 0.11765536739219107, 0.1188241453937885]}, "mutation_prompt": null}
{"id": "0c5dc855-d4be-453b-98e3-d616af03a5ee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        self.CR = 0.9 * (1 - self.evals / self.budget) + 0.1  # Adjust CR based on progress\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance local search efficiency by dynamically adjusting crossover probability based on evaluation progress.", "configspace": "", "generation": 18, "fitness": 0.8824630757155237, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.035. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8368042550542054, 0.8872262735067333, 0.9233586985856326], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "9c86b17e-a5fb-46d1-ae62-b935b4f3be53", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            adaptive_CR = np.random.rand() * self.CR  # Adaptive crossover probability\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (adaptive_CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Integrate adaptive crossover probability to further enhance diversity and convergence in noisy environments.", "configspace": "", "generation": 19, "fitness": 0.8737505722768221, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.048. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8089298533237346, 0.8872469353739572, 0.9250749281327741], "final_y": [0.13953056200830205, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "195e0663-537d-4af3-9bbc-c8d78c84eb82", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n        if self.best_score > 0.9:  # Increase population size if nearing optimum\n            self.population_size = min(self.population_size + 5, 100)\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance diversity by adjusting population size based on convergence rate in DE.", "configspace": "", "generation": 20, "fitness": 0.8824630757155237, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.035. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8368042550542054, 0.8872262735067333, 0.9233586985856326], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "b9b863fe-f170-4407-abbb-213999738f40", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            adaptive_CR = np.random.rand() * self.CR  # Adaptive crossover probability\n            cross_points = np.random.rand(self.dim) < adaptive_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability to enhance diversity and convergence in noisy environments.", "configspace": "", "generation": 21, "fitness": 0.8794845111656983, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.879 with standard deviation 0.025. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.845430178740838, 0.8878705218427091, 0.9051528329135478], "final_y": [0.1268568629948028, 0.11716776430947728, 0.11465517566944561]}, "mutation_prompt": null}
{"id": "986f4457-501d-46a4-ae6a-4cf92ad9a9c8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            # Modified line to adaptively adjust CR\n            cross_points = np.random.rand(self.dim) < (self.CR * (1 + self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance crossover probability adaptation based on best solution score to improve diversity and convergence.", "configspace": "", "generation": 22, "fitness": 0.8824630757155237, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.035. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8368042550542054, 0.8872262735067333, 0.9233586985856326], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "f3d0fbbd-8767-494b-bef9-5fc64e0a6bf1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Initial crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        population_diversity = np.std(self.population, axis=0).mean()\n        adaptive_CR = 0.5 + 0.4 * population_diversity  # Adjust CR based on diversity\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < adaptive_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance global exploration in DE by adjusting CR dynamically based on population diversity.", "configspace": "", "generation": 23, "fitness": 0.8525478083570519, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.853 with standard deviation 0.036. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.803490640520458, 0.887612227502133, 0.8665405570485645], "final_y": [0.15580900418077814, 0.11765536739219107, 0.1188241453937885]}, "mutation_prompt": null}
{"id": "e9742e9c-5f2c-4bca-baf6-64e0c0455efc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c) + np.random.normal(0, 0.1, self.dim), lower_bounds, upper_bounds)  # Added Gaussian noise\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce Gaussian noise to mutation for better exploration in noisy environments.", "configspace": "", "generation": 24, "fitness": 0.8801221891640311, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.038. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8370296668747919, 0.8731194997547673, 0.930217400862534], "final_y": [0.11982621775606739, 0.11891491885413608, 0.1096081320504505]}, "mutation_prompt": null}
{"id": "647b2b29-de58-402b-8c2b-3be013feb25b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            adaptive_CR = self.CR * (1 - (self.evals / self.budget))  # Adaptive crossover\n            cross_points = np.random.rand(self.dim) < adaptive_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n\n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            if self.evals % 100 == 0 and self.population_size > 5:\n                self.population_size -= 1  # Dynamically adjust population size\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance DE by incorporating a dynamic population size adjustment and adaptive crossover probability to boost exploration and convergence.", "configspace": "", "generation": 25, "fitness": 0.8601947220914461, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.050. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.7890453522667528, 0.890445354571191, 0.9010934594363945], "final_y": [0.15218932074615577, 0.11621992467546649, 0.11765167949281352]}, "mutation_prompt": null}
{"id": "91e576cd-a788-4232-b110-8ae93e6f2240", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            adaptive_CR = self.CR * (1 - self.evals / self.budget)  # Dynamically adjust CR\n            cross_points = np.random.rand(self.dim) < adaptive_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n            # Elitism: retain best solution\n            self.population[np.argmin([func(ind) for ind in self.population])] = self.best_solution\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance diversity and convergence by dynamically adjusting crossover probability and incorporating elitism in DE.", "configspace": "", "generation": 26, "fitness": 0.7460835854588813, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.746 with standard deviation 0.047. And the mean value of best solutions found was 0.180 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.6830512382034126, 0.7943219750879833, 0.7608775430852478], "final_y": [0.20726632345844398, 0.1601017669387199, 0.17296403364081692]}, "mutation_prompt": null}
{"id": "a44f642e-41e7-4801-8c43-15cb7d10e49b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            pop_diversity = np.std(self.population, axis=0).mean()  # Calculate population diversity\n            adaptive_CR = self.CR * (1 + pop_diversity)  # Dynamically adjust CR\n            cross_points = np.random.rand(self.dim) < (adaptive_CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance the adaptive mutation strategy by dynamically adjusting the crossover probability (CR) based on population diversity to improve exploration-exploitation balance.", "configspace": "", "generation": 27, "fitness": 0.8824630757155237, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.035. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8368042550542054, 0.8872262735067333, 0.9233586985856326], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "8499b004-e9e0-460c-bb57-430da8be888a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, self.best_solution, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance local search integration by initializing it with the current best solution to improve convergence.", "configspace": "", "generation": 28, "fitness": 0.7777442993197567, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.013. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.7788071535573644, 0.7931436318599447, 0.761282112541961], "final_y": [0.16545889311169248, 0.1601017669387199, 0.17296403364081692]}, "mutation_prompt": null}
{"id": "feaa2808-5776-4c9e-93fc-2378f593cb8c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.7  # Changed Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance diversity and convergence with a dynamic crossover probability in DE for noisy black-box optimization.", "configspace": "", "generation": 29, "fitness": 0.8820534955999841, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.036. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8355755147075862, 0.8872262735067333, 0.9233586985856326], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "8db09450-50eb-4e0d-b41b-d7cab0b5d633", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        diversity = np.std(self.population, axis=0).mean()  # Measure population diversity\n        adaptive_CR = min(1.0, self.CR * (1 + diversity))  # Dynamically adjust crossover probability\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (adaptive_CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance adaptive mutation strategy by dynamically adjusting the crossover probability based on population diversity.", "configspace": "", "generation": 30, "fitness": 0.8824630757155237, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.035. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8368042550542054, 0.8872262735067333, 0.9233586985856326], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "6ecbf470-424a-4c79-b213-c3eaaa5d71fc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1) * np.random.rand())  # Adjust CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance adaptive mutation strategy in DE by dynamically adjusting crossover probability for improved exploration-exploitation balance.", "configspace": "", "generation": 31, "fitness": 0.8746613682956955, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.875 with standard deviation 0.046. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8122672522512099, 0.8886900379491138, 0.9230268146867631], "final_y": [0.13953056200830205, 0.11621778599563126, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "e0b374a6-7a50-4673-a959-e83f32f2f2f7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'ftol': 1e-6})\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance local search by incorporating a faster convergence check in L-BFGS-B to improve solution efficiency.", "configspace": "", "generation": 32, "fitness": 0.8824630757155237, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.035. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8368042550542054, 0.8872262735067333, 0.9233586985856326], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "cca2b223-337d-4e33-8568-295262952b9a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            dynamic_CR = self.CR * (self.best_score + 1) / 2  # Dynamic crossover probability\n            cross_points = np.random.rand(self.dim) < dynamic_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Refine adaptive mutation strategy by dynamically adjusting the crossover probability to improve convergence in noisy environments.", "configspace": "", "generation": 33, "fitness": 0.8796594174372409, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.040. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8272651905846929, 0.8886862470402666, 0.9230268146867631], "final_y": [0.13477441871584106, 0.11622399553425378, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "6c243636-3e8a-49c6-be06-91630cdcf35f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (1 - self.evals/self.budget))  # Dynamic CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance adaptive mutation in DE by adjusting crossover probability dynamically based on convergence speed.", "configspace": "", "generation": 34, "fitness": 0.8445028182616389, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.050. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.7740701247888211, 0.8874710266375259, 0.8719673033585696], "final_y": [0.16019225847722685, 0.11765536739219107, 0.11883000328701776]}, "mutation_prompt": null}
{"id": "e008f6d5-84eb-4dd0-b403-7dc2790be539", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        self.CR = np.random.uniform(0.7, 1.0)  # Dynamic crossover probability\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance local search by dynamically adjusting the crossover probability and incorporating iterative refinement to improve convergence.", "configspace": "", "generation": 35, "fitness": 0.8824630757155237, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.035. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8368042550542054, 0.8872262735067333, 0.9233586985856326], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "e40ede3d-f9a8-4282-aca6-00c82596b8bd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        trial += np.random.normal(0, 0.01, size=trial.shape)  # Added stochastic perturbation\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance local search with a stochastic perturbation to improve exploration around promising solutions.", "configspace": "", "generation": 36, "fitness": 0.8823057825880379, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.035. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8368088229470645, 0.8872439398732002, 0.9228645849438492], "final_y": [0.13477364751236143, 0.11765607759147623, 0.11209467170172005]}, "mutation_prompt": null}
{"id": "59d2162c-c66c-4391-9fb5-1063136c48c9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n\n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F \n            adaptive_CR = np.random.rand() * self.CR  # Self-adaptive crossover probability\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (adaptive_CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n            if self.evals % (self.budget // 10) == 0:  # Dynamic population resizing\n                self.population_size = max(10, self.population_size - 1)\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce self-adaptive crossover probability and dynamic population resizing in DE to enhance adaptability and convergence speed.", "configspace": "", "generation": 37, "fitness": 0.8737505722768221, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.048. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8089298533237346, 0.8872469353739572, 0.9250749281327741], "final_y": [0.13953056200830205, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "2e61ac15-8c77-4a64-8658-20547cdb7c9b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            \n            # Adaptive crossover probability based on population diversity\n            population_diversity = np.var(self.population, axis=0).mean()\n            adaptive_CR = self.CR * (1 + population_diversity)\n\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (adaptive_CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce an adaptive crossover probability in DE based on population diversity to enhance convergence under varying conditions.", "configspace": "", "generation": 38, "fitness": 0.8824630757155237, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.035. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8368042550542054, 0.8872262735067333, 0.9233586985856326], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "5f9407bb-f3a0-4b8a-8378-6320c267eef8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                tournament_size = 2\n                for i in range(self.population_size):\n                    competitors = np.random.choice(self.population_size, tournament_size, replace=False)\n                    winner = max(competitors, key=lambda x: func(self.population[x]))\n                    self.population[i] = self.population[winner]\n                    self.evals += tournament_size  # Adjust evals for additional function evaluations\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce tournament selection in DE to enhance survival of high-quality candidates while maintaining diversity.", "configspace": "", "generation": 39, "fitness": 0.7777442993197567, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.013. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.7788071535573644, 0.7931436318599447, 0.761282112541961], "final_y": [0.16545889311169248, 0.1601017669387199, 0.17296403364081692]}, "mutation_prompt": null}
{"id": "e540c157-4ba6-4414-aae7-18830db4d82a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            dynamic_CR = self.CR * (1 + np.random.rand() * 0.1)  # Dynamic crossover probability\n            cross_points = np.random.rand(self.dim) < dynamic_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a randomized dynamic crossover probability to enhance exploration and convergence balance.", "configspace": "", "generation": 40, "fitness": 0.8400876918014634, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.054. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.7646500658537729, 0.8875028077528393, 0.8681102017977779], "final_y": [0.15980807810708053, 0.11765536739219107, 0.1188241453937885]}, "mutation_prompt": null}
{"id": "f4a981d3-b553-415c-aca7-505a077c03c6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            dynamic_CR = self.CR * (1 - (self.evals / self.budget))  # Modifying this line\n            cross_points = np.random.rand(self.dim) < (dynamic_CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Integrate dynamic adjustment of crossover probability (CR) to balance exploration and exploitation.", "configspace": "", "generation": 41, "fitness": 0.8824630757155237, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.035. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8368042550542054, 0.8872262735067333, 0.9233586985856326], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "2eed0476-59ef-4ac3-aeb1-0f45bfd850f2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Initial crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n                self.CR = min(1.0, self.CR + 0.01)  # Increase CR on improvement\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploitation by increasing the crossover probability dynamically based on an evolving function improvement threshold.", "configspace": "", "generation": 42, "fitness": 0.8824630757155237, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.035. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8368042550542054, 0.8872262735067333, 0.9233586985856326], "final_y": [0.13477441871584106, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "fa4a0046-f9b0-4bc4-a8a7-282a54d6c206", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            adaptive_CR = np.random.rand() * self.CR  # Adaptive crossover probability\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (adaptive_CR * (self.best_score + 1))  # Updated line\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability in DE to further enhance exploration in noisy environments.", "configspace": "", "generation": 43, "fitness": 0.8737505722768221, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.048. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8089298533237346, 0.8872469353739572, 0.9250749281327741], "final_y": [0.13953056200830205, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "d75006c0-83a9-4f50-bc1a-3097a4710022", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            dynamic_CR = self.CR * (1 - (self.evals / self.budget))  # Dynamic crossover probability\n            cross_points = np.random.rand(self.dim) < dynamic_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover probability in DE to improve exploration-exploitation balance.", "configspace": "", "generation": 44, "fitness": 0.8445028182616389, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.050. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.7740701247888211, 0.8874710266375259, 0.8719673033585696], "final_y": [0.16019225847722685, 0.11765536739219107, 0.11883000328701776]}, "mutation_prompt": null}
{"id": "13eabd0b-3cec-4004-801c-07939db95fe8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            adaptive_CR = np.random.rand() * self.CR  # Adaptive crossover probability\n            cross_points = np.random.rand(self.dim) < (adaptive_CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability in DE to improve exploration-exploitation balance in dynamic environments.", "configspace": "", "generation": 45, "fitness": 0.8737505722768221, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.048. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.8089298533237346, 0.8872469353739572, 0.9250749281327741], "final_y": [0.13953056200830205, 0.11765536739219107, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "91404be3-5be7-4b38-8d7f-ac02d87e0f9e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            diversity = np.std(self.population, axis=0).mean()\n            adaptive_CR = self.CR * (1 + diversity)  # Adaptive crossover probability\n            cross_points = np.random.rand(self.dim) < adaptive_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance adaptive crossover probability to dynamically adjust based on population diversity for improved convergence.", "configspace": "", "generation": 46, "fitness": 0.8525478083570519, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.853 with standard deviation 0.036. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.803490640520458, 0.887612227502133, 0.8665405570485645], "final_y": [0.15580900418077814, 0.11765536739219107, 0.1188241453937885]}, "mutation_prompt": null}
{"id": "d5475f88-7d25-489f-9724-ef6b90fdd64f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.rand() * self.F  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            adaptive_CR = self.CR * (1 - (self.evals / self.budget))  # Dynamic crossover probability\n            cross_points = np.random.rand(self.dim) < adaptive_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance adaptive mutation strategy by introducing a dynamic crossover probability in DE to improve exploration and convergence.", "configspace": "", "generation": 47, "fitness": 0.8445028182616389, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.050. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.7740701247888211, 0.8874710266375259, 0.8719673033585696], "final_y": [0.16019225847722685, 0.11765536739219107, 0.11883000328701776]}, "mutation_prompt": null}
{"id": "760638a5-fe90-4b1b-977a-09ef3782e3f2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Improve diversity and exploration by modifying adaptive mutation strategy in HybridMetaheuristic algorithm.", "configspace": "", "generation": 48, "fitness": 0.8861303176504568, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.032. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "584aecca-fcc9-4d70-9da9-f7255d8934c5", "metadata": {"aucs": [0.845381256118849, 0.8886314257442742, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "2de7b230-9960-47e1-a4ef-518f87cb1da0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n\n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce elitism in the population update to maintain high-quality solutions across generations.", "configspace": "", "generation": 49, "fitness": 0.8861303176504568, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.032. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "760638a5-fe90-4b1b-977a-09ef3782e3f2", "metadata": {"aucs": [0.845381256118849, 0.8886314257442742, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "66891a17-9f0c-4818-b26b-3e775575dbb1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * np.random.rand())  # Add randomness to CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover probability to improve diversity and exploration in the HybridMetaheuristic algorithm.", "configspace": "", "generation": 50, "fitness": 0.8320787469727646, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.832 with standard deviation 0.032. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "760638a5-fe90-4b1b-977a-09ef3782e3f2", "metadata": {"aucs": [0.8311900494177715, 0.7931939448395642, 0.871852246660958], "final_y": [0.12567614548475492, 0.15536491384071893, 0.12681201754271676]}, "mutation_prompt": null}
{"id": "8af17716-58c0-4973-bbed-1bd7a15d1f1a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        diversity = np.std(self.population, axis=0).mean()  # Calculate diversity\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1) * diversity)  # Adjust CR with diversity\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration in HybridMetaheuristic by adjusting crossover probability based on diversity.", "configspace": "", "generation": 51, "fitness": 0.8861303176504568, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.032. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "760638a5-fe90-4b1b-977a-09ef3782e3f2", "metadata": {"aucs": [0.845381256118849, 0.8886314257442742, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "e59149a3-5f9d-4c5c-aaca-846c462243b0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Initial crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            # Changed line: Introduce dynamic crossover probability\n            cross_points = np.random.rand(self.dim) < (self.CR * (1 - self.evals / self.budget))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover probability in HybridMetaheuristic to improve exploration-exploitation balance.", "configspace": "", "generation": 52, "fitness": 0.8360402808160016, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.836 with standard deviation 0.053. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "760638a5-fe90-4b1b-977a-09ef3782e3f2", "metadata": {"aucs": [0.7632359689742647, 0.8874539504426326, 0.8574309230311071], "final_y": [0.1601884102303991, 0.11765536739219107, 0.12680821496709604]}, "mutation_prompt": null}
{"id": "136f49f4-a206-4aca-b97b-c7eda483d90f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            # Dynamically adjust crossover probability\n            self.CR = 0.5 * (1 + np.tanh(10 * (self.best_score - func(self.population[i]))))\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance the exploration and exploitation balance by adjusting the crossover probability dynamically in the HybridMetaheuristic algorithm.", "configspace": "", "generation": 53, "fitness": 0.8702700804836292, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.039. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "760638a5-fe90-4b1b-977a-09ef3782e3f2", "metadata": {"aucs": [0.8184245441946552, 0.8796478118548037, 0.9127378854014288], "final_y": [0.12567434187101356, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "e1cc835d-848a-4e96-bc74-dd6fd8e68223", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            # Modified line for dynamic crossover probability\n            current_CR = self.CR * (1 - (self.evals / self.budget))\n            cross_points = np.random.rand(self.dim) < (current_CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover probability based on the evaluation count to improve exploitation in HybridMetaheuristic algorithm.", "configspace": "", "generation": 54, "fitness": 0.8861303176504568, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.032. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "760638a5-fe90-4b1b-977a-09ef3782e3f2", "metadata": {"aucs": [0.845381256118849, 0.8886314257442742, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "12ddb146-3cde-4954-bbd9-e6e7b00795b5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        cr_dynamic = self.CR  # Dynamic crossover probability\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    cr_dynamic *= 0.995  # Dynamically adjust crossover probability\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance the local search by dynamically adjusting the crossover probability based on convergence speed.", "configspace": "", "generation": 55, "fitness": 0.8861303176504568, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.032. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "760638a5-fe90-4b1b-977a-09ef3782e3f2", "metadata": {"aucs": [0.845381256118849, 0.8886314257442742, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance trial selection by introducing a probability-based replacement strategy in HybridMetaheuristic.", "configspace": "", "generation": 56, "fitness": 0.8912098642560662, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.026. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "760638a5-fe90-4b1b-977a-09ef3782e3f2", "metadata": {"aucs": [0.8612588462105357, 0.8879924754694154, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "05056b73-8d5a-4b83-a3ac-cfca9310945f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            if self.evals % 100 == 0:  # Dynamically adjust population size\n                self.population_size = np.clip(int(self.population_size * 1.1), 10, 100)\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic population size adjustment strategy based on convergence progress.", "configspace": "", "generation": 57, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 53 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 53 is out of bounds for axis 0 with size 50')", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {}, "mutation_prompt": null}
{"id": "9506e2e9-f68b-4407-ab3d-dbc26b91feea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(lambda x: func(x) + 0.01 * np.var(x), trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))  # Integrated robustness metric\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Improve local search by incorporating robustness metrics to refine solution quality and stability.", "configspace": "", "generation": 58, "fitness": 0.7867443761684195, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.787 with standard deviation 0.014. And the mean value of best solutions found was 0.163 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.7987360776022902, 0.7939098338226267, 0.7675872170803414], "final_y": [0.15781823825748809, 0.1601017669387199, 0.1703120078621685]}, "mutation_prompt": null}
{"id": "3a8e99fe-a995-40af-b2b6-5bfa7f49f887", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            adaptive_CR = np.random.uniform(0.8, 1.0)  # Adaptive crossover probability\n            cross_points = np.random.rand(self.dim) < (adaptive_CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        if np.random.rand() < 0.5:  # Probabilistic application of local search\n            result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n            self.evals += result.nfev\n            return result.x, result.fun\n        return trial, func(trial)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability and probabilistic local search application to enhance solution diversity and refinement.", "configspace": "", "generation": 59, "fitness": 0.8797684828854403, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.049. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.019.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.9054060788444838, 0.8106332103543954, 0.9232661594574416], "final_y": [0.11012229707633292, 0.1518704689580358, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "f8dbd00d-5c27-4263-8bab-8dd53ac8b81b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            # Dynamically adjust CR based on best_score to improve balance\n            adaptive_CR = self.CR * (1 - np.tanh(self.best_score))  \n            cross_points = np.random.rand(self.dim) < adaptive_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic adaptation of crossover probability (CR) based on current best score to enhance exploration and exploitation balance.", "configspace": "", "generation": 60, "fitness": 0.8438596124599907, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.068. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.7481218043860336, 0.8875240196278918, 0.8959330133660466], "final_y": [0.1601884102303991, 0.11765536739219107, 0.11466187215858148]}, "mutation_prompt": null}
{"id": "912bf8a6-368d-4565-8b64-383db9f1beb4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'eps': 1e-8})\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Optimize with an improved local search by dynamically adjusting step sizes based on convergence.", "configspace": "", "generation": 61, "fitness": 0.8912098642560662, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.026. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8612588462105357, 0.8879924754694154, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "5faa3638-c7de-4d73-ad19-1b22aa450d37", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = min(50, budget // 10)  # Dynamic population size based on budget\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Improve exploration by dynamically adjusting population size based on evaluation budget in HybridMetaheuristic.", "configspace": "", "generation": 62, "fitness": 0.8912098642560662, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.026. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8612588462105357, 0.8879924754694154, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "b3b98cfa-5197-4b6e-9117-7f54bf9e3f80", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            adaptive_CR = self.CR * (1 + 0.5 * np.random.rand())  # Adaptive crossover probability\n            cross_points = np.random.rand(self.dim) < adaptive_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 10})\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability and enhance local search efficiency in HybridMetaheuristic.", "configspace": "", "generation": 63, "fitness": 0.8888819927748278, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.889 with standard deviation 0.003. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8849259258651155, 0.8899179363849941, 0.8918021160743737], "final_y": [0.11888064420507338, 0.11572033630457867, 0.11593069545879209]}, "mutation_prompt": null}
{"id": "b31e1df3-e604-4a57-aabf-00289cfbfa8f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        options = {'eps': 1e-8}  # Adaptive step size for local search\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options=options)\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance local search by implementing an adaptive step size for improved refinement in the HybridMetaheuristic.", "configspace": "", "generation": 64, "fitness": 0.8912098642560662, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.026. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8612588462105357, 0.8879924754694154, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "d55c9a33-5b7b-4fc8-90a6-6338d9a45920", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            adaptive_CR = 0.5 + 0.4 * (self.best_score + 1)  # Adaptive crossover probability\n            cross_points = np.random.rand(self.dim) < adaptive_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        dynamic_bounds = list(zip(bounds.lb, bounds.ub))\n        for dim in range(self.dim):\n            dynamic_bounds[dim] = (trial[dim] - 0.1 * abs(trial[dim]), trial[dim] + 0.1 * abs(trial[dim]))\n        result = minimize(func, trial, method='L-BFGS-B', bounds=dynamic_bounds)\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability and enhance local search exploitation with a dynamic boundary adjustment.", "configspace": "", "generation": 65, "fitness": 0.8080982530341977, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.017. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.7987360776022902, 0.7939098338226267, 0.8316488476776761], "final_y": [0.15781823825748809, 0.1601017669387199, 0.14380703865805244]}, "mutation_prompt": null}
{"id": "142c5072-e6fe-4560-a255-f89b743fc6b0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.2:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce self-adaptive mutation strategy and increase probability-based replacement factor to enhance convergence.", "configspace": "", "generation": 66, "fitness": 0.8909859769914785, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.026. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8612588462105357, 0.8873208136756524, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "acebbfa4-43ef-49aa-8e5e-efaeb213af6a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (np.random.uniform(0.7, 1.0) * (self.best_score + 1)) # Adaptive crossover\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        if np.random.rand() > 0.5:  # Diversified local search strategy\n            result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        else:\n            result = minimize(func, trial, method='TNC', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Integrate adaptive crossover rates and diversify local search strategies to enhance solution refinement.", "configspace": "", "generation": 67, "fitness": 0.8562365760092048, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.026. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8239477472877664, 0.8871971042085195, 0.8575648765313286], "final_y": [0.1395297339537852, 0.11765536739219107, 0.12638434061908788]}, "mutation_prompt": null}
{"id": "c6d38c8f-aa1e-4868-b9f1-27b0ab59b1d7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n        # Adjust population size dynamically\n        self.population_size = max(10, int(50 * (1 - self.evals / self.budget)))\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic population size adjustment strategy based on convergence to improve exploration-exploitation balance.", "configspace": "", "generation": 68, "fitness": 0.8912098642560662, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.026. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8612588462105357, 0.8879924754694154, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "38eea506-bfb1-476b-9f1b-d7d17bc3484e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            stochastic_noise = np.random.uniform(-0.1, 0.1, size=self.dim)  # Added line\n            mutant = np.clip(a + adaptive_F * (b - c) + stochastic_noise, lower_bounds, upper_bounds)  # Modified line\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce stochastic noise to the mutation operation for enhanced exploration in the HybridMetaheuristic.", "configspace": "", "generation": 69, "fitness": 0.8701941046401735, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.038. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8165096548615614, 0.8901187820413043, 0.9039538770176552], "final_y": [0.12878310408661808, 0.11765536739219107, 0.11209170238039323]}, "mutation_prompt": null}
{"id": "00137356-7cc9-43ca-b882-ae6edb5fb9f1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            # Change: Adaptively adjust CR based on best score\n            adaptive_CR = self.CR * (1 + self.best_score)\n            cross_points = np.random.rand(self.dim) < adaptive_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability to improve convergence speed in HybridMetaheuristic.", "configspace": "", "generation": 70, "fitness": 0.8912098642560662, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.026. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8612588462105357, 0.8879924754694154, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "2dd2e853-4754-4d91-8974-acc0538a4001", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            adaptive_CR = np.random.uniform(0.7, 1.0)  # Adaptive crossover probability\n            cross_points = np.random.rand(self.dim) < (adaptive_CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        options = {'ftol': 1e-6}  # Dynamic precision adjustment for refined local search\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options=options)\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability and enhance local search with dynamic precision adjustment for improved convergence.", "configspace": "", "generation": 71, "fitness": 0.8905715455907456, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.025. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8614588464769832, 0.8871971042085195, 0.9230586860867341], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "f4e4135d-37d1-4f4e-a83a-18df9060f7b1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        # Adaptive population size based on budget usage\n        self.population_size = max(10, int(self.budget / 100))  \n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce an adaptive population size strategy to improve exploration in HybridMetaheuristic.", "configspace": "", "generation": 72, "fitness": 0.8910258160910306, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.039. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8412529150120575, 0.8952870285450416, 0.9365375047159926], "final_y": [0.12134874860051703, 0.11765536739219107, 0.11210796224921027]}, "mutation_prompt": null}
{"id": "036c2ef7-2c0f-458c-918b-1c7471d8f2b5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n                self.CR = min(1.0, self.CR + 0.1)  # Adjust CR based on score improvement\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic adjustment of the crossover probability (CR) based on the best score improvement.", "configspace": "", "generation": 73, "fitness": 0.8912098642560662, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.026. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8612588462105357, 0.8879924754694154, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "67ae5092-4ec6-47d6-be5d-94ca3a62e3ea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < np.random.uniform(0.6, 1.0)  # Dynamic crossover probability\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 10}) # Limit local search iterations\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance trial selection by introducing dynamic crossover probability and adaptive local search in HybridMetaheuristic.", "configspace": "", "generation": 74, "fitness": 0.8373655153289884, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.048. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.7745826133737155, 0.8899450077182302, 0.8475689248950196], "final_y": [0.14616008042198325, 0.11572033630457867, 0.13085337038895684]}, "mutation_prompt": null}
{"id": "e9619ad2-a20f-4fe9-bd28-874bf52a7d14", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n        # Dynamic adjustment of crossover probability\n        self.CR = min(1.0, self.CR + 0.05 * (self.best_score > trial_score))\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce dynamic crossover probability adjustment based on generation performance in HybridMetaheuristic.", "configspace": "", "generation": 75, "fitness": 0.8912098642560662, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.026. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8612588462105357, 0.8879924754694154, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "e18ffa8e-bfbc-41f3-88c3-cc94d4f26894", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.initial_population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.initial_population_size, self.dim))\n    \n    def update_population_size(self):\n        self.population_size = max(10, int(self.initial_population_size * (1 - self.evals / self.budget)))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.uniform(0.4, 0.9) * self.F  # Adaptive mutation factor with uniform distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.update_population_size()\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive population size adjustment and refined mutation strategy for enhanced exploration.", "configspace": "", "generation": 76, "fitness": 0.8811630712632273, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.881 with standard deviation 0.040. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8298556664564952, 0.8873214882355347, 0.9263120590976524], "final_y": [0.13477394235883278, 0.11765536739219107, 0.1121267240432875]}, "mutation_prompt": null}
{"id": "9ed492a7-3bc7-43ea-ab8c-d3a8ec9f8db5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        tol = 1e-5 * (1 - (self.evals / self.budget))  # Adaptive tolerance\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), tol=tol)\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance local search by varying the L-BFGS-B tolerance based on current evaluations.", "configspace": "", "generation": 77, "fitness": 0.8330403135237612, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.833 with standard deviation 0.006. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.841069189963956, 0.8254807637939996, 0.832570986813328], "final_y": [0.12619357670630227, 0.11836610865650277, 0.12640849984274405]}, "mutation_prompt": null}
{"id": "9274cef8-055e-4df6-b17f-46e4f2aa2741", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            cross_points = np.random.rand(self.dim) < np.random.uniform(0.5, 1.0)  # Altered line for adaptive CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            adaptive_F *= np.random.uniform(0.9, 1.1)  # Altered line for adaptive F\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability and stochastic adaptive F scaling in the HybridMetaheuristic algorithm for enhanced exploration and exploitation balance.", "configspace": "", "generation": 78, "fitness": 0.8704850088132398, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.022. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8391152176695198, 0.8874191517323811, 0.8849206570378185], "final_y": [0.11163006182726021, 0.11765536739219107, 0.11765414171987942]}, "mutation_prompt": null}
{"id": "e757cf63-0018-4a0f-8b0d-ac0bfba4a22d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    if self.best_score > 0.1:  # Adaptive local search frequency\n                        x_local, score_local = self.local_search(individual, bounds, func)\n                        if score_local > self.best_score:\n                            self.best_score = score_local\n                            self.best_solution = x_local\n                        self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Integrate adaptive local search frequency based on the current performance of the algorithm to improve exploration-exploitation balance.", "configspace": "", "generation": 79, "fitness": 0.8912098642560662, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.026. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8612588462105357, 0.8879924754694154, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "6f189e15-ef92-49b8-8cc1-5a4ab2f8bfe6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        diversity = np.std(self.population, axis=0).mean()  # Calculate population diversity\n        adaptive_CR = 0.5 + 0.4 * (1 - diversity)  # Adaptive crossover probability\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (adaptive_CR * (self.best_score + 1))  # Use adaptive_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability based on population diversity to enhance exploration-exploitation balance.", "configspace": "", "generation": 80, "fitness": 0.8438775999057196, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.069. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.74689121219463, 0.8898497240443376, 0.8948918634781914], "final_y": [0.1601884102303991, 0.11622556348878144, 0.11466187215858148]}, "mutation_prompt": null}
{"id": "744d9cf6-df86-4087-a64b-259dfe5a375e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1) * (self.evals/self.budget))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic adaptation of the crossover probability to enhance exploration.", "configspace": "", "generation": 81, "fitness": 0.8742095209015782, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.047. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8103993735565396, 0.8896911129402678, 0.922538076207927], "final_y": [0.1395297339537852, 0.11622556348878144, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "3f17db3c-d6e0-45e6-9bc4-24bea483fad9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.initial_population_size = 50\n        self.population_size = self.initial_population_size\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.uniform(0.4, 1.0) * self.F  # Adaptive mutation factor with uniform distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n        # Adjust the population size adaptively based on evaluations\n        self.population_size = max(10, int(self.initial_population_size * (1 - self.evals / self.budget)))\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive population size and mutation factor strategy in HybridMetaheuristic for improved exploration and exploitation balance.", "configspace": "", "generation": 82, "fitness": 0.8805759368947214, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.881 with standard deviation 0.039. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.829836296645555, 0.8873373645107123, 0.9245541495278967], "final_y": [0.13476717879196187, 0.11765536739219107, 0.1117640745700137]}, "mutation_prompt": null}
{"id": "499b64f3-958a-4ffa-91fe-999816e3011a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a weighted probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.05 + 0.05 * (trial_score - self.best_score):\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Implement a weighted probability-based replacement strategy to improve trial selection.  ", "configspace": "", "generation": 83, "fitness": 0.8912098642560662, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.026. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8612588462105357, 0.8879924754694154, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "70b9545c-d5da-4fbd-9449-7f0a7ae31217", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        # Start with half the population for exploration\n        effective_population_size = max(self.population_size // 2, 5) if self.evals < self.budget // 2 else self.population_size\n        for i in range(effective_population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            # Conduct local search less frequently to maintain balance\n            if self.evals < self.budget and np.random.rand() < 0.5:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic population size strategy and adjust local search frequency for better exploration-exploitation balance.", "configspace": "", "generation": 84, "fitness": 0.8857724725551984, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.043. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8300901780728753, 0.8937013342808506, 0.9335259053118695], "final_y": [0.13685623838909755, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "31559fe8-05b4-43ae-8cec-1424e4104a82", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n        # Dynamic population size adjustment\n        self.population_size = max(10, int(self.budget / (self.evals + 1) * self.population_size))\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Incorporate a dynamic population size adjustment strategy based on evaluation budget utilization.", "configspace": "", "generation": 85, "fitness": 0.8912098642560662, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.026. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8612588462105357, 0.8879924754694154, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "e4193339-632e-473e-b86b-fc570ec18af3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            dynamic_CR = self.CR * (1 + (self.best_score / (self.best_score + 1)))  # Dynamic CR adjustment\n            cross_points = np.random.rand(self.dim) < dynamic_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance the trial selection strategy by incorporating dynamic scaling of crossover probability based on current performance.", "configspace": "", "generation": 86, "fitness": 0.8912098642560662, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.026. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8612588462105357, 0.8879924754694154, 0.9243782710882472], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "95f11b2b-0611-49ca-a883-55cd373dc55a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            adaptive_CR = np.random.uniform(0.7, 1.0)  # Adaptive crossover probability\n            cross_points = np.random.rand(self.dim) < (adaptive_CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.2:  # Increased the acceptance probability\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability and enhance trial acceptance criteria in HybridMetaheuristic.", "configspace": "", "generation": 87, "fitness": 0.8905715455907456, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.025. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8614588464769832, 0.8871971042085195, 0.9230586860867341], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "3e35d874-9a6a-4ffa-b5df-45fbc0844e6e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            population_diversity = np.std(self.population, axis=0)\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F * (1 + population_diversity.mean())  # Enhanced adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < (0.1 + population_diversity.mean() * 0.1):  # Adjusted probability check\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance the adaptive mutation and trial acceptance with a dynamic scaling factor based on population diversity.", "configspace": "", "generation": 88, "fitness": 0.8647743683461097, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.865 with standard deviation 0.053. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.7920502186898027, 0.8831226467511084, 0.9191502395974175], "final_y": [0.15076469232664536, 0.12199159328144882, 0.11176256956844777]}, "mutation_prompt": null}
{"id": "4e8ff033-6d48-4589-9995-ea4e3e43f0af", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * (self.F + np.random.rand() * 0.2)  # Adjust F dynamically\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1) * np.random.rand())  # Adjust CR dynamically\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance trial mutation in HybridMetaheuristic by introducing a dynamic crossover probability and F-value adjustment based on performance.", "configspace": "", "generation": 89, "fitness": 0.8741736373812395, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.043. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8156988856901011, 0.8871762368242511, 0.9196457896293666], "final_y": [0.13953515239293512, 0.11765536739219107, 0.11176208253172037]}, "mutation_prompt": null}
{"id": "38f930a9-1783-43b8-84df-bb633eb5fec0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (1 - (self.evals / self.budget)))  # Dynamic crossover probability\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover probability strategy to enhance diversity and improve convergence.", "configspace": "", "generation": 90, "fitness": 0.8546487887784213, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.028. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8190614928615241, 0.8874539504426326, 0.8574309230311071], "final_y": [0.14734715911994878, 0.11765536739219107, 0.12680821496709604]}, "mutation_prompt": null}
{"id": "bfad0fb9-3c1e-4a9e-9f11-0c1a85a5984a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            adaptive_CR = np.random.uniform(0.5, 1.0)  # Adaptive crossover probability\n            cross_points = np.random.rand(self.dim) < adaptive_CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce an adaptive crossover probability in HybridMetaheuristic to improve exploration-exploitation balance.", "configspace": "", "generation": 91, "fitness": 0.8797237231942275, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.012. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8632024598480448, 0.8875111785045399, 0.888457531230098], "final_y": [0.11545939277363815, 0.11765536739219107, 0.11109719738780888]}, "mutation_prompt": null}
{"id": "10989bde-e024-4353-a2b7-62c69ae731d7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F * (1 + self.best_score / (abs(self.best_score) + 1))  # Enhanced adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1) * np.random.uniform(0.5, 1.5))  # Dynamic crossover probability\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover probability and adaptive mutation factor scaling for enhanced exploration.", "configspace": "", "generation": 92, "fitness": 0.8816605743030171, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.050. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8145535768441601, 0.8978615461836439, 0.9325665998812476], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "4f215cbc-4d4b-4a01-8868-f05ea23b243c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F * (1 - self.evals / self.budget)  # Dynamic adjustment\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.evals += 1\n    \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic mutation factor adjustment based on the current evaluation ratio to improve adaptability in the HybridMetaheuristic algorithm.", "configspace": "", "generation": 93, "fitness": 0.8868989672392189, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.887 with standard deviation 0.031. And the mean value of best solutions found was 0.114 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8489046139070222, 0.8873208136756524, 0.9244714741349822], "final_y": [0.1126378630710364, 0.11765536739219107, 0.11176159329527713]}, "mutation_prompt": null}
{"id": "52dc0e42-744a-46e5-8d4f-85031699e231", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n            \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 20})\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance trial selection by introducing adaptive mutation factor and selective local search in HybridMetaheuristic.", "configspace": "", "generation": 94, "fitness": 0.8936814192040167, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.894 with standard deviation 0.022. And the mean value of best solutions found was 0.114 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "847a6cf2-d289-4d88-8ae9-58de19c80417", "metadata": {"aucs": [0.8694232847810819, 0.8879924754694154, 0.9236284973615527], "final_y": [0.11314466313779081, 0.11765536739219107, 0.11206342425806681]}, "mutation_prompt": null}
{"id": "b560147e-dd06-4479-a359-4646c490cb1e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.6, 0.2) * self.F  # Adjusted adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.15:  # Increased probability\n                self.population[i] = trial\n            \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 30})  # Increased maxiter\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce learning-based parameter adjustment and enhanced local search in HybridMetaheuristic.", "configspace": "", "generation": 95, "fitness": 0.8854677434448207, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.885 with standard deviation 0.027. And the mean value of best solutions found was 0.114 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "52dc0e42-744a-46e5-8d4f-85031699e231", "metadata": {"aucs": [0.8511801714365543, 0.8873909463076494, 0.9178321125902585], "final_y": [0.11263561530211974, 0.11765536739219107, 0.11176200193259045]}, "mutation_prompt": null}
{"id": "a948cdd1-78bf-4fd5-bc05-a61273701fc6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        self.cr_success = []\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            dynamic_CR = np.mean(self.cr_success) if self.cr_success else self.CR  # Dynamic crossover\n            cross_points = np.random.rand(self.dim) < (dynamic_CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n                self.cr_success.append(dynamic_CR)\n\n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 20})\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Enhance trial selection by introducing a dynamic crossover based on success history in HybridMetaheuristic.", "configspace": "", "generation": 96, "fitness": 0.8936814192040167, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.894 with standard deviation 0.022. And the mean value of best solutions found was 0.114 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "52dc0e42-744a-46e5-8d4f-85031699e231", "metadata": {"aucs": [0.8694232847810819, 0.8879924754694154, 0.9236284973615527], "final_y": [0.11314466313779081, 0.11765536739219107, 0.11206342425806681]}, "mutation_prompt": null}
{"id": "94c1c646-05c0-4f7b-a3e0-1bfc49f64cf9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.clip(np.random.normal(0.6, 0.2) * self.F, 0.5, 1.0)  # Refined adaptive mutation factor\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.15:  # Increased probability\n                self.population[i] = trial\n            \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 30})  # Increased maxiter\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Refine the mutation factor adaptation and enhance the global-local transition for improved optimization.", "configspace": "", "generation": 97, "fitness": 0.8723025725149715, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.046. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "52dc0e42-744a-46e5-8d4f-85031699e231", "metadata": {"aucs": [0.8097355124786694, 0.8873595546568718, 0.9198126504093733], "final_y": [0.13954846768310936, 0.11765536739219107, 0.11213691631079581]}, "mutation_prompt": null}
{"id": "844e8603-43d6-46f5-afa4-707d96828b0b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Initial crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            cross_points = np.random.rand(self.dim) < (self.CR * ((self.best_score + 1) / (i + 1)))  # Adaptive crossover\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n            \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 20})\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Refine solution selection using adaptive crossover probability based on fitness rank for improved convergence.", "configspace": "", "generation": 98, "fitness": 0.8793404337076436, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.879 with standard deviation 0.038. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "52dc0e42-744a-46e5-8d4f-85031699e231", "metadata": {"aucs": [0.8289188824979608, 0.8873208136756524, 0.9217816049493175], "final_y": [0.12578229575219513, 0.11765536739219107, 0.11206342425806681]}, "mutation_prompt": null}
{"id": "8dc797ac-8cae-4f42-b211-027bd7bae868", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lower_bounds, upper_bounds, (self.population_size, self.dim))\n        \n    def differential_evolution_step(self, bounds, func):\n        lower_bounds, upper_bounds = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_F = np.random.normal(0.5, 0.3) * self.F  # Adaptive mutation factor with normal distribution\n            mutant = np.clip(a + adaptive_F * (b - c), lower_bounds, upper_bounds)\n            # Line changed: Introduce dynamic crossover probability based on current evaluations vs. budget\n            cross_points = np.random.rand(self.dim) < (self.CR * (self.best_score + 1) * (1 - self.evals / self.budget))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            trial_score = func(trial)\n            self.evals += 1\n            if trial_score > self.best_score:\n                self.best_score = trial_score\n                self.best_solution = trial\n            # Modify the replacement strategy using a probability-based check\n            if trial_score > func(self.population[i]) or np.random.rand() < 0.1:\n                self.population[i] = trial\n            \n    def local_search(self, trial, bounds, func):\n        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 20})\n        self.evals += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        while self.evals < self.budget:\n            self.differential_evolution_step(bounds, func)\n            if self.evals < self.budget:\n                for i, individual in enumerate(self.population):\n                    x_local, score_local = self.local_search(individual, bounds, func)\n                    if score_local > self.best_score:\n                        self.best_score = score_local\n                        self.best_solution = x_local\n                    self.population[i] = x_local\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover probability to improve solution diversity and exploration in HybridMetaheuristic.", "configspace": "", "generation": 99, "fitness": 0.8936814192040167, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.894 with standard deviation 0.022. And the mean value of best solutions found was 0.114 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "52dc0e42-744a-46e5-8d4f-85031699e231", "metadata": {"aucs": [0.8694232847810819, 0.8879924754694154, 0.9236284973615527], "final_y": [0.11314466313779081, 0.11765536739219107, 0.11206342425806681]}, "mutation_prompt": null}
