{"id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic combining Differential Evolution and Particle Swarm Optimization for efficient exploration and exploitation in black-box optimization.", "configspace": "", "generation": 0, "fitness": 0.9520530889717479, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.952 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.9411881792620419, 0.9504090407145893, 0.9645620469386126], "final_y": [0.16485783647874308, 0.16485781039159297, 0.16485734488682913]}, "mutation_prompt": null}
{"id": "050129fe-32fd-4a3f-a6fb-dacd2229b39c", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.9   # increased inertia weight for better exploration\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # initial crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        dynamic_cr = self.cr  # adaptive crossover rate\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Adjust cr based on fitness improvement\n                    if candidate_fitness < global_best_score:\n                        dynamic_cr = min(1.0, dynamic_cr + 0.1)\n                    else:\n                        dynamic_cr = max(0.1, dynamic_cr - 0.1)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < dynamic_cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic by adjusting inertia weight and incorporating adaptive crossover rate for improved convergence.", "configspace": "", "generation": 1, "fitness": 0.9284334853105071, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.928 with standard deviation 0.011. And the mean value of best solutions found was 0.170 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9265591433910307, 0.9165740288063627, 0.9421672837341278], "final_y": [0.16820531242832748, 0.17389409191266936, 0.16881562772244962]}, "mutation_prompt": null}
{"id": "285158c0-06a7-4eea-ba94-7e8438010d87", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.9   # adaptive inertia weight, updated line\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            self.w = 0.5 + 0.4 * (1 - evaluations / self.budget)  # adaptive inertia weight formula, added line\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic combining adaptive inertia weight and elite selection for improved exploration and exploitation.", "configspace": "", "generation": 2, "fitness": 0.9375035554049216, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.938 with standard deviation 0.018. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9219247797570894, 0.9284601130152166, 0.9621257734424588], "final_y": [0.1668357542879123, 0.16623677306596762, 0.1651352325807337]}, "mutation_prompt": null}
{"id": "5bd95910-619c-47d0-be8a-250fbee635d0", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w_max = 0.9  # maximum inertia weight\n        self.w_min = 0.4  # minimum inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            self.w = self.w_max - ((self.w_max - self.w_min) * evaluations / self.budget)  # Dynamic inertia weight\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "An enhanced Hybrid Metaheuristic by introducing a dynamic inertia weight for improved convergence.", "configspace": "", "generation": 3, "fitness": 0.9411630127664385, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.941 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9260609518726135, 0.9359612583564482, 0.961466828070254], "final_y": [0.1651387784839029, 0.16501953615509557, 0.16523829451325134]}, "mutation_prompt": null}
{"id": "9e798726-4eb6-4d66-b464-4d55b4967311", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w_max = 0.9  # maximum inertia weight\n        self.w_min = 0.4  # minimum inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            w = self.w_max - (self.w_max - self.w_min) * (evaluations / self.budget)\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic integrating adaptive inertia weight for improved convergence in black-box optimization.", "configspace": "", "generation": 4, "fitness": 0.9411630127664385, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.941 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9260609518726135, 0.9359612583564482, 0.961466828070254], "final_y": [0.1651387784839029, 0.16501953615509557, 0.16523829451325134]}, "mutation_prompt": null}
{"id": "349b5f66-eaa3-4c9f-92a8-63bebb28e7e9", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # initial inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Dynamically adjust inertia weight\n            self.w = 0.9 - 0.4 * (evaluations / self.budget)\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhancing HybridMetaheuristic by dynamically adjusting the inertia weight `w` for improved convergence.", "configspace": "", "generation": 5, "fitness": 0.9375035554049217, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.938 with standard deviation 0.018. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9219247797570894, 0.9284601130152169, 0.9621257734424588], "final_y": [0.1668357542879123, 0.16623677306596762, 0.1651352325807337]}, "mutation_prompt": null}
{"id": "37653b4c-dac3-43c4-b80d-e60fad1e94b7", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w_max = 0.9  # maximum inertia weight\n        self.w_min = 0.4  # minimum inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            self.w = self.w_max - (self.w_max - self.w_min) * (evaluations / self.budget)  # adaptive inertia weight\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Improved HybridMetaheuristic with adaptive inertia weight for dynamic balance between exploration and exploitation.", "configspace": "", "generation": 6, "fitness": 0.9411630127664385, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.941 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9260609518726135, 0.9359612583564482, 0.961466828070254], "final_y": [0.1651387784839029, 0.16501953615509557, 0.16523829451325134]}, "mutation_prompt": null}
{"id": "36c2c78f-e150-4ab0-a297-fd4c4586842a", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # adaptive cognitive coefficient\n        self.c2 = 1.5  # adaptive social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                # Adaptively update cognitive and social coefficients\n                self.c1 = 1.5 + 0.5 * (global_best_score / (np.min(personal_best_scores)+1e-9))\n                self.c2 = 1.5 + 0.5 * ((np.max(personal_best_scores)-global_best_score) / (np.max(personal_best_scores)+1e-9))\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic using adaptive control of cognitive and social coefficients for dynamic optimization balance.", "configspace": "", "generation": 7, "fitness": 0.9159009111800488, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.916 with standard deviation 0.031. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.8893452295250444, 0.8994084018669698, 0.9589491021481323], "final_y": [0.1653457033884318, 0.18189013596344406, 0.1649907260720782]}, "mutation_prompt": null}
{"id": "afe0a300-5519-4c62-ae74-762fd11bb336", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.9   # initial inertia weight\n        self.w_min = 0.4  # minimum inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Update inertia weight dynamically based on evaluations\n                self.w = self.w_min + (0.5 * (1 - (evaluations / self.budget)))\n                \n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive inertia weight for improved convergence.", "configspace": "", "generation": 8, "fitness": 0.9421470357923626, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.942 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9285351213819697, 0.9368194003663599, 0.9610865856287578], "final_y": [0.16537665457898032, 0.16517029326040122, 0.16496355629101067]}, "mutation_prompt": null}
{"id": "21a016d7-d0af-4759-8fd9-19ddd7133f74", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w_init = 0.9  # initial inertia weight\n        self.w_final = 0.4  # final inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            w = self.w_init - (self.w_init - self.w_final) * evaluations / self.budget\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced HybridMetaheuristic with adaptive inertia weight for improved convergence.", "configspace": "", "generation": 9, "fitness": 0.9411630127664385, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.941 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9260609518726135, 0.9359612583564482, 0.961466828070254], "final_y": [0.1651387784839029, 0.16501953615509557, 0.16523829451325134]}, "mutation_prompt": null}
{"id": "bf152234-1228-4176-978c-68591744e934", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Adaptive inertia weight\n            self.w = 0.9 - (0.5 * (evaluations / self.budget))\n            \n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic using adaptive parameter tuning for improved convergence.", "configspace": "", "generation": 10, "fitness": 0.9411630127664385, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.941 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9260609518726135, 0.9359612583564482, 0.961466828070254], "final_y": [0.1651387784839029, 0.16501953615509557, 0.16523829451325134]}, "mutation_prompt": null}
{"id": "1e26cb7a-c51c-4226-9dd2-1670368bd5c0", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.9   # initial inertia weight\n        self.w_min = 0.4 # minimum inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            self.w = max(self.w_min, self.w - (0.5 / self.budget))  # adaptively reduce w\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic combining Differential Evolution and Particle Swarm Optimization with adaptive inertia weight for improved convergence.", "configspace": "", "generation": 11, "fitness": 0.9250734226845253, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.925 with standard deviation 0.011. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9248468616796197, 0.9117316017722014, 0.9386418046017548], "final_y": [0.17187106771781546, 0.175127706271495, 0.17068857380519054]}, "mutation_prompt": null}
{"id": "f0dc3307-0cb5-480d-ba44-c61d0e54cfda", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Adapt inertia weight and crossover probability\n            self.w = 0.4 + (0.5 * (self.budget - evaluations) / self.budget)\n            self.cr = 0.6 + (0.3 * evaluations / self.budget)\n            \n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic that adapts inertia weight and crossover probability for improved convergence in black-box optimization.", "configspace": "", "generation": 12, "fitness": 0.9251192964121192, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.925 with standard deviation 0.018. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9127476470785979, 0.9122022103251684, 0.9504080318325913], "final_y": [0.16501320929361318, 0.16509432743308772, 0.1651763921652084]}, "mutation_prompt": null}
{"id": "36d680d1-2a15-4795-9860-4a8ac2ac7c98", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 2.0  # adaptive cognitive coefficient\n        self.c2 = 2.0  # adaptive social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive parameters for improved exploration and exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.9192034649919375, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.919 with standard deviation 0.032. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9006352436741825, 0.8927061397251036, 0.9642690115765268], "final_y": [0.1655349827307796, 0.18197496780459088, 0.16493330266965944]}, "mutation_prompt": null}
{"id": "2f0afa20-f2f5-4102-85ab-15f85613dcb2", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.9   # initial inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Adaptive inertia weight\n                self.w = 0.9 - 0.5 * (evaluations / self.budget)\n                \n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic using adaptive inertia weight for improved optimization in black-box problems.", "configspace": "", "generation": 14, "fitness": 0.9421470357923628, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.942 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9285351213819706, 0.9368194003663599, 0.9610865856287578], "final_y": [0.16537665457898032, 0.16517029326040122, 0.16496355629101067]}, "mutation_prompt": null}
{"id": "7b4d6c4a-2a5a-471a-a0b0-c8394a25be88", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.9   # initial inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.4 + 0.5 * ((self.budget - evaluations) / self.budget)  # adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic with adaptive inertia weight to improve convergence speed and solution quality.", "configspace": "", "generation": 15, "fitness": 0.9421470357923623, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.942 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9285351213819697, 0.9368194003663599, 0.9610865856287573], "final_y": [0.16537665457898032, 0.16517029326040145, 0.16496355629101067]}, "mutation_prompt": null}
{"id": "937daafe-0ddc-4104-87b2-c3ad6bbdd286", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Adaptive inertia weight\n            self.w = 0.9 - (0.5 * evaluations / self.budget)\n            \n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced HybridMetaheuristic with adaptive inertia weight for better convergence.", "configspace": "", "generation": 16, "fitness": 0.9411630127664385, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.941 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9260609518726135, 0.9359612583564482, 0.961466828070254], "final_y": [0.1651387784839029, 0.16501953615509557, 0.16523829451325134]}, "mutation_prompt": null}
{"id": "e78bf62f-a648-4469-92a6-20d015782dba", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    self.cr = 0.9 * (self.budget - evaluations) / self.budget + 0.1 # Adaptive crossover probability\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Efficient hybrid optimization by adaptive inertia and crossover probability adjustment.", "configspace": "", "generation": 17, "fitness": 0.9544904543588193, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.954 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a304f7f-5eef-412b-9941-2d15f50e295b", "metadata": {"aucs": [0.9468443881126202, 0.950891380123655, 0.9657355948401827], "final_y": [0.1648558126371069, 0.16485577518565597, 0.16485582624604644]}, "mutation_prompt": null}
{"id": "61447edc-aaf5-4bcb-bab0-1fa952edf5a3", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    self.cr = 0.9 * (self.budget - evaluations) / self.budget + 0.1 # Adaptive crossover probability\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 2.0 - (1.5 * evaluations / self.budget)  # Dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid optimization with dynamic adjustment of differential weight and cognitive coefficient.", "configspace": "", "generation": 18, "fitness": 0.963746380976354, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e78bf62f-a648-4469-92a6-20d015782dba", "metadata": {"aucs": [0.9632332050013689, 0.9596247570275565, 0.9683811809001364], "final_y": [0.16485578357573638, 0.16485577701684284, 0.16485577608856117]}, "mutation_prompt": null}
{"id": "be6b40d5-9840-4912-a693-24c07a549f5b", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive crossover bounds and adjusted dynamic cognitive coefficient.", "configspace": "", "generation": 19, "fitness": 0.9644350487579666, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "61447edc-aaf5-4bcb-bab0-1fa952edf5a3", "metadata": {"aucs": [0.964195796537335, 0.9603992587782624, 0.9687100909583024], "final_y": [0.16485577696539355, 0.1648559442311318, 0.16485580111879183]}, "mutation_prompt": null}
{"id": "cb353626-5e66-40d6-86ae-31b02590ff3d", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.2 * evaluations / self.budget)  # Adjusted dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introducing elite strategy update for improved convergence and dynamic mutation scaling.", "configspace": "", "generation": 20, "fitness": 0.9499965518280553, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.950 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9411890910097944, 0.9429506949861426, 0.965849869488229], "final_y": [0.16485578196365214, 0.1648557882555084, 0.16485609743588903]}, "mutation_prompt": null}
{"id": "600578df-8702-4b83-b9c6-471cd2231c3b", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n                    self.c2 = 1.5 + (0.5 * evaluations / self.budget)  # Dynamic adjustment of social coefficient\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduced dynamic adjustment of social and cognitive coefficients for improved convergence.", "configspace": "", "generation": 21, "fitness": 0.9619714556558209, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.962 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9616766833959474, 0.9584249551762188, 0.9658127283952967], "final_y": [0.16485579881402013, 0.16485578609209983, 0.16485578042633908]}, "mutation_prompt": null}
{"id": "25b213ee-99e2-4621-b129-d545c820f5cc", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    # Refined mutation strategy\n                    mutant_vector = np.clip(a + self.f * (b - c + global_best_position - a), lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive crossover bounds, adjusted dynamic cognitive coefficient, and refined mutation strategy.", "configspace": "", "generation": 22, "fitness": 0.9493185390823448, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.949 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9614762028104389, 0.9146985832014415, 0.971780831235154], "final_y": [0.16485577268425, 0.18187809684487544, 0.16485577382527483]}, "mutation_prompt": null}
{"id": "a84e5f92-f657-4404-8511-85598b74e540", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.5 * evaluations / self.budget)  # Enhanced dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Stochastic crossover probability\n                    self.cr = np.random.uniform(0.1, 0.9)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive mutation strategy and stochastic crossover adjustment.", "configspace": "", "generation": 23, "fitness": 0.9502529193732491, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.950 with standard deviation 0.022. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9197736848396226, 0.9602680515522996, 0.9707170217278249], "final_y": [0.18187810392826798, 0.1648557794019374, 0.1648557809443577]}, "mutation_prompt": null}
{"id": "11e7bbbb-bd0e-468e-a893-ede505f3e68c", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        # Adjusted population size with dynamic scaling\n        self.population_size = max(5 * dim, 10 + int(dim * np.log(dim)))\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                # Introduced elite learning from global best\n                elite_influence = 0.05 * (global_best_position - candidate_position)\n                candidate_position += elite_influence\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Adaptive hybrid metaheuristic incorporating elite learning strategy and dynamic population size.", "configspace": "", "generation": 24, "fitness": 0.9593422682619089, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9750424882299379, 0.9236409252806259, 0.9793433912751631], "final_y": [0.16485577217209668, 0.1818780967683491, 0.16485577190824363]}, "mutation_prompt": null}
{"id": "12cd2db8-7c69-47db-a125-f594b76ede03", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best with a slight exploration factor\n                if candidate_fitness < personal_best_scores[i] or np.random.rand() < 0.05:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive exploitation strategy by fine-tuning personal best update mechanism.", "configspace": "", "generation": 25, "fitness": 0.9473827288003936, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.024. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9666822930082597, 0.9140551227098426, 0.9614107706830788], "final_y": [0.16485579155237728, 0.18187809752335782, 0.1648557744209571]}, "mutation_prompt": null}
{"id": "bb8d3b3e-fa35-4a35-bfc3-7d84cf2eb834", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    # Blend factor for mutation\n                    blend_factor = 0.7 + 0.3 * np.random.rand() \n                    mutant_vector = a + blend_factor * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced mutation strategy by introducing a blend factor for exploration-exploitation balance.", "configspace": "", "generation": 26, "fitness": 0.9590256537561631, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.951443179295328, 0.9529583462108642, 0.9726754357622974], "final_y": [0.16485599756468483, 0.16485577617699976, 0.1648557848385409]}, "mutation_prompt": null}
{"id": "d6e8e3d9-5ee9-46f2-a57e-13fb4a5802bf", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2, r3 = np.random.rand(), np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i] + r3 * (population[i] - global_best_position)\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * (1 - evaluations / self.budget))\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    self.cr = 0.9 * (self.budget - evaluations) / self.budget + 0.1 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.7 - (1.2 * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Improved hybrid algorithm using adaptive velocity and self-adaptive mutation strategies for enhanced global exploration.", "configspace": "", "generation": 27, "fitness": 0.9275701011593753, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.928 with standard deviation 0.036. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.8774510020425744, 0.9603915798059751, 0.9448677216295767], "final_y": [0.1818877231248892, 0.1648584757175835, 0.16487440780262208]}, "mutation_prompt": null}
{"id": "9eba80a5-45fe-43a9-bf1a-10544425dd2f", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 12 * dim  # Increased population size for better exploration\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        memory = np.copy(population)  # Memory for promising solutions\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    memory_index = np.random.randint(len(memory))\n                    memory_candidate = memory[memory_index]  # Use memory to influence the trial vector\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, memory_candidate)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n                    memory[i] = candidate_position  # Update memory with better solutions\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive dynamic memory and diversified population initialization for improved global exploration.", "configspace": "", "generation": 28, "fitness": 0.9524177972635516, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.952 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9174974737482895, 0.9732131370336952, 0.96654278100867], "final_y": [0.1818780980945598, 0.1648558217829813, 0.16485582727844628]}, "mutation_prompt": null}
{"id": "4e1dc3cd-4ed3-4a11-a4b5-c55eb677b6f2", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.5 * (self.budget - evaluations) / self.budget  # Adjusted adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                \n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.2 * evaluations / self.budget)  # Modified dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    self.cr = 0.85 * (self.budget - evaluations) / self.budget + 0.15  # Modified adaptive crossover probability\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.9 - (1.2 * evaluations / self.budget)  # Further adjusted dynamic cognitive coefficient\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive multi-velocity update and modified dynamic factors for improved convergence.", "configspace": "", "generation": 29, "fitness": 0.9494748541638396, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.949 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9413388840009941, 0.9439828263371111, 0.9631028521534135], "final_y": [0.16485582854510983, 0.16485581700331298, 0.16485592430489682]}, "mutation_prompt": null}
{"id": "3d968be5-99ea-4384-99e7-0ce0f284145d", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * np.sin(np.pi * evaluations / self.budget))  # Improved dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive neighborhood influence and improved mutation strategy.", "configspace": "", "generation": 30, "fitness": 0.956971876480336, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.957 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9429806314384531, 0.9599265107660339, 0.9680084872365209], "final_y": [0.16485582608286165, 0.1648557764801325, 0.16485578566738046]}, "mutation_prompt": null}
{"id": "6b2b1be2-c600-46f6-bf4b-62ab673baeca", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 12 * dim  # Increased population size for better exploration\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.2 * evaluations / self.budget)  # Enhanced dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Refined hybrid metaheuristic with enhanced local exploitation through dynamic population size and adaptive mutation strategy.", "configspace": "", "generation": 31, "fitness": 0.9527410900250542, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.953 with standard deviation 0.020. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9245315393106619, 0.969199999303618, 0.9644917314608827], "final_y": [0.18187810552714656, 0.1648558024772787, 0.16485583304222484]}, "mutation_prompt": null}
{"id": "ab6f4b58-1d2f-4183-99bd-8b4492543126", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.5 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Slight enhancement of the hybrid metaheuristic by fine-tuning adaptive parameters for improved convergence.", "configspace": "", "generation": 32, "fitness": 0.9520096981153213, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.952 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9431500734998151, 0.9481855723244467, 0.9646934485217019], "final_y": [0.16485583115988223, 0.16485578763844733, 0.16485578125933342]}, "mutation_prompt": null}
{"id": "9a510088-51a1-412b-9604-05eb2b6ea43d", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.7 * (self.budget - evaluations) / self.budget + 0.3 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.9 - (1.4 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with optimized dynamic parameters for improved convergence.", "configspace": "", "generation": 33, "fitness": 0.9639121044194133, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.964107648620301, 0.9587092296907846, 0.9689194349471545], "final_y": [0.1648557731446828, 0.16485578452504934, 0.16485577698667642]}, "mutation_prompt": null}
{"id": "43b5b886-5b4c-4c15-8585-96c05bc404f3", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                velocities[i] *= np.random.uniform(0.9, 1.1)  # Dynamic velocity scaling\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.4 * evaluations / self.budget)  # Adjusted differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with dynamic velocity scaling and adaptive differential weight adjustment.", "configspace": "", "generation": 34, "fitness": 0.9284980772225268, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.928 with standard deviation 0.026. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9078587158052851, 0.912125104815829, 0.965510411046466], "final_y": [0.18187810171864394, 0.18187809699731428, 0.16485577893863246]}, "mutation_prompt": null}
{"id": "007dabc7-2437-441d-b57f-f5b70ff3d577", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Enhanced mutation and crossover with neighborhood exploration\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 5, replace=False)\n                    a, b, c, d, e = population[indices]\n                    self.f = 0.6 + (0.4 * np.sin(evaluations / self.budget))  # Enhanced adaptive differential weight\n                    mutant_vector = a + self.f * (b - c) + 0.3 * (d - e)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.9 * (1 - (evaluations / self.budget)**2) + 0.1 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Improved hybrid metaheuristic with enhanced adaptive mutation strategy and selective neighborhood exploration.", "configspace": "", "generation": 35, "fitness": 0.9576158718714948, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.958 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9481615779502645, 0.9548759787341593, 0.9698100589300608], "final_y": [0.16485577828462505, 0.16485579419827512, 0.16485577483010538]}, "mutation_prompt": null}
{"id": "71aedaed-cc2e-4b07-9c8f-2bf85a38e91f", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.2 * evaluations / self.budget)  # Adjusted dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Improved HybridMetaheuristic with adaptive scaling factor for enhanced exploration and exploitation balance.", "configspace": "", "generation": 36, "fitness": 0.9499965518280553, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.950 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9411890910097944, 0.9429506949861426, 0.965849869488229], "final_y": [0.16485578196365214, 0.1648557882555084, 0.16485609743588903]}, "mutation_prompt": null}
{"id": "e3a54b5a-7857-4c6a-a774-2ef5f0b0e3c3", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget  # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 + 0.2 * evaluations / self.budget) * r2 * (global_best_position - population[i]))  # Dynamic social coefficient\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_vector = np.clip(trial_vector, lb, ub)  # Enhanced trial vector selection\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Improved hybrid metaheuristic with dynamic social coefficient and enhanced trial vector selection.", "configspace": "", "generation": 37, "fitness": 0.9637107724881221, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9627724076086827, 0.9611355077275415, 0.9672244021281421], "final_y": [0.16485577731247514, 0.16485577329555268, 0.1648558003802879]}, "mutation_prompt": null}
{"id": "fa2e4b44-a8b1-4f4d-8907-43137e1c8e09", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.5 * (self.budget - evaluations) / self.budget # Enhanced adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.7 + (0.1 * evaluations / self.budget)  # Enhanced dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Refined hybrid metaheuristic with enhanced mutation strategy and fine-tuned inertia weight adaptation.", "configspace": "", "generation": 38, "fitness": 0.9508048832699685, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.951 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9403719316812217, 0.9454004881026196, 0.9666422300260642], "final_y": [0.16485590449495957, 0.16485580548266088, 0.16485580724523075]}, "mutation_prompt": null}
{"id": "3af2f0ec-7ebf-484d-84a6-3f45be846bb7", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.3 + 0.2 * (self.budget - evaluations) / self.budget # Enhanced adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Optimized adaptive crossover probability\n                    self.cr = 0.9 * (self.budget - evaluations) / self.budget + 0.1 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Improved hybrid metaheuristic with an enhanced adaptive inertia weight and optimized crossover strategy for better convergence.", "configspace": "", "generation": 39, "fitness": 0.9640882455924688, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9644886344070089, 0.9609841489300578, 0.9667919534403391], "final_y": [0.1648558461169627, 0.16485580444005798, 0.1648558976145248]}, "mutation_prompt": null}
{"id": "160d17fd-10f0-4ff2-a5c0-7bc4dff1255f", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive parameters, incorporating dynamic inertia weight and refined mutation strategy.", "configspace": "", "generation": 40, "fitness": 0.965370975003455, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be6b40d5-9840-4912-a693-24c07a549f5b", "metadata": {"aucs": [0.9662796855654678, 0.9611428528466022, 0.9686903865982952], "final_y": [0.16485577570693244, 0.1648557741094936, 0.1648557737057783]}, "mutation_prompt": null}
{"id": "2f9525dc-00bc-42e5-9524-8aadde5609e5", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.8 * (self.budget - evaluations) / self.budget + 0.2 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive learning coefficients and strategic elitism.", "configspace": "", "generation": 41, "fitness": 0.965370975003455, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "160d17fd-10f0-4ff2-a5c0-7bc4dff1255f", "metadata": {"aucs": [0.9662796855654678, 0.9611428528466022, 0.9686903865982952], "final_y": [0.16485577570693244, 0.1648557741094936, 0.1648557737057783]}, "mutation_prompt": null}
{"id": "ff1f6166-03d3-477f-a158-21999a66c300", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * (self.budget - evaluations) / self.budget # Adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.9 * (self.budget - evaluations) / self.budget + 0.1 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.8 - (1.3 * evaluations / self.budget)  # Adjusted dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Improved hybrid metaheuristic with enhanced dynamic parameters and refined crossover strategy.", "configspace": "", "generation": 42, "fitness": 0.9657947155637431, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "160d17fd-10f0-4ff2-a5c0-7bc4dff1255f", "metadata": {"aucs": [0.9667850919947606, 0.9616001045941301, 0.9689989501023383], "final_y": [0.16485577391391715, 0.16485577550244757, 0.16485577681088637]}, "mutation_prompt": null}
{"id": "49e221e9-2a5a-44d2-ad16-f7a4ae8abb2a", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.9 * (self.budget - evaluations) / self.budget + 0.1 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Refined inertia and cognitive coefficients with enhanced preservation of diversity in solutions.", "configspace": "", "generation": 43, "fitness": 0.9676615151318284, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ff1f6166-03d3-477f-a158-21999a66c300", "metadata": {"aucs": [0.969369189115102, 0.963610326294721, 0.9700050299856624], "final_y": [0.16485607970891836, 0.16485582417541655, 0.16485614808808924]}, "mutation_prompt": null}
{"id": "2cee681b-4ffc-427e-8968-b77ffdcd4274", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.9 * (self.budget - evaluations) / self.budget + 0.1 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                # Adaptive velocity scaling\n                velocities[i] *= 0.9 + 0.1 * (personal_best_scores[i] / (global_best_score + 1e-10)) \n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce adaptive velocity scaling and enhanced diversity preservation strategies.", "configspace": "", "generation": 44, "fitness": 0.9670791603966578, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.967 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "49e221e9-2a5a-44d2-ad16-f7a4ae8abb2a", "metadata": {"aucs": [0.9678054619528974, 0.9639906778098648, 0.969441341427211], "final_y": [0.1648558075805857, 0.16485584959677868, 0.1648560765925161]}, "mutation_prompt": null}
{"id": "09f85456-e84e-4faf-8453-5e69a8fe9f3e", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.9 * (self.budget - evaluations) / self.budget + 0.1 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n            self.population_size = int(10 * dim * (1 - evaluations / self.budget)) + 1 # Adaptive population size\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduced adaptive population size for enhanced exploration and exploitation balance.", "configspace": "", "generation": 45, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'dim' is not defined\").", "error": "NameError(\"name 'dim' is not defined\")", "parent_id": "49e221e9-2a5a-44d2-ad16-f7a4ae8abb2a", "metadata": {}, "mutation_prompt": null}
{"id": "68625831-42db-43b7-939f-4c2de6f290dd", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]) +\n                                 0.01 * np.random.randn(self.dim))  # Perturbation added here\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.9 * (self.budget - evaluations) / self.budget + 0.1 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduced a small perturbation to velocities, enhancing exploration dynamics in the search space.", "configspace": "", "generation": 46, "fitness": 0.9439727682398901, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.944 with standard deviation 0.026. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "49e221e9-2a5a-44d2-ad16-f7a4ae8abb2a", "metadata": {"aucs": [0.9573539334905764, 0.908043797713649, 0.9665205735154448], "final_y": [0.16485583901551681, 0.18187815258933282, 0.16485627419578275]}, "mutation_prompt": null}
{"id": "1f2ef23c-b0ea-4522-9b31-8f1ee586513a", "solution": "# Description: Optimized adaptive inertia and cognitive coefficients to better balance exploration and exploitation.\n# Code: \nimport numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.4 + 0.1 * np.cos(np.pi * evaluations / self.budget) # Optimized adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.9 * (self.budget - evaluations) / self.budget + 0.1 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.7 + 0.1 * np.cos(np.pi * evaluations / self.budget)  # Optimized dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Optimized adaptive inertia and cognitive coefficients to better balance exploration and exploitation.", "configspace": "", "generation": 47, "fitness": 0.9615217856254356, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.962 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "49e221e9-2a5a-44d2-ad16-f7a4ae8abb2a", "metadata": {"aucs": [0.9586062557051036, 0.9586566316279803, 0.9673024695432229], "final_y": [0.16485671404105418, 0.1648565632648732, 0.1648558469442506]}, "mutation_prompt": null}
{"id": "74453ce7-145b-46ea-894e-119c4a006fd6", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.3 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.2 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Modified adaptive crossover probability\n                    self.cr = 0.9 * (self.budget - evaluations) / self.budget + 0.1 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Improved adaptive dynamic parameters to enhance convergence rate and solution quality.", "configspace": "", "generation": 48, "fitness": 0.9546473792185708, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.955 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "49e221e9-2a5a-44d2-ad16-f7a4ae8abb2a", "metadata": {"aucs": [0.9427093469614074, 0.9525713372074833, 0.9686614534868216], "final_y": [0.16485817585264295, 0.16485726358968444, 0.16485776291473064]}, "mutation_prompt": null}
{"id": "7400447e-129f-4e06-b7a6-c47edfe693da", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.cos(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    self.cr = 0.9 * (self.budget - evaluations) / self.budget + 0.1 \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.sin(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhanced exploration-exploitation balance with adaptive mutation strategy and refined inertial adaptation.", "configspace": "", "generation": 49, "fitness": 0.9648082359126744, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "49e221e9-2a5a-44d2-ad16-f7a4ae8abb2a", "metadata": {"aucs": [0.9673044657420109, 0.9602357797044989, 0.9668844622915133], "final_y": [0.16485577442142085, 0.16485577272903407, 0.16485578066377482]}, "mutation_prompt": null}
{"id": "ddbba17c-9bfe-4337-a2c6-7c10c102ec54", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Self-adaptive crossover probability based on fitness improvement\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Integrate a self-adaptive parameter control mechanism to dynamically adjust the crossover probability based on fitness trends.", "configspace": "", "generation": 50, "fitness": 0.9687446936295004, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "49e221e9-2a5a-44d2-ad16-f7a4ae8abb2a", "metadata": {"aucs": [0.9684646600473596, 0.9662770588726486, 0.9714923619684929], "final_y": [0.1648559685735531, 0.1648558746036195, 0.1648560621824855]}, "mutation_prompt": null}
{"id": "0074b5d7-5f2c-42f2-8cb5-717980cac3c6", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                    # Self-adaptive crossover probability based on fitness improvement\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n                        self.c2 = 1.5 + 0.5 * np.sin(np.pi * evaluations / self.budget)  # Dynamic social coefficient adjustment\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce a dynamic adjustment to the social coefficient and enhance the mutation strategy for refined exploration.", "configspace": "", "generation": 51, "fitness": 0.9662243237958578, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ddbba17c-9bfe-4337-a2c6-7c10c102ec54", "metadata": {"aucs": [0.9665295479537661, 0.9606653717142752, 0.9714780517195318], "final_y": [0.16485647793886016, 0.1648572542649278, 0.16485800477973667]}, "mutation_prompt": null}
{"id": "19fc32ad-ef1a-426c-9710-3100f06795a2", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.cos(np.pi * evaluations / self.budget) # Adaptive inertia weight with cosine\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * np.sin(evaluations / self.budget))  # Dynamic differential weight with sine\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Self-adaptive crossover probability based on fitness improvement\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhance velocity update dynamics and introduce adaptive mutation scaling for refined exploration-exploitation balance.", "configspace": "", "generation": 52, "fitness": 0.9623706619556661, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.962 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ddbba17c-9bfe-4337-a2c6-7c10c102ec54", "metadata": {"aucs": [0.9615682120688134, 0.9557211720653802, 0.9698226017328048], "final_y": [0.1648558334761605, 0.16485577961049047, 0.16485582556821754]}, "mutation_prompt": null}
{"id": "3b3be7f4-f33c-44b5-a287-33035fcb193d", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    diversity = np.std(population, axis=0).mean()\n                    self.f = 0.5 + (0.3 * diversity)  # Adaptive differential weight based on diversity\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Self-adaptive crossover probability based on fitness improvement\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce an adaptive mutation scaling factor based on population diversity to enhance exploration capabilities.", "configspace": "", "generation": 53, "fitness": 0.9596323882836485, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.960 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ddbba17c-9bfe-4337-a2c6-7c10c102ec54", "metadata": {"aucs": [0.9551621468599084, 0.9592705749669134, 0.9644644430241236], "final_y": [0.16485655031607738, 0.16485622939553357, 0.16485647386665558]}, "mutation_prompt": null}
{"id": "756d40be-dedb-4d10-89bf-4652675032cf", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.4 + 0.1 * np.sin(np.pi * evaluations / self.budget) # More responsive adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.2 * evaluations / self.budget)  # Refined dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Self-adaptive crossover probability based on fitness improvement\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhance adaptive learning by refining the inertia weight and mutation strategy for superior convergence.", "configspace": "", "generation": 54, "fitness": 0.959778435017673, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.960 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ddbba17c-9bfe-4337-a2c6-7c10c102ec54", "metadata": {"aucs": [0.9539371424850714, 0.9551296718585299, 0.9702684907094177], "final_y": [0.16485844529607652, 0.16485693364521314, 0.16486086537716627]}, "mutation_prompt": null}
{"id": "e7bcb32f-cd59-43cf-8b9c-2ceac82bef6f", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Self-adaptive crossover probability based on fitness improvement\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n                        self.c2 = 1.4 + 0.3 * np.sin(2 * np.pi * evaluations / self.budget)  # New dynamic social coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Refine the momentum strategy by incorporating adaptive adjustment of the social coefficient for improved convergence.", "configspace": "", "generation": 55, "fitness": 0.9681509488845449, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ddbba17c-9bfe-4337-a2c6-7c10c102ec54", "metadata": {"aucs": [0.9691380363986394, 0.9619072835817603, 0.973407526673235], "final_y": [0.16485641994702227, 0.16485765313863132, 0.16485595319774937]}, "mutation_prompt": null}
{"id": "2c9769eb-149f-460b-8935-3843eec6feae", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.4 + 0.1 * np.sin(2 * np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Self-adaptive crossover probability based on fitness improvement\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.7 + 0.1 * np.cos(2 * np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhance dynamic parameter adjustment by refining the inertia weight and cognitive coefficient over evaluations.", "configspace": "", "generation": 56, "fitness": 0.9620279985362382, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.962 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ddbba17c-9bfe-4337-a2c6-7c10c102ec54", "metadata": {"aucs": [0.9639225382728752, 0.9520003686396348, 0.9701610886962049], "final_y": [0.16485664658215338, 0.16485633942389277, 0.16485587061679996]}, "mutation_prompt": null}
{"id": "4c2400ad-e848-42b8-8199-1d55e2bf0b12", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget)  # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Self-adaptive crossover probability based on fitness improvement\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.015)  # Adjusted crossover probability increment\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02)   # Adjusted crossover probability decrement\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce an adaptive learning rate for velocity update and refine the crossover probability adaptation to further enhance search performance.", "configspace": "", "generation": 57, "fitness": 0.9671817322297563, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.967 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ddbba17c-9bfe-4337-a2c6-7c10c102ec54", "metadata": {"aucs": [0.966601768948321, 0.9607734442274852, 0.974169983513463], "final_y": [0.16485584294956013, 0.16485600048916915, 0.16485597856888423]}, "mutation_prompt": null}
{"id": "57694458-63dc-4417-bf43-6fcced23ee04", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Self-adaptive crossover probability based on fitness improvement\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.3 * np.tan(np.pi * evaluations / self.budget)  # Enhanced stochastic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce stochastic adaptive weighting to dynamically adjust cognitive and social coefficients, enhancing exploration and exploitation balance.", "configspace": "", "generation": 58, "fitness": 0.9683848178461137, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ddbba17c-9bfe-4337-a2c6-7c10c102ec54", "metadata": {"aucs": [0.9690187866855672, 0.9644178821122408, 0.9717177847405332], "final_y": [0.16490169999690696, 0.16498576733789705, 0.16485679882904847]}, "mutation_prompt": null}
{"id": "9270a689-15eb-403f-9961-c4540b19acda", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.4 + 0.1 * np.sin(2 * np.pi * evaluations / self.budget) # Updated adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Self-adaptive crossover probability based on fitness improvement\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Incorporate a refined inertia weight and mutation strategy for improved convergence.", "configspace": "", "generation": 59, "fitness": 0.9614170908770525, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.961 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ddbba17c-9bfe-4337-a2c6-7c10c102ec54", "metadata": {"aucs": [0.962016558272769, 0.9528919835547273, 0.9693427308036613], "final_y": [0.16485623217466006, 0.16485594293104777, 0.1648574476987532]}, "mutation_prompt": null}
{"id": "447aa435-8720-4b73-b66f-8fc0f7711728", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))  # Enhanced cognitive update\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Self-adaptive crossover probability based on fitness improvement\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce refined velocity update using a time-dependent cognitive coefficient adjustment for enhanced exploration and convergence.", "configspace": "", "generation": 60, "fitness": 0.9689979788476141, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ddbba17c-9bfe-4337-a2c6-7c10c102ec54", "metadata": {"aucs": [0.9696205534785047, 0.965569693797658, 0.9718036892666798], "final_y": [0.16485632382480342, 0.16485609922410072, 0.16485593289843914]}, "mutation_prompt": null}
{"id": "6aeb1664-f5d4-4bb1-bb9f-a03663f084db", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))  # Enhanced cognitive update\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Self-adaptive crossover probability based on fitness improvement\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                # Integrate stochastic hierarchical learning\n                random_neighbor = population[np.random.randint(self.population_size)]\n                if np.random.rand() < 0.1 and func(random_neighbor) < personal_best_scores[i]:\n                    personal_best_positions[i] = random_neighbor\n                \n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Integrate stochastic hierarchical learning by incorporating random learning experiences from other particles for enhanced performance.", "configspace": "", "generation": 61, "fitness": 0.9516513836951085, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.952 with standard deviation 0.030. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "447aa435-8720-4b73-b66f-8fc0f7711728", "metadata": {"aucs": [0.9740008827358955, 0.9087946082984075, 0.9721586600510225], "final_y": [0.16485604477439553, 0.18187864912716734, 0.1648574677006045]}, "mutation_prompt": null}
{"id": "a84b5ee0-0c8f-40fb-b494-404971805faa", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))  # Enhanced cognitive update\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Self-adaptive crossover probability based on fitness improvement\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce a refined strategy for adaptive cognitive and social coefficients to balance exploration and exploitation.", "configspace": "", "generation": 62, "fitness": 0.9689979788476141, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "447aa435-8720-4b73-b66f-8fc0f7711728", "metadata": {"aucs": [0.9696205534785047, 0.965569693797658, 0.9718036892666798], "final_y": [0.16485632382480342, 0.16485609922410072, 0.16485593289843914]}, "mutation_prompt": null}
{"id": "fb08ac73-33b0-41f8-87e6-c5fc40d53942", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size  # Adaptive population size\n        self.c1 = 1.5  # cognitive coefficient\n        self.c2 = 1.5  # social coefficient\n        self.w = 0.5   # inertia weight\n        self.f = 0.8   # differential weight\n        self.cr = 0.9  # crossover probability\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Refined adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))  # Enhanced cognitive update\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                # Differential Evolution-like mutation and crossover\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Dynamic differential weight\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    # Self-adaptive crossover probability based on fitness improvement\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    # Refined mutation strategy: Consider trial vector's fitness instead of candidate's\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Enhanced dynamic cognitive coefficient\n\n                # Update the personal best\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                # Update the global best\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n            # Adaptive population size strategy\n            if evaluations % (self.budget // 10) == 0:\n                self.population_size = max(5, int(self.initial_population_size * (1 - evaluations / self.budget)))\n                population = population[:self.population_size]\n                velocities = velocities[:self.population_size]\n                fitness = fitness[:self.population_size]\n                personal_best_positions = personal_best_positions[:self.population_size]\n                personal_best_scores = personal_best_scores[:self.population_size]\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce adaptive population size strategy to balance exploration and exploitation during optimization.", "configspace": "", "generation": 63, "fitness": 0.966296645618837, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "447aa435-8720-4b73-b66f-8fc0f7711728", "metadata": {"aucs": [0.9656368873633719, 0.9628023918614093, 0.9704506576317299], "final_y": [0.1648567743382019, 0.16485591115497755, 0.16485579902389924]}, "mutation_prompt": null}
{"id": "bc8d572b-d7e9-41d7-bd5a-6e0d6d235904", "solution": "import numpy as np\nfrom scipy.stats import norm  # New import for chaos-driven initialization\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Chaos-driven initialization for diverse starting points\n        population = np.clip(np.random.uniform(0, 1, (self.population_size, self.dim)), lb, ub)\n        population = lb + (ub - lb) * norm.cdf(np.random.randn(self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce a diversity boost mechanism by leveraging chaotic maps for enhanced global exploration and convergence.", "configspace": "", "generation": 64, "fitness": 0.9719030056811603, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "447aa435-8720-4b73-b66f-8fc0f7711728", "metadata": {"aucs": [0.9768175620650548, 0.9646668825391248, 0.9742245724393014], "final_y": [0.16485596819053405, 0.16485596982689632, 0.16485674600685807]}, "mutation_prompt": null}
{"id": "b44208ca-f493-4a08-9d12-6fa558616018", "solution": "import numpy as np\nfrom scipy.stats import norm  # New import for chaos-driven initialization\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Chaos-driven initialization for diverse starting points\n        population = np.clip(np.random.uniform(0, 1, (self.population_size, self.dim)), lb, ub)\n        population = lb + (ub - lb) * norm.cdf(np.random.randn(self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.4 + 0.2 * np.sin(2 * np.pi * evaluations / self.budget)  # Modified line for adaptive inertia weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce adaptive inertia weight oscillation for refined balancing of exploration and exploitation. ", "configspace": "", "generation": 65, "fitness": 0.9686467823264691, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "bc8d572b-d7e9-41d7-bd5a-6e0d6d235904", "metadata": {"aucs": [0.9702678777800293, 0.9631111455512448, 0.9725613236481331], "final_y": [0.16485593499360707, 0.16485585589989826, 0.16485692915470518]}, "mutation_prompt": null}
{"id": "1933487e-fed5-416b-8d98-393a98e1f9be", "solution": "import numpy as np\nfrom scipy.stats import norm  # New import for chaos-driven initialization\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Chaos-driven initialization for diverse starting points\n        population = np.clip(np.random.uniform(0, 1, (self.population_size, self.dim)), lb, ub)\n        population = lb + (ub - lb) * norm.cdf(np.random.randn(self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * (evaluations / self.budget)**0.5)  # Non-linear dynamic weight\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Refine the velocity update rule by incorporating a non-linear dynamic weight adaptation for enhanced solution stability.", "configspace": "", "generation": 66, "fitness": 0.9709856799866442, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "bc8d572b-d7e9-41d7-bd5a-6e0d6d235904", "metadata": {"aucs": [0.972303332090248, 0.9654116201445047, 0.97524208772518], "final_y": [0.16485594086418842, 0.16485603431566587, 0.1648559480936117]}, "mutation_prompt": null}
{"id": "2441df5f-8e30-4b38-9d54-a995e51b6603", "solution": "import numpy as np\nfrom scipy.stats import norm  # New import for chaos-driven initialization\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Chaos-driven initialization for diverse starting points\n        population = np.clip(np.random.uniform(0, 1, (self.population_size, self.dim)), lb, ub)\n        population = lb + (ub - lb) * norm.cdf(np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Tweak the chaotic initialization for more diverse and effective initial population distribution by tuning random sampling.", "configspace": "", "generation": 67, "fitness": 0.980604981642298, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "bc8d572b-d7e9-41d7-bd5a-6e0d6d235904", "metadata": {"aucs": [0.9788782466496522, 0.9788003332177735, 0.9841363650594681], "final_y": [0.16485646627989015, 0.16485619496575843, 0.16485580295977897]}, "mutation_prompt": null}
{"id": "2dad227d-759d-41f2-8d59-7debbe5f8716", "solution": "import numpy as np\nfrom scipy.stats import norm  # New import for chaos-driven initialization\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Chaos-driven initialization for diverse starting points\n        population = np.clip(np.random.uniform(0, 1, (self.population_size, self.dim)), lb, ub)\n        population = lb + (ub - lb) * norm.cdf(np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * (1 - global_best_score / (global_best_score + 1)))  # Adaptive mutation scaling\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce adaptive mutation scaling factor to enhance exploration during optimization.", "configspace": "", "generation": 68, "fitness": 0.978902169635845, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2441df5f-8e30-4b38-9d54-a995e51b6603", "metadata": {"aucs": [0.9789844174836285, 0.9781624170962592, 0.9795596743276473], "final_y": [0.16485608720597078, 0.16485579535200034, 0.1648559723780908]}, "mutation_prompt": null}
{"id": "a32349d3-afc7-466c-81e5-fd56a78f0943", "solution": "import numpy as np\nfrom scipy.stats import norm  # New import for chaos-driven initialization\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Chaos-driven initialization for diverse starting points\n        population = np.clip(np.random.uniform(0, 1, (self.population_size, self.dim)), lb, ub)\n        population = lb + (ub - lb) * norm.cdf(np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * np.sin(np.pi * evaluations / self.budget)  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Adjust the inertia weight dynamic range to further explore the search space more efficiently.", "configspace": "", "generation": 69, "fitness": 0.981388361061654, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2441df5f-8e30-4b38-9d54-a995e51b6603", "metadata": {"aucs": [0.9791614649873019, 0.9803717087072464, 0.9846319094904133], "final_y": [0.16485777021367498, 0.16485722678455073, 0.1648557881455247]}, "mutation_prompt": null}
{"id": "bcc95937-144f-41c8-bb7b-bcb9996027b3", "solution": "import numpy as np\nfrom scipy.stats import norm  # New import for chaos-driven initialization\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Chaos-driven initialization for diverse starting points\n        population = np.clip(np.random.uniform(0, 1, (self.population_size, self.dim)), lb, ub)\n        population = lb + (ub - lb) * norm.cdf(np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * np.sin(np.pi * evaluations / self.budget)  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)  # Change the scaling factor evolution pattern\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce adaptive mutation scaling for enhanced exploration in the hybrid algorithm.", "configspace": "", "generation": 70, "fitness": 0.981388361061654, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a32349d3-afc7-466c-81e5-fd56a78f0943", "metadata": {"aucs": [0.9791614649873019, 0.9803717087072464, 0.9846319094904133], "final_y": [0.16485777021367498, 0.16485722678455073, 0.1648557881455247]}, "mutation_prompt": null}
{"id": "b034b470-722d-4418-a0d2-81539503155e", "solution": "import numpy as np\nfrom scipy.stats import norm  # New import for chaos-driven initialization\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Chaos-driven initialization for diverse starting points\n        population = np.clip(np.random.uniform(0, 1, (self.population_size, self.dim)), lb, ub)\n        population = lb + (ub - lb) * norm.cdf(np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * np.sin(np.pi * evaluations / self.budget)  # Adjusted line\n                # Modified velocity update equation for enhanced exploration\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget) * np.random.rand()))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduced dynamic scaling for velocities to enhance exploration by modifying the velocity update equation.", "configspace": "", "generation": 71, "fitness": 0.9581841303104569, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.958 with standard deviation 0.029. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "a32349d3-afc7-466c-81e5-fd56a78f0943", "metadata": {"aucs": [0.9170477730658746, 0.9761817988961898, 0.9813228189693064], "final_y": [0.18187810766275136, 0.16485617684837117, 0.16485586101068028]}, "mutation_prompt": null}
{"id": "ec88b37d-1947-4803-ba05-bc174544d3ad", "solution": "import numpy as np\nfrom scipy.stats import norm  # New import for chaos-driven initialization\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Chaos-driven initialization for diverse starting points\n        population = np.clip(np.random.uniform(0, 1, (self.population_size, self.dim)), lb, ub)\n        population = lb + (ub - lb) * norm.cdf(np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * np.sin(np.pi * evaluations / self.budget)  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))\n                velocities[i] *= np.tanh(0.01 * evaluations)  # New line for adaptive scaling\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce adaptive scaling of velocity to maintain diversity and enhance convergence.", "configspace": "", "generation": 72, "fitness": 0.9803638329551149, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a32349d3-afc7-466c-81e5-fd56a78f0943", "metadata": {"aucs": [0.9828180269016233, 0.9758552281481102, 0.9824182438156113], "final_y": [0.16485595915473494, 0.16485595017621557, 0.1648558007157086]}, "mutation_prompt": null}
{"id": "b9d8fd27-dbcb-4296-848e-6bcddc19c23c", "solution": "import numpy as np\nfrom scipy.stats import norm  # New import for chaos-driven initialization\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Chaos-driven initialization for diverse starting points\n        # Modified line\n        population = lb + (ub - lb) * norm.cdf(np.random.uniform(-1, 1, (self.population_size, self.dim))) * 0.5\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * np.sin(np.pi * evaluations / self.budget)  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce a dynamic adjustment in the chaos-driven initialization to enhance search space exploration.", "configspace": "", "generation": 73, "fitness": 0.6453878322360264, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.645 with standard deviation 0.098. And the mean value of best solutions found was 0.302 (0. is the best) with standard deviation 0.062.", "error": "", "parent_id": "a32349d3-afc7-466c-81e5-fd56a78f0943", "metadata": {"aucs": [0.7023210179022422, 0.726498611897274, 0.5073438669085626], "final_y": [0.25781014790896584, 0.2578100986340798, 0.39009545545881164]}, "mutation_prompt": null}
{"id": "d3e9fcd0-da1f-41cb-b273-5012ff58acb5", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.clip(np.random.uniform(0, 1, (self.population_size, self.dim)), lb, ub)\n        population = lb + (ub - lb) * norm.cdf(np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.6 * np.sin(np.pi * evaluations / self.budget)  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01 + 0.05 * np.random.rand())  # Adjusted line\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Refined inertia weight schedule and diversity preservation improve convergence and solution quality.", "configspace": "", "generation": 74, "fitness": 0.9756204114962247, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a32349d3-afc7-466c-81e5-fd56a78f0943", "metadata": {"aucs": [0.973535097766776, 0.9763120873994411, 0.9770140493224567], "final_y": [0.16485804229032508, 0.1649400918780909, 0.16485851751805125]}, "mutation_prompt": null}
{"id": "bff7cd30-599b-4817-b656-ad3ff28a73de", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.clip(np.random.uniform(0, 1, (self.population_size, self.dim)), lb, ub)\n        population = lb + (ub - lb) * norm.cdf(np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * np.sin(np.pi * evaluations / self.budget)\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + 0.3 * (1 - evaluations / self.budget)  # Adjusted line for adaptive mutation\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Incorporate adaptive mutation factor based on generation progress to explore the search space more effectively.", "configspace": "", "generation": 75, "fitness": 0.9790021141715686, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a32349d3-afc7-466c-81e5-fd56a78f0943", "metadata": {"aucs": [0.9785908155621901, 0.978979942744871, 0.9794355842076444], "final_y": [0.1648574957522082, 0.16485583046637398, 0.164855873364381]}, "mutation_prompt": null}
{"id": "f58e987d-45af-4088-9e21-e6c6058b55ae", "solution": "import numpy as np\nfrom scipy.stats import norm  # New import for chaos-driven initialization\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Chaos-driven initialization for diverse starting points with adaptive scaling\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))  # Modified line\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * np.sin(np.pi * evaluations / self.budget)  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce adaptive scaling for the chaos-driven initialization to enhance starting diversity.", "configspace": "", "generation": 76, "fitness": 0.9909742687224236, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a32349d3-afc7-466c-81e5-fd56a78f0943", "metadata": {"aucs": [0.9865657914660977, 0.9918957457466313, 0.9944612689545418], "final_y": [0.16485597198220336, 0.16485588965600662, 0.16485602241183805]}, "mutation_prompt": null}
{"id": "1423e576-dc70-4a88-b8ec-8d0f79b51ee3", "solution": "import numpy as np\nfrom scipy.stats import norm  # New import for chaos-driven initialization\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Chaos-driven initialization for diverse starting points with adaptive scaling\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))  # Modified line\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * np.sin(np.pi * evaluations / self.budget)  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + (0.3 * evaluations / self.budget)\n                    mutant_vector = a + self.f * ((b - c) * np.random.uniform(0.9, 1.1))  # Adjusted line\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhance convergence by introducing a dynamic search parameter and adaptive mutation scaling.", "configspace": "", "generation": 77, "fitness": 0.9930277135253291, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f58e987d-45af-4088-9e21-e6c6058b55ae", "metadata": {"aucs": [0.9927841236865362, 0.9910930633638372, 0.9952059535256137], "final_y": [0.16485612690677653, 0.16485593449941083, 0.16485581782218983]}, "mutation_prompt": null}
{"id": "ae381ed2-d8a2-48dd-aeb8-a4afbefca7aa", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / self.budget)  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))  # Adjusted line\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.2 * evaluations / self.budget)  # Adjusted line\n                    mutant_vector = a + self.f * ((b - c) * np.random.uniform(0.8, 1.2))  # Adjusted line\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02)  # Adjusted line\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02)  # Adjusted line\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)  # Adjusted line\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Refine convergence by incorporating feedback-based acceleration and stochastic directional shifts.", "configspace": "", "generation": 78, "fitness": 0.9934981188582231, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1423e576-dc70-4a88-b8ec-8d0f79b51ee3", "metadata": {"aucs": [0.9928335315753053, 0.992482516846465, 0.9951783081528994], "final_y": [0.16485593110838215, 0.16485579159312547, 0.16485577444194455]}, "mutation_prompt": null}
{"id": "55578f03-8f3a-4a5f-9026-234a79f08e37", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n        momentum = np.zeros((self.population_size, self.dim))  # Added line\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                momentum[i] = 0.9 * momentum[i] + velocities[i]  # Added line\n                candidate_position = population[i] + momentum[i]  # Adjusted line\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.2 * evaluations / self.budget)\n                    mutant_vector = a + self.f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.03)  # Adjusted line\n                    else:\n                        self.cr = max(0.1, self.cr - 0.03)  # Adjusted line\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.15 * np.cos(np.pi * evaluations / self.budget)  # Adjusted line\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhance convergence by leveraging adaptive momentum and dynamic recombination strategies.", "configspace": "", "generation": 79, "fitness": 0.9806785669333746, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.002. And the mean value of best solutions found was 0.169 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ae381ed2-d8a2-48dd-aeb8-a4afbefca7aa", "metadata": {"aucs": [0.9804250662377039, 0.978221590864996, 0.983389043697424], "final_y": [0.16886435672029732, 0.16914631463229268, 0.16889659325571504]}, "mutation_prompt": null}
{"id": "4b16bd90-56e7-4257-8bc8-56f9ea4b056e", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.4 * (1 - evaluations / self.budget) + 0.1  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.5 + 0.1 * (1 - global_best_score / previous_global_best_score)  # Adjusted line\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.01)  # Adjusted line\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01)  # Adjusted line\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.5  # Adjusted line\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce adaptive mutation scaling and dynamic inertia based on convergence trends to enhance exploration and exploitation balance.", "configspace": "", "generation": 80, "fitness": 0.991734418834747, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ae381ed2-d8a2-48dd-aeb8-a4afbefca7aa", "metadata": {"aucs": [0.990339573216457, 0.9907216242146293, 0.9941420590731546], "final_y": [0.16485578534251422, 0.16485603545715677, 0.16485577583313338]}, "mutation_prompt": null}
{"id": "7c8baf43-7814-448c-8684-cbaf35f8450f", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / self.budget)  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))  # Adjusted line\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.2 * evaluations / self.budget)  # Adjusted line\n                    mutant_vector = a + self.f * ((b - c) * np.random.uniform(0.8, 1.2))  # Adjusted line\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02)  # Adjusted line\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02)  # Adjusted line\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)  # Adjusted line\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n                velocities[i] *= (0.5 + 0.5 * np.random.rand())  # New line for adaptive velocity scaling\n\n            if np.std(personal_best_scores) < 0.01:  # New line for diversity enforcement\n                population += 0.1 * np.random.uniform(lb, ub, population.shape)\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhance convergence by introducing adaptive velocity scaling and diversity enforcement mechanisms.", "configspace": "", "generation": 81, "fitness": 0.9893464122960202, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.989 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ae381ed2-d8a2-48dd-aeb8-a4afbefca7aa", "metadata": {"aucs": [0.9924948675806761, 0.9820902096026031, 0.9934541597047812], "final_y": [0.16509706645250888, 0.16591081523450224, 0.16534526329164123]}, "mutation_prompt": null}
{"id": "6a8c4145-e921-41c8-a2bf-ea6742382507", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / self.budget)  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))  # Adjusted line\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    # Changed line below\n                    self.f = 0.6 + (0.2 * np.std(population) / (ub - lb))  # Adjusted line\n                    mutant_vector = a + self.f * ((b - c) * np.random.uniform(0.8, 1.2))  # Adjusted line\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02)  # Adjusted line\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02)  # Adjusted line\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)  # Adjusted line\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhance exploration by dynamically adjusting the mutation factor based on population diversity.", "configspace": "", "generation": 82, "fitness": 0.9935142798442734, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ae381ed2-d8a2-48dd-aeb8-a4afbefca7aa", "metadata": {"aucs": [0.9928366179728684, 0.9925464870862171, 0.9951597344737347], "final_y": [0.16485578394845513, 0.16485577447045496, 0.1648557780269091]}, "mutation_prompt": null}
{"id": "d3a73f8b-eb4b-450d-8c28-0757e2b1a7bd", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            diversity = np.std(population) / (ub - lb)  # Added line\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * np.sin(np.pi * evaluations / self.budget)  # Changed line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + diversity  # Changed line\n                    mutant_vector = a + self.f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    local_leader = personal_best_positions[np.random.choice(self.population_size)]  # Added line\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector + 0.1 * (local_leader - candidate_position)  # Changed line\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Integrate diversity-driven adaptive parameters and local leader influence for enhanced convergence.", "configspace": "", "generation": 83, "fitness": 0.989815235899124, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.990 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6a8c4145-e921-41c8-a2bf-ea6742382507", "metadata": {"aucs": [0.9923489915576342, 0.9834362265835797, 0.9936604895561585], "final_y": [0.16485577394767914, 0.16485789347092816, 0.1648558460245475]}, "mutation_prompt": null}
{"id": "98d0e58e-9cc8-49f5-9f76-d15ab3524f20", "solution": "import numpy as np\nfrom scipy.stats import norm, levy\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def levy_flights(self, dim):\n        return levy.rvs(size=dim) * 0.01  # New line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.2 * np.std(population) / (ub - lb))\n                    mutant_vector = a + self.f * ((b - c) * np.random.uniform(0.8, 1.2)) + self.levy_flights(self.dim)  # Modified line\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02)\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]):\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                # Dynamic population resizing\n                self.population_size = max(self.initial_population_size // 2, 2 + (self.population_size // 2) * (global_best_score < previous_global_best_score))  # New line\n                \n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Implement adaptive learning strategies by integrating Lévy flight search mechanism and dynamic population size to balance exploration and exploitation.", "configspace": "", "generation": 84, "fitness": 0.9925785834325787, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6a8c4145-e921-41c8-a2bf-ea6742382507", "metadata": {"aucs": [0.9922476059786172, 0.9897242868848573, 0.9957638574342617], "final_y": [0.1648558664492712, 0.16485624015118816, 0.16485577565019982]}, "mutation_prompt": null}
{"id": "857192d1-3ad1-4b7d-b528-e5bc1ebca54f", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))  # Adjusted line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.2 * population_entropy / (ub - lb))  # Adjusted line\n                    mutant_vector = a + self.f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02)\n                    self.cr *= (1.0 + 0.05 * (1 - population_entropy / self.dim))  # Adjusted line\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhance exploration and convergence by dynamically adjusting both mutation and crossover parameters based on population entropy.", "configspace": "", "generation": 85, "fitness": 0.9935733999331083, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6a8c4145-e921-41c8-a2bf-ea6742382507", "metadata": {"aucs": [0.992762899920591, 0.9925515480844634, 0.9954057517942706], "final_y": [0.16485581214469958, 0.16485577254835526, 0.16485578053454153]}, "mutation_prompt": null}
{"id": "296a04d6-67fa-46fa-b161-24445f84fc4a", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))  # Adjusted line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                if np.random.rand() < 0.1:  # Add random escape mechanism\n                    self.w = 0.1 + 0.4 * np.random.rand() \n                else:\n                    self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.2 * population_entropy / (ub - lb))  # Adjusted line\n                    mutant_vector = a + self.f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02)\n                    self.cr *= (1.0 + 0.05 * (1 - population_entropy / self.dim))  # Adjusted line\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Combine adaptive inertia weights and escape mechanisms for enhanced exploration and convergence in black-box optimization.", "configspace": "", "generation": 86, "fitness": 0.9925511238267699, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "857192d1-3ad1-4b7d-b528-e5bc1ebca54f", "metadata": {"aucs": [0.9932860631642055, 0.9892026748534254, 0.9951646334626787], "final_y": [0.16485577626713355, 0.16485578621144936, 0.16485588166005305]}, "mutation_prompt": null}
{"id": "f16870fd-263c-4308-8483-9ffc295fffec", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))\n            dynamic_w = 0.1 + (0.4 * (1 - evaluations / self.budget))  # Changed line\n            dynamic_f = 0.5 + (0.3 * (evaluations / self.budget))  # Changed line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = dynamic_w  # Changed line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = dynamic_f  # Changed line\n                    mutant_vector = a + self.f * (b - c)\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce adaptive inertia weight and mutation scaling factor to enhance convergence speed and solution diversity.", "configspace": "", "generation": 87, "fitness": 0.9914111741382375, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "857192d1-3ad1-4b7d-b528-e5bc1ebca54f", "metadata": {"aucs": [0.9882630140848848, 0.991971663650462, 0.9939988446793658], "final_y": [0.1648558191321603, 0.16485577566284726, 0.16485577236159166]}, "mutation_prompt": null}
{"id": "69bb075d-7c71-4ec6-aa0e-b57ee42a38d3", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))  # Adjusted line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.2 * population_entropy / (ub - lb))  # Adjusted line\n                    mutant_vector = a + self.f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02)\n                    self.cr *= (1.0 + 0.05 * (1 - population_entropy / self.dim))  # Adjusted line\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n                    self.c2 = 1.4 + 0.1 * np.sin(np.pi * evaluations / self.budget)  # Adjusted line\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Refine exploration by dynamically adjusting acceleration coefficients and enhancing diversity through adaptive crossover.", "configspace": "", "generation": 88, "fitness": 0.9928678642244417, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "857192d1-3ad1-4b7d-b528-e5bc1ebca54f", "metadata": {"aucs": [0.9922186185806577, 0.9915461877216368, 0.9948387863710305], "final_y": [0.16485578721856242, 0.16485639046993872, 0.1648558350932684]}, "mutation_prompt": null}
{"id": "eb417fe9-b560-4d63-b04a-c8891960a4e3", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            fitness_variance = np.var(fitness)  # New line\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.4 + 0.1 * np.sin(np.pi * (evaluations / self.budget) * (fitness_variance / (fitness_variance + 1e-10)))  # Changed line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.2 * population_entropy / (ub - lb))\n                    mutant_vector = a + self.f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02)\n                    self.cr *= (1.0 + 0.05 * (1 - population_entropy / self.dim))\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n                fitness[i] = candidate_fitness  # New line\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhance performance by introducing a dynamic inertia weight adjustment and diversity-preserving strategy based on fitness variance.", "configspace": "", "generation": 89, "fitness": 0.9918142772590953, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "857192d1-3ad1-4b7d-b528-e5bc1ebca54f", "metadata": {"aucs": [0.9902630050827066, 0.9910082445311814, 0.9941715821633978], "final_y": [0.16485615290213684, 0.16485637639973705, 0.16485597096065308]}, "mutation_prompt": null}
{"id": "119b0325-6069-40e4-a015-8e72c338f666", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        while evaluations < self.budget:\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))\n            \n            # Adjust the population size dynamically\n            if evaluations % (self.budget // 10) == 0:\n                self.population_size = int(self.population_size * (0.9 + 0.2 * np.exp(-evaluations / self.budget)))\n                if self.population_size < 4:\n                    self.population_size = 4\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.4 * np.sin(np.pi * evaluations / self.budget)  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    self.f = 0.6 + (0.25 * population_entropy / (ub - lb))  # Adjusted line\n                    mutant_vector = a + self.f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.03)  # Adjusted line\n                    else:\n                        self.cr = max(0.1, self.cr - 0.03)  # Adjusted line\n                    self.cr *= (1.0 + 0.05 * (1 - population_entropy / self.dim))\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Leverage adaptive inertia weight and dynamic population adjustment to enhance exploration and exploitation balance.", "configspace": "", "generation": 90, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 104 is out of bounds for axis 0 with size 100').", "error": "IndexError('index 104 is out of bounds for axis 0 with size 100')", "parent_id": "857192d1-3ad1-4b7d-b528-e5bc1ebca54f", "metadata": {}, "mutation_prompt": null}
{"id": "657a3002-d039-4400-bf2b-17c02425758c", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        adapt_rate = 0.9\n        while evaluations < self.budget:\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    dynamic_f = adapt_rate * (0.6 + 0.2 * np.tanh(population_entropy / self.dim))  # Adjusted line\n                    mutant_vector = a + dynamic_f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02 * adapt_rate)  # Adjusted line\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02 * adapt_rate)  # Adjusted line\n                    self.cr *= (1.0 + 0.05 * (1 - population_entropy / self.dim))\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Integrate adaptive learning rates and swarm intelligence to fine-tune exploration and exploitation phases for improved convergence.", "configspace": "", "generation": 91, "fitness": 0.9936591977088082, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "857192d1-3ad1-4b7d-b528-e5bc1ebca54f", "metadata": {"aucs": [0.9932055833973451, 0.9924369028984764, 0.9953351068306029], "final_y": [0.1648557850609299, 0.16485577542004315, 0.1648557745085505]}, "mutation_prompt": null}
{"id": "fa5def94-ed6f-4b8c-8a86-5d6670465a57", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        adapt_rate = 0.9\n        while evaluations < self.budget:\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    dynamic_f = adapt_rate * (0.6 + 0.2 * np.tanh(population_entropy * np.random.rand() / self.dim))  # Adjusted line\n                    mutant_vector = a + dynamic_f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02 * adapt_rate)  # Adjusted line\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02 * adapt_rate)  # Adjusted line\n                    self.cr *= (1.0 + 0.05 * (1 - population_entropy / self.dim))\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < candidate_fitness:  # Adjusted line\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhance exploration with adaptive mutation mechanisms and dynamic selection pressure to improve versatility and convergence.", "configspace": "", "generation": 92, "fitness": 0.9907792748768465, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "657a3002-d039-4400-bf2b-17c02425758c", "metadata": {"aucs": [0.989994062271225, 0.9867085821115601, 0.9956351802477542], "final_y": [0.1648597431626303, 0.16485581785775294, 0.16485577314780886]}, "mutation_prompt": null}
{"id": "a7afceb9-195a-4dd7-aea0-ebe4fc18836f", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        adapt_rate = 0.9\n        while evaluations < self.budget:\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / self.budget)**2  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    dynamic_f = adapt_rate * (0.6 + 0.2 * np.tanh(population_entropy / self.dim))\n                    mutant_vector = a + dynamic_f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02 * adapt_rate)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02 * adapt_rate)\n                    self.cr *= (1.0 + 0.05 * (1 - population_entropy / self.dim))\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce nonlinear modulation of the inertia weight to balance exploration and exploitation for enhanced convergence.", "configspace": "", "generation": 93, "fitness": 0.9938645682923822, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "657a3002-d039-4400-bf2b-17c02425758c", "metadata": {"aucs": [0.9934030368297384, 0.9925816125516419, 0.9956090554957662], "final_y": [0.16485579210595946, 0.1648557880799616, 0.16485577496793147]}, "mutation_prompt": null}
{"id": "a9f3cb90-fd5e-45e2-93c3-c26c681d4fb8", "solution": "import numpy as np\nfrom scipy.stats import norm\n\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n\n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        adapt_rate = 0.9\n        while evaluations < self.budget:\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))\n\n            # Dynamic adjustment of population size and inertia weight for diversification\n            self.population_size = int(self.population_size * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n            self.w = 0.1 + 0.4 * np.sin(np.pi * evaluations / self.budget)**2\n            \n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    dynamic_f = adapt_rate * (0.6 + 0.2 * np.tanh(population_entropy / self.dim))\n                    mutant_vector = a + dynamic_f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02 * adapt_rate)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02 * adapt_rate)\n                    self.cr *= (1.0 + 0.05 * (1 - population_entropy / self.dim))\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]):\n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Integrate dynamic population adjustment and fitness diversity preservation for enhanced global exploration and convergence.", "configspace": "", "generation": 94, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"Cannot take a larger sample than population when 'replace=False'\").", "error": "ValueError(\"Cannot take a larger sample than population when 'replace=False'\")", "parent_id": "a7afceb9-195a-4dd7-aea0-ebe4fc18836f", "metadata": {}, "mutation_prompt": null}
{"id": "ab63f2ae-70a7-4529-aff2-447ed0e5cabd", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        adapt_rate = 0.9\n        while evaluations < self.budget:\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / self.budget)**2 + 0.1 * (1 - population_entropy / self.dim)  # Changed line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    dynamic_f = adapt_rate * (0.6 + 0.2 * np.tanh(population_entropy / self.dim))\n                    mutant_vector = a + dynamic_f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02 * adapt_rate)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02 * adapt_rate)\n                    self.cr *= (1.0 + 0.05 * (1 - population_entropy / self.dim))\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce dynamic adjustment to inertia weight based on population diversity for improved convergence.", "configspace": "", "generation": 95, "fitness": 0.9855749896519872, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.002. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "a7afceb9-195a-4dd7-aea0-ebe4fc18836f", "metadata": {"aucs": [0.986106421418191, 0.9823094046411573, 0.9883091428966133], "final_y": [0.16596785035154404, 0.16565955702416035, 0.16695340493472566]}, "mutation_prompt": null}
{"id": "e142ee57-b770-48b0-b69c-983d838a63ba", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        adapt_rate = 0.9\n        while evaluations < self.budget:\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / self.budget)**2  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    # Change: Introduce adaptive differential weighting factor\n                    dynamic_f = adapt_rate * (0.5 + 0.4 * np.tanh(population_entropy / self.dim))\n                    mutant_vector = a + dynamic_f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.03 * adapt_rate)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.01 * adapt_rate)\n                    self.cr *= (1.0 + 0.03 * (1 - population_entropy / self.dim))\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        # Change: Adjust learning rate dynamically based on evaluation progress\n                        self.c1 = 1.5 + 0.2 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce adaptive particle learning rates and adaptive differential weighting to further balance exploration and exploitation dynamically.", "configspace": "", "generation": 96, "fitness": 0.9935411463632636, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a7afceb9-195a-4dd7-aea0-ebe4fc18836f", "metadata": {"aucs": [0.9926242612129321, 0.9922497983501554, 0.9957493795267034], "final_y": [0.16485595094364736, 0.16485580138339528, 0.16485578459870098]}, "mutation_prompt": null}
{"id": "97afb488-f219-4aa4-b0e4-36cc64ce95cd", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        adapt_rate = 0.9\n        while evaluations < self.budget:\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.2 + 0.3 * np.exp(-evaluations / self.budget)  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    dynamic_f = adapt_rate * (0.6 + 0.3 * np.tanh(population_entropy / self.dim))  # Adjusted line\n                    mutant_vector = a + dynamic_f * ((b - c) * np.random.uniform(0.7, 1.3))  # Adjusted line\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.03 * adapt_rate)  # Adjusted line\n                    else:\n                        self.cr = max(0.1, self.cr - 0.03 * adapt_rate)  # Adjusted line\n                    self.cr *= (1.0 + 0.05 * (1 - population_entropy / self.dim))\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.sin(np.pi * evaluations / self.budget)  # Adjusted line\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Introduce adaptive inertia weight and dynamic crossover rate to balance exploration and exploitation for improved convergence.", "configspace": "", "generation": 97, "fitness": 0.9917641808575496, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a7afceb9-195a-4dd7-aea0-ebe4fc18836f", "metadata": {"aucs": [0.9918291806607944, 0.9892396892650531, 0.9942236726468013], "final_y": [0.16485580315624715, 0.1648558498065642, 0.164855781104722]}, "mutation_prompt": null}
{"id": "c27c160b-49b1-4cfc-98d8-e579a068372d", "solution": "import numpy as np\nfrom scipy.stats import norm, levy\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        adapt_rate = 0.9\n        while evaluations < self.budget:\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / self.budget)**2\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                # Levy flight step\n                levy_step = levy.rvs(size=self.dim)\n                candidate_position = population[i] + velocities[i] + 0.1 * levy_step\n                candidate_position = np.clip(candidate_position, lb, ub)\n                \n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    dynamic_f = adapt_rate * (0.6 + 0.2 * np.tanh(population_entropy / self.dim))\n                    mutant_vector = a + dynamic_f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02 * adapt_rate)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02 * adapt_rate)\n                    self.cr *= (1.0 + 0.05 * (1 - population_entropy / self.dim))\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhance the exploration-exploitation balance by incorporating Levy flights and adaptive learning parameters to improve convergence efficiency.", "configspace": "", "generation": 98, "fitness": 0.9819471441014483, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.007. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "a7afceb9-195a-4dd7-aea0-ebe4fc18836f", "metadata": {"aucs": [0.9890101053052428, 0.9723815556073769, 0.9844497713917254], "final_y": [0.1653834501876068, 0.1667542898497265, 0.16596502277210834]}, "mutation_prompt": null}
{"id": "8ef5a2fd-762b-4b1b-89ef-fe553fa1d0ff", "solution": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.f = 0.8\n        self.cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = lb + (ub - lb) * norm.cdf(self.w * np.random.uniform(-1, 1, (self.population_size, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        personal_best_positions = population.copy()\n        personal_best_scores = fitness.copy()\n        global_best_position = population[np.argmin(fitness)]\n        global_best_score = np.min(fitness)\n        \n        evaluations = self.population_size\n        previous_global_best_score = global_best_score\n\n        adapt_rate = 0.9\n        while evaluations < self.budget:\n            population_entropy = -np.sum(np.log(np.var(population, axis=0, ddof=1) + 1e-10))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.1 + 0.3 * np.sin(np.pi * evaluations / (self.budget * 0.95))**2  # Adjusted line\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 (self.c2 * r2 * (global_best_position - population[i]))\n                                 * (0.9 + 0.1 * np.cos(np.pi * evaluations / self.budget)))\n                candidate_position = population[i] + velocities[i]\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                evaluations += 1\n\n                if evaluations < self.budget:\n                    indices = np.random.choice(self.population_size, 3, replace=False)\n                    a, b, c = population[indices]\n                    dynamic_f = adapt_rate * (0.6 + 0.2 * np.tanh(population_entropy / self.dim))\n                    mutant_vector = a + dynamic_f * ((b - c) * np.random.uniform(0.8, 1.2))\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    \n                    if candidate_fitness < previous_global_best_score:\n                        self.cr = min(1.0, self.cr + 0.02 * adapt_rate)\n                    else:\n                        self.cr = max(0.1, self.cr - 0.02 * adapt_rate)\n                    self.cr *= (1.0 + 0.05 * (1 - population_entropy / self.dim))\n                    trial_vector = np.where(np.random.rand(self.dim) < self.cr, mutant_vector, candidate_position)\n                    trial_fitness = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_fitness < min(candidate_fitness, personal_best_scores[i]): \n                        candidate_position = trial_vector\n                        candidate_fitness = trial_fitness\n                        self.c1 = 1.6 + 0.1 * np.cos(np.pi * evaluations / self.budget)\n\n                if candidate_fitness < personal_best_scores[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_scores[i] = candidate_fitness\n\n                if candidate_fitness < global_best_score:\n                    global_best_position = candidate_position\n                    global_best_score = candidate_fitness\n\n                previous_global_best_score = global_best_score\n                population[i] = candidate_position\n\n        return global_best_position", "name": "HybridMetaheuristic", "description": "Enhance convergence by refining the adjustment of the inertia weight sinusoidal modulation.", "configspace": "", "generation": 99, "fitness": 0.9938763797052554, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a7afceb9-195a-4dd7-aea0-ebe4fc18836f", "metadata": {"aucs": [0.9934278355666698, 0.992470623412838, 0.9957306801362585], "final_y": [0.16485579401013506, 0.16485581635508795, 0.16485577215148262]}, "mutation_prompt": null}
