{"id": "e887bbd8-90f8-4329-af37-f01e8529df89", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.1  # Radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "A hybrid evolutionary algorithm combining differential evolution and local search to efficiently explore and exploit the search space for optimal layer configurations.", "configspace": "", "generation": 0, "fitness": 0.5596936138135012, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.560 with standard deviation 0.068. And the mean value of best solutions found was 0.359 (0. is the best) with standard deviation 0.042.", "error": "", "parent_id": null, "metadata": {"aucs": [0.49666061503708203, 0.6542120460787166, 0.5282081803247051], "final_y": [0.4073425911909888, 0.3041766564788797, 0.3648444205860307]}, "mutation_prompt": null}
{"id": "6ea41964-0dd4-41cf-baf6-3abce5ab5c5d", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.1  # Radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced HybridBraggOptimizer using adaptive parameter tuning for improved convergence and performance.", "configspace": "", "generation": 1, "fitness": 0.5751308269282256, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.575 with standard deviation 0.097. And the mean value of best solutions found was 0.356 (0. is the best) with standard deviation 0.059.", "error": "", "parent_id": "e887bbd8-90f8-4329-af37-f01e8529df89", "metadata": {"aucs": [0.7120699212875593, 0.5132482394956681, 0.5000743200014495], "final_y": [0.27255660637199863, 0.3906314939781711, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "9a915638-2fee-4fa5-baf7-14ab10409359", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.1  # Radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced HybridBraggOptimizer using mixed adaptive mutation strategies and improved local search for better exploration and exploitation.", "configspace": "", "generation": 2, "fitness": 0.5816233292229763, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.582 with standard deviation 0.095. And the mean value of best solutions found was 0.335 (0. is the best) with standard deviation 0.059.", "error": "", "parent_id": "6ea41964-0dd4-41cf-baf6-3abce5ab5c5d", "metadata": {"aucs": [0.7120699212875593, 0.4887423048434897, 0.5440577615378798], "final_y": [0.27255660637199863, 0.41379549904142665, 0.3175031218700666]}, "mutation_prompt": null}
{"id": "53dc96c1-365e-492e-8fb8-46f01f18ab06", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.1  # Radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            self.cr = np.random.uniform(0.7, 0.95)  # Adaptive CR\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Introduced adaptive crossover probability to improve exploration-exploitation balance.", "configspace": "", "generation": 3, "fitness": 0.5018158288618442, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.502 with standard deviation 0.011. And the mean value of best solutions found was 0.401 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "9a915638-2fee-4fa5-baf7-14ab10409359", "metadata": {"aucs": [0.5166308617405935, 0.4887423048434897, 0.5000743200014495], "final_y": [0.3835626147047787, 0.41379549904142665, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "b9f5a372-2ac6-46cc-9469-a49ab55fe606", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 8 * dim  # Reduced population size for faster convergence\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.1  # Radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        self.dynamic_local_search_factor = 1.0  # Factor to adjust local search frequency\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.dynamic_local_search_factor * self.local_search_radius,\n                                                               self.dynamic_local_search_factor * self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 2 == 0:  # Adjust mutation frequency\n                self.hybrid_mutation(bounds)\n            self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.dynamic_local_search_factor = 0.5 + 0.5 * (func_counter / self.budget)  # Dynamic local search adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced Differential Evolution with Adaptive Population Size and Dynamic Local Search for Efficient Optimization.", "configspace": "", "generation": 4, "fitness": 0.528000302745587, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.528 with standard deviation 0.028. And the mean value of best solutions found was 0.346 (0. is the best) with standard deviation 0.056.", "error": "", "parent_id": "9a915638-2fee-4fa5-baf7-14ab10409359", "metadata": {"aucs": [0.5511873068421984, 0.4887423048434897, 0.5440712965510727], "final_y": [0.2762205676405698, 0.41379549904142665, 0.3476024595199704]}, "mutation_prompt": null}
{"id": "12046f6b-eae4-43e7-a9e0-847557bbc454", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.1  # Radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.cr = np.random.uniform(0.7, 0.95)  # Adaptive CR\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved crossover probability adaptation for enhanced exploration and convergence.", "configspace": "", "generation": 5, "fitness": 0.5329184186680801, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.533 with standard deviation 0.031. And the mean value of best solutions found was 0.374 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "9a915638-2fee-4fa5-baf7-14ab10409359", "metadata": {"aucs": [0.49666061503708203, 0.5295715749979366, 0.5725230659692215], "final_y": [0.4073425911909888, 0.3768441612016715, 0.33767524724048936]}, "mutation_prompt": null}
{"id": "a342db1e-a603-476e-9cdf-a6d8999d12cc", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.1  # Radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.6, 0.9)  # Adaptive F (modified)\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced adaptive differential weight strategy for improved solution quality.", "configspace": "", "generation": 6, "fitness": 0.5295325256905273, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.530 with standard deviation 0.050. And the mean value of best solutions found was 0.385 (0. is the best) with standard deviation 0.035.", "error": "", "parent_id": "9a915638-2fee-4fa5-baf7-14ab10409359", "metadata": {"aucs": [0.5997809522266431, 0.4887423048434897, 0.5000743200014495], "final_y": [0.3360640233118063, 0.41379549904142665, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "be9f53dd-a019-4a1a-8e31-37a56665d918", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.1  # Radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced HybridBraggOptimizer using mixed adaptive mutation strategies and improved local search for better exploration and exploitation.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "9a915638-2fee-4fa5-baf7-14ab10409359", "metadata": {"aucs": [0.7120699212875593, 0.4887423048434897, 0.5440577615378798], "final_y": [0.27255660637199863, 0.41379549904142665, 0.3175031218700666]}, "mutation_prompt": null}
{"id": "b86850d2-6d7a-418a-947b-8f4914f1cece", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.15  # Radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced HybridBraggOptimizer with increased local search radius and adaptive crossover probability for improved convergence.", "configspace": "", "generation": 8, "fitness": 0.5901061575232308, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.590 with standard deviation 0.087. And the mean value of best solutions found was 0.325 (0. is the best) with standard deviation 0.051.", "error": "", "parent_id": "9a915638-2fee-4fa5-baf7-14ab10409359", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5438345727061451], "final_y": [0.27255660637199863, 0.3946714880547304, 0.3078785148677792]}, "mutation_prompt": null}
{"id": "983b057d-0694-4d6d-999d-4e917c7e602e", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.15  # Radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            self.cr = np.random.uniform(0.7, 0.9)  # Dynamic CR adjustment\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Introduced dynamic crossover probability adjustment during the differential evolution step for improved adaptability to the search landscape.", "configspace": "", "generation": 9, "fitness": 0.49912408557991467, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.499 with standard deviation 0.010. And the mean value of best solutions found was 0.400 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "b86850d2-6d7a-418a-947b-8f4914f1cece", "metadata": {"aucs": [0.49666061503708203, 0.4887423048434897, 0.5119693368591725], "final_y": [0.4073425911909888, 0.41379549904142665, 0.37978265764981034]}, "mutation_prompt": null}
{"id": "fa26ef89-cdaa-4e8f-898b-fb7eff4d6fdb", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.base_population_size = 10 * dim  # Base population size is scaled with dimension\n        self.adaptive_population_size = self.base_population_size  # Start with base size\n        self.cr = 0.9  # Increased crossover probability for better diversity\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.1  # Reduced radius for more granular local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.adaptive_population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.adaptive_population_size):\n            idxs = [idx for idx in range(self.adaptive_population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.4, 1.0)  # Broadened range for adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.adaptive_population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def multi_strategy_local_search(self, bounds, func):\n        for i in range(self.adaptive_population_size):\n            perturb = np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(self.population[i] + perturb, bounds.lb, bounds.ub)\n            exploration_candidate = np.clip(self.population[i] + 2 * perturb, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n            elif func(exploration_candidate) > func(self.population[i]):\n                self.population[i] = exploration_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.adaptive_population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.adaptive_population_size\n            if func_counter < self.budget:\n                self.multi_strategy_local_search(bounds, func)\n                func_counter += self.adaptive_population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Augmented HybridBraggOptimizer with adaptive population size and multi-strategy local search for enhanced convergence and exploration.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'HybridBraggOptimizer' object has no attribute 'hybrid_mutation'\").", "error": "AttributeError(\"'HybridBraggOptimizer' object has no attribute 'hybrid_mutation'\")", "parent_id": "b86850d2-6d7a-418a-947b-8f4914f1cece", "metadata": {}, "mutation_prompt": null}
{"id": "a1a32b83-702b-4675-b597-216936d17091", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.15  # Radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved HybridBraggOptimizer with dynamic local search radius adjustment for enhanced exploration-exploitation balance.", "configspace": "", "generation": 11, "fitness": 0.5901068339887926, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.590 with standard deviation 0.087. And the mean value of best solutions found was 0.325 (0. is the best) with standard deviation 0.051.", "error": "", "parent_id": "b86850d2-6d7a-418a-947b-8f4914f1cece", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5438366021028305], "final_y": [0.27255660637199863, 0.3946714880547304, 0.30787459512124593]}, "mutation_prompt": null}
{"id": "e21fdfc4-52cc-4f56-bd09-b1c2534e5eff", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.15  # Radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            step_size = np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)  # Adjust step size\n            candidate = self.population[i] + step_size\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced local search strategy with adaptive step size control for improved exploitation of promising regions.", "configspace": "", "generation": 12, "fitness": 0.5901068339887926, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.590 with standard deviation 0.087. And the mean value of best solutions found was 0.325 (0. is the best) with standard deviation 0.051.", "error": "", "parent_id": "a1a32b83-702b-4675-b597-216936d17091", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5438366021028305], "final_y": [0.27255660637199863, 0.3946714880547304, 0.30787459512124593]}, "mutation_prompt": null}
{"id": "db6b7d51-2401-41de-b277-88465cc4e45a", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.15  # Radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            diversity = np.mean(np.std(self.population, axis=0))  # New line for diversity measure\n            self.cr = 0.5 + (0.5 * diversity)  # New line for adaptive crossover probability\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced HybridBraggOptimizer with adaptive crossover probability based on population diversity.", "configspace": "", "generation": 13, "fitness": 0.5697678344635438, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.570 with standard deviation 0.101. And the mean value of best solutions found was 0.357 (0. is the best) with standard deviation 0.061.", "error": "", "parent_id": "a1a32b83-702b-4675-b597-216936d17091", "metadata": {"aucs": [0.7120699212875593, 0.4887423048434897, 0.5084912772595827], "final_y": [0.27255660637199863, 0.41379549904142665, 0.38398855906934737]}, "mutation_prompt": null}
{"id": "bffae429-8dd6-4870-949d-3003ea60fdf2", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = int(10 * dim * (1 + np.random.uniform(-0.1, 0.1)))  # Adaptive population size adjustment\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.15  # Radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced HybridBraggOptimizer with adaptive population size adjustment for improved convergence.", "configspace": "", "generation": 14, "fitness": 0.5550130207342608, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.555 with standard deviation 0.052. And the mean value of best solutions found was 0.352 (0. is the best) with standard deviation 0.025.", "error": "", "parent_id": "a1a32b83-702b-4675-b597-216936d17091", "metadata": {"aucs": [0.48193973863227735, 0.581389022728493, 0.6017103008420125], "final_y": [0.3860115201058135, 0.34594174643479136, 0.32510140947162436]}, "mutation_prompt": null}
{"id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "HybridBraggOptimizer with adaptive local search radius initialization for improved exploration-exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.5926442578738175, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.593 with standard deviation 0.085. And the mean value of best solutions found was 0.307 (0. is the best) with standard deviation 0.028.", "error": "", "parent_id": "a1a32b83-702b-4675-b597-216936d17091", "metadata": {"aucs": [0.7120699212875593, 0.5223335181534001, 0.5435293341804928], "final_y": [0.27255660637199863, 0.3404393115324811, 0.308468646698681]}, "mutation_prompt": null}
{"id": "02e02a03-1de4-41aa-a88b-0a1ebb5c1b40", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.95  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhance exploration-exploitation balance by adjusting local search radius decay rate to 0.95 for improved convergence.", "configspace": "", "generation": 16, "fitness": 0.5900090425872526, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.590 with standard deviation 0.087. And the mean value of best solutions found was 0.325 (0. is the best) with standard deviation 0.051.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5435432278982106], "final_y": [0.27255660637199863, 0.3946714880547304, 0.30844176077307417]}, "mutation_prompt": null}
{"id": "29e119e5-2eb2-41d4-8ee6-1d0e0cae3497", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            self.cr = np.random.uniform(0.6, 0.9)  # Adaptive CR\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved strategy with adaptive crossover probability to enhance exploration and exploitation phases effectively.", "configspace": "", "generation": 17, "fitness": 0.5017477126283205, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.502 with standard deviation 0.010. And the mean value of best solutions found was 0.398 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.503421526599912, 0.4887423048434897, 0.5130793064415599], "final_y": [0.3935006061952714, 0.41379549904142665, 0.38786994799463137]}, "mutation_prompt": null}
{"id": "38a6d843-66d2-4cc6-9e13-8c6df91019bc", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.9  # Adaptive crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.cr = np.random.uniform(0.7, 0.9)  # Adaptive CR\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n    \n    def multi_phase_local_search(self, bounds, func):\n        for i in range(self.population_size):\n            for _ in range(3):\n                candidate = self.population[i] + np.random.uniform(-self.local_search_radius*0.5, self.local_search_radius*0.5, self.dim)\n                candidate = np.clip(candidate, bounds.lb, bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.multi_phase_local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced HybridBraggOptimizer with adaptive crossover probability and multi-phase local search for improved search depth and convergence rate.", "configspace": "", "generation": 18, "fitness": 0.5058351636573734, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.506 with standard deviation 0.019. And the mean value of best solutions found was 0.399 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.49666061503708203, 0.4887423048434897, 0.5321025710915486], "final_y": [0.4073425911909888, 0.41379549904142665, 0.3757288588403276]}, "mutation_prompt": null}
{"id": "ae42ec73-bd88-49a6-a0d0-f3fb0c2b14b6", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Initial crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            self.cr = np.random.uniform(0.7, 0.9)  # Adaptive CR\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced HybridBraggOptimizer with adaptive crossover probability for dynamic exploration-exploitation balance.", "configspace": "", "generation": 19, "fitness": 0.4998277967480023, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.500 with standard deviation 0.011. And the mean value of best solutions found was 0.399 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.49666061503708203, 0.4887423048434897, 0.5140804703634354], "final_y": [0.4073425911909888, 0.41379549904142665, 0.37545049849619627]}, "mutation_prompt": null}
{"id": "84e5f083-66a4-43dd-8002-6c98e9e48736", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.cr = 0.9  # Increase crossover probability for better diversity\n        self.f = 0.8\n        self.local_search_radius = 0.3\n        self.diversity_factor = 0.1  # Introduce diversity factor\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        # Add diversity in initialization\n        additional_diverse_population = np.random.uniform(bounds.lb, bounds.ub, (int(self.population_size * self.diversity_factor), self.dim))\n        self.population = np.concatenate((self.population, additional_diverse_population))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            new_population[i] = trial if func(trial) > func(self.population[i]) else self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += len(self.population)  # Corrected population size count\n\n        while func_counter < self.budget:\n            if func_counter % 2 == 0:  # Increase frequency of mutation\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += len(self.population)\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98  # Faster reduction of local search radius\n                self.local_search(bounds, func)\n                func_counter += len(self.population)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved hybrid optimizer using dynamic local search and diverse initialization for enhanced global exploration.", "configspace": "", "generation": 20, "fitness": 0.5270281353114264, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.527 with standard deviation 0.043. And the mean value of best solutions found was 0.376 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.505092319847275, 0.4887423048434897, 0.5872497812435147], "final_y": [0.3944833290596722, 0.41379549904142665, 0.3208397187414027]}, "mutation_prompt": null}
{"id": "0b435b74-f6dd-4aab-b885-a234b67fc6d0", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.9  # Increased crossover probability for better exploration\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced exploration by increasing crossover probability.", "configspace": "", "generation": 21, "fitness": 0.5693640688571631, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.569 with standard deviation 0.101. And the mean value of best solutions found was 0.341 (0. is the best) with standard deviation 0.049.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.49240376602854385, 0.5036185192553859], "final_y": [0.27255660637199863, 0.37108448421559936, 0.38044450891090853]}, "mutation_prompt": null}
{"id": "b1e1a7ca-314a-4be2-8712-f54f58a62fab", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.9  # Refined adaptive crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "HybridBraggOptimizer with a refined adaptive crossover probability for better exploration-exploitation balance.", "configspace": "", "generation": 22, "fitness": 0.5693640688571631, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.569 with standard deviation 0.101. And the mean value of best solutions found was 0.341 (0. is the best) with standard deviation 0.049.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.49240376602854385, 0.5036185192553859], "final_y": [0.27255660637199863, 0.37108448421559936, 0.38044450891090853]}, "mutation_prompt": null}
{"id": "98d3539d-1dee-4966-b2b5-75516c5588c4", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            self.cr = np.random.uniform(0.7, 0.9)  # Dynamic CR adjustment\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced local search by dynamically adjusting the crossover probability during the evolution process.", "configspace": "", "generation": 23, "fitness": 0.5021953916891163, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.502 with standard deviation 0.006. And the mean value of best solutions found was 0.403 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.49666061503708203, 0.5098512400288175, 0.5000743200014495], "final_y": [0.4073425911909888, 0.39777991120329037, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "9b251818-2302-4654-89da-2514254b504e", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Initial Crossover probability\n        self.f = 0.8  # Initial Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_f = np.random.uniform(0.6, 1.2)  # Refined adaptive F\n            mutant = np.clip(a + adaptive_f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n            self.cr *= 0.98  # Gradual decay of crossover probability\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced HybridBraggOptimizer with adaptive mutation strategy and dynamic crossover for improved performance in complex optimization landscapes.", "configspace": "", "generation": 24, "fitness": 0.5414980516980165, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.541 with standard deviation 0.017. And the mean value of best solutions found was 0.369 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.55868365550043, 0.5190387727765985, 0.5467717268170209], "final_y": [0.36283982441988116, 0.3913214329742435, 0.3516499381037731]}, "mutation_prompt": null}
{"id": "bf2ff6c9-7c36-4330-b8d8-5379f81dae48", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n                self.cr = min(1.0, self.cr + 0.01)  # Adaptive CR based on success\n            else:\n                new_population[i] = self.population[i]\n                self.cr = max(0.1, self.cr - 0.01)  # Adaptive CR based on failure\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhance the adaptive mutation strategy by varying the crossover probability dynamically based on the success of previous iterations.", "configspace": "", "generation": 25, "fitness": 0.5755194066216656, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.576 with standard deviation 0.097. And the mean value of best solutions found was 0.357 (0. is the best) with standard deviation 0.060.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5000743200014495], "final_y": [0.27255660637199863, 0.3946714880547304, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "defb83fa-b7ce-4503-b93b-97c773d25a68", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.9  # Adjusted crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.6, 1.0)  # Adjusted adaptive F range\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98  # Modified dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced exploration-exploitation balance through adaptive mutation and local search parameter tuning.", "configspace": "", "generation": 26, "fitness": 0.5364770182447043, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.536 with standard deviation 0.038. And the mean value of best solutions found was 0.370 (0. is the best) with standard deviation 0.019.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.5867521852656694, 0.49567110425227956, 0.5270077652161638], "final_y": [0.3443313999572074, 0.3859005940549266, 0.3810765328921015]}, "mutation_prompt": null}
{"id": "95e504fc-e1a8-477c-a95a-55f325d78a49", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.995  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced local search by adjusting the local search radius decay factor for improved convergence.", "configspace": "", "generation": 27, "fitness": 0.5926396411125506, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.593 with standard deviation 0.085. And the mean value of best solutions found was 0.307 (0. is the best) with standard deviation 0.028.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.5223209419985126, 0.5435280600515802], "final_y": [0.27255660637199863, 0.34051923284581986, 0.3084711124005267]}, "mutation_prompt": null}
{"id": "33b6ce82-6588-4b53-a864-6d46f7dc9913", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                if self.local_search_radius < 0.1:  # Change: dynamically increase cr when local search radius is low\n                    self.cr = 0.9  # Increase the crossover probability\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced exploitation by dynamically increasing crossover probability when local search radius is low.", "configspace": "", "generation": 28, "fitness": 0.5926442578738175, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.593 with standard deviation 0.085. And the mean value of best solutions found was 0.307 (0. is the best) with standard deviation 0.028.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.5223335181534001, 0.5435293341804928], "final_y": [0.27255660637199863, 0.3404393115324811, 0.308468646698681]}, "mutation_prompt": null}
{"id": "78a1c9ff-b3dd-4ef2-bab4-ff0a454fe636", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.85  # Adjusted crossover probability for better diversity\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Slightly adjusted crossover probability for improved solution diversity in HybridBraggOptimizer.", "configspace": "", "generation": 29, "fitness": 0.5831670183972831, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.583 with standard deviation 0.091. And the mean value of best solutions found was 0.351 (0. is the best) with standard deviation 0.055.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5230171553283021], "final_y": [0.27255660637199863, 0.3946714880547304, 0.38463536832576095]}, "mutation_prompt": null}
{"id": "6098da06-fbfc-4d76-bc8b-42a410d870ca", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.6, 1.2) * (b - c), bounds.lb, bounds.ub)  # Changed step size here\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "HybridBraggOptimizer with adaptive mutation step size for improved exploration.", "configspace": "", "generation": 30, "fitness": 0.5759935441477428, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.576 with standard deviation 0.096. And the mean value of best solutions found was 0.356 (0. is the best) with standard deviation 0.059.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5014967325796814], "final_y": [0.27255660637199863, 0.3946714880547304, 0.40138683935377795]}, "mutation_prompt": null}
{"id": "0358d0b1-4702-471c-9383-1ed8670acbff", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            weight = np.random.uniform(0.5, 1.5)  # Dynamic weighting adjustment\n            new_candidate = np.clip(a + weight * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved mutation strategy with dynamic weighting for enhanced diversity and convergence.", "configspace": "", "generation": 31, "fitness": 0.5755194066216656, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.576 with standard deviation 0.097. And the mean value of best solutions found was 0.357 (0. is the best) with standard deviation 0.060.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5000743200014495], "final_y": [0.27255660637199863, 0.3946714880547304, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "c7cb2e26-951b-45e1-9304-33e3489c842e", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.6, 1.2)  # Changed range for adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced adaptive differential weight range for improved exploration.", "configspace": "", "generation": 32, "fitness": 0.5391005869779922, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.539 with standard deviation 0.015. And the mean value of best solutions found was 0.365 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.55868365550043, 0.5362477855945733, 0.5223703198389735], "final_y": [0.36283982441988116, 0.369757122266153, 0.36125242155892334]}, "mutation_prompt": null}
{"id": "42304837-4317-449b-bee1-15ea644fa682", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.85  # Adjusted crossover probability for better performance\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Adjusted crossover probability for improved exploration-exploitation trade-off.", "configspace": "", "generation": 33, "fitness": 0.5831670183972831, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.583 with standard deviation 0.091. And the mean value of best solutions found was 0.351 (0. is the best) with standard deviation 0.055.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5230171553283021], "final_y": [0.27255660637199863, 0.3946714880547304, 0.38463536832576095]}, "mutation_prompt": null}
{"id": "9b1cae8a-2520-4b7b-ba22-3637a9158a18", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            self.population[i] = np.clip(a + np.random.uniform(-1.0, 1.0) * (b - c), bounds.lb, bounds.ub)  # Added randomness\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "HybridBraggOptimizer with dynamic hybrid mutation incorporating randomness for increased exploration.", "configspace": "", "generation": 34, "fitness": 0.5777267958063347, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.578 with standard deviation 0.095. And the mean value of best solutions found was 0.349 (0. is the best) with standard deviation 0.056.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.5210361461299952, 0.5000743200014495], "final_y": [0.27255660637199863, 0.3700039365291854, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "84308ca2-d4d9-4731-ae88-1e689a055602", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.9  # Crossover probability changed from 0.8 to 0.9 for better exploration\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improve exploration by using a larger adaptive crossover probability for differential evolution.", "configspace": "", "generation": 35, "fitness": 0.5693640688571631, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.569 with standard deviation 0.101. And the mean value of best solutions found was 0.341 (0. is the best) with standard deviation 0.049.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.49240376602854385, 0.5036185192553859], "final_y": [0.27255660637199863, 0.37108448421559936, 0.38044450891090853]}, "mutation_prompt": null}
{"id": "420351be-657c-42e6-a09f-cb905087eefe", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            self.cr = 0.5 + 0.5 * (func_counter / self.budget)  # Dynamic crossover probability\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved exploration by introducing a dynamic crossover probability based on iteration count.", "configspace": "", "generation": 36, "fitness": 0.5270963116375212, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.527 with standard deviation 0.041. And the mean value of best solutions found was 0.383 (0. is the best) with standard deviation 0.033.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.49666061503708203, 0.5845539998740321, 0.5000743200014495], "final_y": [0.4073425911909888, 0.3369483899097586, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "565e499d-1e64-466a-b7f9-f267a8a9afe4", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.5)  # Adaptive F with increased range for diversity\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved exploration by increasing the adaptive differential weight range for better solution diversity.", "configspace": "", "generation": 37, "fitness": 0.5876011800949751, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.588 with standard deviation 0.090. And the mean value of best solutions found was 0.341 (0. is the best) with standard deviation 0.051.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.706489519785739, 0.48896660990959984, 0.5673474105895864], "final_y": [0.2754085363429454, 0.4003031457323537, 0.34767949623362737]}, "mutation_prompt": null}
{"id": "431fe127-abc0-4b80-880a-9b63974c8fd8", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            self.cr = np.random.uniform(0.4, 0.9)  # Adaptive CR\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Introduce stochastic adaptive crossover probability in differential evolution for enhanced solution diversity.", "configspace": "", "generation": 38, "fitness": 0.5159086894550816, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.516 with standard deviation 0.016. And the mean value of best solutions found was 0.375 (0. is the best) with standard deviation 0.019.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.5330474568985621, 0.4944317040627624, 0.5202469074039202], "final_y": [0.35246667456481096, 0.3997473721422846, 0.37372485686484247]}, "mutation_prompt": null}
{"id": "fc2ff70c-c596-4448-a35b-6d75b8f72ff5", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        self.learning_rate = 0.1  # Added learning rate for adaptive changes\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 0.9)  # Adjusted adaptive F range\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def selective_local_search(self, bounds, func, step_size):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:  # Add stochastic selection for local search\n                candidate = self.population[i] + np.random.uniform(-step_size, step_size, self.dim)\n                candidate = np.clip(candidate, bounds.lb, bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98  # Adjusted dynamic adjustment\n                self.selective_local_search(bounds, func, self.learning_rate * self.local_search_radius)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "An enhanced HybridBraggOptimizer integrating adaptive learning rates and selective local search to improve convergence speed.", "configspace": "", "generation": 39, "fitness": 0.5817814311488158, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.582 with standard deviation 0.092. And the mean value of best solutions found was 0.339 (0. is the best) with standard deviation 0.051.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7109704848955116, 0.5045927459330841, 0.5297810626178514], "final_y": [0.27311614054884426, 0.39832043116152094, 0.344216223090531]}, "mutation_prompt": null}
{"id": "4d783556-c77c-4b67-8e3f-b0d67d4b7025", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.995  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "HybridBraggOptimizer with refined adaptive local search decay to enhance exploitation.", "configspace": "", "generation": 40, "fitness": 0.5926396411125506, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.593 with standard deviation 0.085. And the mean value of best solutions found was 0.307 (0. is the best) with standard deviation 0.028.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.5223209419985126, 0.5435280600515802], "final_y": [0.27255660637199863, 0.34051923284581986, 0.3084711124005267]}, "mutation_prompt": null}
{"id": "7d805e47-73de-42b4-866f-799b1a936dfc", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            self.cr = np.random.uniform(0.7, 0.9)  # Dynamic CR adjustment\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Introduced dynamic adjustment of crossover probability to enhance exploration-exploitation balance.", "configspace": "", "generation": 41, "fitness": 0.4998277967480023, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.500 with standard deviation 0.011. And the mean value of best solutions found was 0.399 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.49666061503708203, 0.4887423048434897, 0.5140804703634354], "final_y": [0.4073425911909888, 0.41379549904142665, 0.37545049849619627]}, "mutation_prompt": null}
{"id": "8a1932f8-fc5e-4872-8198-75a01d896544", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.cr = 0.8  \n        self.f = 0.8  \n        self.local_search_radius = 0.3  \n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < (0.5 + np.random.rand() * 0.5)  # Adaptive crossover probability\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def probabilistic_selection(self, scores):\n        probabilities = (scores - np.min(scores)) / (np.ptp(scores) + 1e-10)\n        selected_indices = np.random.choice(self.population_size, self.population_size, p=probabilities/np.sum(probabilities))\n        self.population = self.population[selected_indices]\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            self.evaluate_population(func)\n            scores = np.array([func(ind) for ind in self.population])  # Evaluate scores for selection\n            self.probabilistic_selection(scores)\n            \n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced HybridBraggOptimizer using probabilistic selection pressure and adaptive crossover for improved convergence.", "configspace": "", "generation": 42, "fitness": 0.5833858449093993, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.583 with standard deviation 0.089. And the mean value of best solutions found was 0.336 (0. is the best) with standard deviation 0.073.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7062879044101549, 0.5437953103165936, 0.5000743200014495], "final_y": [0.2343496716821658, 0.36719553427277773, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "bdf04c04-43f1-4e74-80ab-5d1e87af95c6", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99 * (1 + 0.01 * (self.best_score - 0.5))  # Dynamic adjustment based on best score\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved local search by adjusting the adaptive local search radius dynamically based on the current best solution's fitness.", "configspace": "", "generation": 43, "fitness": 0.5926397302808223, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.593 with standard deviation 0.085. And the mean value of best solutions found was 0.307 (0. is the best) with standard deviation 0.028.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.5223211617140093, 0.5435281078408982], "final_y": [0.27255660637199863, 0.34051783639538336, 0.3084710199180588]}, "mutation_prompt": null}
{"id": "3592085b-dc7e-4046-8649-c714baf219b2", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            self.cr = 0.5 + 0.4 * np.std(self.population)  # Dynamic adjustment based on diversity\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Introduced a dynamic crossover probability adjustment based on population diversity.  ", "configspace": "", "generation": 44, "fitness": 0.5699925031266199, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.570 with standard deviation 0.101. And the mean value of best solutions found was 0.356 (0. is the best) with standard deviation 0.061.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.4887423048434897, 0.5091652832488104], "final_y": [0.27255660637199863, 0.41379549904142665, 0.382343184963712]}, "mutation_prompt": null}
{"id": "f9a11805-6749-4e69-b97d-b15b45feeef7", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Incorporate adaptive mutation scaling based on generation progress for enhanced convergence.", "configspace": "", "generation": 45, "fitness": 0.5926442578738175, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.593 with standard deviation 0.085. And the mean value of best solutions found was 0.307 (0. is the best) with standard deviation 0.028.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.5223335181534001, 0.5435293341804928], "final_y": [0.27255660637199863, 0.3404393115324811, 0.308468646698681]}, "mutation_prompt": null}
{"id": "f83ad5bb-1b4a-4a1d-ab09-1de400408da9", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            self.cr = np.random.uniform(0.5, 1.0)  # Adaptive CR\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhance exploration-exploitation balance by dynamically adjusting crossover probability.", "configspace": "", "generation": 46, "fitness": 0.5054174606938021, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.505 with standard deviation 0.018. And the mean value of best solutions found was 0.376 (0. is the best) with standard deviation 0.050.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.49666061503708203, 0.4887423048434897, 0.5308494622008346], "final_y": [0.4073425911909888, 0.41379549904142665, 0.30551491836831557]}, "mutation_prompt": null}
{"id": "5f195a81-26f3-4eb0-a1ff-fa3a8233422b", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.cr = 0.9  # Increased crossover probability\n        self.f = 0.5  # Lower initial differential weight for diversity\n        self.local_search_radius = 0.3\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.3, 0.9)  # More adaptive F range\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def multi_strategy_mutation(self, bounds):\n        for i in range(self.population_size):\n            if i % 2 == 0:  # Alternate mutation strategies\n                idxs = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[idxs]\n                new_candidate = np.clip(a + np.random.uniform(0.2, 0.8) * (b - c), bounds.lb, bounds.ub)\n            else:\n                new_candidate = self.population[i] + np.random.normal(0, 0.1, self.dim)  # Gaussian mutation\n            self.population[i] = new_candidate\n\n    def fitness_scaling(self, scores):\n        min_score = np.min(scores)\n        scaled_scores = scores - min_score + 1e-5  # Avoid division by zero\n        return scaled_scores\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.multi_strategy_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.95  # More aggressive adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced HybridBraggOptimizer with multi-strategy mutation and fitness-scaling for improved solution diversity and convergence.", "configspace": "", "generation": 47, "fitness": 0.5380191924246033, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.538 with standard deviation 0.047. And the mean value of best solutions found was 0.366 (0. is the best) with standard deviation 0.033.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.5100677056601162, 0.6039155516122441, 0.5000743200014495], "final_y": [0.3692105750677812, 0.32384552399049393, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "29a0c046-da09-4c7e-9bbb-d9f4c3a9ebfa", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.cr = 0.9  # Dynamic increase\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved exploration by increasing the crossover probability dynamically over iterations.", "configspace": "", "generation": 48, "fitness": 0.5693640688571631, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.569 with standard deviation 0.101. And the mean value of best solutions found was 0.341 (0. is the best) with standard deviation 0.049.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.49240376602854385, 0.5036185192553859], "final_y": [0.27255660637199863, 0.37108448421559936, 0.38044450891090853]}, "mutation_prompt": null}
{"id": "aaef4826-08ab-45f5-94b3-674945ac7e06", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.cr = np.std(self.population) / np.mean(self.population)  # Dynamically adjust crossover probability\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved exploration-exploitation balance by dynamically adjusting crossover probability based on population diversity.", "configspace": "", "generation": 49, "fitness": 0.5144224497855316, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.514 with standard deviation 0.017. And the mean value of best solutions found was 0.380 (0. is the best) with standard deviation 0.031.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.49666061503708203, 0.5095572529131616, 0.5370494814063514], "final_y": [0.4073425911909888, 0.3957271581629983, 0.33628795308601556]}, "mutation_prompt": null}
{"id": "fd8c19a5-c9e4-42d3-9fb0-78e2ac83af6c", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = np.random.uniform(0.5, 1.0)  # Adaptive F\n            self.cr = np.random.uniform(0.7, 0.9)  # Adaptive CR\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "HybridBraggOptimizer with dynamic crossover probability adjustment for enhanced exploration-exploitation balance.", "configspace": "", "generation": 50, "fitness": 0.4998277967480023, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.500 with standard deviation 0.011. And the mean value of best solutions found was 0.399 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.49666061503708203, 0.4887423048434897, 0.5140804703634354], "final_y": [0.4073425911909888, 0.41379549904142665, 0.37545049849619627]}, "mutation_prompt": null}
{"id": "43e31a6d-1cff-4959-a000-66a8e05629ae", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.99, 0.9, self.population_size)\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.05\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced HybridBraggOptimizer utilizes adaptive mutation strategies and a novel convergence acceleration mechanism for improved optimization performance.", "configspace": "", "generation": 51, "fitness": 0.5998537492577488, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.600 with standard deviation 0.083. And the mean value of best solutions found was 0.338 (0. is the best) with standard deviation 0.050.", "error": "", "parent_id": "6ca462bf-b688-48d9-8b7d-9d54ba186247", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5730773479096994], "final_y": [0.27255660637199863, 0.3946714880547304, 0.3470602753577471]}, "mutation_prompt": null}
{"id": "e97c1eb6-26c9-4051-88a3-558c5d5eae40", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            self.cr = 0.6 + np.random.rand() * 0.4  # Adaptive CR\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.99, 0.9, self.population_size)\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.05\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Refined HybridBraggOptimizer incorporates an enhanced hybrid mutation with adaptive crossover probability to maximize solution diversity and convergence rates.", "configspace": "", "generation": 52, "fitness": 0.5141825312434, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.514 with standard deviation 0.031. And the mean value of best solutions found was 0.393 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "43e31a6d-1cff-4959-a000-66a8e05629ae", "metadata": {"aucs": [0.49666061503708203, 0.4887423048434897, 0.5571446738496283], "final_y": [0.4073425911909888, 0.41379549904142665, 0.3589827347726047]}, "mutation_prompt": null}
{"id": "5db39eec-ed31-4546-8bbf-d3f308b4d935", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.99, 0.9, self.population_size)\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced HybridBraggOptimizer with slight increase in convergence acceleration factor for improved optimization performance.", "configspace": "", "generation": 53, "fitness": 0.6570443560360145, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.657 with standard deviation 0.079. And the mean value of best solutions found was 0.295 (0. is the best) with standard deviation 0.042.", "error": "", "parent_id": "43e31a6d-1cff-4959-a000-66a8e05629ae", "metadata": {"aucs": [0.7120699212875593, 0.5459700227308752, 0.7130931240896088], "final_y": [0.27255660637199863, 0.35401830381695587, 0.2576989347376385]}, "mutation_prompt": null}
{"id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "A refined hybrid optimizer for black-box optimization with an improved local search radius adjustment.", "configspace": "", "generation": 54, "fitness": 0.6570511524997967, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.657 with standard deviation 0.079. And the mean value of best solutions found was 0.295 (0. is the best) with standard deviation 0.042.", "error": "", "parent_id": "5db39eec-ed31-4546-8bbf-d3f308b4d935", "metadata": {"aucs": [0.7120699212875593, 0.5459907481866756, 0.7130927880251551], "final_y": [0.27255660637199863, 0.3539930293636343, 0.25769936857673736]}, "mutation_prompt": null}
{"id": "ee5e042d-6fab-44ea-b6a5-615fe11ab1f6", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Adaptive F with improved exploration\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved adaptive step scaling in differential evolution for enhanced exploration.", "configspace": "", "generation": 55, "fitness": 0.6012686057003351, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.601 with standard deviation 0.076. And the mean value of best solutions found was 0.334 (0. is the best) with standard deviation 0.053.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.5867521852656694, 0.5159612558658906, 0.7010923759694454], "final_y": [0.3443313999572074, 0.3935475088258399, 0.26519454557712685]}, "mutation_prompt": null}
{"id": "9ba32100-b351-46db-9ecf-eb2a07d8817d", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.07  # Changed from 0.06 to 0.07\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "A refined hybrid optimizer for black-box optimization with an improved local search radius adjustment and convergence acceleration.", "configspace": "", "generation": 56, "fitness": 0.5755194066216656, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.576 with standard deviation 0.097. And the mean value of best solutions found was 0.357 (0. is the best) with standard deviation 0.060.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5000743200014495], "final_y": [0.27255660637199863, 0.3946714880547304, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "b09e8603-0d02-447b-94d5-190e98c20977", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.9  # Adjusted crossover probability\n        self.f = 0.6  # Adjusted differential weight\n        self.local_search_radius = 0.3  # Initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.3 + np.random.rand() * 0.7  # More varied adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 2 == 0:  # More frequent global search\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved exploration by varying adaptive parameters and emphasizing global search in early stages.", "configspace": "", "generation": 57, "fitness": 0.5332851937751802, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.533 with standard deviation 0.044. And the mean value of best solutions found was 0.380 (0. is the best) with standard deviation 0.030.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.5026688594229658, 0.5951862016802469, 0.5020005202223281], "final_y": [0.3990422687877324, 0.3379154168432613, 0.4027856450445134]}, "mutation_prompt": null}
{"id": "c14fed0b-78fc-4677-ad16-e2e754f2054c", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.85  # Slightly increased crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.35  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.6, 1.2) * (b - c), bounds.lb, bounds.ub)  # Refined mutation scaling\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced local search radius adjustment and refined mutation strategy for improved convergence.", "configspace": "", "generation": 58, "fitness": 0.5755194066216656, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.576 with standard deviation 0.097. And the mean value of best solutions found was 0.357 (0. is the best) with standard deviation 0.060.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5000743200014495], "final_y": [0.27255660637199863, 0.3946714880547304, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "1a2e1f75-5bd3-4ff5-bb35-980fcf91b417", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        beta = 0.1  # New parameter to enhance exploration\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor * (1 + beta), \n                                                              self.local_search_radius * radius_factor * (1 + beta), self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Introducing a new parameter `beta` to enhance exploration during local search for improved optimization.", "configspace": "", "generation": 59, "fitness": 0.6570390356888615, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.657 with standard deviation 0.079. And the mean value of best solutions found was 0.295 (0. is the best) with standard deviation 0.042.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.7120699212875593, 0.5459488690666647, 0.7130983167123605], "final_y": [0.27255660637199863, 0.3540441023264782, 0.2576958170029202]}, "mutation_prompt": null}
{"id": "e71dcbe0-7a94-45e5-a928-144280c06a5a", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < (self.cr + 0.1)  # Enhanced crossover\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Refined adaptive differential evolution with enhanced crossover strategy to improve convergence.", "configspace": "", "generation": 60, "fitness": 0.6172705590221251, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.617 with standard deviation 0.074. And the mean value of best solutions found was 0.322 (0. is the best) with standard deviation 0.039.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.7120699212875593, 0.5322484692912972, 0.6074932864875187], "final_y": [0.27255660637199863, 0.36821851721781207, 0.3242670940367183]}, "mutation_prompt": null}
{"id": "96148d1c-10bd-403e-abb3-73230531861e", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.4 + np.random.rand() * 0.6  # Slightly widened range for adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Introduced slight alteration in mutation strategy to enhance exploration by widening differential weight range.", "configspace": "", "generation": 61, "fitness": 0.5964995066169653, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.596 with standard deviation 0.089. And the mean value of best solutions found was 0.338 (0. is the best) with standard deviation 0.061.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.5575216112173953, 0.5127632073155456, 0.7192137013179547], "final_y": [0.3636271652831693, 0.39587418736689506, 0.2542740135179764]}, "mutation_prompt": null}
{"id": "d656af02-ab7f-4dc5-a2cf-4e14b558d848", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.normal(0, 0.1, self.dim)  # Changed from uniform to normal distribution\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced convergence acceleration by refining the perturbation strategy.", "configspace": "", "generation": 62, "fitness": 0.5755194066216656, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.576 with standard deviation 0.097. And the mean value of best solutions found was 0.357 (0. is the best) with standard deviation 0.060.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5000743200014495], "final_y": [0.27255660637199863, 0.3946714880547304, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "52c50b03-3a77-4775-a133-02b4e9867fff", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand()  # Adaptive F with expanded range\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced mutation strategy by expanding the range of the scaling factor for diversity.", "configspace": "", "generation": 63, "fitness": 0.6007667930810331, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.601 with standard deviation 0.086. And the mean value of best solutions found was 0.335 (0. is the best) with standard deviation 0.051.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.706489519785739, 0.4951229470193037, 0.6006879124380566], "final_y": [0.2754085363429454, 0.3997604377583375, 0.3288739385361452]}, "mutation_prompt": null}
{"id": "0a0c62d0-aee8-4bc5-9917-a848111f5f85", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.97, 0.87, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced exploitation phase by slightly increasing the local search radius factor to explore promising areas more thoroughly.", "configspace": "", "generation": 64, "fitness": 0.6570483564630358, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.657 with standard deviation 0.079. And the mean value of best solutions found was 0.295 (0. is the best) with standard deviation 0.042.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.7120699212875593, 0.545981479285087, 0.7130936688164613], "final_y": [0.27255660637199863, 0.3540043324587998, 0.25769877183091694]}, "mutation_prompt": null}
{"id": "8e610028-6f76-4663-bdb8-1416344722a6", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.85  # Crossover probability - slightly increased\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced crossover probability to improve exploration and convergence in the search process.", "configspace": "", "generation": 65, "fitness": 0.575835328391366, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.576 with standard deviation 0.097. And the mean value of best solutions found was 0.353 (0. is the best) with standard deviation 0.058.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.7120699212875593, 0.5153617438850892, 0.5000743200014495], "final_y": [0.27255660637199863, 0.38268185398075427, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "9d3c7ce9-5934-4fff-bd05-77c481a638fe", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.08  # Enhanced acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced convergence acceleration factor for improved solution exploration and exploitation balance.", "configspace": "", "generation": 66, "fitness": 0.5755194066216656, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.576 with standard deviation 0.097. And the mean value of best solutions found was 0.357 (0. is the best) with standard deviation 0.060.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5000743200014495], "final_y": [0.27255660637199863, 0.3946714880547304, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "f3a2772f-59a6-4e22-acc8-588ccacdf4d4", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.cr = 0.8\n        self.f = 0.8\n        self.local_search_radius = 0.3\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.cr = 0.7 + 0.3 * np.random.rand()  # Adaptive CR\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            phase_shift = np.random.uniform(-0.1, 0.1, self.dim)  # Phase-guided mutation\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c) + phase_shift, bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Introducing a phase-guided mutation and adaptive crossover strategy to enhance exploration and convergence.", "configspace": "", "generation": 67, "fitness": 0.49629579178650723, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.496 with standard deviation 0.006. And the mean value of best solutions found was 0.408 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.49666061503708203, 0.4887423048434897, 0.5034844554789502], "final_y": [0.4073425911909888, 0.41379549904142665, 0.4019651565865232]}, "mutation_prompt": null}
{"id": "22be7992-26c9-4245-bf39-8b13b3af4574", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.9  # Crossover probability increased for diversity\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            adaptive_factor = np.random.rand() * 0.5  # Adaptive scaling factor\n            self.f = 0.5 + adaptive_factor  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.9, 0.85, self.population_size)  # Adjusted range lower-bound for improved search precision\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.07  # Slight increase in acceleration factor for faster convergence\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98  # Faster dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "The refined optimizer employs adaptive mutation scaling and an elitist local search to enhance performance and convergence in black-box optimization.", "configspace": "", "generation": 68, "fitness": 0.5670635902104996, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.567 with standard deviation 0.103. And the mean value of best solutions found was 0.350 (0. is the best) with standard deviation 0.058.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.7120699212875593, 0.4887423048434897, 0.5003785445004497], "final_y": [0.27255660637199863, 0.41379549904142665, 0.3630473773117756]}, "mutation_prompt": null}
{"id": "4e67df05-caf3-4309-bc94-edeb7f27fb0a", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            self.cr = 0.6 + np.random.rand() * 0.4  # Dynamic crossover probability\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved exploitation through dynamic crossover and mutation diversification for enhanced local search efficiency.", "configspace": "", "generation": 69, "fitness": 0.49699711446825495, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.497 with standard deviation 0.002. And the mean value of best solutions found was 0.400 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.49666061503708203, 0.4942564083662334, 0.5000743200014495], "final_y": [0.4073425911909888, 0.38865246867452585, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "8580c48b-3150-4d67-a86c-4c2007f720c8", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted perturbation range\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced convergence acceleration by adjusting the perturbation range for faster convergence.", "configspace": "", "generation": 70, "fitness": 0.6570417879458886, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.657 with standard deviation 0.079. And the mean value of best solutions found was 0.295 (0. is the best) with standard deviation 0.042.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.7120699212875593, 0.5459515339735914, 0.7131039085765151], "final_y": [0.27255660637199863, 0.35404085216537795, 0.2576934513586815]}, "mutation_prompt": null}
{"id": "1936e55a-8d87-4e8a-9b3d-025caf3f4ef0", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            self.cr = 0.5 + np.random.rand() * 0.5  # Adaptive CR\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Introduced adaptive crossover probability to improve exploration and exploitation balance.", "configspace": "", "generation": 71, "fitness": 0.5013773912651961, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.501 with standard deviation 0.012. And the mean value of best solutions found was 0.382 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.4974206623930386, 0.4887423048434897, 0.51796920655906], "final_y": [0.34465184021682627, 0.41379549904142665, 0.3890287750311079]}, "mutation_prompt": null}
{"id": "d231d96a-0e44-49bd-b2a8-9dc79f1f717a", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.995  # Dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced local search efficiency via strategic adjustment of the local search radius decrement rate.", "configspace": "", "generation": 72, "fitness": 0.6570508115670081, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.657 with standard deviation 0.079. And the mean value of best solutions found was 0.295 (0. is the best) with standard deviation 0.042.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.7120699212875593, 0.5459800423277905, 0.7131024710856741], "final_y": [0.27255660637199863, 0.3540060848092701, 0.2576940410383062]}, "mutation_prompt": null}
{"id": "7e71e752-7e07-4fd6-925a-85ff046804f8", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Introducing a slight randomness in the local search radius adjustment to enhance exploration.", "configspace": "", "generation": 73, "fitness": 0.6627781869819857, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.663 with standard deviation 0.071. And the mean value of best solutions found was 0.283 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "072411dc-dd64-4eb6-87ae-e1264e9dce4e", "metadata": {"aucs": [0.7120699212875593, 0.5630576858410994, 0.7132069538172983], "final_y": [0.27255660637199863, 0.31849741626007466, 0.2576570827632988]}, "mutation_prompt": null}
{"id": "79b0db1a-4472-4af3-a609-8dae564b3be2", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06 + np.random.uniform(-0.005, 0.005)  # Introduce slight randomness\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Introduce randomness in convergence acceleration factor to enhance exploration while maintaining convergence speed.", "configspace": "", "generation": 74, "fitness": 0.6016079340968803, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.602 with standard deviation 0.081. And the mean value of best solutions found was 0.335 (0. is the best) with standard deviation 0.047.", "error": "", "parent_id": "7e71e752-7e07-4fd6-925a-85ff046804f8", "metadata": {"aucs": [0.7120699212875593, 0.5199306049047037, 0.572823276098378], "final_y": [0.27255660637199863, 0.38517994496063557, 0.34727493016732247]}, "mutation_prompt": null}
{"id": "1b618ac0-02ea-4f3d-832a-1866800b1dac", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.04 + np.random.uniform(0, 0.04)  # More dynamic acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Incorporate a more dynamic convergence acceleration factor to enhance exploration and exploitation balance.", "configspace": "", "generation": 75, "fitness": 0.5997690586539751, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.600 with standard deviation 0.083. And the mean value of best solutions found was 0.338 (0. is the best) with standard deviation 0.050.", "error": "", "parent_id": "7e71e752-7e07-4fd6-925a-85ff046804f8", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.572823276098378], "final_y": [0.27255660637199863, 0.3946714880547304, 0.34727493016732247]}, "mutation_prompt": null}
{"id": "d87d53d1-1456-4152-9e8a-f69262721cd6", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            self.cr = 0.7 + np.random.rand() * 0.2  # Adaptive crossover probability\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Adjust the crossover probability adaptively to enhance exploration and exploitation balance.", "configspace": "", "generation": 76, "fitness": 0.5232717736372258, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.523 with standard deviation 0.026. And the mean value of best solutions found was 0.368 (0. is the best) with standard deviation 0.034.", "error": "", "parent_id": "7e71e752-7e07-4fd6-925a-85ff046804f8", "metadata": {"aucs": [0.5104797297866072, 0.5592612711236209, 0.5000743200014495], "final_y": [0.37574593484511654, 0.32351669229837876, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "3f8ab5da-79b3-4a68-89c2-87a25a670508", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.cr = 0.9  # Increased crossover probability to enhance diversification\n        self.f = 0.8\n        self.local_search_radius = 0.3\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n    \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.4 + np.random.rand() * 0.6  # Broader range for adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                               self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.07  # Slightly increased acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98 + np.random.uniform(-0.02, 0.02)  # More dynamic adjustment\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced exploration and exploitation balance by introducing dynamic crossover rates and adaptive search radius adjustment.", "configspace": "", "generation": 77, "fitness": 0.5274732548506863, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.527 with standard deviation 0.024. And the mean value of best solutions found was 0.376 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "7e71e752-7e07-4fd6-925a-85ff046804f8", "metadata": {"aucs": [0.5582930549280616, 0.524052389622548, 0.5000743200014495], "final_y": [0.3518372227608716, 0.3701953038305571, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "9c043bb7-417f-46c4-9525-d7c09a6288b6", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Reduced initial population size for efficiency\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Adaptive F with increased variance\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.9, 0.8, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 4, replace=False)\n            a, b, c, d = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c + d), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.08  # Further increased acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 2 == 0:  # Adjusted pattern for mutation\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98 + np.random.uniform(-0.02, 0.02)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Introduced adaptive population size and improved local search strategy with dynamic exploration-exploitation balance.", "configspace": "", "generation": 78, "fitness": 0.4632111557791962, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.463 with standard deviation 0.034. And the mean value of best solutions found was 0.434 (0. is the best) with standard deviation 0.027.", "error": "", "parent_id": "7e71e752-7e07-4fd6-925a-85ff046804f8", "metadata": {"aucs": [0.41584610300173286, 0.4887423048434897, 0.4850450594923661], "final_y": [0.47239265783783124, 0.41379549904142665, 0.41645432498274326]}, "mutation_prompt": null}
{"id": "0539d2d2-0be2-43a3-a893-a80e241a1f93", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 12 * dim  # Increased population size\n        self.cr = 0.9  # Slightly increased crossover probability\n        self.f = 0.7  # Adjusted differential weight\n        self.local_search_radius = 0.25  # Fine-tuned radius for local search\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n        self.chaotic_factor = np.random.rand()  # Initialize chaotic factor\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + self.chaotic_map() * 0.5  # Chaotic adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def chaotic_map(self):\n        self.chaotic_factor = (self.chaotic_factor * 3.57) % 1  # Logistic map\n        return self.chaotic_factor\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.9, 0.8, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.07  # Adjusted acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhancing exploration by introducing chaotic maps and adaptive strategies for parameter control.", "configspace": "", "generation": 79, "fitness": 0.553442057729161, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.553 with standard deviation 0.019. And the mean value of best solutions found was 0.354 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "7e71e752-7e07-4fd6-925a-85ff046804f8", "metadata": {"aucs": [0.5442896940794202, 0.5360615640040622, 0.5799749151040007], "final_y": [0.3706314376317006, 0.34169340930484915, 0.34954404120482696]}, "mutation_prompt": null}
{"id": "16a33392-9ced-4e03-a8b1-7ca7f43eba95", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.cr = 0.8\n        self.f = 0.8\n        self.local_search_radius = 0.3\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            stochastic_perturbation = np.random.uniform(-0.05, 0.05, self.dim)  # Added line\n            new_candidate += stochastic_perturbation  # Added line\n            new_candidate = np.clip(new_candidate, bounds.lb, bounds.ub)  # Modified line\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99 + np.random.uniform(-0.01, 0.01)\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Integrating stochastic perturbations to increase exploration and escape local optima.", "configspace": "", "generation": 80, "fitness": 0.5812653162572069, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.581 with standard deviation 0.093. And the mean value of best solutions found was 0.350 (0. is the best) with standard deviation 0.055.", "error": "", "parent_id": "7e71e752-7e07-4fd6-925a-85ff046804f8", "metadata": {"aucs": [0.7120699212875593, 0.5265440256998238, 0.5051820017842374], "final_y": [0.27255660637199863, 0.38060395054454466, 0.3962676108745772]}, "mutation_prompt": null}
{"id": "d85f53a7-3a3f-46bc-978b-bf6c4502e787", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98, 0.88, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Adjust the search radius factor range in local search to improve exploration-exploitation balance.", "configspace": "", "generation": 81, "fitness": 0.6627927073912717, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.663 with standard deviation 0.070. And the mean value of best solutions found was 0.283 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "7e71e752-7e07-4fd6-925a-85ff046804f8", "metadata": {"aucs": [0.7120699212875593, 0.5630957991537856, 0.7132124017324701], "final_y": [0.27255660637199863, 0.31844390760869723, 0.25765457445762463]}, "mutation_prompt": null}
{"id": "3b455d98-a276-41bb-a83d-4bbf644bf02f", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.55 + np.random.rand() * 0.4  # Adaptive F, minor adjustment\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98, 0.88, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Introduce a minor adjustment in the mutation factor to fine-tune exploration and exploitation balance.", "configspace": "", "generation": 82, "fitness": 0.6394220490807959, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.639 with standard deviation 0.074. And the mean value of best solutions found was 0.297 (0. is the best) with standard deviation 0.030.", "error": "", "parent_id": "d85f53a7-3a3f-46bc-978b-bf6c4502e787", "metadata": {"aucs": [0.6871491606969309, 0.5352967216167182, 0.6958202649287386], "final_y": [0.28552558152436536, 0.33763320721750345, 0.26744914218640636]}, "mutation_prompt": null}
{"id": "6025fc3c-41ac-4a89-a49e-b333af2d5bf6", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c) + np.random.uniform(-0.1, 0.1, self.dim), bounds.lb, bounds.ub)  # Change\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98, 0.88, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Introduced a random factor to the adaptive differential evolution step to enhance exploration and prevent premature convergence.", "configspace": "", "generation": 83, "fitness": 0.5217321458490104, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.522 with standard deviation 0.017. And the mean value of best solutions found was 0.381 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "d85f53a7-3a3f-46bc-978b-bf6c4502e787", "metadata": {"aucs": [0.5240916590908373, 0.5410304584547446, 0.5000743200014495], "final_y": [0.3840452814553109, 0.3540869799853098, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "856f7710-bbed-41ed-9ef5-7bbc4ccdf11a", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98 + np.random.uniform(-0.02, 0.02), \n                                           0.88 + np.random.uniform(-0.02, 0.02), self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.99 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhance the adaptiveness of local search by introducing stochastic adjustment to search radius range.", "configspace": "", "generation": 84, "fitness": 0.6460900208797274, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.646 with standard deviation 0.093. And the mean value of best solutions found was 0.309 (0. is the best) with standard deviation 0.061.", "error": "", "parent_id": "d85f53a7-3a3f-46bc-978b-bf6c4502e787", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.711786162775635], "final_y": [0.27255660637199863, 0.3946714880547304, 0.25843533444637146]}, "mutation_prompt": null}
{"id": "c3e923a6-9927-46d2-ad84-0362583448d6", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98, 0.88, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved the local search radius decay rate to enhance exploitation in later stages of the optimization.", "configspace": "", "generation": 85, "fitness": 0.6628123283585454, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.663 with standard deviation 0.070. And the mean value of best solutions found was 0.283 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "d85f53a7-3a3f-46bc-978b-bf6c4502e787", "metadata": {"aucs": [0.7120699212875593, 0.5631563625203074, 0.7132107012677694], "final_y": [0.27255660637199863, 0.3183588990102494, 0.2576552856871197]}, "mutation_prompt": null}
{"id": "654fc375-64b4-4ffd-a12e-4bd43e9c829b", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adjusted Adaptive F for improved balance\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98, 0.88, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced the balance between exploration and exploitation by adjusting the adaptive differential weight scaling.", "configspace": "", "generation": 86, "fitness": 0.5695231094414133, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.570 with standard deviation 0.081. And the mean value of best solutions found was 0.349 (0. is the best) with standard deviation 0.054.", "error": "", "parent_id": "c3e923a6-9927-46d2-ad84-0362583448d6", "metadata": {"aucs": [0.49740572487984414, 0.5286822756827234, 0.6824813277616724], "final_y": [0.40390669496934406, 0.36759792521745827, 0.276483060905682]}, "mutation_prompt": null}
{"id": "2a9e3cda-047c-457e-858c-bad62dc625eb", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.85  # Adjusted crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98, 0.88, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Fine-tune the crossover probability to enhance the exploration-exploitation balance for better convergence.", "configspace": "", "generation": 87, "fitness": 0.5755194066216656, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.576 with standard deviation 0.097. And the mean value of best solutions found was 0.357 (0. is the best) with standard deviation 0.060.", "error": "", "parent_id": "c3e923a6-9927-46d2-ad84-0362583448d6", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5000743200014495], "final_y": [0.27255660637199863, 0.3946714880547304, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "493eafd7-f94e-4025-b75d-ff084689147b", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98, 0.88, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved the local search radius decay rate to enhance exploitation in later stages of the optimization.", "configspace": "", "generation": 86, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "c3e923a6-9927-46d2-ad84-0362583448d6", "metadata": {"aucs": [0.7120699212875593, 0.5631563625203074, 0.7132107012677694], "final_y": [0.27255660637199863, 0.3183588990102494, 0.2576552856871197]}, "mutation_prompt": null}
{"id": "4407ca3f-88cc-4e55-a453-7f05e203913b", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.cr = 0.7 + np.random.rand() * 0.3  # Adjusted crossover probability\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98, 0.88, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhance exploitation by adjusting the crossover probability during differential evolution steps.", "configspace": "", "generation": 89, "fitness": 0.5319949471917395, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.532 with standard deviation 0.053. And the mean value of best solutions found was 0.383 (0. is the best) with standard deviation 0.037.", "error": "", "parent_id": "c3e923a6-9927-46d2-ad84-0362583448d6", "metadata": {"aucs": [0.6071682167302794, 0.4887423048434897, 0.5000743200014495], "final_y": [0.3305492756430477, 0.41379549904142665, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "10312ddf-c17f-4bd1-8aed-50c5ba7beeed", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 12 * dim  # Increased population size for diversity\n        self.cr = 0.9  # Increased crossover probability\n        self.f = 0.7  # Slightly reduced differential weight\n        self.local_search_radius = 0.4  # Increased initial radius for enhanced local exploration\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Wider adaptive F range\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  \n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.4, 0.9) * (b - c), bounds.lb, bounds.ub)  # Adjusted factors\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.08  # Increased acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.97 + np.random.uniform(-0.02, 0.02)  # Adjusted randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "The algorithm refines local exploration and adaptive differential evolution to enhance convergence and diversity.", "configspace": "", "generation": 90, "fitness": 0.5482315940217509, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.548 with standard deviation 0.042. And the mean value of best solutions found was 0.371 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "c3e923a6-9927-46d2-ad84-0362583448d6", "metadata": {"aucs": [0.49666061503708203, 0.5485253517668502, 0.5995088152613206], "final_y": [0.4073425911909888, 0.36757811505057125, 0.3368376117662111]}, "mutation_prompt": null}
{"id": "e509c377-8e3a-462f-8a1a-555da1e3f625", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.85, self.population_size)  # Adjusted range\n        adaptive_learning_rate = 0.02 * (self.budget - len(self.population)) / self.budget\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + adaptive_learning_rate * np.random.uniform(\n                -self.local_search_radius * radius_factor, \n                self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.6, 1.2) * (b - c), bounds.lb, bounds.ub)  # Adjusted range\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced balance between exploration and exploitation by refining the mutation strategy and integrating adaptive learning rate for local search.", "configspace": "", "generation": 91, "fitness": 0.5859354044656059, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.586 with standard deviation 0.089. And the mean value of best solutions found was 0.349 (0. is the best) with standard deviation 0.054.", "error": "", "parent_id": "c3e923a6-9927-46d2-ad84-0362583448d6", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5313223135332703], "final_y": [0.27255660637199863, 0.3946714880547304, 0.3791053256691086]}, "mutation_prompt": null}
{"id": "df4af98c-051e-48e9-bb8f-ed4c49a7b665", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98, 0.88, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.55, 1.05) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Refined mutation process by slightly increasing the combination range of selected individuals.", "configspace": "", "generation": 92, "fitness": 0.6386539550149047, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.639 with standard deviation 0.075. And the mean value of best solutions found was 0.311 (0. is the best) with standard deviation 0.047.", "error": "", "parent_id": "c3e923a6-9927-46d2-ad84-0362583448d6", "metadata": {"aucs": [0.7120699212875593, 0.5361079775195221, 0.667783966237633], "final_y": [0.27255660637199863, 0.3768184855561637, 0.28438246138294987]}, "mutation_prompt": null}
{"id": "5fa869b5-aa7d-4858-bd6f-b6b6578df937", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98, 0.88, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.985 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced exploitation by slightly increasing the local search radius decay rate to fine-tune convergence.", "configspace": "", "generation": 93, "fitness": 0.6628026419015521, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.663 with standard deviation 0.070. And the mean value of best solutions found was 0.283 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "c3e923a6-9927-46d2-ad84-0362583448d6", "metadata": {"aucs": [0.7120699212875593, 0.5631264530169517, 0.7132115514001451], "final_y": [0.27255660637199863, 0.3184008780697416, 0.2576549299863883]}, "mutation_prompt": null}
{"id": "c31dc4a8-4fe2-40bf-8ce2-88b7b85c0b3f", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98, 0.88, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.cr = 0.7 + 0.3 * np.std(self.population) / np.mean(self.population)  # Dynamic crossover probability\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced solution diversity by adjusting crossover probability dynamically based on population variance.", "configspace": "", "generation": 94, "fitness": 0.6764691034476383, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.676 with standard deviation 0.051. And the mean value of best solutions found was 0.281 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "c3e923a6-9927-46d2-ad84-0362583448d6", "metadata": {"aucs": [0.7120699212875593, 0.6041266877875862, 0.7132107012677694], "final_y": [0.27255660637199863, 0.31301402557236313, 0.2576552856871197]}, "mutation_prompt": null}
{"id": "ca379f33-b1e7-4ef1-bf42-09ebf121cda2", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.95, 0.88, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.cr = 0.7 + 0.3 * np.std(self.population) / np.mean(self.population)  # Dynamic crossover probability\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced search efficiency by refining the local search strategy, reducing the search radius factor decay rate from 0.98 to 0.95.", "configspace": "", "generation": 95, "fitness": 0.6764547175953695, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.676 with standard deviation 0.051. And the mean value of best solutions found was 0.281 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "c31dc4a8-4fe2-40bf-8ce2-88b7b85c0b3f", "metadata": {"aucs": [0.7120699212875593, 0.6040821146086292, 0.7132121168899199], "final_y": [0.27255660637199863, 0.313050077844201, 0.2576542644088544]}, "mutation_prompt": null}
{"id": "ff355005-4fd0-4035-bce3-f16f0926ce32", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98, 0.88, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c) + np.random.uniform(-0.05, 0.05, self.dim), bounds.lb, bounds.ub)  # stochastic perturbation added\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.cr = 0.7 + 0.3 * np.std(self.population) / np.mean(self.population)  # Dynamic crossover probability\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced search dynamics by incorporating stochastic perturbation into the mutation process.", "configspace": "", "generation": 96, "fitness": 0.592550439518738, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.593 with standard deviation 0.085. And the mean value of best solutions found was 0.339 (0. is the best) with standard deviation 0.047.", "error": "", "parent_id": "c31dc4a8-4fe2-40bf-8ce2-88b7b85c0b3f", "metadata": {"aucs": [0.7120699212875593, 0.5209414060445772, 0.5446399912240777], "final_y": [0.27255660637199863, 0.3762195742811121, 0.3669077202805213]}, "mutation_prompt": null}
{"id": "56c6ca10-4b16-459b-933f-e894dbfbb303", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic to determine population size\n        self.cr = 0.8  # Crossover probability\n        self.f = 0.8  # Differential weight\n        self.local_search_radius = 0.3  # Increased initial radius for local search exploitation\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Adaptive F\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98, 0.88, self.population_size)  # Adjusted range\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.06  # Slight increase in acceleration factor\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98 + np.random.uniform(-0.01, 0.01)  # Dynamic adjustment with randomness\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.cr = 0.7 + 0.3 * np.std(self.population) / np.mean(self.population)  # Dynamic crossover probability\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced solution diversity by adjusting crossover probability dynamically based on population variance.", "configspace": "", "generation": 95, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "c31dc4a8-4fe2-40bf-8ce2-88b7b85c0b3f", "metadata": {"aucs": [0.7120699212875593, 0.6041266877875862, 0.7132107012677694], "final_y": [0.27255660637199863, 0.31301402557236313, 0.2576552856871197]}, "mutation_prompt": null}
{"id": "011f8f92-6de0-4b8c-af55-6820f76973bf", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.cr = 0.8\n        self.f = 0.8\n        self.local_search_radius = 0.3\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.4 + np.random.rand() * 0.6  # Adapted F range\n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.98, 0.88, self.population_size)\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.4, 1.2) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.07  # Slightly higher acceleration\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.15, 0.15, self.dim)  # Wider perturbation range\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.98 + np.random.uniform(-0.01, 0.01)\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.cr = 0.7 + 0.3 * np.std(self.population) / np.mean(self.population)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Improved exploration and convergence by integrating a mutation scaling factor and different local search strategies.", "configspace": "", "generation": 98, "fitness": 0.5234776928097624, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.523 with standard deviation 0.025. And the mean value of best solutions found was 0.388 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "c31dc4a8-4fe2-40bf-8ce2-88b7b85c0b3f", "metadata": {"aucs": [0.5575216112173953, 0.5128371472104425, 0.5000743200014495], "final_y": [0.3636271652831693, 0.39413048630966785, 0.4051421831226424]}, "mutation_prompt": null}
{"id": "a67ba7b4-655a-4afb-949e-ef480f261181", "solution": "import numpy as np\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.cr = 0.8\n        self.f = 0.8\n        self.local_search_radius = 0.3\n        self.population = None\n        self.best_solution = None\n        self.best_score = float('-inf')\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmax(scores)\n        if scores[best_idx] > self.best_score:\n            self.best_score = scores[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n\n    def adaptive_differential_evolution_step(self, bounds, func):\n        new_population = np.zeros_like(self.population)\n        for i in range(self.population_size):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5 \n            mutant = np.clip(a + self.f * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n            if func(trial) > func(self.population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = self.population[i]\n        self.population = new_population\n\n    def local_search(self, bounds, func):\n        search_radius_factor = np.linspace(0.96, 0.9, self.population_size)\n        for i, radius_factor in enumerate(search_radius_factor):\n            candidate = self.population[i] + np.random.uniform(-self.local_search_radius * radius_factor, \n                                                              self.local_search_radius * radius_factor, self.dim)\n            candidate = np.clip(candidate, bounds.lb, bounds.ub)\n            if func(candidate) > func(self.population[i]):\n                self.population[i] = candidate\n\n    def hybrid_mutation(self, bounds):\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            new_candidate = np.clip(a + np.random.uniform(0.5, 1.0) * (b - c), bounds.lb, bounds.ub)\n            self.population[i] = new_candidate\n\n    def convergence_acceleration(self, func):\n        acceleration_factor = 0.08\n        for i in range(self.population_size):\n            if np.random.random() < acceleration_factor:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = self.population[i] + perturbation\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n                if func(candidate) > func(self.population[i]):\n                    self.population[i] = candidate\n\n    def __call__(self, func):\n        func_counter = 0\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        self.evaluate_population(func)\n        func_counter += self.population_size\n\n        while func_counter < self.budget:\n            if func_counter % 3 == 0:\n                self.hybrid_mutation(bounds)\n            else:\n                self.adaptive_differential_evolution_step(bounds, func)\n            func_counter += self.population_size\n            if func_counter < self.budget:\n                self.local_search_radius *= 0.985\n                self.local_search(bounds, func)\n                func_counter += self.population_size\n                self.convergence_acceleration(func)\n            self.cr = 0.6 + 0.4 * np.std(self.population) / np.mean(self.population)\n            self.evaluate_population(func)\n\n        return self.best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced exploration and exploitation by introducing a multi-strategy approach combining perturbation tactics and using an adaptive learning rate.", "configspace": "", "generation": 99, "fitness": 0.5755194066216656, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.576 with standard deviation 0.097. And the mean value of best solutions found was 0.357 (0. is the best) with standard deviation 0.060.", "error": "", "parent_id": "c31dc4a8-4fe2-40bf-8ce2-88b7b85c0b3f", "metadata": {"aucs": [0.7120699212875593, 0.5144139785759878, 0.5000743200014495], "final_y": [0.27255660637199863, 0.3946714880547304, 0.4051421831226424]}, "mutation_prompt": null}
