{"id": "23599a47-85b6-44fd-b756-7803503622b9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.3  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution with a periodicity-enforcing penalty and local refinement using BFGS for efficient exploration and exploitation of black-box optimization landscapes. ", "configspace": "", "generation": 0, "fitness": 0.913518947032164, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.914 with standard deviation 0.044. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8790000474979267, 0.8864112757139847, 0.9751455178845805], "final_y": [0.164859231287261, 0.16485648968423472, 0.16485692415691455]}, "mutation_prompt": null}
{"id": "f7073ea4-e4c3-49b1-8a55-065413ea1350", "solution": "# Description: Improved hybrid metaheuristic by adjusting local search probability for a better balance between exploration and exploitation.\n# Code:\nimport numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Improved hybrid metaheuristic by adjusting local search probability for a better balance between exploration and exploitation.", "configspace": "", "generation": 1, "fitness": 0.9212149264483168, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.921 with standard deviation 0.042. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "23599a47-85b6-44fd-b756-7803503622b9", "metadata": {"aucs": [0.8728877471903437, 0.9156109022683409, 0.9751461298862661], "final_y": [0.16485761838722823, 0.16485648968423472, 0.16485625102240975]}, "mutation_prompt": null}
{"id": "f9f574b4-0986-470e-8acb-2c7656a4130b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget)  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - num_evals / self.budget)  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic with adaptive mutation and periodic penalty scaling for improved exploration and exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.9411945360967361, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.941 with standard deviation 0.031. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f7073ea4-e4c3-49b1-8a55-065413ea1350", "metadata": {"aucs": [0.899265539231209, 0.9491374155205389, 0.9751806535384601], "final_y": [0.16485668935277353, 0.16485613042875458, 0.16485646945990529]}, "mutation_prompt": null}
{"id": "c1b8bc04-5b59-4709-9c84-12a29804844c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.85  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget)  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - num_evals / self.budget)  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Refined metaheuristic optimization by slightly increasing the mutation factor for broader exploration and improving convergence speed.", "configspace": "", "generation": 3, "fitness": 0.9352274405934543, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.037. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f9f574b4-0986-470e-8acb-2c7656a4130b", "metadata": {"aucs": [0.8858856886619588, 0.9453114739835755, 0.9744851591348286], "final_y": [0.16485591486892748, 0.16485583910949964, 0.16485691024900084]}, "mutation_prompt": null}
{"id": "cf77abc9-1d37-465d-bc56-5a6971c8968f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.3 + 0.2 * (np.random.rand())  # Adjusted dynamic probability for local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget)  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) * 0.9  # Adaptive penalty refinement\n                trial_fitness = func(trial) + periodicity_penalty * (1 - num_evals / self.budget)  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic with dynamic local search probability and adaptive periodicity penalty refinement for better optimization performance.", "configspace": "", "generation": 4, "fitness": 0.9189721699339012, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.919 with standard deviation 0.027. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f9f574b4-0986-470e-8acb-2c7656a4130b", "metadata": {"aucs": [0.9409926757349291, 0.8809497366319157, 0.9349740974348586], "final_y": [0.1648577577362974, 0.1648569828034825, 0.16485649688701098]}, "mutation_prompt": null}
{"id": "3e311359-466a-41ac-b1fb-b0ec629a486d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget)  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - num_evals / self.budget)  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                self.local_search_probability = 0.5 * (1 - num_evals / self.budget)  # Dynamic local search probability\n                if np.random.rand() < self.local_search_probability:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic with adaptive mutation, periodic penalty scaling, and dynamic local search probability for improved exploration and exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.939751692392563, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.940 with standard deviation 0.032. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f9f574b4-0986-470e-8acb-2c7656a4130b", "metadata": {"aucs": [0.8973817445575429, 0.9466926790816856, 0.9751806535384601], "final_y": [0.16485599379768723, 0.164856520950174, 0.16485646945990529]}, "mutation_prompt": null}
{"id": "2b68e77f-5b3a-46bc-bc78-d0f2b54d7827", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget)  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - num_evals / self.budget)  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search with dynamic probability\n                dynamic_local_search_prob = self.local_search_probability * (1 - num_evals / self.budget)\n                if np.random.rand() < dynamic_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic with adaptive mutation, periodic penalty scaling, and dynamic local search probability for improved exploration-exploitation balance.", "configspace": "", "generation": 6, "fitness": 0.939751692392563, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.940 with standard deviation 0.032. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f9f574b4-0986-470e-8acb-2c7656a4130b", "metadata": {"aucs": [0.8973817445575429, 0.9466926790816856, 0.9751806535384601], "final_y": [0.16485599379768723, 0.164856520950174, 0.16485646945990529]}, "mutation_prompt": null}
{"id": "bbcf0bf2-fe29-428e-9d7e-bd4ee30e715c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 120.0  # Increased penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget)  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - num_evals / self.budget)  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability:\n                    # Guided local search initialization\n                    local_res = minimize(func, mutant, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Improved hybrid metaheuristic with reinforced periodic pressure and guided local exploration for enhanced solution accuracy.", "configspace": "", "generation": 7, "fitness": 0.939209954059875, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.026. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f9f574b4-0986-470e-8acb-2c7656a4130b", "metadata": {"aucs": [0.9147796459955743, 0.9277834054937413, 0.9750668106903091], "final_y": [0.1648562266534962, 0.1648560228716044, 0.16485646945990529]}, "mutation_prompt": null}
{"id": "ce62d0d8-e6e2-4ebc-9fd7-3faa43748ab3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget)  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Improved adaptive mutation with localized penalty scaling for enhanced exploration.", "configspace": "", "generation": 8, "fitness": 0.9413226160224966, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.941 with standard deviation 0.031. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f9f574b4-0986-470e-8acb-2c7656a4130b", "metadata": {"aucs": [0.8996497749142636, 0.9491374196147659, 0.9751806535384601], "final_y": [0.16485668264908848, 0.1648561065586025, 0.16485646945990529]}, "mutation_prompt": null}
{"id": "b419d679-b45a-444a-b44b-58efc52b7080", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget)  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                adaptive_CR = self.CR * (1 + num_evals / self.budget)  # Adaptive crossover probability\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty / (len(solution) // period)  # Normalized penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduced adaptive crossover probability and improved periodicity penalty calculation to enhance exploration and exploitation balance.", "configspace": "", "generation": 9, "fitness": 0.9364482488628253, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.936 with standard deviation 0.029. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ce62d0d8-e6e2-4ebc-9fd7-3faa43748ab3", "metadata": {"aucs": [0.9063806875562743, 0.9277834054937413, 0.9751806535384601], "final_y": [0.16485641616056923, 0.1648560228716044, 0.16485646945990529]}, "mutation_prompt": null}
{"id": "c09793e3-29f3-4333-99c8-d5d9ac49f19a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.6  # Increased probability to invoke local search\n        self.penalty_factor = 120.0  # Adjusted penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget)  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Improved hybrid metaheuristic with enhanced local search probability and adaptive penalty factor for better exploration and exploitation balance.", "configspace": "", "generation": 10, "fitness": 0.9141735938668716, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.914 with standard deviation 0.043. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "ce62d0d8-e6e2-4ebc-9fd7-3faa43748ab3", "metadata": {"aucs": [0.8737720165553462, 0.8948650503936773, 0.9738837146515913], "final_y": [0.18187918389072888, 0.16485727593731425, 0.16485675408793365]}, "mutation_prompt": null}
{"id": "6c61600b-2583-4a4d-9d9f-967732db97c2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget)  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + (0.5 + 0.5 * np.tanh(-fitness[i])) * periodicity_penalty  # Adjusted penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability and num_evals < self.budget:  # Ensure budget is not exceeded\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with improved adaptive search balancing to optimize periodicity and local search application.", "configspace": "", "generation": 11, "fitness": 0.9411945360967361, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.941 with standard deviation 0.031. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ce62d0d8-e6e2-4ebc-9fd7-3faa43748ab3", "metadata": {"aucs": [0.899265539231209, 0.9491374155205389, 0.9751806535384601], "final_y": [0.16485668935277353, 0.16485613042875458, 0.16485646945990529]}, "mutation_prompt": null}
{"id": "a1cbde6c-51be-4572-a8dd-c9e9973caee6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive mutation rate and crossover strategy for improved exploration and exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.9430541129129355, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.943 with standard deviation 0.025. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ce62d0d8-e6e2-4ebc-9fd7-3faa43748ab3", "metadata": {"aucs": [0.9138024473255818, 0.940187217871318, 0.9751726735419068], "final_y": [0.16485734619587988, 0.16485675397523525, 0.16485731387907088]}, "mutation_prompt": null}
{"id": "6cc3bce8-a030-4147-873e-f932ad6d34cf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        penalty += np.sum(np.sin(solution))  # Sinusoidal periodicity penalty\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduced a sinusoidal periodicity penalty to better guide solutions towards periodic configurations.", "configspace": "", "generation": 13, "fitness": 0.9430541129129355, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.943 with standard deviation 0.025. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a1cbde6c-51be-4572-a8dd-c9e9973caee6", "metadata": {"aucs": [0.9138024473255818, 0.940187217871318, 0.9751726735419068], "final_y": [0.16485734619587988, 0.16485675397523525, 0.16485731387907088]}, "mutation_prompt": null}
{"id": "ad8217bb-9b20-429a-ae34-71c1f8e9caf2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 50.0  # Penalty factor for non-periodicity (adjusted)\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Improved optimized performance by adjusting the periodicity penalty factor for better handling of periodic solutions.", "configspace": "", "generation": 14, "fitness": 0.9430541129129355, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.943 with standard deviation 0.025. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a1cbde6c-51be-4572-a8dd-c9e9973caee6", "metadata": {"aucs": [0.9138024473255818, 0.940187217871318, 0.9751726735419068], "final_y": [0.16485734619587988, 0.16485675397523525, 0.16485731387907088]}, "mutation_prompt": null}
{"id": "67d6b7ec-ebbd-43b0-b1c8-fb2436c451dc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability_base = 0.5  # Base probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search with dynamic probability\n                if np.random.rand() < (self.local_search_probability_base * (num_evals / self.budget)):\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Integrate dynamic local search probability to enhance local exploitation during optimization.", "configspace": "", "generation": 15, "fitness": 0.8285343267891078, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.829 with standard deviation 0.083. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "a1cbde6c-51be-4572-a8dd-c9e9973caee6", "metadata": {"aucs": [0.9036932841760125, 0.7127090530660651, 0.8692006431252456], "final_y": [0.16486095005961887, 0.2578102722569643, 0.18187945174355957]}, "mutation_prompt": null}
{"id": "514750f6-8d64-4417-9b4e-53ce7ba9feb9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial = trial + 0.01 * np.sin(np.linspace(0, 2 * np.pi, len(trial)))  # Cyclic modulation\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a cyclic periodicity encouragement by modulating trial solutions with sine function.", "configspace": "", "generation": 16, "fitness": 0.9387327789473252, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.026. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a1cbde6c-51be-4572-a8dd-c9e9973caee6", "metadata": {"aucs": [0.9079520535481648, 0.9356141426271035, 0.9726321406667076], "final_y": [0.16485599199494505, 0.1648566838103761, 0.16485641260315387]}, "mutation_prompt": null}
{"id": "111414da-5d71-4c08-a3aa-14caeec27f71", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduced dynamic local search probability for better exploitation near optimal solutions.", "configspace": "", "generation": 17, "fitness": 0.9472385301060279, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.021. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a1cbde6c-51be-4572-a8dd-c9e9973caee6", "metadata": {"aucs": [0.9245321914439458, 0.9420098998480093, 0.9751734990261283], "final_y": [0.16485641210369872, 0.1648560965415088, 0.16485608458432388]}, "mutation_prompt": null}
{"id": "696d300f-a45f-400d-8190-0e5d239850ab", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n        self.elite_fraction = 0.1  # Fraction of elite solutions\n        self.diversity_threshold = 0.1  # Diversity threshold for rerandomization\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            \n            # Elite selection\n            elite_count = int(self.elite_fraction * self.pop_size)\n            elite_indices = np.argsort(fitness)[:elite_count]\n            new_population[:elite_count] = population[elite_indices]\n\n            # Diversity preservation\n            diversity = np.std(population, axis=0)\n            if np.all(diversity < self.diversity_threshold):\n                new_population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduced elite selection and diversity preservation to enhance exploration and exploitation balance.", "configspace": "", "generation": 18, "fitness": 0.9472384597247969, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.021. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9245321914439458, 0.9420098469606226, 0.9751733407698223], "final_y": [0.16485641210369872, 0.16485636135083637, 0.16485665239064107]}, "mutation_prompt": null}
{"id": "0b4be26c-6469-444c-91c1-82ebdd30646f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))),\n                                         options={'maxiter': 10, 'ftol': 1e-4})  # Limited iterations and early stopping\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Improved periodicity penalty with variable period length\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty_contrib = np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n            penalty += penalty_contrib * (1 + penalty_contrib)  # Scaled penalty\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search with contextual adaptation and improved periodicity penalty calculation for optimal exploration and exploitation.", "configspace": "", "generation": 19, "fitness": 0.9431620639636605, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.943 with standard deviation 0.041. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.8864884216325094, 0.960867210518038, 0.9821305597404344], "final_y": [0.18166269499638055, 0.1648570842818906, 0.16498439271280074]}, "mutation_prompt": null}
{"id": "3fae75aa-140a-4e39-bd0d-eb66f3f31b5f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.7  # Adjusted probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search probability adaptation for improved exploitation near promising regions.", "configspace": "", "generation": 20, "fitness": 0.9139210518056683, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.914 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9252545389356815, 0.9018179071578102, 0.9146907093235132], "final_y": [0.16485634557779327, 0.16485708701721202, 0.16485709537505677]}, "mutation_prompt": null}
{"id": "4cb7efd1-01a9-440c-b172-7f1535873858", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.3  # Probability to invoke local search\n        self.penalty_factor = 50.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced periodicity handling and adaptive local search probability to explore solution space more effectively.", "configspace": "", "generation": 21, "fitness": 0.9397712348029454, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.940 with standard deviation 0.033. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.8959467360948361, 0.9481942947720933, 0.9751726735419068], "final_y": [0.16485666904483642, 0.16485926153101949, 0.16485731387907088]}, "mutation_prompt": null}
{"id": "39beee48-5435-46fd-9bd8-ea4a80e3c383", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness) * 0.5)  # Adjusted adaptive penalty\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget) * 1.2:  # Adjusted dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptivity in local search and penalty application for better balance between exploration and exploitation.", "configspace": "", "generation": 22, "fitness": 0.9363641256055955, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.936 with standard deviation 0.026. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9138034100291235, 0.9232070188812221, 0.9720819479064409], "final_y": [0.16485654019089024, 0.164859134387497, 0.164856421264704]}, "mutation_prompt": null}
{"id": "f1b96e42-fb03-4708-93c7-4afd6f962df8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial, fitness)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution, fitness):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        spread = np.std(fitness)  # Compute standard deviation of fitness\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty * (1 + spread)  # Adaptive penalty based on fitness spread\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced diversity by using an adaptive periodicity penalty based on current fitness spread.", "configspace": "", "generation": 23, "fitness": 0.9472385301060279, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.021. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9245321914439458, 0.9420098998480093, 0.9751734990261283], "final_y": [0.16485641210369872, 0.1648560965415088, 0.16485608458432388]}, "mutation_prompt": null}
{"id": "dff7ba8f-7870-4249-b306-1d66e493c8e8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.6  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget + 0.1))  # Slightly modified dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                thickness_penalty = self.calculate_thickness_penalty(trial)  # New penalty\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness)) + thickness_penalty\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def calculate_thickness_penalty(self, solution):  # New method\n        penalty = 0\n        target_thickness = 0.5 * (bounds.lb + bounds.ub)  # Assume ideal thickness is mid-range\n        penalty += np.sum((solution - target_thickness) ** 2)\n        return penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced dynamic adaptation of mutation and crossover parameters and incorporated penalty for non-optimal layer thickness.", "configspace": "", "generation": 24, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'bounds' is not defined\").", "error": "NameError(\"name 'bounds' is not defined\")", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {}, "mutation_prompt": null}
{"id": "9883e025-fe4c-4216-8362-6982188b13f2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 150.0  # Adjusted penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced periodicity penalty to better encourage periodic solutions in Differential Evolution.", "configspace": "", "generation": 25, "fitness": 0.9472385301060279, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.021. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9245321914439458, 0.9420098998480093, 0.9751734990261283], "final_y": [0.16485641210369872, 0.1648560965415088, 0.16485608458432388]}, "mutation_prompt": null}
{"id": "c53d17e6-bd2c-4822-b9fc-0de8e0b4ba20", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.7  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.CR  # Static crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive strategies for DE mutation and crossover, and improved periodicity handling for better exploration and exploitation balance.", "configspace": "", "generation": 26, "fitness": 0.9275791582469276, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.928 with standard deviation 0.048. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.8615619228599866, 0.9459696884505024, 0.975205863430294], "final_y": [0.1818794181450254, 0.1648563689156327, 0.16485825279948818]}, "mutation_prompt": null}
{"id": "8868605d-3784-405d-9ef3-3e78afee1348", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))), tol=1e-9)\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Increased precision of local search in promising regions by reducing tolerance in `minimize` calls.", "configspace": "", "generation": 27, "fitness": 0.934774715539334, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.032. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.8964217978168671, 0.9341569032738565, 0.9737454455272784], "final_y": [0.16485577207110125, 0.16485577221852865, 0.16485577233340754]}, "mutation_prompt": null}
{"id": "79f6d126-ae21-419f-90c0-161d31072b51", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget) ** 0.5)  # Enhanced adaptive crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness)) ** 0.5  # Enhanced adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive crossover rate and penalty scaling for improved solution quality.", "configspace": "", "generation": 28, "fitness": 0.9423512721979733, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.942 with standard deviation 0.029. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9051341294509333, 0.946747013552207, 0.9751726735907793], "final_y": [0.16485680301381256, 0.16485761559773104, 0.16485729572039332]}, "mutation_prompt": null}
{"id": "a3d8977b-4d09-42f6-a14d-03e090a9db14", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - fitness[i] / np.min(fitness)):  # Enhanced adaptation mechanism\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptation mechanism in local search probability to improve convergence performance.", "configspace": "", "generation": 29, "fitness": 0.9430541129129355, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.943 with standard deviation 0.025. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9138024473255818, 0.940187217871318, 0.9751726735419068], "final_y": [0.16485734619587988, 0.16485675397523525, 0.16485731387907088]}, "mutation_prompt": null}
{"id": "b331b186-d490-4811-a6c7-6719120d226d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        self.penalty_factor = 100.0 * (1 - penalty / len(solution))  # Dynamic adjustment\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search strategy by adjusting the penalty factor dynamically to improve convergence.", "configspace": "", "generation": 30, "fitness": 0.9472384207336381, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.021. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9245321942488751, 0.942009924295257, 0.9751731436567819], "final_y": [0.16485637202021752, 0.16485628766637572, 0.16485702893307264]}, "mutation_prompt": null}
{"id": "5a9f7fa9-0bca-4666-9a93-62bb1bc48d9e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5 * (np.var(fitness) / (np.mean(fitness) + 1e-9))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive mutation factor by introducing an exploration-exploitation balance term.", "configspace": "", "generation": 31, "fitness": 0.8057891357997006, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.085. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9240837272244441, 0.7283063804261773, 0.7649772997484803], "final_y": [0.16485780872967537, 0.1648568008586313, 0.16485678774211354]}, "mutation_prompt": null}
{"id": "75125bbe-beb2-47f2-bbd2-c7c4ba2b98c3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget) ** 0.5)  # Further adaptive crossover rate adjustment\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduced further adaptive crossover probability adjustment to enhance exploration-exploitation balance.", "configspace": "", "generation": 32, "fitness": 0.9352504013496913, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.038. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.8838286307707768, 0.9467496177988243, 0.9751729554794729], "final_y": [0.18187896068530363, 0.1648561614198535, 0.16485689316498187]}, "mutation_prompt": null}
{"id": "90da279a-54e8-4ff3-8d00-8463189b86c8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - (fitness[i] - np.min(fitness)) / (np.max(fitness) - np.min(fitness))):  # Enhanced local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Improved local search integration by optimizing its influence based on fitness improvement.", "configspace": "", "generation": 33, "fitness": 0.8672654688101534, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.867 with standard deviation 0.032. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9119183213496063, 0.8505683305027386, 0.8393097545781152], "final_y": [0.1818788833385917, 0.1648582980829142, 0.20044579500811655]}, "mutation_prompt": null}
{"id": "df7c096f-3bf6-41dd-8452-7d178d6bf9b7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * ((1 - num_evals / self.budget) ** 0.7)  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive mutation factor for better exploration in later stages.", "configspace": "", "generation": 34, "fitness": 0.9389014192881548, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.030. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9006293385498494, 0.9408679884598479, 0.9752069308547672], "final_y": [0.16485654291534924, 0.1648565821688892, 0.16485655220165218]}, "mutation_prompt": null}
{"id": "bbca24da-8061-460c-8eb7-98487b1af7f4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2) / (period**2)  # Enhanced penalty calculation\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced periodicity penalty calculation for better periodic structure optimization.", "configspace": "", "generation": 35, "fitness": 0.9472385301060279, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.021. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9245321914439458, 0.9420098998480093, 0.9751734990261283], "final_y": [0.16485641210369872, 0.1648560965415088, 0.16485608458432388]}, "mutation_prompt": null}
{"id": "c97f9b8b-1c54-4954-a1da-62dbbd912b68", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 50.0  # Reduced penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                if np.random.rand() < self.local_search_probability * np.exp(-0.1 * num_evals / self.budget):\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period, period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced periodicity handling and adaptive local improvement for superior solution quality.", "configspace": "", "generation": 36, "fitness": 0.9430541129129355, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.943 with standard deviation 0.025. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9138024473255818, 0.940187217871318, 0.9751726735419068], "final_y": [0.16485734619587988, 0.16485675397523525, 0.16485731387907088]}, "mutation_prompt": null}
{"id": "82ecdc93-bbff-4403-8b66-ed90623f3a40", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - np.roll(solution[i:i+period], 1, axis=0))**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Updated the periodicity penalty calculation to better capture and penalize non-periodic structures.", "configspace": "", "generation": 37, "fitness": 0.9472385301060279, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.021. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9245321914439458, 0.9420098998480093, 0.9751734990261283], "final_y": [0.16485641210369872, 0.1648560965415088, 0.16485608458432388]}, "mutation_prompt": null}
{"id": "7e182dd5-bfc1-48fc-ab71-4c5fc2d9545c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 50.0  # Reduced Penalty factor for non-periodicity (changed)\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.3  # Modified adaptive mutation factor (changed)\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum(np.abs(solution[i:i+period] - solution[i+period:i+2*period]))  # Used absolute difference (changed)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced periodicity penalty and adaptive search parameters to improve solution convergence.", "configspace": "", "generation": 38, "fitness": 0.9161596662916626, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.916 with standard deviation 0.043. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9015703205344147, 0.8717860053720328, 0.9751226729685404], "final_y": [0.164856633471681, 0.16485631597616845, 0.16485681837046573]}, "mutation_prompt": null}
{"id": "ce60322d-e8d9-4ee3-94a1-d83fe20994fd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  # Enhanced adaptive mutation factor\n                neighborhood_perturbation = np.random.normal(scale=0.1, size=self.dim)  # New line: added neighborhood-based perturbation\n                mutant = np.clip(a + adaptive_F * (b - c) + neighborhood_perturbation, bounds.lb, bounds.ub)  # Modified line\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  # Dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adaptive penalty scaling\n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Local search\n                if np.random.rand() < self.local_search_probability * (1 - num_evals / self.budget):  # Dynamic local search probability\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation strategy by incorporating neighborhood-based perturbation to improve exploration.", "configspace": "", "generation": 39, "fitness": 0.9258347220273954, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.926 with standard deviation 0.040. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.8979508427030372, 0.9827092409210981, 0.8968440824580507], "final_y": [0.16485781322058324, 0.16485727176676024, 0.1648566992021956]}, "mutation_prompt": null}
{"id": "31d131e3-537b-4e9c-9fb9-1a472754e4f3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20  # Population size for Differential Evolution\n        self.F = 0.8  # Mutation factor for DE\n        self.CR = 0.9  # Crossover probability for DE\n        self.local_search_probability = 0.5  # Probability to invoke local search\n        self.penalty_factor = 100.0  # Penalty factor for non-periodicity\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation and crossover\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 - num_evals / self.budget) ** 0.5  \n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))  \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Apply periodicity penalty\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))  \n                num_evals += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                # Enhanced local search strategy\n                adaptive_local_prob = self.local_search_probability * (1 - num_evals / self.budget) \n                if np.random.rand() < adaptive_local_prob: \n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        # Define periodic structures and compare\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced modularity and adaptive synergy between global and local search strategies to improve convergence on complex optimization landscapes.", "configspace": "", "generation": 40, "fitness": 0.9472385301060279, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.021. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.9245321914439458, 0.9420098998480093, 0.9751734990261283], "final_y": [0.16485641210369872, 0.1648560965415088, 0.16485608458432388]}, "mutation_prompt": null}
{"id": "af4268a6-4e73-4b2b-bb6c-0bedb0c23330", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.8\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-num_evals/self.budget)  # Adaptive local search probability\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced the adaptive mutation strategy and introduced an adaptive hybrid local search mechanism for improved convergence.", "configspace": "", "generation": 41, "fitness": 0.9521475323652043, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.952 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "111414da-5d71-4c08-a3aa-14caeec27f71", "metadata": {"aucs": [0.986496220703471, 0.9391311141848768, 0.9308152622072651], "final_y": [0.16485940878806515, 0.1648565997640754, 0.18187872273703187]}, "mutation_prompt": null}
{"id": "6d7388ff-dd16-4272-83d6-fac3fa0043c1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-num_evals/self.budget)  # Adaptive local search probability\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  # Adjusted periodicity encouragement\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance periodicity encouragement and fine-tune adaptive mutation for improved reflectivity optimization.", "configspace": "", "generation": 42, "fitness": 0.9688678971555392, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.027. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "af4268a6-4e73-4b2b-bb6c-0bedb0c23330", "metadata": {"aucs": [0.9866563214060827, 0.9894700028833354, 0.9304773671771994], "final_y": [0.16485764829082716, 0.16485615975071055, 0.18187855090890115]}, "mutation_prompt": null}
{"id": "b2999f7d-c58d-4a45-bce0-d79e01da37de", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 + 0.2 * np.cos(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-num_evals/self.budget)  # Adaptive local search probability\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)  # Adjusted periodicity encouragement\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance adaptive mutation and penalty strategies to optimize periodic solutions' convergence rate and solution quality.", "configspace": "", "generation": 43, "fitness": 0.9308520650467348, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.931 with standard deviation 0.037. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6d7388ff-dd16-4272-83d6-fac3fa0043c1", "metadata": {"aucs": [0.8821819071609115, 0.9394141333712512, 0.9709601546080414], "final_y": [0.16485633762912, 0.1648568650149258, 0.16485602745585903]}, "mutation_prompt": null}
{"id": "753f9c24-ef25-4a38-9fc4-071324bdf580", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * np.abs(np.cos(np.pi * num_evals / self.budget))  # Improved adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-num_evals/self.budget)  # Adaptive local search probability\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.2  # Fine-tuned periodicity encouragement\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Improve periodicity encouragement and fine-tune mutation strategy for better optimization performance.", "configspace": "", "generation": 44, "fitness": 0.9229536343826278, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.923 with standard deviation 0.018. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6d7388ff-dd16-4272-83d6-fac3fa0043c1", "metadata": {"aucs": [0.9029805908991959, 0.9188076851011012, 0.9470726271475862], "final_y": [0.16485624140932276, 0.16485659953862164, 0.1648596140678258]}, "mutation_prompt": null}
{"id": "31388175-6b79-4716-ad99-67feb178eb55", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (0.5 + 0.5 * np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-num_evals/self.budget)  # Adaptive local search probability\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  # Adjusted periodicity encouragement\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Refine hybrid strategy by enhancing periodicity term and adaptive mutation for optimal photonic structure reflectivity.", "configspace": "", "generation": 45, "fitness": 0.948803176637167, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.949 with standard deviation 0.047. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6d7388ff-dd16-4272-83d6-fac3fa0043c1", "metadata": {"aucs": [0.9861670550414551, 0.8830160993367004, 0.9772263755333453], "final_y": [0.16485583382112423, 0.16485703487362757, 0.16485607648282485]}, "mutation_prompt": null}
{"id": "d9904725-cf18-43b8-b4cb-922179081f59", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                \n                adaptive_CR = self.CR * (1 + 0.5 * np.sin(np.pi * num_evals / self.budget))  # Adaptive crossover rate\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                dynamic_penalty_factor = self.penalty_factor * (1 + num_evals / self.budget)  # Dynamic penalty scaling\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) * dynamic_penalty_factor\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-num_evals/self.budget)  # Adaptive local search probability\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  # Adjusted periodicity encouragement\n        return penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduce dynamic penalty scaling and adaptive crossover for improved search efficiency in reflectivity optimization.", "configspace": "", "generation": 46, "fitness": 0.9688673200464045, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.027. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "6d7388ff-dd16-4272-83d6-fac3fa0043c1", "metadata": {"aucs": [0.9866563214060827, 0.9894697869653525, 0.9304758517677787], "final_y": [0.16485764829082716, 0.164856355538899, 0.18187965869192335]}, "mutation_prompt": null}
{"id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Fine-tune periodicity encouragement and local search adaptation for enhanced reflectivity optimization.", "configspace": "", "generation": 47, "fitness": 0.9688680516794106, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.027. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "6d7388ff-dd16-4272-83d6-fac3fa0043c1", "metadata": {"aucs": [0.9866569952431016, 0.9894697926179306, 0.9304773671771994], "final_y": [0.16485709315674657, 0.16485632686704355, 0.18187855090890115]}, "mutation_prompt": null}
{"id": "19598f11-97ac-483b-8869-6e51f72a6e37", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (0.5 + np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.7 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Fine-tuned periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance adaptive mutation strategy and fine-tune penalty scaling for improved optimization performance.", "configspace": "", "generation": 48, "fitness": 0.9473876579901553, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.049. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9876013762541342, 0.877676768189874, 0.9768848295264579], "final_y": [0.16485665417646322, 0.1648607165153433, 0.16485702506708977]}, "mutation_prompt": null}
{"id": "7cf96b27-25c4-42e9-b631-165257c49222", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.7 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Penalty scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.2 * num_evals/self.budget)  # Decay adjustment\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Refine periodicity encouragement and local search adaptation for enhanced reflectivity optimization.", "configspace": "", "generation": 49, "fitness": 0.9688680497952178, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.027. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9866569952431016, 0.9894697869653525, 0.9304773671771994], "final_y": [0.16485709315674657, 0.164856355538899, 0.18187855090890115]}, "mutation_prompt": null}
{"id": "d28a503c-3b5b-482a-9c45-e62c010a3cd5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget) + 0.5)  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.8  # Increased periodicity sensitivity\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance modular solution development and adaptive periodicity to improve optimization performance.", "configspace": "", "generation": 50, "fitness": 0.9473876579901553, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.049. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9876013762541342, 0.877676768189874, 0.9768848295264579], "final_y": [0.16485665417646322, 0.1648607165153433, 0.16485702506708977]}, "mutation_prompt": null}
{"id": "934d89e4-218d-493a-af01-f0f45cbff6ee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * np.sin(np.pi * num_evals / self.budget))  # Adaptive crossover strategy\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration via adaptive crossover strategy for improved reflectivity optimization.", "configspace": "", "generation": 51, "fitness": 0.929699171585009, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.930 with standard deviation 0.015. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9473347266141023, 0.9320934013949306, 0.9096693867459941], "final_y": [0.16485768109632226, 0.16485645923868375, 0.18187967278953165]}, "mutation_prompt": null}
{"id": "f49b75ff-999e-47bb-bf22-6ae28bc29f40", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 50.0  # Reduced penalty factor for better flexibility\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (1 + np.sin(np.pi * num_evals / self.budget) / 2)  # Further enhancement to adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Further enhanced adaptive mutation and penalty adjustment for improved convergence and solution quality.", "configspace": "", "generation": 52, "fitness": 0.9180789713994589, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.918 with standard deviation 0.025. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.8836356421606811, 0.9310047956225134, 0.9395964764151824], "final_y": [0.1648567207937891, 0.16485763423296318, 0.16485636515153546]}, "mutation_prompt": null}
{"id": "7a0f563a-b5d1-4f09-a3c4-08d556739394", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.6 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.3 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Refine the weighting of periodicity penalty and local search scaling for improved convergence.", "configspace": "", "generation": 53, "fitness": 0.9688680497952178, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.027. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9866569952431016, 0.9894697869653525, 0.9304773671771994], "final_y": [0.16485709315674657, 0.164856355538899, 0.18187855090890115]}, "mutation_prompt": null}
{"id": "1f94bace-c95d-4b6f-bb17-0ce6fdf13134", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.cos(np.pi * num_evals / self.budget))  # Refined adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.75 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a refined adaptive mutation strategy and periodicity penalty for improved optimization performance.", "configspace": "", "generation": 54, "fitness": 0.9156896469216341, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.916 with standard deviation 0.009. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9038655256602695, 0.9188069862693108, 0.924396428835322], "final_y": [0.16485585162820982, 0.16485707949914785, 0.1818783663092004]}, "mutation_prompt": null}
{"id": "2a2c7b5a-95bd-494c-ad86-53c0d45271d8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget) + 0.3)  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive strategies and mutation with periodicity alignment for improved optimization.", "configspace": "", "generation": 55, "fitness": 0.9675873149882775, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.028. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9889581798105662, 0.9280210318449484, 0.985782733309318], "final_y": [0.16485703782240058, 0.16485626623573357, 0.1648563935102476]}, "mutation_prompt": null}
{"id": "888855cc-8d55-4a62-9de4-ad6524910ea3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)  # Removed unnecessary power operation\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance periodicity evaluation by refining the periodicity penalty function for better convergence.", "configspace": "", "generation": 56, "fitness": 0.9688680516794106, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.027. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9866569952431016, 0.9894697926179306, 0.9304773671771994], "final_y": [0.16485709315674657, 0.16485632686704355, 0.18187855090890115]}, "mutation_prompt": null}
{"id": "d0ebe268-9468-415a-98e0-fd04f94fc21a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.5 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Fine-tune adaptation in DE and periodicity penalty for enhanced optimization efficiency.", "configspace": "", "generation": 57, "fitness": 0.9688680516794106, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.027. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9866569952431016, 0.9894697926179306, 0.9304773671771994], "final_y": [0.16485709315674657, 0.16485632686704355, 0.18187855090890115]}, "mutation_prompt": null}
{"id": "8d34aaf2-066b-447d-8917-d1ea2d46b61d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.cos(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.6 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance convergence by refining mutation strategy and periodicity penalty for improved optimization.", "configspace": "", "generation": 58, "fitness": 0.9156896469216341, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.916 with standard deviation 0.009. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9038655256602695, 0.9188069862693108, 0.924396428835322], "final_y": [0.16485585162820982, 0.16485707949914785, 0.1818783663092004]}, "mutation_prompt": null}
{"id": "fe0fcf02-164e-47b4-8f6a-d75d1748de9d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1.5 - num_evals / self.budget))  # Adaptive crossover probability\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + np.sqrt(periodicity_penalty) * (1 - fitness[i] / np.min(fitness))  # Enhanced periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduces adaptive crossover probability and enhanced periodicity penalty scaling for improved solution quality.", "configspace": "", "generation": 59, "fitness": 0.9609623669063027, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.961 with standard deviation 0.038. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9866574008166199, 0.9894697869653525, 0.9067599129369356], "final_y": [0.1648572431824502, 0.164856355538899, 0.18187932631011705]}, "mutation_prompt": null}
{"id": "f18eb5a9-f0b6-4539-9101-0e331cdf8fb1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                adaptive_CR = self.CR * (1 - num_evals / self.budget)  # Adaptive crossover rate\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduced an adaptive crossover rate to improve exploration-exploitation balance.", "configspace": "", "generation": 60, "fitness": 0.9688680516794106, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.027. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9866569952431016, 0.9894697926179306, 0.9304773671771994], "final_y": [0.16485709315674657, 0.16485632686704355, 0.18187855090890115]}, "mutation_prompt": null}
{"id": "66c25d61-3350-4058-8206-3edd6353d881", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.cos(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.2 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Refine adaptive mutation strategy and enhance local search integration for improved convergence and solution quality.", "configspace": "", "generation": 61, "fitness": 0.9156893727682585, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.916 with standard deviation 0.009. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9038655256602695, 0.9188069609578685, 0.9243956316866375], "final_y": [0.16485585162820982, 0.16485728619158424, 0.18187870768086922]}, "mutation_prompt": null}
{"id": "2e67e45a-f0b5-4dab-bddd-e53f86266577", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.6  # Adjusted local search probability\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.5 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.0 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Adaptive differential evolution with enhanced local search integration for optimal reflectivity configuration.", "configspace": "", "generation": 62, "fitness": 0.9653452395974726, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.025. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9866589877115347, 0.9300412635872475, 0.9793354674936355], "final_y": [0.1648559237295617, 0.16485657249882812, 0.16485646870617987]}, "mutation_prompt": null}
{"id": "8114e00b-9452-4707-85b6-c7a724b5da58", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.cos(np.pi * num_evals / (2 * self.budget)) + 1)  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**0.5  # Improved penalty calculation\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive mutation and improved periodic penalty calculation for optimized reflectivity.", "configspace": "", "generation": 63, "fitness": 0.9169358774484335, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.917 with standard deviation 0.020. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.8905166226313296, 0.9213586163919242, 0.9389323933220468], "final_y": [0.16486313118971718, 0.16485731305131301, 0.1648560089877944]}, "mutation_prompt": null}
{"id": "c9f13f5c-0e08-4b8a-9889-ae20b7667df8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (0.5 + 0.5 * np.sin(np.pi * num_evals / self.budget))  # Fine-tuned mutation factor scaling\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Fine-tune mutation factor scaling to improve convergence and solution quality.", "configspace": "", "generation": 64, "fitness": 0.9488032642556127, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.949 with standard deviation 0.047. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9861660329079888, 0.8830173843255037, 0.9772263755333453], "final_y": [0.16485703930082984, 0.16485614371569746, 0.16485607648282485]}, "mutation_prompt": null}
{"id": "0f366881-b07e-4654-bf5a-616ad358a255", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget) + 0.1)  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (self.CR * (1 - num_evals / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance adaptive mutation strategy based on evaluation progress to improve optimization precision.", "configspace": "", "generation": 65, "fitness": 0.9674026014461882, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.967 with standard deviation 0.023. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.985161946175927, 0.9344027097531361, 0.9826431484095013], "final_y": [0.1648561271153952, 0.16485662059266726, 0.16485729882461042]}, "mutation_prompt": null}
{"id": "e27287e0-c4c3-43ca-9344-c10b592da321", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Adaptive crossover probability fine-tuned\n                adaptive_CR = self.CR * (1 - np.cos((np.pi * num_evals) / (2 * self.budget)))\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Fine-tuned periodicity penalty application\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration by introducing adaptive crossover probability and fine-tune periodicity penalty application for improved solution quality.", "configspace": "", "generation": 66, "fitness": 0.932889450573334, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.933 with standard deviation 0.021. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9574572934776437, 0.9064562197961139, 0.9347548384462443], "final_y": [0.16485681372409255, 0.1818790111455867, 0.16485678999733966]}, "mutation_prompt": null}
{"id": "5aef683f-f6c1-4427-95eb-4a71ffb6e411", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration by adjusting crossover rate dynamically to balance exploration and exploitation.", "configspace": "", "generation": 67, "fitness": 0.9750065685681918, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.019. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2c2285ae-1ae7-4c86-b0ed-eef0eade3c65", "metadata": {"aucs": [0.9866563214060827, 0.9894697869653525, 0.94889359733314], "final_y": [0.16485764829082716, 0.164856355538899, 0.16486191215027757]}, "mutation_prompt": null}
{"id": "35e8edc2-c7c4-4c5a-b2e5-9c83f714d56a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                scaled_penalty_factor = self.penalty_factor * (1 - num_evals / self.budget)  # Adaptive penalty scaling\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness)) * scaled_penalty_factor\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive penalty factor scaling to enhance convergence by adjusting the periodicity penalty over time.", "configspace": "", "generation": 68, "fitness": 0.9750065685681918, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.019. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5aef683f-f6c1-4427-95eb-4a71ffb6e411", "metadata": {"aucs": [0.9866563214060827, 0.9894697869653525, 0.94889359733314], "final_y": [0.16485764829082716, 0.164856355538899, 0.16486191215027757]}, "mutation_prompt": null}
{"id": "99efcdef-55a9-4b82-9752-77ead1ce579e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.6 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a slight increase in local search probability decay to enhance convergence speed by encouraging more focused exploitation.", "configspace": "", "generation": 69, "fitness": 0.975007125166249, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.019. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5aef683f-f6c1-4427-95eb-4a71ffb6e411", "metadata": {"aucs": [0.9866579912002547, 0.9894697869653525, 0.94889359733314], "final_y": [0.16485611261392907, 0.164856355538899, 0.16486191215027757]}, "mutation_prompt": null}
{"id": "9bd46aba-a1e3-4b81-a57a-2dc6b6187d00", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.8 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance convergence speed by dynamically adjusting the local search decay factor based on budget consumption.", "configspace": "", "generation": 70, "fitness": 0.9763465329083519, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.017. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "99efcdef-55a9-4b82-9752-77ead1ce579e", "metadata": {"aucs": [0.9866579912002547, 0.9894697869653525, 0.9529118205594488], "final_y": [0.16485611261392907, 0.164856355538899, 0.1648568718801623]}, "mutation_prompt": null}
{"id": "87f55a37-7bd3-4c89-aedc-9e888c111914", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.5 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Fine-tune adaptive local search probability to improve convergence and exploitability.", "configspace": "", "generation": 71, "fitness": 0.9750065685681918, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.019. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9bd46aba-a1e3-4b81-a57a-2dc6b6187d00", "metadata": {"aucs": [0.9866563214060827, 0.9894697869653525, 0.94889359733314], "final_y": [0.16485764829082716, 0.164856355538899, 0.16486191215027757]}, "mutation_prompt": null}
{"id": "5be6c01e-36d1-45cc-8051-fccf77c8adcc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.8 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.25  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance convergence speed and accuracy by adjusting the periodicity penalty exponent for increased solution modularity.", "configspace": "", "generation": 72, "fitness": 0.9763465329083519, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.017. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9bd46aba-a1e3-4b81-a57a-2dc6b6187d00", "metadata": {"aucs": [0.9866579912002547, 0.9894697869653525, 0.9529118205594488], "final_y": [0.16485611261392907, 0.164856355538899, 0.1648568718801623]}, "mutation_prompt": null}
{"id": "9741d0b1-a996-43a4-bbb3-32200e22e239", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                adaptive_penalty_factor = self.penalty_factor * (1 + 0.5 * (num_evals / self.budget))  # Adaptive penalty factor\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.8 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Implement an adaptive penalty factor to more effectively encourage periodic solutions in multilayer photonic structures.", "configspace": "", "generation": 73, "fitness": 0.9763465329083519, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.017. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9bd46aba-a1e3-4b81-a57a-2dc6b6187d00", "metadata": {"aucs": [0.9866579912002547, 0.9894697869653525, 0.9529118205594488], "final_y": [0.16485611261392907, 0.164856355538899, 0.1648568718801623]}, "mutation_prompt": null}
{"id": "0913177f-197b-4bd5-b599-0dc95e3031fc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (0.5 + 0.5 * np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.8 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Refine adaptive mutation strategy to strategically influence exploration and exploitation balance.", "configspace": "", "generation": 74, "fitness": 0.9488032541059307, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.949 with standard deviation 0.047. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9bd46aba-a1e3-4b81-a57a-2dc6b6187d00", "metadata": {"aucs": [0.9861670865132127, 0.8830163002712339, 0.9772263755333453], "final_y": [0.16485674392916216, 0.1648569023260038, 0.16485607648282485]}, "mutation_prompt": null}
{"id": "32c2c7e6-bc1d-431c-ad79-09d5d7b3db79", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial)\n                trial_fitness = func(trial) + 0.8 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted periodicity scaling\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-1.8 * num_evals/self.budget)  # Fine-tuned local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        adaptive_penalty_factor = self.penalty_factor * (1 + 0.5 * (num_evals / self.budget))  # Dynamically adjusted penalty factor\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return adaptive_penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Refine local search integration by dynamically adjusting the penalty factor based on budget consumption for improved convergence.", "configspace": "", "generation": 75, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'num_evals' is not defined\").", "error": "NameError(\"name 'num_evals' is not defined\")", "parent_id": "9bd46aba-a1e3-4b81-a57a-2dc6b6187d00", "metadata": {}, "mutation_prompt": null}
{"id": "8e5362ac-7b9b-49fa-af46-05c8994f565d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim  # Normalized periodicity scaling\n                trial_fitness = func(trial) + 0.6 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Reduced periodicity influence\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-2.0 * num_evals/self.budget)  # More aggressive local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Optimize Bragg mirror reflectivity by enhancing adaptive local search and periodicity penalty integration.", "configspace": "", "generation": 76, "fitness": 0.9768707601528095, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.016. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9bd46aba-a1e3-4b81-a57a-2dc6b6187d00", "metadata": {"aucs": [0.9866563881523697, 0.9894697869653525, 0.9544861053407064], "final_y": [0.16485758518713378, 0.164856355538899, 0.16485656207911237]}, "mutation_prompt": null}
{"id": "2cb4814d-ab21-4050-8f46-0072b8d61efe", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim  # Normalized periodicity scaling\n                trial_fitness = func(trial) + 0.5 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Reduced periodicity influence\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-2.0 * num_evals/self.budget)  # More aggressive local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance Bragg mirror optimization by fine-tuning periodicity penalty and adaptive mutation dynamics.", "configspace": "", "generation": 77, "fitness": 0.9768707601528095, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.016. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8e5362ac-7b9b-49fa-af46-05c8994f565d", "metadata": {"aucs": [0.9866563881523697, 0.9894697869653525, 0.9544861053407064], "final_y": [0.16485758518713378, 0.164856355538899, 0.16485656207911237]}, "mutation_prompt": null}
{"id": "7088cfa8-017c-4ef9-a8dd-62ee6451af4f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.9  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 150.0  # Increased penalty factor\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim  # Normalized periodicity scaling\n                trial_fitness = func(trial) + 0.6 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Reduced periodicity influence\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-2.0 * num_evals/self.budget)  # More aggressive local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Improve reflectivity optimization via enhanced adaptive mutation strategy and increased penalty factor for periodicity.", "configspace": "", "generation": 78, "fitness": 0.9766208314400172, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.016. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8e5362ac-7b9b-49fa-af46-05c8994f565d", "metadata": {"aucs": [0.9865809723575281, 0.9889340368795305, 0.954347485082993], "final_y": [0.16485765565270116, 0.16485667244127222, 0.1648559510232278]}, "mutation_prompt": null}
{"id": "7efdf902-4448-4cd0-a459-d5e00be7152e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim  # Normalized periodicity scaling\n                trial_fitness = func(trial) + 0.6 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Reduced periodicity influence\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget)  # More aggressive local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Refine adaptive local search strategy by increasing decay aggressiveness for faster convergence.", "configspace": "", "generation": 79, "fitness": 0.9780166299539275, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8e5362ac-7b9b-49fa-af46-05c8994f565d", "metadata": {"aucs": [0.9866563214060827, 0.9894697869653525, 0.9579237814903474], "final_y": [0.16485764829082716, 0.164856355538899, 0.16485724089602083]}, "mutation_prompt": null}
{"id": "0c2f2361-9c93-4483-9a1e-0c72b4d26ff0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.cos(np.pi * num_evals / self.budget))  # Modified adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim  # Normalized periodicity scaling\n                trial_fitness = func(trial) + 0.6 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Reduced periodicity influence\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-2.5 * num_evals/self.budget)  # Adjusted local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Adjust decay aggressiveness and adaptive mutation for improved convergence.", "configspace": "", "generation": 80, "fitness": 0.9253909599388527, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.925 with standard deviation 0.016. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7efdf902-4448-4cd0-a459-d5e00be7152e", "metadata": {"aucs": [0.9099028710373666, 0.9188078788333492, 0.9474621299458426], "final_y": [0.1648560998108436, 0.1648567303335795, 0.1648594814092157]}, "mutation_prompt": null}
{"id": "d65494d5-2be4-4717-bd51-b5a29fa20e21", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / (self.dim - num_evals/self.budget)  # Normalized periodicity scaling\n                trial_fitness = func(trial) + 0.5 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Reduced periodicity influence\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget)  # More aggressive local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance adaptive local search strategy using a more dynamic penalty scaling and refined periodicity assessment for improved convergence.", "configspace": "", "generation": 81, "fitness": 0.9780166299539275, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7efdf902-4448-4cd0-a459-d5e00be7152e", "metadata": {"aucs": [0.9866563214060827, 0.9894697869653525, 0.9579237814903474], "final_y": [0.16485764829082716, 0.164856355538899, 0.16485724089602083]}, "mutation_prompt": null}
{"id": "3e12b2cf-18a3-428e-8092-0aca2344e6e7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim  # Normalized periodicity scaling\n                adaptive_penalty_factor = self.penalty_factor * (1 - num_evals / self.budget)  # Adaptive penalty reduction\n                trial_fitness = func(trial) + 0.6 * periodicity_penalty * adaptive_penalty_factor * (1 - fitness[i] / np.min(fitness))  # Reduced periodicity influence\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget)  # More aggressive local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive penalty reduction to enhance periodicity exploitation.", "configspace": "", "generation": 82, "fitness": 0.9780166299539275, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7efdf902-4448-4cd0-a459-d5e00be7152e", "metadata": {"aucs": [0.9866563214060827, 0.9894697869653525, 0.9579237814903474], "final_y": [0.16485764829082716, 0.164856355538899, 0.16485724089602083]}, "mutation_prompt": null}
{"id": "92d613ca-6fd6-404f-aa1a-b523f3db9e69", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (0.5 + 0.5 * np.cos(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim  # Normalized periodicity scaling\n                trial_fitness = func(trial) + 0.5 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Reduced periodicity influence\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget)  # More aggressive local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Improve periodicity penalty scaling and mutation factor adaptation for enhanced convergence.", "configspace": "", "generation": 83, "fitness": 0.9218378807154539, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.922 with standard deviation 0.021. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "7efdf902-4448-4cd0-a459-d5e00be7152e", "metadata": {"aucs": [0.8935444514116906, 0.930440952385037, 0.9415282383496341], "final_y": [0.18187895789745578, 0.16485636056452768, 0.16485675222654272]}, "mutation_prompt": null}
{"id": "2b733b89-7d79-4eac-8e88-153dbf45d306", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  \n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim\n                trial_fitness = func(trial) + 0.6 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget)\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            self.pop_size = max(10, self.pop_size - int(num_evals / self.budget))  # Dynamic population size adjustment\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance mutation strategy by dynamically adjusting the population size, improving exploration-exploitation balance.", "configspace": "", "generation": 84, "fitness": 0.9780166299539275, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7efdf902-4448-4cd0-a459-d5e00be7152e", "metadata": {"aucs": [0.9866563214060827, 0.9894697869653525, 0.9579237814903474], "final_y": [0.16485764829082716, 0.164856355538899, 0.16485724089602083]}, "mutation_prompt": null}
{"id": "392f722a-8e1c-4eea-9664-b15de24aae3d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / (1 + self.dim / 2)  # Improved periodicity scaling \n                trial_fitness = func(trial) + 0.6 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Reduced periodicity influence\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget)  # More aggressive local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Refine adaptive local search strategy by introducing dynamic mutation factor and improving periodicity penalty calculation.", "configspace": "", "generation": 85, "fitness": 0.9780166299539275, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7efdf902-4448-4cd0-a459-d5e00be7152e", "metadata": {"aucs": [0.9866563214060827, 0.9894697869653525, 0.9579237814903474], "final_y": [0.16485764829082716, 0.164856355538899, 0.16485724089602083]}, "mutation_prompt": null}
{"id": "b750a937-d7d9-4ffb-b830-d65d7d36db5d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.cos(np.pi * num_evals / self.budget))  # Changed from sin to cos for different mutation adaptation\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / (self.dim ** 1.1)  # Adjust penalty scaling exponent\n                trial_fitness = func(trial) + 0.6 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget)\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance adaptive mutation and periodicity penalty for improved convergence precision.", "configspace": "", "generation": 86, "fitness": 0.935460112499201, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.012. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7efdf902-4448-4cd0-a459-d5e00be7152e", "metadata": {"aucs": [0.9203141339702969, 0.9373602664003612, 0.948705937126945], "final_y": [0.16485691471106878, 0.16485938916718412, 0.16485918972330582]}, "mutation_prompt": null}
{"id": "6380827f-6ff3-446e-9a10-b0d98e5c17e9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))  # Enhanced adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                # Dynamic crossover rate to balance exploration and exploitation\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim  # Normalized periodicity scaling\n                trial_fitness = func(trial) + 0.6 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Reduced periodicity influence\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-2.5 * num_evals/self.budget)  # Slightly adjusted local search decay\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance adaptive local search by modifying decay rate for increased convergence efficiency.", "configspace": "", "generation": 87, "fitness": 0.9780166299539275, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7efdf902-4448-4cd0-a459-d5e00be7152e", "metadata": {"aucs": [0.9866563214060827, 0.9894697869653525, 0.9579237814903474], "final_y": [0.16485764829082716, 0.164856355538899, 0.16485724089602083]}, "mutation_prompt": null}
{"id": "92a29082-c576-4e03-9cc5-12f4b5e088b6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))**1.2  # Adjust mutation factor decay\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim\n                trial_fitness = func(trial) + 0.5 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjust penalty influence\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget)\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance the adaptive local search strategy by fine-tuning the mutation factor decay and adjusting the periodicity penalty influence for improved convergence.", "configspace": "", "generation": 88, "fitness": 0.9780460703446993, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.013. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7efdf902-4448-4cd0-a459-d5e00be7152e", "metadata": {"aucs": [0.9864398439608907, 0.9878486136553061, 0.9598497534179008], "final_y": [0.1648566192424138, 0.1648565504930234, 0.16485842908828474]}, "mutation_prompt": null}
{"id": "04853936-f0e8-4cd9-be32-a7031e973fd2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n        best_fitness = np.min(fitness)  # Track best fitness\n        \n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))**1.2\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Adaptive penalty based on progress\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim\n                trial_fitness = func(trial) + 0.3 * periodicity_penalty * (1 - fitness[i] / best_fitness)\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n                    best_fitness = min(best_fitness, trial_fitness)\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-2.5 * num_evals/self.budget)\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n                        best_fitness = min(best_fitness, local_fitness)\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.2  # Enhance periodicity detection\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive population dynamics and enhanced periodicity detection to refine convergence and leverage constructive interference principles.", "configspace": "", "generation": 89, "fitness": 0.9764862104433929, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "92a29082-c576-4e03-9cc5-12f4b5e088b6", "metadata": {"aucs": [0.98643984265732, 0.9878486136553061, 0.955170175017553], "final_y": [0.16485665579653253, 0.1648565504930234, 0.16485643135683858]}, "mutation_prompt": null}
{"id": "137acff4-7e23-44a4-bc63-adbad97fceea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))**1.5  # Adjust mutation factor decay\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim\n                dynamic_penalty_factor = self.penalty_factor * (num_evals / self.budget)\n                trial_fitness = func(trial) + 0.5 * dynamic_penalty_factor * periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget)\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Improve mutation factor adaptiveness and add a dynamic penalty factor for better convergence.", "configspace": "", "generation": 90, "fitness": 0.9724659956367088, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.019. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "92a29082-c576-4e03-9cc5-12f4b5e088b6", "metadata": {"aucs": [0.9863912032189367, 0.985230645992968, 0.9457761376982217], "final_y": [0.1648576274019139, 0.1648563079322477, 0.16485784215590615]}, "mutation_prompt": null}
{"id": "ebf8539d-0ad5-41e6-9507-c60a485883b1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))**1.2  # Adjust mutation factor decay\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim\n                trial_fitness = func(trial) + 0.5 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjust penalty influence\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                diversity = np.std(population, axis=0).mean()  # Calculate diversity\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget) * (1 + diversity)\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Fine-tune adaptive local search probability by incorporating its dependence on population diversity for enhanced convergence.", "configspace": "", "generation": 91, "fitness": 0.8795929829785036, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.111. And the mean value of best solutions found was 0.196 (0. is the best) with standard deviation 0.044.", "error": "", "parent_id": "92a29082-c576-4e03-9cc5-12f4b5e088b6", "metadata": {"aucs": [0.9807222990981924, 0.7247218134127591, 0.9333348364245591], "final_y": [0.16485666691134826, 0.25781056720859274, 0.16485974012143678]}, "mutation_prompt": null}
{"id": "65dce803-913f-4b63-9d2b-315e214800ea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85  # Adjusted adaptive mutation strategy\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                a, b, c = population[np.random.choice(self.pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))**1.2  # Adjust mutation factor decay\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim\n                trial_fitness = func(trial) + 0.5 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjust penalty influence\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-4.0 * num_evals/self.budget)  # Adjust trigger condition\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5  \n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a dynamic penalty adaptation strategy and optimize the local search trigger condition for enhanced convergence.", "configspace": "", "generation": 92, "fitness": 0.9775743936169462, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "92a29082-c576-4e03-9cc5-12f4b5e088b6", "metadata": {"aucs": [0.9864402443131056, 0.9878486136553061, 0.9584343228824269], "final_y": [0.16485596569230854, 0.1648565504930234, 0.1648606339668004]}, "mutation_prompt": null}
{"id": "cf662206-8fe0-41e1-8a6a-ce90c20183be", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n        self.dynamic_pop_size = lambda evals: max(5, int(self.pop_size * (1 - evals / self.budget)))  # Dynamic pop size\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            current_pop_size = self.dynamic_pop_size(num_evals)\n            for i in range(current_pop_size):\n                a, b, c = population[np.random.choice(current_pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))**1.2\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial = self.frequency_domain_transform(trial)  # Apply frequency domain transformation\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim\n                trial_fitness = func(trial) + 0.5 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget)\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def frequency_domain_transform(self, solution):\n        transformed = np.fft.ifft(np.fft.fft(solution) * np.exp(-0.1 * np.arange(self.dim)))\n        return np.real(transformed)\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Integrate a dynamic population size and frequency domain transformation to improve exploration and convergence in multilayer photonic structures optimization.", "configspace": "", "generation": 93, "fitness": 0.9792920686931504, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "92a29082-c576-4e03-9cc5-12f4b5e088b6", "metadata": {"aucs": [0.9942541906317169, 0.9695471122830774, 0.9740749031646573], "final_y": [0.16485589692486435, 0.16485601671810157, 0.1648560496039756]}, "mutation_prompt": null}
{"id": "98c8ce19-f2a7-47dc-804e-351e40e65854", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n        self.dynamic_pop_size = lambda evals: max(5, int(self.pop_size * (1 - evals / self.budget)))  # Dynamic pop size\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            current_pop_size = self.dynamic_pop_size(num_evals)\n            for i in range(current_pop_size):\n                a, b, c = population[np.random.choice(current_pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))**1.2\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.2 * (num_evals / self.budget)  # Updated CR calculation\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial = self.frequency_domain_transform(trial)\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim\n                trial_fitness = func(trial) + self.penalty_factor * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted penalty factor\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget)\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def frequency_domain_transform(self, solution):\n        transformed = np.fft.ifft(np.fft.fft(solution) * np.exp(-0.1 * np.arange(self.dim)))\n        return np.real(transformed)\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5\n        return penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced periodicity penalty and adaptive crossover strategy to improve convergence in multilayer photonic structures optimization.", "configspace": "", "generation": 94, "fitness": 0.9785000708995266, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cf662206-8fe0-41e1-8a6a-ce90c20183be", "metadata": {"aucs": [0.9942541906317169, 0.9671711189022058, 0.9740749031646573], "final_y": [0.16485589692486435, 0.16485597568774912, 0.1648560496039756]}, "mutation_prompt": null}
{"id": "d266ce4d-518e-4f3d-b833-3a62451a8eb6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n        self.dynamic_pop_size = lambda evals: max(5, int(self.pop_size * (1 - evals / self.budget)))  # Dynamic pop size\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            current_pop_size = self.dynamic_pop_size(num_evals)\n            for i in range(current_pop_size):\n                a, b, c = population[np.random.choice(current_pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))**1.2\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial = self.frequency_domain_transform(trial)  # Apply frequency domain transformation\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim\n                trial_fitness = func(trial) + 0.3 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted penalty weight\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-4.0 * num_evals/self.budget)  # Adjusted decay factor\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def frequency_domain_transform(self, solution):\n        transformed = np.fft.ifft(np.fft.fft(solution) * np.exp(-0.1 * np.arange(self.dim)))\n        return np.real(transformed)\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance solution periodicity preservation and adaptability by refining the periodicity penalty function and adaptive local search probability.", "configspace": "", "generation": 95, "fitness": 0.9792918037832142, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cf662206-8fe0-41e1-8a6a-ce90c20183be", "metadata": {"aucs": [0.9942541906317169, 0.9695469443238083, 0.9740742763941178], "final_y": [0.16485589692486435, 0.16485623101314895, 0.16485594162281814]}, "mutation_prompt": null}
{"id": "5e752553-58fa-429b-b093-f9553617375f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n        self.dynamic_pop_size = lambda evals: max(5, int(self.pop_size * (1 - evals / self.budget)))  # Dynamic pop size\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            current_pop_size = self.dynamic_pop_size(num_evals)\n            for i in range(current_pop_size):\n                a, b, c = population[np.random.choice(current_pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))**1.2\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial = self.frequency_domain_transform(trial)  # Apply frequency domain transformation\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim\n                trial_fitness = func(trial) + 0.5 * periodicity_penalty * (1 - fitness[i] / np.min(fitness)) * (1 - num_evals / self.budget)\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget)\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def frequency_domain_transform(self, solution):\n        transformed = np.fft.ifft(np.fft.fft(solution) * np.exp(-0.1 * np.arange(self.dim)))\n        return np.real(transformed)\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Introduced adaptive periodicity penalty scaling to enhance convergence towards periodic solutions while adhering to the 1.4% modification constraint.", "configspace": "", "generation": 96, "fitness": 0.9792920686931504, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cf662206-8fe0-41e1-8a6a-ce90c20183be", "metadata": {"aucs": [0.9942541906317169, 0.9695471122830774, 0.9740749031646573], "final_y": [0.16485589692486435, 0.16485601671810157, 0.1648560496039756]}, "mutation_prompt": null}
{"id": "fed092ea-70b6-4e04-92bc-453f22f83db3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n        self.dynamic_pop_size = lambda evals: max(5, int(self.pop_size * (1 - evals / self.budget)))  # Dynamic pop size\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            current_pop_size = self.dynamic_pop_size(num_evals)\n            for i in range(current_pop_size):\n                a, b, c = population[np.random.choice(current_pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / self.budget))**1.2\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial = self.frequency_domain_transform(trial)  # Apply frequency domain transformation\n\n                periodicity_penalty = self.calculate_periodicity_penalty(trial) / self.dim\n                trial_fitness = func(trial) + 0.4 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))  # Adjusted penalty factor\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-2.5 * num_evals/self.budget)  # Adjusted decay rate\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def frequency_domain_transform(self, solution):\n        transformed = np.fft.ifft(np.fft.fft(solution) * np.exp(-0.1 * np.arange(self.dim)))\n        return np.real(transformed)\n\n    def calculate_periodicity_penalty(self, solution):\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**1.5\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance periodicity encouragement and dynamic local search strategies to refine solution convergence in multilayer photonic structures optimization.", "configspace": "", "generation": 97, "fitness": 0.9792918997351467, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cf662206-8fe0-41e1-8a6a-ce90c20183be", "metadata": {"aucs": [0.9942541906317169, 0.9695466054090659, 0.9740749031646573], "final_y": [0.16485589692486435, 0.1648561878614938, 0.1648560496039756]}, "mutation_prompt": null}
{"id": "6b59ba1e-9b87-4ab8-b0e3-ad3ebd99a126", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n        self.dynamic_pop_size = lambda evals: max(5, int(self.pop_size * (1 - evals / self.budget)))\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            current_pop_size = self.dynamic_pop_size(num_evals)\n            for i in range(current_pop_size):\n                a, b, c = population[np.random.choice(current_pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.cos(np.pi * num_evals / (2 * self.budget)))**1.2  # Changed line\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.1 * (num_evals / self.budget)\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial = self.frequency_domain_transform(trial)\n\n                periodicity_penalty = self.calculate_periodicity_penalty_v2(trial) / self.dim  # Changed line\n                trial_fitness = func(trial) + 0.5 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget)\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def frequency_domain_transform(self, solution):\n        transformed = np.fft.ifft(np.fft.fft(solution) * np.exp(-0.1 * np.arange(self.dim)))\n        return np.real(transformed)\n\n    def calculate_periodicity_penalty_v2(self, solution):  # Changed line\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            penalty += np.sum((solution[i:i+period] - solution[i+period:i+2*period])**2)**2  # Changed line\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance the dynamic adaptation of parameters and introduce a more sophisticated periodicity penalty to improve convergence and solution quality.", "configspace": "", "generation": 98, "fitness": 0.9800024755777509, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.012. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cf662206-8fe0-41e1-8a6a-ce90c20183be", "metadata": {"aucs": [0.9881007904788149, 0.9888095606026315, 0.9630970756518065], "final_y": [0.1648559505157774, 0.16485592020860118, 0.16485579685375173]}, "mutation_prompt": null}
{"id": "1b8bec98-7e06-4036-b0dc-f06c0aa2f48a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.F = 0.85\n        self.CR = 0.9\n        self.local_search_probability = 0.5\n        self.penalty_factor = 100.0\n        self.dynamic_pop_size = lambda evals: max(5, int(self.pop_size * (1 - evals / self.budget)))\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        num_evals = self.pop_size\n\n        while num_evals < self.budget:\n            new_population = np.copy(population)\n            current_pop_size = self.dynamic_pop_size(num_evals)\n            for i in range(current_pop_size):\n                a, b, c = population[np.random.choice(current_pop_size, 3, replace=False)]\n                adaptive_F = self.F * (np.sin(np.pi * num_evals / (2 * self.budget)))**1.5  # Changed line\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                dynamic_CR = self.CR * (1 - num_evals / self.budget) + 0.2 * (num_evals / self.budget)  # Changed line\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial = self.frequency_domain_transform(trial)\n\n                periodicity_penalty = self.calculate_periodicity_penalty_v3(trial) / self.dim  # Changed line\n                trial_fitness = func(trial) + 0.5 * periodicity_penalty * (1 - fitness[i] / np.min(fitness))\n                num_evals += 1\n\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n\n                adaptive_local_search_prob = self.local_search_probability * np.exp(-3.0 * num_evals/self.budget)\n                if np.random.rand() < adaptive_local_search_prob:\n                    local_res = minimize(func, trial, method='L-BFGS-B', bounds=np.array(list(zip(bounds.lb, bounds.ub))))\n                    local_fitness = local_res.fun\n                    num_evals += local_res.nfev\n                    if local_fitness < fitness[i]:\n                        new_population[i] = local_res.x\n                        fitness[i] = local_fitness\n\n                if num_evals >= self.budget:\n                    break\n            population = new_population\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def frequency_domain_transform(self, solution):\n        transformed = np.fft.ifft(np.fft.fft(solution) * np.exp(-0.1 * np.arange(self.dim)))\n        return np.real(transformed)\n\n    def calculate_periodicity_penalty_v3(self, solution):  # Changed line\n        period = 2\n        penalty = 0\n        for i in range(0, len(solution) - period):\n            segment = solution[i:i+2*period]\n            segment_entropy = -np.sum(segment * np.log2(segment + 1e-9))  # Changed line\n            penalty += segment_entropy  # Changed line\n        return self.penalty_factor * penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        return best_solution, best_fitness", "name": "HybridMetaheuristicOptimizer", "description": "Enhance parameter adaptation and incorporate entropy-based periodicity for improved convergence in complex landscapes.", "configspace": "", "generation": 99, "fitness": 0.976017479245684, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.013. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6b59ba1e-9b87-4ab8-b0e3-ad3ebd99a126", "metadata": {"aucs": [0.9942440754179823, 0.9670877105231117, 0.9667206517959581], "final_y": [0.16485597554311737, 0.16485611087751817, 0.16485604746519122]}, "mutation_prompt": null}
