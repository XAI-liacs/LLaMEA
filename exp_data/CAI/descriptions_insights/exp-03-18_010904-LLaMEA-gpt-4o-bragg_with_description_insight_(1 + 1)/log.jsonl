{"id": "18d79811-b547-434e-af51-3353f15d26df", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.1\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size//2, replace=False, p=fitness/fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1, parent2 = parents[i], parents[i+1]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Hybrid Genetic Algorithm combining global exploration with local search to optimize multilayer structures by leveraging periodicity and symmetry.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 194, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 288, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 149, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 56, in __call__\nIndexError: index 25 is out of bounds for axis 0 with size 25\n.", "error": "IndexError('index 25 is out of bounds for axis 0 with size 25')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 194, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 288, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 149, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 56, in __call__\nIndexError: index 25 is out of bounds for axis 0 with size 25\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "6f91ce2d-e53b-4283-aeab-eb6c75711358", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.1\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size//2, replace=False, p=fitness/fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        adaptive_mutation_rate = self.mutation_rate * (1 - self.func_evals / self.budget)  # Adaptive mutation rate\n        if np.random.rand() < adaptive_mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1, parent2 = parents[i], parents[i+1]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Refined Hybrid Genetic Algorithm incorporating adaptive mutation rate adjustment for enhanced exploration and convergence.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 25 is out of bounds for axis 0 with size 25').", "error": "IndexError('index 25 is out of bounds for axis 0 with size 25')", "parent_id": "18d79811-b547-434e-af51-3353f15d26df", "metadata": {}, "mutation_prompt": null}
{"id": "9a0c8461-a5d1-42ca-9673-7d1ec6afb206", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.1\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size//2, replace=False, p=(1/fitness)/((1/fitness).sum()))\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1, parent2 = parents[i], parents[i+1]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Improved Hybrid Genetic Algorithm with refined parent selection for enhanced diversity and periodicity in multilayer optimization.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 25 is out of bounds for axis 0 with size 25').", "error": "IndexError('index 25 is out of bounds for axis 0 with size 25')", "parent_id": "18d79811-b547-434e-af51-3353f15d26df", "metadata": {}, "mutation_prompt": null}
{"id": "a6cf56f4-3074-43bc-a9c5-7793ba3c963d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.1\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Improved genetic algorithm with a fix for offspring parent assignments and optimized parent selection to prevent out-of-bounds errors and enhance performance.", "configspace": "", "generation": 3, "fitness": 0.9809527977759154, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "18d79811-b547-434e-af51-3353f15d26df", "metadata": {"aucs": [0.9843079547803478, 0.9743451093928138, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485780306043385, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "ab5937f2-136f-4d36-9065-2a7729f9e23e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.1\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=(1/(1 + fitness)) / np.sum(1/(1 + fitness)))\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhanced parent selection strategy to increase diversity and explore the search space more effectively.", "configspace": "", "generation": 4, "fitness": 0.9763648129144946, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cf56f4-3074-43bc-a9c5-7793ba3c963d", "metadata": {"aucs": [0.977215274344917, 0.9758246603979159, 0.9760545040006511], "final_y": [0.16485729075155675, 0.16485792096174823, 0.16485666011226174]}, "mutation_prompt": null}
{"id": "2a7b0462-e0d1-4209-9b17-b5eb8d51c976", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.1\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opposite_population = lb + ub - population  # Symmetric initialization\n        return np.vstack((population, opposite_population))[:self.population_size]\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhanced Hybrid Genetic Algorithm with symmetric initialization to improve search space exploration.", "configspace": "", "generation": 5, "fitness": 0.9809527977759154, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cf56f4-3074-43bc-a9c5-7793ba3c963d", "metadata": {"aucs": [0.9843079547803478, 0.9743451093928138, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485780306043385, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "300d516f-8e5e-4365-9dda-ff22c23215f8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.1\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        diversity = np.std(child)\n        adaptive_mutation_rate = self.mutation_rate * (1 + 0.5 * diversity) # Changed line\n        if np.random.rand() < adaptive_mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhanced exploration by improving mutation strategy to adaptively adjust mutation rates based on diversity.", "configspace": "", "generation": 6, "fitness": 0.9807163542086972, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cf56f4-3074-43bc-a9c5-7793ba3c963d", "metadata": {"aucs": [0.9863585379013454, 0.9715851717068831, 0.9842053530178632], "final_y": [0.16485777577090388, 0.16485602230583563, 0.16485607145507342]}, "mutation_prompt": null}
{"id": "95921347-707d-446b-99f3-9d18b991da14", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.1\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        def periodic_cost(x):\n            return func(x) + np.sum(np.diff(x[:self.dim//2])**2)  # Encourage periodicity\n        result = minimize(periodic_cost, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhanced hybrid genetic algorithm with improved local search integration for periodicity and local minima escape.", "configspace": "", "generation": 7, "fitness": 0.9243620776243273, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.924 with standard deviation 0.015. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "a6cf56f4-3074-43bc-a9c5-7793ba3c963d", "metadata": {"aucs": [0.9450257696890543, 0.9158771200419872, 0.9121833431419403], "final_y": [0.16932086683787828, 0.1786617256889993, 0.17237275656069684]}, "mutation_prompt": null}
{"id": "bc067081-5f38-4c9f-8537-45eaf475eeda", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.1\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop  # Quasi-oppositional initialization\n        return np.vstack((pop, opp_pop))[:self.population_size]\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def enforce_periodicity(self, individual):\n        period = 2\n        individual[:period] = np.mean(individual[:period])\n        individual[period:self.dim] = np.tile(individual[:period], self.dim // period)\n        return individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return self.enforce_periodicity(population[0])", "name": "HybridGeneticOptimizer", "description": "Enhanced genetic algorithm with quasi-oppositional initialization and periodicity constraint for improved convergence and solution quality.", "configspace": "", "generation": 8, "fitness": 0.9809527977759154, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cf56f4-3074-43bc-a9c5-7793ba3c963d", "metadata": {"aucs": [0.9843079547803478, 0.9743451093928138, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485780306043385, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "4fff5c16-ca02-469e-b14b-4e2e3abc49ac", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.1\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            diversity_factor = int(np.std(parent1) * self.dim)  # Added line for dynamic crossover point\n            point = np.random.randint(1, self.dim) if diversity_factor < 1 else diversity_factor\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhanced crossover strategy by dynamically adapting crossover points based on population diversity to improve exploration and exploitation balance.", "configspace": "", "generation": 9, "fitness": 0.9526359073169868, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.953 with standard deviation 0.037. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cf56f4-3074-43bc-a9c5-7793ba3c963d", "metadata": {"aucs": [0.9006837449345004, 0.9730186478618754, 0.9842053291545846], "final_y": [0.16485657128908893, 0.164860019457576, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "8e24b62f-63cb-42b2-9a1a-a45483d56ef9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.1\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        # Line changed for fitness scaling\n        fitness_scaled = fitness - fitness.min() + 1e-6  # Avoid division by zero\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness_scaled / fitness_scaled.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduced fitness scaling by subtracting the minimum fitness to enhance selection pressure and improve convergence speed.", "configspace": "", "generation": 10, "fitness": 0.9422356914831366, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.942 with standard deviation 0.031. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "a6cf56f4-3074-43bc-a9c5-7793ba3c963d", "metadata": {"aucs": [0.910876674794905, 0.9316250704999198, 0.9842053291545846], "final_y": [0.16485638449799855, 0.1818804950148526, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "26eadd6d-075f-4be0-8e6e-9e6367f1034c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Incorporate diversity enhancement by slightly increasing mutation rate to improve exploration and avoid premature convergence in the optimization process.", "configspace": "", "generation": 11, "fitness": 0.9809538696777113, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cf56f4-3074-43bc-a9c5-7793ba3c963d", "metadata": {"aucs": [0.9843079547803478, 0.9743483250982016, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485780383441784, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "f6d0763e-f5dc-4fb9-b752-d5f100e56395", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 100})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhanced the local search by increasing the max iterations of L-BFGS-B method to improve convergence precision.", "configspace": "", "generation": 12, "fitness": 0.9809538696777113, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "26eadd6d-075f-4be0-8e6e-9e6367f1034c", "metadata": {"aucs": [0.9843079547803478, 0.9743483250982016, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485780383441784, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "71e1c441-d595-41eb-bf61-e3dbeb1f8ec4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 55  # Increased population size for enhanced exploration\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.10  # Reduced mutation rate to improve convergence balance\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance exploration by increasing population size slightly and reducing mutation rate to balance diversity and convergence.", "configspace": "", "generation": 13, "fitness": 0.9805527515604094, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "26eadd6d-075f-4be0-8e6e-9e6367f1034c", "metadata": {"aucs": [0.9839550114983414, 0.9746286529382724, 0.9830745902446146], "final_y": [0.1648568818594306, 0.16485742615395294, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "30337a03-81f1-4b09-9f08-f5cfc5ab7150", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        diversity_factor = np.std(child)  # Dynamic mutation based on population diversity\n        if np.random.rand() < self.mutation_rate * (1 + diversity_factor):\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance exploration by introducing dynamic mutation based on diversity while preserving the original framework.", "configspace": "", "generation": 14, "fitness": 0.9807163462542711, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "26eadd6d-075f-4be0-8e6e-9e6367f1034c", "metadata": {"aucs": [0.9863585379013454, 0.9715851717068831, 0.9842053291545846], "final_y": [0.16485777577090388, 0.16485602230583563, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "d2817bee-4615-4918-a724-22c1ac87171a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size - 1]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Retain the best individual from the previous generation (elitism)\n            best_of_prev_gen = np.argmin(fitness)\n            if fitness[best_of_prev_gen] < fitness[0]:\n                population[0] = combined_population[best_of_prev_gen]\n                fitness[0] = combined_fitness[best_of_prev_gen]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce elitism by retaining the best individual from the previous generation to the next, ensuring high-quality solutions are preserved.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "26eadd6d-075f-4be0-8e6e-9e6367f1034c", "metadata": {}, "mutation_prompt": null}
{"id": "d8d7207e-0e9f-4c38-a2c3-a7fce81efa1c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n\n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n            # Adjust crossover rate based on convergence\n            self.crossover_rate = min(1.0, 0.5 + 0.5 * (self.func_evals / self.budget))\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce a dynamic crossover rate based on convergence speed to balance exploration and exploitation adaptively.", "configspace": "", "generation": 16, "fitness": 0.9809426870639761, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "26eadd6d-075f-4be0-8e6e-9e6367f1034c", "metadata": {"aucs": [0.9843079547803478, 0.974314777256996, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485780423011553, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "411739f6-bcbf-4ad2-af79-3397bb9563d9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 100})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance the local search process by increasing the local optimization iteration limit, improving fine-tuning near promising solutions.", "configspace": "", "generation": 17, "fitness": 0.9809538696777113, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "26eadd6d-075f-4be0-8e6e-9e6367f1034c", "metadata": {"aucs": [0.9843079547803478, 0.9743483250982016, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485780383441784, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "ff8551cd-27b9-4870-9a63-98671a34fe12", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Adaptive mutation rate based on fitness diversity\n        fitness_std = np.std(self.current_fitness)\n        adaptive_mutation_rate = min(max(0.1 * fitness_std, 0.01), 0.3)\n        if np.random.rand() < adaptive_mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        self.current_fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, self.current_fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((self.current_fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            self.current_fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                self.current_fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduced adaptive mutation rate to dynamically balance exploration and exploitation based on current fitness diversity.", "configspace": "", "generation": 18, "fitness": 0.9667127848116174, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.967 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "26eadd6d-075f-4be0-8e6e-9e6367f1034c", "metadata": {"aucs": [0.9843079547803478, 0.9316250704999198, 0.9842053291545846], "final_y": [0.16485687662560167, 0.1818804950148526, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "0f75c508-573f-4939-ac33-2959875ea1a5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        diversity = np.std(child)\n        adjusted_mutation_rate = self.mutation_rate * (1 + diversity)  # Dynamic adjustment\n        if np.random.rand() < adjusted_mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance convergence by dynamically adjusting mutation rates based on diversity metrics within the population.", "configspace": "", "generation": 19, "fitness": 0.9807163462542711, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "26eadd6d-075f-4be0-8e6e-9e6367f1034c", "metadata": {"aucs": [0.9863585379013454, 0.9715851717068831, 0.9842053291545846], "final_y": [0.16485777577090388, 0.16485602230583563, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "83a5f8ca-0df4-4e55-b2b2-fbd8e50677b3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            # Focus mutation towards boundaries\n            child[idx] = np.random.uniform(lb, lb + 0.1 * (ub - lb)) if np.random.rand() < 0.5 else np.random.uniform(ub - 0.1 * (ub - lb), ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Adjust mutation strategy to focus on boundary regions for better exploration in parameter space.", "configspace": "", "generation": 20, "fitness": 0.966996419929818, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.967 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "26eadd6d-075f-4be0-8e6e-9e6367f1034c", "metadata": {"aucs": [0.9851588601349495, 0.9316250704999198, 0.9842053291545846], "final_y": [0.1648565805037192, 0.1818804950148526, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "1ceedf86-9d3c-49bd-a011-685375295cbd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point1, point2 = sorted(np.random.randint(1, self.dim, 2))  # Two-point crossover\n            child1 = np.concatenate([parent1[:point1], parent2[point1:point2], parent1[point2:]])\n            child2 = np.concatenate([parent2[:point1], parent1[point1:point2], parent2[point2:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Slightly refine the crossover strategy by using a two-point crossover to potentially enhance offspring diversity.", "configspace": "", "generation": 21, "fitness": 0.9701297262730378, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.970 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "26eadd6d-075f-4be0-8e6e-9e6367f1034c", "metadata": {"aucs": [0.9612544482561182, 0.964929401408411, 0.9842053291545846], "final_y": [0.1648560136227546, 0.1648566363040337, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "4d018deb-749d-4de1-b513-a85458cfa802", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 60  # Increase population size to improve exploration\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance exploration by increasing the population size for broader search space sampling.", "configspace": "", "generation": 22, "fitness": 0.9671393555419238, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.967 with standard deviation 0.013. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "26eadd6d-075f-4be0-8e6e-9e6367f1034c", "metadata": {"aucs": [0.9507099182138001, 0.9685587081846045, 0.9821494402273665], "final_y": [0.16485637777140483, 0.16485768187580907, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "229c1968-40c7-45f4-a826-5ee08cc117d4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce adaptive crossover rate based on generation progress to balance exploration and exploitation.", "configspace": "", "generation": 23, "fitness": 0.9814446184127915, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "26eadd6d-075f-4be0-8e6e-9e6367f1034c", "metadata": {"aucs": [0.9843079547803478, 0.9758205713034424, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485822588696275, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "79421e04-d0ca-4f95-9e3f-17a955f06598", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Initial mutation rate\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        adaptive_mutation_rate = self.mutation_rate * (1 - progress)  # Decrease mutation rate over time\n        if np.random.rand() < adaptive_mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Implement a progressive mutation rate that decreases over time to enhance exploitation in later stages of the optimization process.", "configspace": "", "generation": 24, "fitness": 0.9814445228902389, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "229c1968-40c7-45f4-a826-5ee08cc117d4", "metadata": {"aucs": [0.9843079547803478, 0.975820284735784, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485822516418247, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "aeebb3a5-8d1a-4f05-86e7-f9d9670b9694", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 100})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Improve local search by increasing max iterations in L-BFGS-B for better fine-tuning.", "configspace": "", "generation": 25, "fitness": 0.9814446184127915, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "229c1968-40c7-45f4-a826-5ee08cc117d4", "metadata": {"aucs": [0.9843079547803478, 0.9758205713034424, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485822588696275, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "0b7c6b04-fd60-41c1-b24d-846b057ac854", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        adaptive_mutation_rate = self.mutation_rate * (1 - progress) + 0.05 * progress\n        if np.random.rand() < adaptive_mutation_rate:  # Adjust mutation rate\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance mutation diversity by dynamically adjusting the mutation rate based on generation progress.", "configspace": "", "generation": 26, "fitness": 0.9814445889729851, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "229c1968-40c7-45f4-a826-5ee08cc117d4", "metadata": {"aucs": [0.9843079547803478, 0.9758204829840229, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485822589590815, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "0ef8df8a-302d-446a-b6ac-4e0139cc6375", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)]  # Preserve the best individual\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce elitism by preserving the best individual in the population to maintain top solutions through generations.", "configspace": "", "generation": 27, "fitness": 0.9814446184127915, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "229c1968-40c7-45f4-a826-5ee08cc117d4", "metadata": {"aucs": [0.9843079547803478, 0.9758205713034424, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485822588696275, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "187427f5-db46-4f29-a39a-3d44cbd7bb3e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        remaining_budget = self.budget - self.func_evals\n        max_iter = min(50, remaining_budget)  # Updated line\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': max_iter})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance exploitation by adjusting the local search iterations based on the remaining budget.", "configspace": "", "generation": 28, "fitness": 0.9814446184127915, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "229c1968-40c7-45f4-a826-5ee08cc117d4", "metadata": {"aucs": [0.9843079547803478, 0.9758205713034424, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485822588696275, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "8cfa8f0e-b9e6-424c-87e9-140340efd18f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        adaptive_mutation_rate = self.mutation_rate * (1 - progress) + 0.05 * progress  # Adjust mutation rate based on progress\n        if np.random.rand() < adaptive_mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce a dynamically adaptive mutation rate based on the convergence progress to maintain diversity in the population.", "configspace": "", "generation": 29, "fitness": 0.9814445889729851, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "229c1968-40c7-45f4-a826-5ee08cc117d4", "metadata": {"aucs": [0.9843079547803478, 0.9758204829840229, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485822589590815, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "9105a353-ebf2-4015-bb85-7a87a7ac8772", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        tournament_size = 3\n        selected_indices = []\n        for _ in range(self.population_size):\n            participants = np.random.choice(np.arange(self.population_size), tournament_size, replace=False)\n            best = participants[np.argmin(fitness[participants])]\n            selected_indices.append(best)\n        return population[selected_indices]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Slightly adjust the selection strategy to use tournament selection for improved genetic diversity and exploration.", "configspace": "", "generation": 30, "fitness": 0.9780098604614547, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "229c1968-40c7-45f4-a826-5ee08cc117d4", "metadata": {"aucs": [0.9690704387437274, 0.9854912171470009, 0.9794679254936356], "final_y": [0.16485762711891117, 0.16485727840017528, 0.16485776912323868]}, "mutation_prompt": null}
{"id": "77e09133-ae6e-41bd-a0e7-d5352add2eca", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50, 'eps': 1e-8})  # changed eps for step-size adaptation\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Local optimization on best individual to fine-tune using gradient information\n            best_individual = population[0]\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, best_individual, bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce step-size adaptation in local search to improve fine-tuning efficiency near promising regions.  ", "configspace": "", "generation": 31, "fitness": 0.9814446184127915, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "229c1968-40c7-45f4-a826-5ee08cc117d4", "metadata": {"aucs": [0.9843079547803478, 0.9758205713034424, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485822588696275, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "6615925f-e94b-40da-8290-b69b06924097", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce elitism to preserve best solutions across generations for enhanced performance.", "configspace": "", "generation": 32, "fitness": 0.9814446945070223, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "229c1968-40c7-45f4-a826-5ee08cc117d4", "metadata": {"aucs": [0.9843079547803478, 0.9758207995861344, 0.9842053291545846], "final_y": [0.16485687662560167, 0.1648582254523565, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "06264001-9139-4e18-8fa6-e78c136c61cd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            # changed line: introduce periodic perturbation\n            child[idx] = np.random.uniform(lb, ub) * (1 + 0.1 * np.sin(2 * np.pi * idx / self.dim))\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhanced periodicity enforcement by modifying mutation for better exploration of constructive interference.", "configspace": "", "generation": 33, "fitness": 0.9814348735322921, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6615925f-e94b-40da-8290-b69b06924097", "metadata": {"aucs": [0.9843082695718872, 0.9757910218704046, 0.9842053291545846], "final_y": [0.16485672833905596, 0.16485741853603864, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "579d035d-99e3-4b89-b30e-d3b37677c941", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        diversity = np.std(child)  # Adapt mutation rate based on population diversity\n        adaptive_mutation_rate = self.mutation_rate + 0.1 * diversity\n        if np.random.rand() < adaptive_mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Improve exploration by adapting mutation rate based on population diversity.", "configspace": "", "generation": 34, "fitness": 0.9812789634366266, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6615925f-e94b-40da-8290-b69b06924097", "metadata": {"aucs": [0.9863594351539804, 0.9732721260013146, 0.9842053291545846], "final_y": [0.16485742832280592, 0.16485820458754874, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "2ab62fbb-41fe-455a-bbeb-7f5658b4be38", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size // 2, self.dim))\n        opp_pop = lb + ub - pop  # Quasi-Oppositional Initialization\n        return np.vstack((pop, opp_pop))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhancing exploration by introducing Quasi-Oppositional initialization to better sample the search space.", "configspace": "", "generation": 35, "fitness": 0.9618450020653464, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.962 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6615925f-e94b-40da-8290-b69b06924097", "metadata": {"aucs": [0.9718432370268385, 0.953297232877153, 0.9603945362920476], "final_y": [0.16485901689215987, 0.1648635630091817, 0.16486344565044553]}, "mutation_prompt": null}
{"id": "6e430cfc-4a52-4c1c-814a-820e47eb4dab", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Base mutation rate\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        diversity = np.std(child)  # Calculate diversity as standard deviation\n        dynamic_mutation_rate = self.mutation_rate + 0.1 * (1 - diversity)\n        if np.random.rand() < dynamic_mutation_rate:  # Use dynamic mutation rate\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce dynamic mutation rates based on diversity to improve exploration and convergence.", "configspace": "", "generation": 36, "fitness": 0.9667127848116174, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.967 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "6615925f-e94b-40da-8290-b69b06924097", "metadata": {"aucs": [0.9843079547803478, 0.9316250704999198, 0.9842053291545846], "final_y": [0.16485687662560167, 0.1818804950148526, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "91f5e8f2-50a6-4890-a3fb-d6973d2ef746", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress**2) + 0.2 * progress**2  # More aggressive adaptation\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        adaptive_mutation_rate = self.mutation_rate * (1 - progress) + 0.1 * progress  # Gradual mutation rate change\n        if np.random.rand() < adaptive_mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance adaptive crossover and mutation strategies for improved exploration and convergence.", "configspace": "", "generation": 37, "fitness": 0.9806895108189736, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6615925f-e94b-40da-8290-b69b06924097", "metadata": {"aucs": [0.9843079547803478, 0.9735552485219882, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485647793766722, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "0ab32aa4-7e06-4fd7-b71c-4fce1474aac3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget  # Dynamic mutation adjustment\n        adaptive_mutation_rate = self.mutation_rate * (1 - progress) + 0.1 * progress\n        if np.random.rand() < adaptive_mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n            # Introduce periodicity bias in best individual's adjustment\n            if self.func_evals < self.budget:\n                period = self.dim // 2  # Example periodicity heuristic\n                adjustment = np.sin(np.arange(self.dim) * (2 * np.pi / period)) * 0.01\n                population[0] = np.clip(population[0] + adjustment, bounds.lb, bounds.ub)\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Incorporate layered periodicity guidance and dynamic mutation to enhance exploration and exploitation balance.", "configspace": "", "generation": 38, "fitness": 0.980648207248651, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6615925f-e94b-40da-8290-b69b06924097", "metadata": {"aucs": [0.9843101882789818, 0.9734287616482697, 0.9842056718187014], "final_y": [0.16485604381703034, 0.16485653604970674, 0.16485600225931918]}, "mutation_prompt": null}
{"id": "d4e653ad-6c75-4fc1-b172-19ce1047088c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget  # New line for strategic mutation rate\n        if np.random.rand() < self.mutation_rate * (1 - 0.5 * progress):\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce a strategic mutation rate adaptation to enhance diversity and convergence balance.", "configspace": "", "generation": 39, "fitness": 0.9814444606585386, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6615925f-e94b-40da-8290-b69b06924097", "metadata": {"aucs": [0.9843079547803478, 0.9758200980406833, 0.9842053291545846], "final_y": [0.16485687662560167, 0.16485852494774977, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "6f714913-788e-4cc8-b51c-52816dd17785", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        maxiter = min(50, self.budget - self.func_evals)  # Adjust maxiter based on remaining budget\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': maxiter})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance local search efficiency by adjusting L-BFGS-B maximum iterations based on remaining budget.", "configspace": "", "generation": 40, "fitness": 0.9814446945070223, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6615925f-e94b-40da-8290-b69b06924097", "metadata": {"aucs": [0.9843079547803478, 0.9758207995861344, 0.9842053291545846], "final_y": [0.16485687662560167, 0.1648582254523565, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance diversity by dynamically adjusting mutation rate based on population diversity.", "configspace": "", "generation": 41, "fitness": 0.9815346046439771, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6615925f-e94b-40da-8290-b69b06924097", "metadata": {"aucs": [0.9863587889487954, 0.9740396792657026, 0.9842053457174335], "final_y": [0.16485735451059103, 0.16485913042954614, 0.16485607145507342]}, "mutation_prompt": null}
{"id": "59f42f95-6719-4d43-b6fc-c7d4ecd31d8a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.initial_population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(len(population)), size=len(population), replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        dynamic_population_size = self.initial_population_size  # Dynamic adjustment based on progress\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:dynamic_population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce dynamic population size adjustment to enhance exploration and exploitation balance.  ", "configspace": "", "generation": 42, "fitness": 0.9815346046439771, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9863587889487954, 0.9740396792657026, 0.9842053457174335], "final_y": [0.16485735451059103, 0.16485913042954614, 0.16485607145507342]}, "mutation_prompt": null}
{"id": "ee5510e7-3f25-45ed-91c7-f35a28d1474d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Adjust mutation rate based on fitness improvement\n        fitness_improvement = max(0, 1 - np.std(child))\n        self.mutation_rate = 0.12 + 0.05 * fitness_improvement\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce adaptive mutation rate depending on fitness improvement, enhancing exploration capabilities.", "configspace": "", "generation": 43, "fitness": 0.9814446945070223, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9843079547803478, 0.9758207995861344, 0.9842053291545846], "final_y": [0.16485687662560167, 0.1648582254523565, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "092f5a3d-ddbc-4e50-814e-928c1bd7f0f7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Ensure symmetric layer thicknesses\n        for i in range(0, self.dim, 2):\n            if np.random.rand() < self.mutation_rate:\n                lb, ub = bounds.lb[i], bounds.ub[i]\n                value = np.random.uniform(lb, ub)\n                child[i] = value\n                if i+1 < self.dim:\n                    child[i+1] = value\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Implement periodic mutation by ensuring mutated children have symmetric layer thicknesses to promote constructive interference.", "configspace": "", "generation": 44, "fitness": 0.9696631431863102, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.970 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9509777785117578, 0.9738063218925883, 0.9842053291545846], "final_y": [0.16485891957618914, 0.1648588275384021, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "a90b93fa-7fb9-4560-ac83-afcf537b185b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n        self.diversity_threshold = 0.1  # New parameter to control adaptive behavior\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def adaptive_population_size(self, diversity, bounds):\n        if diversity < self.diversity_threshold:\n            self.population_size = min(self.population_size + 5, 100)\n        else:\n            self.population_size = max(self.population_size - 5, 20)\n        return self.initialize_population(bounds)\n\n    def diversity_measure(self, population):\n        return np.mean(np.std(population, axis=0))\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n            diversity = self.diversity_measure(population)\n            population = self.adaptive_population_size(diversity, bounds)\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Integrate multi-objective optimization with adaptive population size to balance exploration and exploitation for enhanced diversity.", "configspace": "", "generation": 45, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {}, "mutation_prompt": null}
{"id": "598ea914-26de-43ab-9928-e021545fd854", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim - 1)  # Change point selection for more diverse crossover\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Fine-tune the crossover point in the crossover function to improve solution diversity.", "configspace": "", "generation": 46, "fitness": 0.9684602489754047, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9501566116496218, 0.9710188061220079, 0.9842053291545846], "final_y": [0.16485841726643335, 0.1648581278407666, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "1779d950-d0a2-436e-bb56-27bf0d1632a8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (adaptive elitism)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Hybrid optimization that dynamically adjusts mutation rate and incorporates adaptive elitism to improve solution diversity and convergence.", "configspace": "", "generation": 47, "fitness": 0.9815346046439771, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9863587889487954, 0.9740396792657026, 0.9842053457174335], "final_y": [0.16485735451059103, 0.16485913042954614, 0.16485607145507342]}, "mutation_prompt": null}
{"id": "35cded63-f3e0-4c08-bf9b-00baa9c50c4a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            # Change: Adjust population size dynamically based on remaining budget\n            self.population_size = int(50 * (1 - self.func_evals / self.budget) + 10)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Incorporate a dynamic population size adjustment to enhance exploration and exploitation balance as the optimization progresses.", "configspace": "", "generation": 48, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {}, "mutation_prompt": null}
{"id": "4f8ec804-2edb-4de4-b80d-60bd0b8b797c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity and average fitness\n        avg_fitness = np.mean([func(ind) for ind in population])\n        self.mutation_rate = 0.12 + 0.05 * np.std(child) + 0.01 * avg_fitness\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce adaptive mutation rate by also considering the average fitness of the population.", "configspace": "", "generation": 49, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {}, "mutation_prompt": null}
{"id": "96da8cd7-19ee-47f5-a816-51f74e2b5611", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point1, point2 = sorted(np.random.randint(1, self.dim, 2))  # Change: Improved crossover logic by using two crossover points\n            child1 = np.concatenate([parent1[:point1], parent2[point1:point2], parent1[point2:]])\n            child2 = np.concatenate([parent2[:point1], parent1[point1:point2], parent2[point2:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance diversity by dynamically adjusting mutation rate based on population diversity with improved crossover logic.", "configspace": "", "generation": 50, "fitness": 0.9719246202133162, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.013. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9537975545838288, 0.9777709489773929, 0.9842053570787267], "final_y": [0.1648562970537255, 0.16485902198536784, 0.16485607145507342]}, "mutation_prompt": null}
{"id": "8a984d30-139d-4542-ba49-e5a8300e8180", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        ranks = np.argsort(np.argsort(fitness))\n        selection_prob = (ranks + 1) / ranks.sum()\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=selection_prob)\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n\n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Refine population selection by integrating rank-based selection to emphasize high-quality solutions.", "configspace": "", "generation": 51, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('probabilities do not sum to 1').", "error": "ValueError('probabilities do not sum to 1')", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {}, "mutation_prompt": null}
{"id": "a68682f6-b4e8-456a-8b82-1de12c4f04b5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim - 1)  # Change: Adapt crossover point\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance diversity by dynamically adjusting mutation rate based on population diversity and adapt crossover point to balance local and global search.", "configspace": "", "generation": 52, "fitness": 0.9684602489754047, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9501566116496218, 0.9710188061220079, 0.9842053291545846], "final_y": [0.16485841726643335, 0.1648581278407666, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "29e0f6ae-23de-45a0-86ab-3302c1c5a1b6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def adaptive_population(self, current_best, bounds):\n        # Adaptive population size strategy\n        diversity = np.std(current_best)\n        self.population_size = int(np.clip(50 + 10 * diversity, 30, 70))\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            self.adaptive_population(population[np.argmin(fitness)].copy(), bounds)  # Add adaptive population size\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance solution diversity by adding competitive co-evolution and adaptive population size strategies.", "configspace": "", "generation": 53, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {}, "mutation_prompt": null}
{"id": "5aa55baa-b635-4257-bf71-ede49f3268a8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        # Change: Adjust mutation rate based on fitness variance for adaptive scaling\n        fitness_variance = np.var([func(child) for child in self.initialize_population(bounds)])\n        adaptive_mutation_rate = self.mutation_rate * (1 + fitness_variance)\n        if np.random.rand() < adaptive_mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce adaptive mutation scaling based on fitness variance to maintain exploration while converging.", "configspace": "", "generation": 54, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {}, "mutation_prompt": null}
{"id": "1d95d6cb-e7c1-4d9a-88b5-e56c237134ac", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child) + 0.02 * np.var(child)  # Added variance-based adjustment\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce adaptive mutation based on dynamic diversity measures to further enhance solution exploration.", "configspace": "", "generation": 55, "fitness": 0.9812789634366266, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9863594351539804, 0.9732721260013146, 0.9842053291545846], "final_y": [0.16485742832280592, 0.16485820458754874, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "95a25c6b-2199-4c0d-b1b5-8f5d628eb9ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        # Introduce periodic recombination to enhance solution periodicity\n        point = np.random.randint(1, self.dim)\n        child1 = np.array([parent1[i] if i % 2 == 0 else parent2[i] for i in range(self.dim)])\n        child2 = np.array([parent2[i] if i % 2 == 0 else parent1[i] for i in range(self.dim)])\n        return child1, child2\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Refine crossover by introducing periodic recombination to enhance solution periodicity for Bragg mirrors.", "configspace": "", "generation": 56, "fitness": 0.9661724744384221, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.019. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9406021145054392, 0.9737099796552425, 0.9842053291545846], "final_y": [0.16485664679767364, 0.16485952403106807, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "3a4f5a22-57e1-4016-b989-18bc3522ee1e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.3 * progress  # Modified adaptive crossover rate\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance the genetic algorithm by tuning the adaptive crossover rate for improved diversity control.", "configspace": "", "generation": 57, "fitness": 0.9815345335084352, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.986358626379336, 0.9740396449913851, 0.9842053291545846], "final_y": [0.16485773414475968, 0.16485917052642562, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "bf77d83c-d943-47f6-8d61-a49183c69254", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            # Adapt mutation step size based on fitness variance\n            step_size = 0.1 * np.std(bounds.ub - bounds.lb)\n            child[idx] = np.clip(child[idx] + step_size * np.random.randn(), lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce dynamic mutation step size by adapting it to the fitness variance, enhancing exploration-exploitation balance.", "configspace": "", "generation": 58, "fitness": 0.8860647310469808, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.104. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.7423637934864379, 0.9316250704999198, 0.9842053291545846], "final_y": [0.25781339118751434, 0.1818804950148526, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "598f1062-880a-4f4f-907c-4043630a3795", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 100})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Improve local search efficiency by increasing L-BFGS-B max iterations from 50 to 100.", "configspace": "", "generation": 59, "fitness": 0.9815346046439771, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9863587889487954, 0.9740396792657026, 0.9842053457174335], "final_y": [0.16485735451059103, 0.16485913042954614, 0.16485607145507342]}, "mutation_prompt": null}
{"id": "e1418eb1-233d-4050-865a-fcb110c55345", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Introduce a periodic adjustment to mutation rate to encourage exploration\n        if self.func_evals % 100 < 10:  # For 10% of the evaluations, increase the mutation rate\n            self.mutation_rate = 0.3\n        else:\n            self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Adjust mutation strategy to periodically induce higher variability for escaping local minima.", "configspace": "", "generation": 60, "fitness": 0.9814305405265982, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9863586234664796, 0.9737276605176091, 0.9842053375957057], "final_y": [0.16485773414475968, 0.16485845812774036, 0.16485607145507342]}, "mutation_prompt": null}
{"id": "a37d3214-bb69-4d03-8583-d2d080b2af9b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        # Change: Adjust local search intensity dynamically\n        intensity = int(50 * (1 - self.func_evals / self.budget))\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': intensity})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Improve local search efficacy by dynamically adjusting local search intensity based on budget remaining.", "configspace": "", "generation": 61, "fitness": 0.9815346046439771, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9863587889487954, 0.9740396792657026, 0.9842053457174335], "final_y": [0.16485735451059103, 0.16485913042954614, 0.16485607145507342]}, "mutation_prompt": null}
{"id": "a2c9e429-c3a1-478d-915d-53238ad9cb55", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        fitness_diversity = np.std(self.evaluate_population(lambda x: 0, population))  # Calculate fitness diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child) + 0.01 * fitness_diversity  # Added adaptive factor based on fitness diversity\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Refine the mutation strategy by making it dependent on both standard deviation of the population and an adaptive factor based on fitness diversity to improve exploration.", "configspace": "", "generation": 62, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {}, "mutation_prompt": null}
{"id": "e8140dae-efcd-48ed-9817-d69822bbd6a3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 55  # Changed from 50 to improve exploration\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Improve global exploration by increasing population size to enhance diversity and convergence.", "configspace": "", "generation": 63, "fitness": 0.9743195933378298, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9861304967465044, 0.9742732939575418, 0.9625549893094432], "final_y": [0.16485613842979374, 0.16485828372796496, 0.1648605846073291]}, "mutation_prompt": null}
{"id": "ec0a899f-2a9b-4393-a5d2-1d91fefca9b2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            diversity_factor = np.std(parent1) / (np.mean(parent1) + 1e-6)  # Added line\n            point = int(np.random.randint(1, self.dim) * diversity_factor)  # Modified line\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Foster exploration by integrating a variable crossover point based on population diversity.", "configspace": "", "generation": 64, "fitness": 0.9804818024709218, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9835916554916978, 0.9736484067511662, 0.9842053451699014], "final_y": [0.16485730511775132, 0.16485595392031482, 0.16485607145507342]}, "mutation_prompt": null}
{"id": "2cb67008-e91b-440d-96a3-797658b3abc0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        # Change: Adjust crossover rate based on population diversity\n        self.crossover_rate = 0.7 + 0.05 * np.std(parent1)\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance diversity by dynamically adjusting crossover rate based on population diversity.", "configspace": "", "generation": 65, "fitness": 0.9807557956638565, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9863594363228868, 0.971702596190734, 0.9842053544779488], "final_y": [0.16485742832280592, 0.16485689498134204, 0.16485607145507342]}, "mutation_prompt": null}
{"id": "de31da5c-cc56-4bec-bcf4-44ff56a6be57", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Adjust mutation rate based on mean fitness variability\n        self.mutation_rate = 0.12 + 0.05 * np.std(np.mean([func(ind) for ind in self.initialize_population(bounds)]))\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance diversity by adjusting mutation rate based on mean fitness variability.", "configspace": "", "generation": 66, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {}, "mutation_prompt": null}
{"id": "a804ec12-e61b-4f5a-83e5-2300e3e7b5f4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim) + int(np.std(parent1))\n            point = min(point, self.dim - 1)  # Ensure crossover point is within valid range\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance exploration by introducing a dynamic crossover point based on population diversity.", "configspace": "", "generation": 67, "fitness": 0.9802758884867062, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9863587215757333, 0.9702635976194195, 0.9842053462649656], "final_y": [0.16485739281474665, 0.1648561403185107, 0.16485607145507342]}, "mutation_prompt": null}
{"id": "ba353fc2-d87c-42c8-b96e-ee5c05312bec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = int(np.round(np.std(parent1) / np.linalg.norm(parent1) * self.dim))  # Changed line\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance exploration by adjusting crossover points based on population diversity.", "configspace": "", "generation": 68, "fitness": 0.9590974570032936, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.028. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9194099228733544, 0.9736771189819418, 0.9842053291545846], "final_y": [0.16486249434705158, 0.16485822444930576, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "007e786d-cf11-4ad6-94b1-a30c7aa8439c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        diversity_factor = np.std(population)  # Change: Promote more diversity\n        self.mutation_rate = 0.12 + 0.05 * diversity_factor\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhanced genetic algorithm with adaptive elitism and diversity promotion for robust optimization.", "configspace": "", "generation": 69, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {}, "mutation_prompt": null}
{"id": "eefe5d14-2f05-4fb4-9d3a-17a84da37769", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = 0.12 + 0.05 * np.std(child) * (1 - progress)  # Change: Combined progress-based and diversity-based adjustment\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduced adaptive mutation rate based on both progress and diversity to enhance exploration and exploitation balance.", "configspace": "", "generation": 70, "fitness": 0.9815342912409132, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9863585768665567, 0.9740389677015981, 0.9842053291545846], "final_y": [0.16485774776774276, 0.16485934017988613, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "9547505a-0ffd-4a20-8ae7-30ed9c8bc43b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity and convergence rate\n        convergence_rate = np.std(self.evaluate_population(func, np.array([child])))\n        self.mutation_rate = 0.12 + 0.05 * np.std(child) + 0.02 * convergence_rate\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Dynamically adjust the mutation rate by incorporating both diversity and convergence rate for enhancing exploration.", "configspace": "", "generation": 71, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {}, "mutation_prompt": null}
{"id": "b06d321e-3d4c-4577-a7a7-f62001cd21ba", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Change: Ensure best individual is retained dynamically based on fitness improvement\n            if min(fitness) > best_fitness:\n                population[-1] = best_individual\n                fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance solution retention by implementing dynamic elitism based on fitness improvement. ", "configspace": "", "generation": 72, "fitness": 0.9791466876811622, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9863586275556799, 0.9668760846600726, 0.9842053508277342], "final_y": [0.16485773414475968, 0.1648581326492925, 0.16485607145507342]}, "mutation_prompt": null}
{"id": "e03f31b4-bc29-4063-9333-340b8410c0ea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        fitness_diversity = np.std(self.evaluate_population(func, [child]))\n        self.mutation_rate = 0.12 + 0.1 * fitness_diversity  # Modified line to include fitness diversity\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce adaptive mutation based on fitness diversity to improve convergence speed and solution quality.", "configspace": "", "generation": 73, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {}, "mutation_prompt": null}
{"id": "cc184682-0eb8-44d9-a849-ae491431340b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        # Change: Increase crossover rate adaptively based on standard deviation of population\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance solution quality by adaptively adjusting the crossover rate based on the diversity of the population.", "configspace": "", "generation": 74, "fitness": 0.9818612730796789, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6798d71-7c73-4d50-827a-1ecd09e9c149", "metadata": {"aucs": [0.9863585379013454, 0.9750199327913412, 0.9842053485463499], "final_y": [0.16485777577090388, 0.1648571145889467, 0.16485607145507342]}, "mutation_prompt": null}
{"id": "ed934b5d-4ebc-45c1-9112-523860b103b9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Scale mutation based on interquartile range (IQR) of population\n        population_diversity = np.std(child) * (np.percentile(child, 75) - np.percentile(child, 25))\n        self.mutation_rate = 0.12 + 0.05 * population_diversity\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhance solution quality by incorporating adaptive mutation scaling based on population diversity metrics.", "configspace": "", "generation": 75, "fitness": 0.9818589995941442, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cc184682-0eb8-44d9-a849-ae491431340b", "metadata": {"aucs": [0.9863586306926019, 0.975013038935246, 0.9842053291545846], "final_y": [0.16485773414475968, 0.1648606220113461, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "4e5f63f7-c2e4-4cd8-bdf8-080c5c64df83", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Change: Implement quasi-oppositional initialization\n        opposite_population = lb + ub - population\n        combined_population = np.vstack((population, opposite_population))\n        # Select the best half based on random sampling\n        return combined_population[np.random.choice(len(combined_population), self.population_size, replace=False)]\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            # Change: Force periodicity by using tailored crossover\n            period = self.dim // 10\n            child1 = np.concatenate([parent1[i:i+period] if i % 2 == 0 else parent2[i:i+period] for i in range(0, self.dim, period)])\n            child2 = np.concatenate([parent2[i:i+period] if i % 2 == 0 else parent1[i:i+period] for i in range(0, self.dim, period)])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        self.mutation_rate = 0.12 + 0.05 * np.std(child)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Improve solution quality by incorporating quasi-oppositional initialization and encouraging periodicity through tailored crossover operations.", "configspace": "", "generation": 76, "fitness": 0.9761435631089063, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cc184682-0eb8-44d9-a849-ae491431340b", "metadata": {"aucs": [0.9802639045441708, 0.9725331535810773, 0.9756336312014711], "final_y": [0.1648574346746744, 0.1648592730426448, 0.1648602746425818]}, "mutation_prompt": null}
{"id": "d4eaae16-0bcd-440d-87bf-c96472ba5ebf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        # Change: Increase crossover rate adaptively based on standard deviation of population\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity and iteration progress\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.12 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce a small adaptive component to mutation rate based on iteration progress to enhance exploration-exploitation balance.", "configspace": "", "generation": 77, "fitness": 0.9818613403879151, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cc184682-0eb8-44d9-a849-ae491431340b", "metadata": {"aucs": [0.9863585379013454, 0.975020154107815, 0.9842053291545846], "final_y": [0.16485777577090388, 0.16485745282522335, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "ba1a1aba-c44c-41a0-bb11-7f3f60335831", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.12 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_indices = np.argsort(fitness)[:2]  # Ensure top 2 individuals are preserved\n            best_individuals = population[best_indices].copy()\n            best_fitnesses = fitness[best_indices]\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-2:] = best_individuals  # Preserve top 2 individuals\n            fitness[-2:] = best_fitnesses\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce elitism by preserving the top 2 individuals instead of just the best one to maintain diversity and quality.", "configspace": "", "generation": 78, "fitness": 0.9814186468007553, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d4eaae16-0bcd-440d-87bf-c96472ba5ebf", "metadata": {"aucs": [0.9863585379013454, 0.973692073346336, 0.9842053291545846], "final_y": [0.16485777577090388, 0.16485734285128073, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "2b48f960-152e-415f-9c3f-2ee430e781b7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.12 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Change: Adaptive population size\n            self.population_size = int(50 * (1 - 0.5 * (self.func_evals / self.budget)))\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce adaptive population size based on convergence progress to further enhance exploration-exploitation balance.", "configspace": "", "generation": 79, "fitness": 0.9808361716182487, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d4eaae16-0bcd-440d-87bf-c96472ba5ebf", "metadata": {"aucs": [0.9863585379013454, 0.9719446477988158, 0.9842053291545846], "final_y": [0.16485777577090388, 0.16485836335097348, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "e69b2f98-fd4d-4bc6-8739-a3daa45f7c29", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        diversity = np.mean(np.std(np.vstack([child]), axis=0))  # Diversity measure added\n        self.mutation_rate = (0.12 + 0.05 * diversity) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce a diversity measure to adaptively adjust mutation rate for enhanced exploration.", "configspace": "", "generation": 80, "fitness": 0.9684571305986616, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d4eaae16-0bcd-440d-87bf-c96472ba5ebf", "metadata": {"aucs": [0.9576337675758088, 0.9635322950655915, 0.9842053291545846], "final_y": [0.1648566209931066, 0.1648566363040337, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "7f00e5b2-bd0a-42f2-b776-1c8002f56597", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        # Change: Increase crossover rate adaptively based on standard deviation of population\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.2 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        # Change: Adjust mutation rate based on population diversity and iteration progress\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.12 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            # Preserve the best individual (elitism addition)\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            # Combine population and offspring, and select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Reduce population size dynamically\n            self.population_size = int(self.population_size * 0.99)\n\n            # Ensure best individual is retained \n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            # Local optimization on best individual to fine-tune using gradient information\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduce dynamic population size reduction to focus computation on promising regions as optimization progresses.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "d4eaae16-0bcd-440d-87bf-c96472ba5ebf", "metadata": {}, "mutation_prompt": null}
{"id": "6367d125-02c5-4392-a625-e43cd1e4f1ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)  # Change 1: Modified adaptive crossover rate\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)  # Change 2: Adjusted mutation rate formula\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "This algorithm uses an adaptive mutation rate and dynamically adjusts the crossover strategy to enhance solution diversity and exploration-exploitation balance.", "configspace": "", "generation": 82, "fitness": 0.9818616369414026, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d4eaae16-0bcd-440d-87bf-c96472ba5ebf", "metadata": {"aucs": [0.9863586268834834, 0.9750209547861399, 0.9842053291545846], "final_y": [0.16485773414475968, 0.16485726296033465, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "cef0c186-b6e6-4407-a3f2-1444209f71ad", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)  # Change 1: Modified adaptive crossover rate\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)  # Change 2: Adjusted mutation rate formula\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else best_individual  # Use best_individual for elite recombination\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhancing exploration by introducing elite recombination that uses the best solutions from previous generations to influence new generations.", "configspace": "", "generation": 83, "fitness": 0.9818616369414026, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6367d125-02c5-4392-a625-e43cd1e4f1ec", "metadata": {"aucs": [0.9863586268834834, 0.9750209547861399, 0.9842053291545846], "final_y": [0.16485773414475968, 0.16485726296033465, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "61e2177b-189b-4647-97eb-b44e530f9587", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)  # Change 1: Modified adaptive crossover rate\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)  # Change 2: Adjusted mutation rate formula\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 100})  # Increased maxiter from 50 to 100\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "This algorithm refines the local search process by increasing the maximum iterations of L-BFGS-B to enhance solution accuracy.", "configspace": "", "generation": 84, "fitness": 0.9818616369414026, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6367d125-02c5-4392-a625-e43cd1e4f1ec", "metadata": {"aucs": [0.9863586268834834, 0.9750209547861399, 0.9842053291545846], "final_y": [0.16485773414475968, 0.16485726296033465, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "8a27e4ea-f7e9-4825-aebb-f93dbc1827bb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            progress = self.func_evals / self.budget  # Dynamically adapt population size\n            self.population_size = int(50 * (1 - progress) + 20 * progress)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Improved algorithm by dynamically adapting the population size based on the progress to potentially enhance exploration and exploitation balance.", "configspace": "", "generation": 85, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "6367d125-02c5-4392-a625-e43cd1e4f1ec", "metadata": {}, "mutation_prompt": null}
{"id": "54d2b1b7-d6f4-43a0-b0d4-ca12027c75c6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)  # Change 1: Modified adaptive crossover rate\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)  # Change 2: Adjusted mutation rate formula\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "This algorithm uses an adaptive mutation rate and dynamically adjusts the crossover strategy to enhance solution diversity and exploration-exploitation balance, with a slight modification to improve convergence speed.", "configspace": "", "generation": 86, "fitness": 0.9818616369414026, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6367d125-02c5-4392-a625-e43cd1e4f1ec", "metadata": {"aucs": [0.9863586268834834, 0.9750209547861399, 0.9842053291545846], "final_y": [0.16485773414475968, 0.16485726296033465, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "81e26824-ee49-4165-bd7e-d563af2a5a94", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub) + 0.1 * np.sin(2 * np.pi * idx / self.dim)  # Change: Encourage periodicity\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "This algorithm enhances its adaptive mutation strategy with periodicity encouragement to further exploit constructive interference.", "configspace": "", "generation": 87, "fitness": 0.9818640839008016, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6367d125-02c5-4392-a625-e43cd1e4f1ec", "metadata": {"aucs": [0.9863526793076703, 0.9750342432401495, 0.9842053291545846], "final_y": [0.16485673380427746, 0.1648573497803576, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "6d66216d-002b-4149-b3ea-6fd413a7c665", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point1, point2 = np.sort(np.random.choice(self.dim, 2, replace=False))  # Change: Adapt crossover points\n            child1 = np.concatenate([parent1[:point1], parent2[point1:point2], parent1[point2:]])\n            child2 = np.concatenate([parent2[:point1], parent1[point1:point2], parent2[point2:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub) + 0.1 * np.sin(2 * np.pi * idx / self.dim)  # Change: Encourage periodicity\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhanced adaptive crossover strategy to better explore solution space, while preserving periodicity.", "configspace": "", "generation": 88, "fitness": 0.9801920946386864, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "81e26824-ee49-4165-bd7e-d563af2a5a94", "metadata": {"aucs": [0.9838431036442703, 0.9725278511172042, 0.9842053291545846], "final_y": [0.16485670829489962, 0.16485725532758977, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "d6174844-9f8f-418b-8590-a5e044459c31", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            amplitude = 0.1 * np.sin(2 * np.pi * idx / self.dim) * (1 + 0.1 * np.sin(np.pi * progress))  # Change: Dynamically adjusted mutation amplitude\n            child[idx] = np.random.uniform(lb, ub) + amplitude\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "This algorithm enhances its adaptive mutation strategy with periodicity encouragement to further exploit constructive interference, now with dynamically adjusted mutation amplitude to better explore local minima.", "configspace": "", "generation": 89, "fitness": 0.9818588939411274, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "81e26824-ee49-4165-bd7e-d563af2a5a94", "metadata": {"aucs": [0.9863539571755326, 0.9750173954932649, 0.9842053291545846], "final_y": [0.16485775895819232, 0.1648630891045606, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "19c0323a-6542-4ed5-af8d-9e19bb4ca24d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12  # Slightly increased mutation rate to enhance diversity\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            child[idx] = np.random.uniform(lb, ub) + 0.1 * np.sin(2 * np.pi * idx / self.dim)  # Change: Encourage periodicity\n            child += np.random.normal(0, 0.02, size=self.dim)  # Change: Added Gaussian noise\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduced a noise-resistant mutation strategy that adds a small Gaussian perturbation, enhancing robustness against local minima.", "configspace": "", "generation": 90, "fitness": 0.9770076013830259, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "81e26824-ee49-4165-bd7e-d563af2a5a94", "metadata": {"aucs": [0.9703422697571346, 0.9764752052373585, 0.9842053291545846], "final_y": [0.16485599950985974, 0.16485742667449088, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "4aa38509-32dc-4ac7-be2f-8480a9c4f281", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            perturbation = 0.1 * np.sin(2 * np.pi * idx / self.dim) * (1 + np.cos(2 * np.pi * progress))\n            child[idx] = np.clip(np.random.uniform(lb, ub) + perturbation, lb, ub)  # Enhanced periodicity with symmetry\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Improved mutation strategy by incorporating adaptive periodicity and symmetry for enhanced optimization performance.", "configspace": "", "generation": 91, "fitness": 0.9818647276298472, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "81e26824-ee49-4165-bd7e-d563af2a5a94", "metadata": {"aucs": [0.9863609202347604, 0.9750279335001966, 0.9842053291545846], "final_y": [0.16485756400792728, 0.1648575777092497, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "4bc065b9-acea-405b-a3fa-24e4886e706b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        fitness_based_adjustment = np.exp(-1 * np.min(self.evaluate_population(lambda x: x, [child])))\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress) * fitness_based_adjustment\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            perturbation = 0.1 * np.sin(2 * np.pi * idx / self.dim) * (1 + np.cos(2 * np.pi * progress))\n            child[idx] = np.clip(np.random.uniform(lb, ub) + perturbation, lb, ub)  # Enhanced periodicity with symmetry\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhanced mutation strategy by including dynamic mutation rate adjustment based on individual fitness.", "configspace": "", "generation": 92, "fitness": 0.9749063316133534, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4aa38509-32dc-4ac7-be2f-8480a9c4f281", "metadata": {"aucs": [0.9630020047590232, 0.9775116987066634, 0.9842052913743732], "final_y": [0.1648585781834493, 0.16485695687192548, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "fecd45e1-35b7-410c-837c-bbce7341da79", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            perturbation = 0.1 * np.sin(2 * np.pi * idx / self.dim) * (1 + np.cos(2 * np.pi * progress))\n            child[idx] = np.clip(np.random.uniform(lb, ub) + perturbation, lb, ub)  # Enhanced periodicity with symmetry\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + int(50 * (1 + 0.1 * np.std(fitness))) <= self.budget:  # Adaptive local search frequency\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduces adaptive local search frequency based on convergence speed to enhance solution refinement.", "configspace": "", "generation": 93, "fitness": 0.9818647276298472, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4aa38509-32dc-4ac7-be2f-8480a9c4f281", "metadata": {"aucs": [0.9863609202347604, 0.9750279335001966, 0.9842053291545846], "final_y": [0.16485756400792728, 0.1648575777092497, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "bc184674-a678-4c94-9030-55e543cee373", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            perturbation = 0.15 * np.sin(2 * np.pi * idx / self.dim) * (1 + np.cos(2 * np.pi * progress))  # Enhanced periodicity\n            child[idx] = np.clip(np.random.uniform(lb, ub) + perturbation, lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Enhanced the mutation perturbation calculation to increase periodicity exploitation.", "configspace": "", "generation": 94, "fitness": 0.9818769962366557, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4aa38509-32dc-4ac7-be2f-8480a9c4f281", "metadata": {"aucs": [0.986359753230346, 0.9750659063250366, 0.9842053291545846], "final_y": [0.16485782765049284, 0.16485767822966024, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "b1bd653b-f186-44b9-9461-66a486c5fbd2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        # Change made here: Inverse scaling of fitness for selection.\n        scaled_fitness = 1 / (1 + fitness)\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=scaled_fitness / scaled_fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            perturbation = 0.15 * np.sin(2 * np.pi * idx / self.dim) * (1 + np.cos(2 * np.pi * progress))  # Enhanced periodicity\n            child[idx] = np.clip(np.random.uniform(lb, ub) + perturbation, lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Refined parent selection by scaling fitness inversely for better exploration.", "configspace": "", "generation": 95, "fitness": 0.9742355759934135, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "bc184674-a678-4c94-9030-55e543cee373", "metadata": {"aucs": [0.9614409908982272, 0.9770604079274285, 0.9842053291545846], "final_y": [0.1648563069946012, 0.16485810394935307, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "09a70aa1-2d16-46c7-b56e-a86c8d3b479c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            perturbation = 0.15 * np.sin(2 * np.pi * idx / self.dim) * (1 + np.cos(2 * np.pi * progress + np.pi/4))  # Enhanced periodicity with phase shift\n            child[idx] = np.clip(np.random.uniform(lb, ub) + perturbation, lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Utilized adaptive mutation with a cosine function to enhance exploration and periodic pattern exploitation.", "configspace": "", "generation": 96, "fitness": 0.9818591613212327, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "bc184674-a678-4c94-9030-55e543cee373", "metadata": {"aucs": [0.9863478809985783, 0.9750242738105352, 0.9842053291545846], "final_y": [0.16485806688803606, 0.16485862425486364, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "0933ac2e-af2a-4bff-80bc-839384c0c478", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate:\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            perturbation = 0.15 * np.sin(2 * np.pi * idx / self.dim) * (1 + np.cos(2 * np.pi * progress + 0.1))  # Enhanced periodicity\n            child[idx] = np.clip(np.random.uniform(lb, ub) + perturbation, lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Improved periodic mutation perturbation to enhance convergence reliability.", "configspace": "", "generation": 97, "fitness": 0.9818689597168323, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "bc184674-a678-4c94-9030-55e543cee373", "metadata": {"aucs": [0.9863551950853863, 0.9750463543645199, 0.9842053297005907], "final_y": [0.16485693058206619, 0.16485755108478484, 0.16485613765670482]}, "mutation_prompt": null}
{"id": "71f8c68f-12ac-4507-bf87-925429052ab6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2)\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate * (0.5 + 0.5 * np.sin(np.pi * progress)):  # Self-adaptive scaling\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            perturbation = 0.15 * np.sin(2 * np.pi * idx / self.dim) * (1 + np.cos(2 * np.pi * progress))  # Enhanced periodicity\n            child[idx] = np.clip(np.random.uniform(lb, ub) + perturbation, lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n        \n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduced a self-adaptive mutation rate scaling with progress to enhance exploration-exploitation trade-off.", "configspace": "", "generation": 98, "fitness": 0.9820709042068004, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "bc184674-a678-4c94-9030-55e543cee373", "metadata": {"aucs": [0.9863616949552716, 0.9756456885105451, 0.9842053291545846], "final_y": [0.1648564719041098, 0.16485726066454065, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "6cba4b1e-9d87-4191-94d7-2a9589d107f2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.12\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(ind) for ind in population])\n        self.func_evals += len(population)\n        return fitness\n\n    def select_parents(self, population, fitness):\n        idx = np.random.choice(np.arange(self.population_size), size=self.population_size, replace=True, p=fitness / fitness.sum())\n        return population[idx]\n\n    def crossover(self, parent1, parent2):\n        progress = self.func_evals / self.budget\n        adaptive_crossover_rate = self.crossover_rate * (1 - progress) + 0.25 * progress + 0.1 * np.std(parent1 - parent2) * (0.5 + 0.5 * np.sin(2 * np.pi * progress)) # Sinusoidal adaptation\n        if np.random.rand() < adaptive_crossover_rate:\n            point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, child, bounds):\n        progress = self.func_evals / self.budget\n        self.mutation_rate = (0.15 + 0.05 * np.std(child)) * (1 - 0.5 * progress)\n        if np.random.rand() < self.mutation_rate * (0.5 + 0.5 * np.sin(np.pi * progress)):  # Self-adaptive scaling\n            idx = np.random.randint(self.dim)\n            lb, ub = bounds.lb[idx], bounds.ub[idx]\n            perturbation = 0.15 * np.sin(2 * np.pi * idx / self.dim) * (1 + np.cos(2 * np.pi * progress))  # Enhanced periodicity\n            child[idx] = np.clip(np.random.uniform(lb, ub) + perturbation, lb, ub)\n        return child\n\n    def local_search(self, func, individual, bounds):\n        result = minimize(func, individual, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B', options={'maxiter': 50})\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.initialize_population(bounds)\n        fitness = self.evaluate_population(func, population)\n\n        while self.func_evals < self.budget:\n            best_individual = population[np.argmin(fitness)].copy()\n            best_fitness = min(fitness)\n\n            parents = self.select_parents(population, fitness)\n            offspring = []\n\n            for i in range(0, len(parents), 2):\n                parent1 = parents[i]\n                parent2 = parents[i+1] if i+1 < len(parents) else parents[0]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, bounds)\n                child2 = self.mutate(child2, bounds)\n                offspring.extend([child1, child2])\n\n            offspring = np.array(offspring)\n            offspring_fitness = self.evaluate_population(func, offspring)\n\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.concatenate((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            population[-1] = best_individual\n            fitness[-1] = best_fitness\n\n            if self.func_evals + 50 <= self.budget:\n                population[0] = self.local_search(func, population[0], bounds)\n                fitness[0] = func(population[0])\n                self.func_evals += 1\n\n        return population[0]", "name": "HybridGeneticOptimizer", "description": "Introduced a sinusoidal adaptation in crossover probability to better balance exploration and exploitation phases.", "configspace": "", "generation": 99, "fitness": 0.9820708319904802, "feedback": "The algorithm HybridGeneticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "71f8c68f-12ac-4507-bf87-925429052ab6", "metadata": {"aucs": [0.9863616949552716, 0.9756454718615843, 0.9842053291545846], "final_y": [0.1648564719041098, 0.16485738698635244, 0.16485613926677423]}, "mutation_prompt": null}
