{"id": "cb2bd04d-a468-494d-b0b9-7a9fa5056d18", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid Differential Evolution and local optimizer that leverages constructive interference principles for periodic solutions in multilayer optimization.", "configspace": "", "generation": 0, "fitness": 0.6956463762754262, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.696 with standard deviation 0.064. And the mean value of best solutions found was 0.221 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": null, "metadata": {"aucs": [0.7168250659019004, 0.608439082979207, 0.7616749799451711], "final_y": [0.21884795188044392, 0.24340676259895833, 0.20088504838088084]}, "mutation_prompt": null}
{"id": "37e09b63-c406-486e-90e8-73946e3c47ea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'maxiter': 100})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "An enhanced hybrid optimizer integrating Differential Evolution with a periodicity-encouraging local search for improved multilayer optimization.", "configspace": "", "generation": 1, "fitness": 0.6956463762754262, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.696 with standard deviation 0.064. And the mean value of best solutions found was 0.221 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "cb2bd04d-a468-494d-b0b9-7a9fa5056d18", "metadata": {"aucs": [0.7168250659019004, 0.608439082979207, 0.7616749799451711], "final_y": [0.21884795188044392, 0.24340676259895833, 0.20088504838088084]}, "mutation_prompt": null}
{"id": "9542ff36-151f-430f-a126-fa57628e12fc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        x0 = x0 - np.sign(np.gradient(func(x0))) * 0.01  # Line changed: Initial guess adjustment\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved local search by adding a gradient-based initial guess adjustment in the hybrid DE-local optimizer for multilayer optimization.", "configspace": "", "generation": 2, "fitness": 0.6956463762754262, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.696 with standard deviation 0.064. And the mean value of best solutions found was 0.221 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "cb2bd04d-a468-494d-b0b9-7a9fa5056d18", "metadata": {"aucs": [0.7168250659019004, 0.608439082979207, 0.7616749799451711], "final_y": [0.21884795188044392, 0.24340676259895833, 0.20088504838088084]}, "mutation_prompt": null}
{"id": "640c7142-2a75-4927-8e28-a06a4ad88823", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                # Encourage periodicity by adjusting periodic constraints\n                trial = np.clip(trial, bounds.lb, bounds.ub)  # Adjusted line\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "An enhanced hybrid optimizer that incorporates a strategy to encourage periodicity by adjusting trial selection during differential evolution for effective multilayer design optimization.", "configspace": "", "generation": 3, "fitness": 0.6956463762754262, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.696 with standard deviation 0.064. And the mean value of best solutions found was 0.221 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "cb2bd04d-a468-494d-b0b9-7a9fa5056d18", "metadata": {"aucs": [0.7168250659019004, 0.608439082979207, 0.7616749799451711], "final_y": [0.21884795188044392, 0.24340676259895833, 0.20088504838088084]}, "mutation_prompt": null}
{"id": "0159ca51-d531-4550-b96d-5365533228cd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + np.random.normal(0, 0.01, self.dim), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation strategy in Differential Evolution to improve exploration and convergence in multilayer optimization.", "configspace": "", "generation": 4, "fitness": 0.6896487811872735, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.690 with standard deviation 0.020. And the mean value of best solutions found was 0.220 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "cb2bd04d-a468-494d-b0b9-7a9fa5056d18", "metadata": {"aucs": [0.6810326789262762, 0.7177342894143318, 0.6701793752212122], "final_y": [0.2306918464836446, 0.1873008386421302, 0.24151128200600236]}, "mutation_prompt": null}
{"id": "c62f0aca-889e-47b2-97e9-21c7ddcf9b75", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            F = np.clip(0.5 + 0.5 * np.cos(np.pi * self.current_evaluations / self.budget), 0.5, 1.0) # Adaptive F\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial) + 0.1 * np.sin(np.pi * np.sum(trial)) # Encourage periodicity\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced periodicity encouragement and adaptive mutation strategy within a hybrid Differential Evolution and local optimizer framework for multilayer optimization.", "configspace": "", "generation": 5, "fitness": 0.6239884026712628, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.624 with standard deviation 0.081. And the mean value of best solutions found was 0.261 (0. is the best) with standard deviation 0.028.", "error": "", "parent_id": "cb2bd04d-a468-494d-b0b9-7a9fa5056d18", "metadata": {"aucs": [0.5398665975057431, 0.5993977211947036, 0.7327008893133415], "final_y": [0.2677840919334382, 0.2906578749556842, 0.22323200273728816]}, "mutation_prompt": null}
{"id": "73f454cf-6696-46a0-93df-5f6e20d0bcfd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        # Quasi-oppositional initialization for better exploration\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This algorithm enhances the hybrid optimization strategy by introducing a quasi-oppositional initialization to improve diversity and exploration in the search space.", "configspace": "", "generation": 6, "fitness": 0.7054071942449331, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.705 with standard deviation 0.053. And the mean value of best solutions found was 0.221 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "cb2bd04d-a468-494d-b0b9-7a9fa5056d18", "metadata": {"aucs": [0.7233315522695658, 0.6330755753294632, 0.7598144551357704], "final_y": [0.21884795188044392, 0.24340676259895833, 0.20088504838088084]}, "mutation_prompt": null}
{"id": "cab43d5e-ad12-403f-915f-036e5598c285", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        # Quasi-oppositional initialization for better exploration\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (CR * (1 - self.current_evaluations / self.budget))  # Adaptive CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This algorithm enhances the hybrid optimization strategy by introducing a quasi-oppositional initialization and adaptive crossover probability for improved exploration and exploitation balance.", "configspace": "", "generation": 7, "fitness": 0.6979610393915352, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.698 with standard deviation 0.052. And the mean value of best solutions found was 0.215 (0. is the best) with standard deviation 0.028.", "error": "", "parent_id": "73f454cf-6696-46a0-93df-5f6e20d0bcfd", "metadata": {"aucs": [0.7695422926922997, 0.6480029243471146, 0.6763379011351913], "final_y": [0.18420814291623577, 0.25283358803717415, 0.2082408733118496]}, "mutation_prompt": null}
{"id": "138efaeb-392e-47dc-ae5a-996a03339ac8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        # Quasi-oppositional initialization for better exploration\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = F + np.random.rand() * 0.2  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This algorithm enhances the hybrid optimization strategy by introducing a quasi-oppositional initialization and adaptive mutation to improve diversity and exploration in the search space.", "configspace": "", "generation": 8, "fitness": 0.6210497498785762, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.621 with standard deviation 0.055. And the mean value of best solutions found was 0.268 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "73f454cf-6696-46a0-93df-5f6e20d0bcfd", "metadata": {"aucs": [0.6863122410406038, 0.6245024909282504, 0.5523345176668744], "final_y": [0.2579420774696326, 0.27862692916765963, 0.26618000324690383]}, "mutation_prompt": null}
{"id": "bb9bfb00-efc0-4b1d-85ce-30ba40b14b71", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):  # Changed CR from 0.9 to 0.95\n        # Quasi-oppositional initialization for better exploration\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This algorithm improves the hybrid optimization strategy with a quasi-oppositional initialization by fine-tuning crossover rates in Differential Evolution for enhanced convergence.", "configspace": "", "generation": 9, "fitness": 0.7089063126588386, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.709 with standard deviation 0.030. And the mean value of best solutions found was 0.184 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "73f454cf-6696-46a0-93df-5f6e20d0bcfd", "metadata": {"aucs": [0.7348921577179365, 0.6668318491905976, 0.7249949310679819], "final_y": [0.16771942336540613, 0.2123228917528015, 0.1714108614028319]}, "mutation_prompt": null}
{"id": "e711496a-21bc-4621-be5a-6c19f0a69716", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        # Quasi-oppositional initialization for better exploration\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)  # Dynamic population sizing\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This improved hybrid optimizer enhances the quasi-oppositional initialization by employing adaptive mutation strategies and a dynamic population sizing in Differential Evolution for superior exploration and convergence.", "configspace": "", "generation": 10, "fitness": 0.777830749928476, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.022. And the mean value of best solutions found was 0.218 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "bb9bfb00-efc0-4b1d-85ce-30ba40b14b71", "metadata": {"aucs": [0.7905226724375499, 0.7466001150211871, 0.7963694623266911], "final_y": [0.21599396205051413, 0.23442787311911106, 0.20344943109389468]}, "mutation_prompt": null}
{"id": "8cd9c3ac-e93d-4744-8012-0dfe700b05ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        # Quasi-oppositional initialization for better exploration\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.9 + 0.1 * np.random.rand()  # Adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)  # Dynamic population sizing\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This refined hybrid optimizer further enhances exploration by dynamically adjusting both the crossover rate and mutation factors in Differential Evolution for improved convergence.", "configspace": "", "generation": 11, "fitness": 0.8782142412748793, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.026. And the mean value of best solutions found was 0.190 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "e711496a-21bc-4621-be5a-6c19f0a69716", "metadata": {"aucs": [0.8437267337827662, 0.906655472443942, 0.8842605175979299], "final_y": [0.2014565302442115, 0.18283427590191692, 0.18717526159037112]}, "mutation_prompt": null}
{"id": "1eabe432-81ce-4417-85e6-6e0751bbb06b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        # Quasi-oppositional initialization for better exploration\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.9 + 0.1 * np.random.rand()  # Adaptive crossover strategy\n                strategy = np.random.choice(['rand1', 'best1', 'current-to-best1'])\n\n                if strategy == 'rand1':\n                    mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                elif strategy == 'best1':\n                    mutant = np.clip(best + F * (b - c), bounds.lb, bounds.ub)\n                else:  # current-to-best1\n                    mutant = np.clip(pop[i] + F * (best - pop[i]) + F * (b - c), bounds.lb, bounds.ub)\n\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)  # Dynamic population sizing\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration via adaptive choice of the DE strategy for improved convergence in complex landscapes.", "configspace": "", "generation": 12, "fitness": 0.8625646224294012, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.075. And the mean value of best solutions found was 0.195 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "8cd9c3ac-e93d-4744-8012-0dfe700b05ec", "metadata": {"aucs": [0.8923976490694455, 0.9361523771499813, 0.7591438410687765], "final_y": [0.19382733525287876, 0.1750645590290214, 0.21719000916008468]}, "mutation_prompt": null}
{"id": "6ca5ea61-82c5-4f78-a362-8f2c812ee244", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        # Quasi-oppositional initialization for better exploration\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.sin(np.pi * np.random.rand())  # Sinusoidal mutation strategy\n                CR = 0.9 + 0.1 * np.random.rand()  # Adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)  # Dynamic population sizing\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "By incorporating a sinusoidal bias in the mutation strategy of Differential Evolution, the algorithm aims to enhance its ability to explore periodic solutions for problems with wave-like landscapes.", "configspace": "", "generation": 13, "fitness": 0.8593050357201074, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.072. And the mean value of best solutions found was 0.190 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "8cd9c3ac-e93d-4744-8012-0dfe700b05ec", "metadata": {"aucs": [0.7676588694709672, 0.9430855341103992, 0.867170703578956], "final_y": [0.21723326592983694, 0.1676053020226319, 0.18414739095210975]}, "mutation_prompt": null}
{"id": "6517cece-eb84-41c7-b674-a4fc2548d2df", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        # Quasi-oppositional initialization for better exploration\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.9 + 0.1 * np.random.rand()  # Adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial) + np.std(trial)  # Encouraging periodic solutions\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)  # Dynamic population sizing\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This refined hybrid optimizer further enhances exploration by dynamically adjusting both the crossover rate and mutation factors in Differential Evolution for improved convergence, now with periodicity encouragement through dynamic weighting in cost function.", "configspace": "", "generation": 14, "fitness": 0.6095200027116247, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.610 with standard deviation 0.035. And the mean value of best solutions found was 0.307 (0. is the best) with standard deviation 0.038.", "error": "", "parent_id": "8cd9c3ac-e93d-4744-8012-0dfe700b05ec", "metadata": {"aucs": [0.6507001898751505, 0.566173963101189, 0.6116858551585345], "final_y": [0.265207029035483, 0.35756917149445966, 0.2993255357559176]}, "mutation_prompt": null}
{"id": "594441f3-2f0e-4974-9e56-d1c0b97f05bc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()\n                CR = 0.9 + 0.1 * np.random.rand()\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                # Enforcing periodicity in potential solutions\n                trial = trial - np.mod(trial, 2 * (bounds.ub[0] - bounds.lb[0]) / self.dim)\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                # Dynamically adjust trial acceptance criteria\n                if trial_fitness < func(pop[i]) or np.random.rand() < 0.1:\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This strategy refines solution quality by incorporating periodicity enforcement and dynamic adjustment of trial acceptance criteria in Differential Evolution.", "configspace": "", "generation": 15, "fitness": 0.5509414398465605, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.551 with standard deviation 0.064. And the mean value of best solutions found was 0.370 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "8cd9c3ac-e93d-4744-8012-0dfe700b05ec", "metadata": {"aucs": [0.6418235370186985, 0.5081586275780816, 0.5028421549429012], "final_y": [0.3134298421743149, 0.3992702095910784, 0.3987689707523039]}, "mutation_prompt": null}
{"id": "84a61a30-3eaa-4e47-8472-c1348e3d9a2a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n        self.island_count = 3  # Introduce island model for diversity\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        # Quasi-oppositional initialization for better exploration\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.9 + 0.1 * np.random.rand()  # Adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            if self.current_evaluations % 50 == 0:  # Periodic migration\n                self.migrate(pop, bounds)\n\n            pop_size = max(pop_size // 2, 10)  # Dynamic population sizing\n\n        return best\n\n    def migrate(self, pop, bounds):  # Island model migration\n        island_size = len(pop) // self.island_count\n        for i in range(self.island_count):\n            np.random.shuffle(pop[i*island_size: (i+1)*island_size])\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This enhanced hybrid optimizer combines quasi-oppositional DE with a dynamic island model to improve convergence by effectively balancing exploration and exploitation.", "configspace": "", "generation": 16, "fitness": 0.8650751043918055, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.865 with standard deviation 0.022. And the mean value of best solutions found was 0.179 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "8cd9c3ac-e93d-4744-8012-0dfe700b05ec", "metadata": {"aucs": [0.8358292307675725, 0.8873574668966471, 0.8720386155111965], "final_y": [0.1972475288393486, 0.1693893819043938, 0.17063402925145477]}, "mutation_prompt": null}
{"id": "f356d88f-ced8-4604-85a1-a7283076ae9e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.85 + 0.15 * np.random.rand()  # Adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'learning_rate': 0.1})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This refined hybrid optimizer enhances exploration and convergence by introducing adaptive learning rates in the local search phase and stochastic adjustments in population evolution.", "configspace": "", "generation": 17, "fitness": 0.8922813057467737, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.892 with standard deviation 0.035. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "8cd9c3ac-e93d-4744-8012-0dfe700b05ec", "metadata": {"aucs": [0.843407958844725, 0.9190154621140703, 0.9144204962815257], "final_y": [0.18893502743294277, 0.16776764823772627, 0.1708411068999014]}, "mutation_prompt": null}
{"id": "100a9caa-bc77-4416-bed1-98e97613ad36", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.85 + 0.15 * np.random.rand()  # Adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2 + 1, 10)  # Adjust population size dynamically\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'learning_rate': 0.05})  # Adjust learning rate\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This refined hybrid optimizer enhances exploration and convergence by integrating dynamic population sizing and learning rate adjustments during the differential evolution phase.", "configspace": "", "generation": 18, "fitness": 0.8409592485169991, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.841 with standard deviation 0.053. And the mean value of best solutions found was 0.190 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "f356d88f-ced8-4604-85a1-a7283076ae9e", "metadata": {"aucs": [0.8508640199320558, 0.7713605626761498, 0.9006531629427914], "final_y": [0.17793556806874755, 0.22596751744525545, 0.166638965682533]}, "mutation_prompt": null}
{"id": "d4fb9f47-558c-4867-9acc-a0bb7510adc8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.85 + 0.15 * np.random.rand()  # Adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            # Adaptive population size scaling\n            pop_size = max(int(pop_size * 0.9), 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'learning_rate': 0.1})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This refined hybrid optimizer incorporates adaptive population size scaling and enhanced stochastic learning rates to improve exploration and convergence efficiency.", "configspace": "", "generation": 19, "fitness": 0.8792879883617933, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.879 with standard deviation 0.049. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.019.", "error": "", "parent_id": "f356d88f-ced8-4604-85a1-a7283076ae9e", "metadata": {"aucs": [0.8109351840908995, 0.9230063431318235, 0.9039224378626567], "final_y": [0.2078026742866349, 0.1654500179448829, 0.172182001630794]}, "mutation_prompt": null}
{"id": "70904f52-7216-4fd1-891e-02927a8787c9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.85 + 0.15 * np.random.rand()  # Adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        dynamic_lr = 0.1 / (1 + 0.1 * self.current_evaluations / self.budget)  # Dynamic learning rate\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'learning_rate': dynamic_lr})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This optimized hybrid method refines the exploration and convergence by introducing dynamic learning rates during local search for enhanced adaptability and improved solution tightening.", "configspace": "", "generation": 20, "fitness": 0.8922813057467737, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.892 with standard deviation 0.035. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "f356d88f-ced8-4604-85a1-a7283076ae9e", "metadata": {"aucs": [0.843407958844725, 0.9190154621140703, 0.9144204962815257], "final_y": [0.18893502743294277, 0.16776764823772627, 0.1708411068999014]}, "mutation_prompt": null}
{"id": "989fa070-ef34-4294-af0a-5a42af7bec37", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.85 + 0.15 * np.random.rand()  # Adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                # Apply periodicity constraint to align with known near-optimal structures\n                trial[int(self.dim/2):] = trial[:int(self.dim/2)]\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'learning_rate': 0.1})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This refined hybrid optimizer introduces a periodicity constraint within the differential evolution phase to encourage solutions aligning with known near-optimal periodic structures.", "configspace": "", "generation": 21, "fitness": 0.8398603150609114, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.130. And the mean value of best solutions found was 0.216 (0. is the best) with standard deviation 0.060.", "error": "", "parent_id": "f356d88f-ced8-4604-85a1-a7283076ae9e", "metadata": {"aucs": [0.9246440812497303, 0.6564923391373921, 0.9384445247956118], "final_y": [0.17321753620330882, 0.30062762099013207, 0.17288887332581482]}, "mutation_prompt": null}
{"id": "7ef1178b-a375-476e-9bc4-4fdd68abe955", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        self.current_evaluations += len(pop)\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(len(pop)) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.5 * np.random.rand()  # More adaptive mutation strategy\n                CR = 0.9  # Fixed crossover probability for stability\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 5)  # Smaller minimum population size for diversity\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'maxfun': self.budget - self.current_evaluations})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This improved hybrid optimizer enhances convergence and diversity by combining adaptive differential evolution with opposition-based learning and integrating a frequency-modulated local search phase.", "configspace": "", "generation": 22, "fitness": 0.7227485298744902, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.723 with standard deviation 0.010. And the mean value of best solutions found was 0.256 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "f356d88f-ced8-4604-85a1-a7283076ae9e", "metadata": {"aucs": [0.7087926959232105, 0.7294095986706133, 0.7300432950296469], "final_y": [0.2725676519819862, 0.23920208337283588, 0.25588455851014547]}, "mutation_prompt": null}
{"id": "5dfa3765-8ad3-403a-af70-da71a8923e70", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            temperature = 1.0 - (self.current_evaluations / self.budget)  # New line\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.85 + 0.15 * np.random.rand()  # Adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]) or np.random.rand() < np.exp((func(pop[i]) - trial_fitness) / temperature):  # Modified line\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'learning_rate': 0.1})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhances the optimizer by introducing a temperature-based probabilistic acceptance criterion during evolution, allowing more exploration in early stages.", "configspace": "", "generation": 23, "fitness": 0.5663921490205429, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.566 with standard deviation 0.056. And the mean value of best solutions found was 0.361 (0. is the best) with standard deviation 0.036.", "error": "", "parent_id": "f356d88f-ced8-4604-85a1-a7283076ae9e", "metadata": {"aucs": [0.6418235370186985, 0.5081586275780816, 0.5491942824648485], "final_y": [0.3134298421743149, 0.3992702095910784, 0.37047852031713646]}, "mutation_prompt": null}
{"id": "e2df3b0e-5c8f-4ce5-aac9-9df9afda8cdd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.85 + 0.15 * np.random.rand()  # Adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(int(pop_size * 0.9), 10)  # Dynamic adjustment to population size\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'learning_rate': 0.15})  # Adaptive learning rate\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This refined hybrid optimizer enhances exploration and convergence by introducing adaptive learning rates in the local search phase and stochastic adjustments in population evolution with a dynamic adjustment to the population size based on solution progress.", "configspace": "", "generation": 24, "fitness": 0.8792879883617933, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.879 with standard deviation 0.049. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.019.", "error": "", "parent_id": "f356d88f-ced8-4604-85a1-a7283076ae9e", "metadata": {"aucs": [0.8109351840908995, 0.9230063431318235, 0.9039224378626567], "final_y": [0.2078026742866349, 0.1654500179448829, 0.172182001630794]}, "mutation_prompt": null}
{"id": "6e1bc75c-e609-4ffe-af17-399863aec8e0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.85 + 0.15 * np.random.rand()  # Adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        decay_rate = 0.99\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'learning_rate': 0.1 * decay_rate})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This refined hybrid optimizer enhances local refinement by adding a decay factor to adaptive learning rates, improving convergence in complex landscapes.", "configspace": "", "generation": 25, "fitness": 0.8922813057467737, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.892 with standard deviation 0.035. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "f356d88f-ced8-4604-85a1-a7283076ae9e", "metadata": {"aucs": [0.843407958844725, 0.9190154621140703, 0.9144204962815257], "final_y": [0.18893502743294277, 0.16776764823772627, 0.1708411068999014]}, "mutation_prompt": null}
{"id": "6fde2040-19d6-4681-abca-292d950e253b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()\n                CR = 0.85 + 0.15 * np.random.rand()\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                trial = trial * np.sin(np.linspace(0, np.pi, self.dim))  # Encourage periodic patterns\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'learning_rate': 0.1})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This refined hybrid optimizer improves exploration and convergence by employing adaptive learning rates in the local search phase and stochastic adjustments in population evolution while enhancing periodicity of solutions.", "configspace": "", "generation": 26, "fitness": 0.5663921490205429, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.566 with standard deviation 0.056. And the mean value of best solutions found was 0.361 (0. is the best) with standard deviation 0.036.", "error": "", "parent_id": "f356d88f-ced8-4604-85a1-a7283076ae9e", "metadata": {"aucs": [0.6418235370186985, 0.5081586275780816, 0.5491942824648485], "final_y": [0.3134298421743149, 0.3992702095910784, 0.37047852031713646]}, "mutation_prompt": null}
{"id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm introduces a refined adaptive crossover strategy and enhanced local search options to better explore promising regions and improve convergence.", "configspace": "", "generation": 27, "fitness": 0.8981021074170911, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.898 with standard deviation 0.036. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "f356d88f-ced8-4604-85a1-a7283076ae9e", "metadata": {"aucs": [0.9159846721341064, 0.848027979399979, 0.9302936707171879], "final_y": [0.1650350530687993, 0.17537467441016363, 0.1648568227031285]}, "mutation_prompt": null}
{"id": "e4f33eb2-ff9e-4d25-90d7-0f079abf0a19", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * (1 - self.current_evaluations / self.budget)  # Time-varying mutation\n                CR = 0.7 + 0.25 * np.random.rand()  \n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(int(pop_size * 0.9), 10)  # Adaptive population resizing\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm integrates a time-varying mutation factor and adaptive population resizing to enhance convergence and exploration.", "configspace": "", "generation": 28, "fitness": 0.7881758234313764, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.788 with standard deviation 0.020. And the mean value of best solutions found was 0.181 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.7602493620678066, 0.8021243856359346, 0.8021537225903881], "final_y": [0.18960841340417767, 0.18198803633840321, 0.17160824118485618]}, "mutation_prompt": null}
{"id": "5c8b7836-4fe0-4465-965a-294f7d5f9180", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.8 + 0.15 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm enhances performance by introducing a symmetry-preserving strategy and refining the adaptive crossover to improve convergence in complex landscapes.", "configspace": "", "generation": 29, "fitness": 0.8116366614571993, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.075. And the mean value of best solutions found was 0.198 (0. is the best) with standard deviation 0.043.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.8679406873109841, 0.8610167886053233, 0.7059525084552902], "final_y": [0.1667740032471442, 0.16962708862120468, 0.2583934701721097]}, "mutation_prompt": null}
{"id": "ee4bafa7-7b68-4357-8ea3-739fedc9b8bb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand() * (1 - self.current_evaluations / self.budget)  # Dynamic mutation strategy\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm refines the adaptive mutation strategy by introducing a dynamically decreasing factor to enhance exploration and exploitation balance.", "configspace": "", "generation": 30, "fitness": 0.8723113501046664, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.044. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.8096699116559465, 0.9033568510633471, 0.9039072875947057], "final_y": [0.20044567078176945, 0.16650905559041107, 0.16536789415183217]}, "mutation_prompt": null}
{"id": "77781bcd-923b-4a34-906f-bb94a297c009", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-9})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm improves convergence by enhancing local search using a more precise stopping criteria during optimization.", "configspace": "", "generation": 31, "fitness": 0.8981021074170911, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.898 with standard deviation 0.036. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.9159846721341064, 0.848027979399979, 0.9302936707171879], "final_y": [0.1650350530687993, 0.17537467441016363, 0.1648568227031285]}, "mutation_prompt": null}
{"id": "bc3f0506-b77d-4e6c-ac72-9c87b46a2b5e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import basinhopping\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.6 + 0.3 * np.random.rand()  # Slightly refined adaptive mutation\n                CR = 0.75 + 0.2 * np.random.rand()  # Slightly refined adaptive crossover\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        minimizer_kwargs = {\"method\": \"L-BFGS-B\", \"bounds\": bounds}\n        # Added basin hopping to enhance local search capabilities\n        result = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs, niter=10)\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm enhances adaptive crossover and mutation strategies and extends local search with basin hopping for better convergence and exploration.", "configspace": "", "generation": 32, "fitness": 0.8679072990783662, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.032. And the mean value of best solutions found was 0.179 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.8368077488526098, 0.8549566080660369, 0.9119575403164517], "final_y": [0.18223312478367149, 0.1881786854853642, 0.16531632491504644]}, "mutation_prompt": null}
{"id": "cf380fea-cd65-4d00-aabf-1ab34770db6e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 3, 10)  # Dynamic population size adjustment\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-8})  # Aggressive termination criterion\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The optimized algorithm incorporates a dynamic population size adjustment and a more aggressive local search termination criterion.", "configspace": "", "generation": 33, "fitness": 0.8981021074170911, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.898 with standard deviation 0.036. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.9159846721341064, 0.848027979399979, 0.9302936707171879], "final_y": [0.1650350530687993, 0.17537467441016363, 0.1648568227031285]}, "mutation_prompt": null}
{"id": "bf61148b-7b48-4348-9e96-6393b93f9bc8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += 2 * pop_size  # Adjust for double population size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 15)  # Improved dynamic reduction strategy\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-8})  # Increased precision\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm enhances exploration and convergence by incorporating dynamic population adaptation and improved local search precision.", "configspace": "", "generation": 34, "fitness": 0.8417159692338486, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.842 with standard deviation 0.013. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.8588136312657821, 0.8391706082691254, 0.8271636681666386], "final_y": [0.16612652915817472, 0.1657421993816779, 0.16503231513318228]}, "mutation_prompt": null}
{"id": "f598a75a-b2bb-4ae0-b9e5-bd0ea13705ab", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            pop_size = min(max(pop_size // 2, 10), self.budget - self.current_evaluations) # Dynamic pop adjustment\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size * 2) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.6 + 0.2 * np.random.rand()  # Slightly refined adaptive mutation\n                CR = 0.8 + 0.15 * np.random.rand()  # Refined crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a dynamic population adjustment strategy and leverage symmetry to enhance exploration and convergence.", "configspace": "", "generation": 35, "fitness": 0.6568353994417224, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.657 with standard deviation 0.034. And the mean value of best solutions found was 0.277 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.6743229396637171, 0.6098990695412276, 0.6862841891202226], "final_y": [0.25644144206387276, 0.3082192092269499, 0.26736774998673296]}, "mutation_prompt": null}
{"id": "a0bb51a4-0ce5-4934-8543-19b7ff288ac6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        # Creating a second population for enhanced diversity\n        second_pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += len(pop)  # Adjust evaluation count\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n                \n                # Choosing from both populations\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                combined_pop = np.concatenate((pop, second_pop))\n                a, b, c = combined_pop[np.random.choice(indices, 3, replace=False)]\n\n                F = 0.6 + 0.3 * np.random.rand()  # Slightly adjusted adaptive mutation strategy\n                CR = 0.65 + 0.3 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        # Adaptive local search based on proximity to the best solution\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm enhances exploration by implementing a dual-population strategy and leverages adaptive local search to refine solutions near local optima.", "configspace": "", "generation": 36, "fitness": 0.6524773256443349, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.652 with standard deviation 0.048. And the mean value of best solutions found was 0.275 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.71534465591205, 0.64426596145295, 0.5978213595680049], "final_y": [0.25749885667627403, 0.28403328854482723, 0.28299372448657356]}, "mutation_prompt": null}
{"id": "afdcd7cd-503f-4a9f-be3d-a9eec73f1bd1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de + 0.01, bounds)  # Small tweak in initialization\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm introduces a refined adaptive crossover strategy with slight enhancements in the local search initialization to improve convergence.", "configspace": "", "generation": 37, "fitness": 0.8981021074170911, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.898 with standard deviation 0.036. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.9159846721341064, 0.848027979399979, 0.9302936707171879], "final_y": [0.1650350530687993, 0.17537467441016363, 0.1648568227031285]}, "mutation_prompt": null}
{"id": "a109730d-71b0-43a1-95ed-c6396267d604", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = int(pop_size * 0.9)  # Adaptive population reduction\n            if pop_size < 10:  # Maintain a minimum population size\n                pop_size = 10\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-7})  # More precise tolerance\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A refined hybrid metaheuristic optimizer using adaptive population management and enhanced local search to boost convergence and solution quality.", "configspace": "", "generation": 38, "fitness": 0.802489270813091, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.024. And the mean value of best solutions found was 0.198 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.7713999271347104, 0.806998117176077, 0.8290697681284854], "final_y": [0.22064882824743615, 0.20566063175206672, 0.1671435824378673]}, "mutation_prompt": null}
{"id": "6a30fd10-336f-4942-b310-b69fdffc78a6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)  # Adjust population size based on performance\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm introduces a refined adaptive crossover strategy and enhanced local search options to better explore promising regions and improve convergence by dynamically adjusting the population size based on solution quality.", "configspace": "", "generation": 39, "fitness": 0.8981021074170911, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.898 with standard deviation 0.036. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.9159846721341064, 0.848027979399979, 0.9302936707171879], "final_y": [0.1650350530687993, 0.17537467441016363, 0.1648568227031285]}, "mutation_prompt": null}
{"id": "f5698b54-e223-43d5-819e-09dcb9de8b00", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                decay_factor = 1 - (self.current_evaluations / self.budget)\n                F = 0.5 + 0.4 * np.random.rand() * decay_factor  # Adaptive mutation with decay\n                CR = 0.7 + 0.25 * np.random.rand() * decay_factor  # Adaptive crossover with decay\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "An improved hybrid optimizer that incorporates a decay function for mutation and crossover, alongside a diverse initialization strategy to enhance convergence and robustness.", "configspace": "", "generation": 40, "fitness": 0.8224350867896527, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.086. And the mean value of best solutions found was 0.196 (0. is the best) with standard deviation 0.044.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.8356467668685155, 0.9199475583125558, 0.7117109351878865], "final_y": [0.16489203907983851, 0.16485597254230433, 0.2578238462458111]}, "mutation_prompt": null}
{"id": "1756c291-938f-4565-aa31-e2b51be34e8c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.4 * np.random.rand()  # Adaptive mutation strategy\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size - 1, 10)  # Adjusted population reduction strategy\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm introduces a refined adaptive crossover strategy and enhanced local search options to better explore promising regions and improve convergence, with an adjustment to the population reduction strategy to maintain diversity.", "configspace": "", "generation": 41, "fitness": 0.8733986831477593, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.065. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.9115660784314517, 0.7816614636846375, 0.9269685073271888], "final_y": [0.16529099691237292, 0.1820049062304403, 0.16491944624163468]}, "mutation_prompt": null}
{"id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm integrates a refined adaptive crossover strategy and enhanced local search options with improved adaptive mutation factor initialization to better explore promising regions and improve convergence.", "configspace": "", "generation": 42, "fitness": 0.9039143298509194, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.034. And the mean value of best solutions found was 0.174 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "009d5cbd-f9b7-43e9-8a53-54fa5fda0737", "metadata": {"aucs": [0.8582248608948482, 0.9378998614948131, 0.9156182671630968], "final_y": [0.18907658171148434, 0.16575417188900832, 0.1678418524303007]}, "mutation_prompt": null}
{"id": "4a25fb05-6c7d-4b3f-aa73-7a5df718f3ad", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()\n                CR = 0.7 + 0.25 * np.random.rand()\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            if self.current_evaluations % 100 == 0:  # Added random reinitialization for exploration\n                pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration by incorporating random reinitialization to improve convergence from diverse starting points.", "configspace": "", "generation": 43, "fitness": 0.7400433801975366, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.740 with standard deviation 0.030. And the mean value of best solutions found was 0.245 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.7752345553743546, 0.7024593033538947, 0.7424362818643606], "final_y": [0.2412493779025261, 0.26493478990856634, 0.22889131573212973]}, "mutation_prompt": null}
{"id": "94f19965-6b1c-43ec-a16e-e3e18a291ea7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        fitness_scores = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness_scores)\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand() \n                CR = 0.7 + 0.25 * np.random.rand() \n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial) + 0.1 * np.sum(np.sin(2 * np.pi * trial))  # Periodicity-promoting term\n                self.current_evaluations += 1\n\n                if trial_fitness < fitness_scores[i]:\n                    pop[i] = trial\n                    fitness_scores[i] = trial_fitness\n                    if trial_fitness < fitness_scores[best_idx]:\n                        best_idx = i\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm employs an enhanced selection strategy with a periodicity-promoting term in the cost function to encourage constructive interference, improving the optimization of multilayer photonic structures.", "configspace": "", "generation": 44, "fitness": 0.7370768303914229, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.737 with standard deviation 0.054. And the mean value of best solutions found was 0.238 (0. is the best) with standard deviation 0.044.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.6639057390729605, 0.7527902207365431, 0.7945345313647654], "final_y": [0.2952318014890932, 0.22822071891965878, 0.18984901670681886]}, "mutation_prompt": null}
{"id": "0d87f472-2be1-4048-b4da-1909b9dce612", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.std(pop) / (bounds.ub - bounds.lb)  # Adapt F based on diversity\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm enhances exploration by incorporating mutation factor adaptation based on population diversity to improve convergence and solution quality.", "configspace": "", "generation": 45, "fitness": 0.8786959997326486, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.879 with standard deviation 0.029. And the mean value of best solutions found was 0.180 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8779842536523836, 0.9144713662866187, 0.8436323792589432], "final_y": [0.18333356792510913, 0.16743747493733951, 0.18952579262785485]}, "mutation_prompt": null}
{"id": "56650a7a-c6a3-4c06-a5a4-46aaa2d1530a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(int(pop_size * 0.9), 10)  # Dynamic population size adjustment\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Integrates a refined adaptive crossover strategy with enhanced local search options and introduces a dynamic population size adjustment for better exploration and convergence.", "configspace": "", "generation": 46, "fitness": 0.8760454557431864, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.046. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.869780407551887, 0.8226760665885479, 0.9356798930891241], "final_y": [0.18194131950066428, 0.18330929597962908, 0.1656458448935736]}, "mutation_prompt": null}
{"id": "092f214f-8a6b-4484-8232-c85627cf58dc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand() * (1 - self.current_evaluations/self.budget)  # Dynamic crossover rate adjustment\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Further optimizes the crossover strategy by introducing a dynamic crossover rate adjustment based on iteration progress.", "configspace": "", "generation": 47, "fitness": 0.8126236870201545, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.113. And the mean value of best solutions found was 0.214 (0. is the best) with standard deviation 0.050.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8419871368332603, 0.933381649982614, 0.6625022742445896], "final_y": [0.19147114662792608, 0.16719313667766145, 0.28320411920125177]}, "mutation_prompt": null}
{"id": "2904e2b5-3c2f-4bc3-8dbd-7e844bb182dd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // (2 + self.current_evaluations // self.budget), 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced by implementing dynamic population resizing to improve convergence efficiency.", "configspace": "", "generation": 48, "fitness": 0.9039143298509194, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.034. And the mean value of best solutions found was 0.174 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8582248608948482, 0.9378998614948131, 0.9156182671630968], "final_y": [0.18907658171148434, 0.16575417188900832, 0.1678418524303007]}, "mutation_prompt": null}
{"id": "7ecb23a5-fed8-4302-b525-ede567380cbe", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            fitness_variance = np.var([func(ind) for ind in pop])\n            pop_size = max(int(pop_size * (1 - fitness_variance)), 10)  # Adjusted based on fitness variance\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploitation phase by incorporating dynamic population resizing based on fitness variance.", "configspace": "", "generation": 49, "fitness": 0.7666926521676459, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.767 with standard deviation 0.041. And the mean value of best solutions found was 0.178 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.7573072908180496, 0.7214853642598937, 0.8212853014249942], "final_y": [0.1910405840824032, 0.17416379962673023, 0.16731341531161703]}, "mutation_prompt": null}
{"id": "00f75f04-aeb8-4dc1-8fdf-871ffe90656c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            if self.current_evaluations / self.budget < 0.5:  # Dynamic population scaling\n                pop_size = min(30, pop_size * 2)\n\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            if np.abs(func(best) - trial_fitness) < 1e-5:  # Adaptive local search initiation\n                break\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhances exploration through dynamic population scaling and adaptive local search initiation based on convergence patterns.", "configspace": "", "generation": 50, "fitness": 0.7366187103967173, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.737 with standard deviation 0.043. And the mean value of best solutions found was 0.195 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.7216276414432983, 0.693223049425784, 0.7950054403210696], "final_y": [0.22854555757518435, 0.18590285100144754, 0.17154724156183565]}, "mutation_prompt": null}
{"id": "322c041d-db1c-4104-b8a9-ccc3366f5833", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand() * (1 - np.std(pop, axis=0).mean())  # Dynamically adjust mutation scale\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm introduces adaptive crossover and mutation strategies, now refining exploration by dynamically adjusting mutation scale based on population diversity.", "configspace": "", "generation": 51, "fitness": 0.6548553318166349, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.655 with standard deviation 0.036. And the mean value of best solutions found was 0.238 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.7057974055138612, 0.6281131381457106, 0.6306554517903331], "final_y": [0.2485557155381123, 0.24017876730695797, 0.22620540358228347]}, "mutation_prompt": null}
{"id": "5cce5441-fb07-417b-8df3-e566afe706a3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(int(0.9 * pop_size), 10)  # Adaptive population size adjustment\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm includes an adaptive population size adjustment to maintain diversity and improve exploration.", "configspace": "", "generation": 52, "fitness": 0.8760454557431864, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.046. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.869780407551887, 0.8226760665885479, 0.9356798930891241], "final_y": [0.18194131950066428, 0.18330929597962908, 0.1656458448935736]}, "mutation_prompt": null}
{"id": "5a193cce-239d-4d50-aa69-8861c37f8060", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                # Replaced the static F with a dynamic one\n                F = 0.5 + 0.5 * np.random.rand()  # Dynamic scaling factor for mutation\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The solution enhances mutation diversity by introducing a dynamic scaling factor, enabling the algorithm to adaptively adjust the mutation effect based on the optimization landscape.", "configspace": "", "generation": 53, "fitness": 0.8993471324371853, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.899 with standard deviation 0.004. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8933682333699489, 0.9022315216227983, 0.9024416423188087], "final_y": [0.17072482412410273, 0.16557402898869267, 0.16486059195184732]}, "mutation_prompt": null}
{"id": "0c753c6a-7b96-4491-97db-f84dfa30286e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n        self.elite_solutions = []\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.elite_solutions.append(best)\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            new_pop = []\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()  # Improved adaptive mutation factor initialization\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial) + self.periodicity_penalty(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    new_pop.append(trial)\n                    if trial_fitness < func(best):\n                        best = trial\n                else:\n                    new_pop.append(pop[i])\n\n            pop = np.array(new_pop)\n            pop_size = max(len(pop), 10)\n\n            self.elite_solutions.append(best)\n\n        return best\n\n    def periodicity_penalty(self, solution):\n        return 0.01 * np.std(solution[:self.dim//2] - solution[self.dim//2:])\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "EnhancedHybridOptimizer", "description": "An Enhanced Hybrid Metaheuristic Optimizer incorporating periodicity encouragement and elite preservation to improve convergence and solution quality.", "configspace": "", "generation": 54, "fitness": 0.6548120156935692, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.655 with standard deviation 0.017. And the mean value of best solutions found was 0.261 (0. is the best) with standard deviation 0.031.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.6482464097582845, 0.6376764345126681, 0.6785132028097551], "final_y": [0.2978141690968976, 0.2221108800382915, 0.2621986109070792]}, "mutation_prompt": null}
{"id": "bc52c059-912c-437d-9b60-1c50429d548d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            diversity = np.std(pop, axis=0).mean()  # Calculate diversity as mean standard deviation\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  \n                CR = 0.7 + 0.25 * np.random.rand()  \n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(min(int(pop_size * (1 + diversity / 10)), pop_size), 10)  # Adaptive resizing\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Integrates adaptive population resizing and diversity preservation mechanisms to enhance global exploration and convergence robustness.", "configspace": "", "generation": 55, "fitness": 0.8280318675768467, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.032. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8209193603680999, 0.8701868418505048, 0.7929894005119353], "final_y": [0.17139834852008362, 0.1651504508227516, 0.18262784999785064]}, "mutation_prompt": null}
{"id": "aa24f494-d9c1-4812-9068-b0fc3b3b8623", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = min(max(pop_size // 2, 10), pop_size * 2)  # Dynamically adapt population size\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration by dynamically adapting the population size during the Differential Evolution process.", "configspace": "", "generation": 56, "fitness": 0.9039143298509194, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.034. And the mean value of best solutions found was 0.174 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8582248608948482, 0.9378998614948131, 0.9156182671630968], "final_y": [0.18907658171148434, 0.16575417188900832, 0.1678418524303007]}, "mutation_prompt": null}
{"id": "ac084253-9a1d-45ae-9a83-466772b2f67b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, max(5, self.current_evaluations // 100))  # Dynamic adjustment of population size\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm introduces a dynamic population size adjustment strategy to ensure diverse exploration and efficient solution refinement during the optimization process.", "configspace": "", "generation": 57, "fitness": 0.7380289099892471, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.738 with standard deviation 0.076. And the mean value of best solutions found was 0.228 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.843421163322902, 0.664931078811636, 0.705734487833203], "final_y": [0.2109760812486574, 0.247146518985075, 0.2255841079907005]}, "mutation_prompt": null}
{"id": "515adfe6-e7b6-44d4-8e7f-07747e396e2b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='TNC', options={'xtol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance the local search by fine-tuning the method and convergence criteria to improve solution refinement.", "configspace": "", "generation": 58, "fitness": 0.9039143298509194, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.034. And the mean value of best solutions found was 0.174 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8582248608948482, 0.9378998614948131, 0.9156182671630968], "final_y": [0.18907658171148434, 0.16575417188900832, 0.1678418524303007]}, "mutation_prompt": null}
{"id": "88377f2f-652f-4b3d-b19b-5c32c0ac4969", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-8})  # Enhanced termination condition\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing the local search phase by adjusting the termination condition for better fine-tuning near promising solutions.", "configspace": "", "generation": 59, "fitness": 0.9039143298509194, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.034. And the mean value of best solutions found was 0.174 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8582248608948482, 0.9378998614948131, 0.9156182671630968], "final_y": [0.18907658171148434, 0.16575417188900832, 0.1678418524303007]}, "mutation_prompt": null}
{"id": "589df2a3-f45d-49d9-ac59-18a41c897dea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < (CR + 0.1 * np.sin(self.current_evaluations)) # Modified line for improved diversity\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced structured population diversity by altering crossover strategy for improved convergence.", "configspace": "", "generation": 60, "fitness": 0.8400460341018569, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.077. And the mean value of best solutions found was 0.180 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.819507496695534, 0.9430464915327587, 0.7575841140772777], "final_y": [0.19310035956447025, 0.1675104338253499, 0.17834812235306174]}, "mutation_prompt": null}
{"id": "3f0b2a37-c56c-442e-98c6-9da1f82d3ccb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.3 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6, 'maxls': 10})  # Enhanced local search\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm refines the adaptive mutation factor and crossover strategy, and enhances local search by incorporating a termination criterion based on solution improvement rate.", "configspace": "", "generation": 61, "fitness": 0.8889939544802159, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.889 with standard deviation 0.046. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8449430354195345, 0.9526935998687959, 0.8693452281523175], "final_y": [0.19503135369009728, 0.1657326392405296, 0.18404831637418761]}, "mutation_prompt": null}
{"id": "00187563-cab1-4aa3-b6f4-c0234f6c4c61", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        pop += np.sin(np.linspace(0, 2*np.pi, self.dim)) * 0.1  # Encourage periodicity\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.6 + 0.3 * np.random.rand()  # Enhanced mutation factor initialization\n                CR = 0.65 + 0.2 * np.random.rand()  # Further adjusted crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-7})  # Improved precision\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm enhances global exploration with periodicity encouragement and improved local search by adjusting the initialization and mutation strategies.", "configspace": "", "generation": 62, "fitness": 0.8714549215548653, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.012. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8641875622122228, 0.8882459222987074, 0.8619312801536655], "final_y": [0.16498927453123902, 0.16487230323942126, 0.16800785383813732]}, "mutation_prompt": null}
{"id": "89a6946b-50f6-402e-b42e-bb0fcbd66691", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c) + np.random.normal(0, 0.1, self.dim), bounds.lb, bounds.ub)  # Gaussian perturbation added\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm refines its mutation strategy by introducing a Gaussian-based perturbation to enhanced adaptability.", "configspace": "", "generation": 63, "fitness": 0.8625256287881718, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.034. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8445694512464318, 0.8330666341867847, 0.9099408009312989], "final_y": [0.1714199760885391, 0.1924306036837934, 0.16519186522811213]}, "mutation_prompt": null}
{"id": "b9f5fddc-c6b1-42c8-aa4a-7c8a219edf87", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, np.mean([best_de], axis=0), bounds)  # Change: Improved local search initial solution\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved local search initial solution to enhance convergence on promising regions.", "configspace": "", "generation": 64, "fitness": 0.9039143298509194, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.034. And the mean value of best solutions found was 0.174 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8582248608948482, 0.9378998614948131, 0.9156182671630968], "final_y": [0.18907658171148434, 0.16575417188900832, 0.1678418524303007]}, "mutation_prompt": null}
{"id": "0e312fb7-b7be-49da-9f95-629381131db4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            # Adaptive population size reduction based on fitness diversity\n            fitness_values = [func(ind) for ind in pop]\n            if np.std(fitness_values) < 0.01:  # Reduce population size if diversity is low\n                pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm enhances convergence by introducing adaptive population size reduction based on fitness diversity to maintain exploration and exploitation balance.", "configspace": "", "generation": 65, "fitness": 0.7307326870789476, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.731 with standard deviation 0.035. And the mean value of best solutions found was 0.207 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.7414536781546905, 0.7676814525510152, 0.6830629305311366], "final_y": [0.2167637166377674, 0.1793112079748702, 0.22469141656422298]}, "mutation_prompt": null}
{"id": "e4ad9846-d961-43ca-8752-ca8a3d170a39", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            pop = np.random.permutation(pop)  # Introduced permutation for diversity\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive strategy with refined exploration and exploitation through population diversity maintenance.", "configspace": "", "generation": 66, "fitness": 0.6708520972019304, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.671 with standard deviation 0.035. And the mean value of best solutions found was 0.250 (0. is the best) with standard deviation 0.030.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.7190465633907435, 0.6380608756738293, 0.6554488525412183], "final_y": [0.21235291311880133, 0.25202132472850514, 0.2865865509163573]}, "mutation_prompt": null}
{"id": "3adb4baa-4b9a-4bd1-bf1f-75fde4c24150", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()  # Increased adaptive mutation factor range\n                CR = 0.7 + 0.25 * np.random.rand()\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(int(pop_size * 0.9), 10)  # Adaptive population size reduction\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduced adaptive population size reduction and refined the mutation strategy for better convergence.", "configspace": "", "generation": 67, "fitness": 0.898263428858663, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.898 with standard deviation 0.022. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8708684988228832, 0.9243144451963317, 0.8996073425567739], "final_y": [0.18195285771536218, 0.16495665233519208, 0.1661466633278661]}, "mutation_prompt": null}
{"id": "73303cbf-d196-48e4-b779-bca7cfb3cdb9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.6 + 0.3 * np.random.rand()  # Modified adaptive mutation factor initialization\n                CR = 0.75 + 0.2 * np.random.rand()  # Modified adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhances the convergence speed by adjusting the mutation factor and crossover strategy, along with a dynamic population size.", "configspace": "", "generation": 68, "fitness": 0.8679072990783662, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.032. And the mean value of best solutions found was 0.179 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8368077488526098, 0.8549566080660369, 0.9119575403164517], "final_y": [0.18223312478367149, 0.1881786854853642, 0.16531632491504644]}, "mutation_prompt": null}
{"id": "6b16b71f-1202-4f4b-beee-ebadfecefbd5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            if self.current_evaluations < self.budget:  # New line to implement the opposition-based learning\n                opp_best = bounds.ub + bounds.lb - best  # New line to implement the opposition-based learning\n                if func(opp_best) < func(best):  # New line to implement the opposition-based learning\n                    best = opp_best  # New line to implement the opposition-based learning\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced HybridMetaheuristicOptimizer now incorporates opposition-based learning to improve solution diversity and convergence speed.", "configspace": "", "generation": 69, "fitness": 0.8985521883936763, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.899 with standard deviation 0.034. And the mean value of best solutions found was 0.174 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.852641106437408, 0.933372178954154, 0.9096432797894671], "final_y": [0.18908202025683252, 0.16578236375364452, 0.16791997235752054]}, "mutation_prompt": null}
{"id": "c98db2d1-c795-420e-bcc7-a436e0a712c2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(int(pop_size * 0.95), 10)  # Dynamic adjustment of population size\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a dynamic population size adjustment strategy to enhance exploration and exploitation balance.", "configspace": "", "generation": 70, "fitness": 0.8909514057461568, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.024. And the mean value of best solutions found was 0.172 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.863592157021757, 0.8874664179419307, 0.9217956422747827], "final_y": [0.18534325828551845, 0.1657904628642498, 0.16563056617280336]}, "mutation_prompt": null}
{"id": "25f6bcc7-3177-4740-b188-3eb9f03dc1b4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()  # Modified adaptive mutation factor\n                CR = 0.6 + 0.3 * np.random.rand()  # Altered crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            if self.current_evaluations < self.budget * 0.5:\n                pop_size = max(pop_size // 2, 10)  # Adaptive population resizing\n\n        return best\n\n    def periodic_constraint(self, solution):\n        return np.tile(solution[:self.dim // 2], 2)  # Enforcing periodic solutions\n\n    def local_search(self, func, x0, bounds):\n        x0 = self.periodic_constraint(x0)  # Apply periodicity before local search\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm refines the differential evolution and local search integration by introducing periodicity constraints and a novel modular initialization strategy with adaptive population resizing for enhanced convergence.", "configspace": "", "generation": 71, "fitness": 0.8758429170856282, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.034. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8440293035866093, 0.9236145567228635, 0.8598848909474116], "final_y": [0.17634452669343303, 0.16497195673636678, 0.1705409237116221]}, "mutation_prompt": null}
{"id": "19e327ba-c659-4cc5-a12a-f24523dad373", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 1.5, 10)  # Adjusted population size reduction strategy\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm integrates an adaptive population size reduction with refined local search to improve convergence efficiency.  ", "configspace": "", "generation": 72, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'float' object cannot be interpreted as an integer\").", "error": "TypeError(\"'float' object cannot be interpreted as an integer\")", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {}, "mutation_prompt": null}
{"id": "a0202b42-47ab-4ef1-be0a-24db52483465", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = int(max(pop_size * 0.9, 10))  # Refined decrease strategy for population size\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced with a dynamic blending strategy for mutation vectors and a refined decrease strategy for population size.", "configspace": "", "generation": 73, "fitness": 0.8760454557431864, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.046. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.869780407551887, 0.8226760665885479, 0.9356798930891241], "final_y": [0.18194131950066428, 0.18330929597962908, 0.1656458448935736]}, "mutation_prompt": null}
{"id": "abfb7394-311b-44c3-9cff-302d1dbefc0e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            if self.current_evaluations < self.budget and np.random.rand() < 0.05:  # Random restart mechanism\n                pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n                best_idx = np.argmin([func(ind) for ind in pop])\n                best = pop[best_idx]\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Integrate a random restart mechanism in the differential evolution to enhance exploration and avoid premature convergence.", "configspace": "", "generation": 74, "fitness": 0.8039634310869879, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.018. And the mean value of best solutions found was 0.191 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8065593994706264, 0.7802731447828793, 0.8250577490074582], "final_y": [0.20789856640412163, 0.1880567127678492, 0.17661667650439228]}, "mutation_prompt": null}
{"id": "086ab1c9-12fb-4609-832f-b9728c55e25a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.6 + 0.25 * np.random.rand()  # Further adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "This algorithm refines the HybridMetaheuristicOptimizer by adjusting the mutation and crossover strategies to improve exploration and convergence.", "configspace": "", "generation": 75, "fitness": 0.8814183227410638, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.881 with standard deviation 0.033. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.8394601595288087, 0.9190474297965034, 0.8857473788978794], "final_y": [0.1824349498866713, 0.16498577275844195, 0.1655558666180056]}, "mutation_prompt": null}
{"id": "1d6afc27-b143-4cac-bb65-edf6b09d7b1a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        half_pop_size = pop_size // 2\n        pop = np.random.uniform(bounds.lb, bounds.ub, (half_pop_size, self.dim))\n        # Encourage periodic solution by initializing some individuals with repeating patterns\n        periodic_init = np.tile(np.random.uniform(bounds.lb, bounds.ub, (half_pop_size, self.dim // 2)), 2)\n        pop = np.concatenate((pop, periodic_init))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm introduces periodic solution encouragement with tailored initialization to enhance convergence to near-optimal solutions.", "configspace": "", "generation": 76, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (10, 5) and arg 1 with shape (10,).').", "error": "ValueError('shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (10, 5) and arg 1 with shape (10,).')", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {}, "mutation_prompt": null}
{"id": "e6c69bc9-4454-475a-9746-c59a89bf03d2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.2 * np.random.rand()  # Improved adaptive mutation factor initialization\n                CR = 0.7 + 0.25 * np.random.rand()  # Adjusted adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            # Encourage periodicity by averaging every two layers\n            pop = 0.5 * (pop + np.roll(pop, shift=1, axis=1))\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced differential evolution algorithm with a periodicity constraint to encourage constructive interference and improve convergence towards optimal solutions.", "configspace": "", "generation": 77, "fitness": 0.9569511679981103, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.957 with standard deviation 0.002. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "59dcaf2e-8a25-470e-9473-fef04d98bc2c", "metadata": {"aucs": [0.9579378756030124, 0.9583629557677713, 0.9545526726235475], "final_y": [0.17277783849765438, 0.17302275021516744, 0.1727095405285518]}, "mutation_prompt": null}
{"id": "a7b46839-cf76-4268-8d85-eeb638db5602", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.75 + 0.2 * np.random.rand()  # Slightly increased adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            # Encourage periodicity by averaging every three layers\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A refined hybrid metaheuristic algorithm with improved adaptive parameters and periodic adjustment to enhance convergence on complex optimization landscapes.", "configspace": "", "generation": 78, "fitness": 0.9805859312274503, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e6c69bc9-4454-475a-9746-c59a89bf03d2", "metadata": {"aucs": [0.9813787851599862, 0.9784282421324398, 0.9819507663899247], "final_y": [0.1648557719046978, 0.1648557719046978, 0.1648557719046978]}, "mutation_prompt": null}
{"id": "37375f76-6626-440b-be88-ccd53fd6aab1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            # Encourage periodicity by averaging every three layers\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Refined hybrid metaheuristic algorithm with enhanced adaptive crossover strategy for improved convergence.", "configspace": "", "generation": 79, "fitness": 0.9821891322864523, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a7b46839-cf76-4268-8d85-eeb638db5602", "metadata": {"aucs": [0.9822277089798732, 0.9821633643808433, 0.98217632349864], "final_y": [0.1648557719046978, 0.1648557719046978, 0.1648557719046978]}, "mutation_prompt": null}
{"id": "ecf9ee5e-8602-4d27-904d-ab1ddf27ca0c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 5)  # Changed to allow smaller population size\n            # Encourage periodicity by averaging every two layers\n            pop = (1/2) * pop + (1/2) * np.roll(pop, shift=1, axis=1)  # Modified periodic averaging\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic optimizer with dynamic population adjustment and improved periodicity encouragement for better convergence.", "configspace": "", "generation": 80, "fitness": 0.9589494485255577, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.001. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9603580371169385, 0.9595221822023232, 0.9569681262574117], "final_y": [0.1730576622609239, 0.17313402229853947, 0.17304976112675252]}, "mutation_prompt": null}
{"id": "02380ec9-8d4c-433c-9fba-70f28676713e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            # Encourage periodicity by averaging every two layers\n            pop = 0.5 * pop + 0.5 * np.roll(pop, shift=1, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced periodicity preservation by averaging every two layers instead of three in mutation phase.", "configspace": "", "generation": 81, "fitness": 0.9571372718041277, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.957 with standard deviation 0.003. And the mean value of best solutions found was 0.172 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9549704336578616, 0.9614802712383717, 0.9549611105161498], "final_y": [0.17273807896443671, 0.1717543280424344, 0.17262430213503188]}, "mutation_prompt": null}
{"id": "d5e1ebcb-6f23-4d77-9caf-623266e96417", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            # Encourage periodicity by averaging every three layers\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced diversity in population initialization for broader exploration and faster convergence.", "configspace": "", "generation": 82, "fitness": 0.9821891322864523, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9822277089798732, 0.9821633643808433, 0.98217632349864], "final_y": [0.1648557719046978, 0.1648557719046978, 0.1648557719046978]}, "mutation_prompt": null}
{"id": "eb8bf80c-e0db-4d9d-a91c-a040706a529d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.6 + 0.2 * np.random.rand()  # Adjusted adaptive mutation factor range\n                CR = 0.85 + 0.1 * np.random.rand()  # Slightly refined adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            # Encourage periodicity more consistently\n            pop = (1/2) * pop + (1/2) * np.roll(pop, shift=1, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved mutation strategy and periodicity enforcement for enhanced exploration and exploitation balance.", "configspace": "", "generation": 83, "fitness": 0.9583587561508272, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.958 with standard deviation 0.003. And the mean value of best solutions found was 0.172 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9605381292988765, 0.9607939077578364, 0.9537442313957685], "final_y": [0.17254169050790835, 0.17153360917430704, 0.17232785785506854]}, "mutation_prompt": null}
{"id": "730e43cb-5609-4fb4-a38c-4d3ef8f09fa0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 8)  # Adjusted population size reduction to 8\n            # Encourage periodicity by averaging every three layers\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Advanced hybrid metaheuristic optimizer with adaptive population size strategy for improved convergence efficiency.", "configspace": "", "generation": 84, "fitness": 0.9175086418868079, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.918 with standard deviation 0.078. And the mean value of best solutions found was 0.188 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.8075330654294188, 0.980456540223665, 0.96453632000734], "final_y": [0.2283238300129682, 0.1648557719046978, 0.17011231219734646]}, "mutation_prompt": null}
{"id": "12390fe9-8d7d-4c5f-9696-85bda21b97c5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n        \n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1) + np.random.normal(0, 0.01, pop.shape)  # Diversity maintenance\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved the diversity maintenance by modifying the population update strategy to enhance the exploration-exploitation balance.", "configspace": "", "generation": 85, "fitness": 0.9820157645037594, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9866856986759118, 0.978495240699366, 0.9808663541360004], "final_y": [0.16485577791487005, 0.1648557759671968, 0.16485577647795402]}, "mutation_prompt": null}
{"id": "5f42f882-8e88-4077-ac3b-f73853b44caa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                # Encourage periodicity: Compute differential over shifted indices\n                periodicity_shift = np.roll(pop[i], shift=2)\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, periodicity_shift)\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            # Dynamic population scaling based on budget usage\n            pop_size = max(pop_size // 2, int(10 * (self.budget - self.current_evaluations) / self.budget))\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic with dynamic population scaling and periodicity-inspired mutation to improve convergence.", "configspace": "", "generation": 86, "fitness": 0.9806244302364987, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9810736300747929, 0.9833915938574797, 0.9774080667772237], "final_y": [0.16489157131546328, 0.1648557719046978, 0.16485579588476718]}, "mutation_prompt": null}
{"id": "9ccf1780-49c1-439a-9c12-3430c7b2a9a6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=30, F=0.85, CR=0.9):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        pop = np.concatenate((pop, bounds.ub + bounds.lb - pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                adaptive_F = 0.4 + 0.5 * np.random.rand()\n                adaptive_CR = 0.75 + 0.25 * np.random.rand()\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 15)\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-7})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced Differential Evolution with Adaptive Periodicity and Local Search Refinement for Optimizing Multilayer Structures.", "configspace": "", "generation": 87, "fitness": 0.9718690141906675, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9778892721395515, 0.9737472365394196, 0.9639705338930311], "final_y": [0.1648558724170066, 0.1648578343275937, 0.16489127654347224]}, "mutation_prompt": null}
{"id": "87e18e66-8634-4d4c-a900-f334383aafb9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.85 + 0.1 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 3, 8)  # Dynamic population resizing\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1)\n            # Introduce periodicity constraint\n            for j in range(self.dim):\n                if j % 2 == 0:\n                    pop[:, j] = pop[:, j-1]\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Adaptive multi-strategy hybrid optimizer leveraging dynamic population size and periodicity enforcement for enhanced exploration and fine-tuning.", "configspace": "", "generation": 88, "fitness": 0.9600204212600918, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.960 with standard deviation 0.001. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9595809090315693, 0.9586557965308015, 0.9618245582179048], "final_y": [0.1729022927291367, 0.17315951670530394, 0.17218846137301536]}, "mutation_prompt": null}
{"id": "f3c7cc18-28dd-45fa-a0f7-6ba115ad1d2d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            # Change population size dynamically to enhance exploration and convergence balance\n            pop_size = max(int(pop_size * 0.9), 10)\n            # Encourage periodicity by averaging every three layers\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduced dynamic population size adjustment in Differential Evolution to enhance exploration and convergence balance.", "configspace": "", "generation": 89, "fitness": 0.9745761026300505, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.003. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.970356535284126, 0.9773695062169245, 0.9760022663891006], "final_y": [0.16692139407821804, 0.1648557719046978, 0.16485577190469758]}, "mutation_prompt": null}
{"id": "b1154e2d-fe43-422f-ad57-040c5fecc14d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.35 * np.random.rand()  # Fine-tuned adaptive mutation factor\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            # Encourage periodicity by averaging every three layers\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Fine-tune adaptive mutation strategy in the differential evolution phase for improved exploration and convergence balance.", "configspace": "", "generation": 90, "fitness": 0.9814225152421775, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9797975464281565, 0.9817557636071378, 0.9827142356912383], "final_y": [0.1648557719046978, 0.1648557719046978, 0.16485577190469758]}, "mutation_prompt": null}
{"id": "4f0e5f58-3f36-44ba-af3e-9e26321a0fc4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.6 + 0.2 * np.random.rand()  # Enhanced adaptive mutation factor\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            # Encourage periodicity by averaging every three layers\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive mutation factor for better exploration in differential evolution phase.", "configspace": "", "generation": 91, "fitness": 0.9810118661312276, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9799120459125992, 0.98309277984269, 0.9800307726383934], "final_y": [0.16492762363296742, 0.16485577190469758, 0.1648557719046978]}, "mutation_prompt": null}
{"id": "3aac7245-6ebd-41a3-9fee-3e1b1652d0b1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            # Encourage periodicity by averaging every two layers instead of three\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=1, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved periodicity enforcement by averaging layers more effectively to achieve better reflectivity.", "configspace": "", "generation": 92, "fitness": 0.9542583045462912, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.954 with standard deviation 0.003. And the mean value of best solutions found was 0.172 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9542270494160839, 0.9579092314946551, 0.950638632728135], "final_y": [0.17159318304108473, 0.1729405753852713, 0.17282344524668258]}, "mutation_prompt": null}
{"id": "32fbc12e-16d0-4bef-97c3-b7fba71d24a7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.6 + 0.3 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.85 + 0.10 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            # Encourage periodicity by averaging every two layers\n            pop = (1/2) * pop + (1/2) * np.roll(pop, shift=1, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation and periodicity encouragement strategies to improve convergence and solution quality.", "configspace": "", "generation": 93, "fitness": 0.9579577969094292, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.958 with standard deviation 0.001. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9588360773964703, 0.9579318101360921, 0.9571055031957256], "final_y": [0.17262052322655386, 0.17273294539365258, 0.17235632921373145]}, "mutation_prompt": null}
{"id": "02f2cc10-ab03-4314-b0b8-9f93f553ae3a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            # Encourage periodicity by averaging every three layers\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        x0 = np.clip(x0, bounds.lb, bounds.ub)  # Improved initialization for local search\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic with improved local search initialization for better convergence.", "configspace": "", "generation": 94, "fitness": 0.9821891322864523, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9822277089798732, 0.9821633643808433, 0.98217632349864], "final_y": [0.1648557719046978, 0.1648557719046978, 0.1648557719046978]}, "mutation_prompt": null}
{"id": "cb3206c9-5067-4e93-9c36-73ea6f8c9f91", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            # Encourage periodicity by averaging every three layers\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Refined hybrid metaheuristic algorithm with enhanced adaptive crossover strategy for improved convergence.", "configspace": "", "generation": 80, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9822277089798732, 0.9821633643808433, 0.98217632349864], "final_y": [0.1648557719046978, 0.1648557719046978, 0.1648557719046978]}, "mutation_prompt": null}
{"id": "f3720a25-ff13-4118-b278-bcf61afdc8aa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size * 3 // 4, 10)  # Adaptive population size adjustment\n            # Encourage periodicity by averaging every two layers\n            pop = (1/2) * pop + (1/2) * np.roll(pop, shift=1, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved hybrid metaheuristic with refined periodicity encouragement and adaptive population size for enhanced optimization efficiency.", "configspace": "", "generation": 96, "fitness": 0.9526811858932431, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.953 with standard deviation 0.003. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9490036465456421, 0.9555335644665831, 0.9535063466675044], "final_y": [0.1730964576679097, 0.1724263528546861, 0.17257897951285184]}, "mutation_prompt": null}
{"id": "86f6f1c2-5e1d-430f-938c-15075ec9d976", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()  # Slightly modified adaptive mutation factor range\n                CR = 0.75 + 0.2 * np.random.rand()  # Slightly broadened adaptive crossover range\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            pop = (3/4) * pop + (1/4) * np.roll(pop, shift=2, axis=1)  # Enhanced periodicity encouragement\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved hybrid metaheuristic optimizer with enhanced periodicity encouragement and adaptive parameters for better convergence.", "configspace": "", "generation": 97, "fitness": 0.98078913994456, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9838205232586181, 0.9789050184985231, 0.9796418780765385], "final_y": [0.16485577190469758, 0.16485577190716105, 0.16485577192289314]}, "mutation_prompt": null}
{"id": "a693b951-c63a-48a4-b2f6-5bf414bc5610", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                \n                # Change adaptive mutation factor strategy\n                F *= 1.0 + 0.5 * ((self.budget - self.current_evaluations) / self.budget)\n                \n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            # Encourage periodicity by averaging every three layers\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved hybrid metaheuristic with dynamic adaptive mutation factor for enhanced exploration.", "configspace": "", "generation": 98, "fitness": 0.9785832763595912, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.9768104193392008, 0.9801275190105209, 0.9788118907290521], "final_y": [0.1648557719046978, 0.1648557719046978, 0.1648557719046978]}, "mutation_prompt": null}
{"id": "f1ffc209-3626-4882-9c88-26876a22c505", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.95):\n        pop = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opp_pop = bounds.ub + bounds.lb - pop\n        pop = np.concatenate((pop, opp_pop))\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            for i in range(pop_size):\n                if self.current_evaluations >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.25 * np.random.rand()  # Adjusted adaptive mutation factor initialization\n                CR = 0.8 + 0.15 * np.random.rand()  # Enhanced adaptive crossover strategy\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, pop[i])\n\n                trial_fitness = func(trial)\n                self.current_evaluations += 1\n\n                if trial_fitness < func(pop[i]):\n                    pop[i] = trial\n                    if trial_fitness < func(best):\n                        best = trial\n\n            pop_size = max(pop_size // 2, 10)\n            pop = (2/3) * pop + (1/3) * np.roll(pop, shift=2, axis=1) * (0.5 + 0.5 * np.sin(np.arange(self.dim)*np.pi)) # Encourage periodicity\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        best_de = self.differential_evolution(func, func.bounds)\n\n        if self.current_evaluations < self.budget:\n            best_solution = self.local_search(func, best_de, bounds)\n        else:\n            best_solution = best_de\n\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive crossover strategy for improved convergence with periodic encouragement.", "configspace": "", "generation": 99, "fitness": 0.6575021293780413, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.658 with standard deviation 0.084. And the mean value of best solutions found was 0.307 (0. is the best) with standard deviation 0.046.", "error": "", "parent_id": "37375f76-6626-440b-be88-ccd53fd6aab1", "metadata": {"aucs": [0.6418235370186985, 0.5638373026251977, 0.7668455484902277], "final_y": [0.3134298421743149, 0.3600832827120468, 0.246819238649031]}, "mutation_prompt": null}
