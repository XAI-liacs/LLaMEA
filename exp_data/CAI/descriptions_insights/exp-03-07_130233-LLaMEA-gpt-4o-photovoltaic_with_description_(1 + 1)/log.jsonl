{"id": "129e3e18-b538-424a-96b3-2e82a06300f7", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                self.velocities[i] = self.velocities[i] + cognitive_component + social_component\n                \n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Adaptive Quantum-inspired Particle Swarm Optimization (AQPSO) leveraging quantum superposition and adaptive learning for enhanced explorations and exploitations.", "configspace": "", "generation": 0, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "3893fde6-5da6-4606-afbb-ba20af323495", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n        self.inertia_weight = 0.9  # New line: Initialize inertia weight\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                self.velocities[i] = self.inertia_weight * self.velocities[i] + cognitive_component + social_component  # Modified line: Added inertia weight\n\n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n            self.inertia_weight *= 0.99  # New line: Dynamically adjust inertia weight\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced AQPSO with dynamic inertia weight for improved convergence and exploration-exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "129e3e18-b538-424a-96b3-2e82a06300f7", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "fd2bbe5d-fdd7-4d3b-8930-1ca63190be47", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        inertia_weight = 0.9  # Initial inertia weight\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                self.velocities[i] = inertia_weight * self.velocities[i] + cognitive_component + social_component\n                \n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n            \n            # Dynamic inertia weight update\n            inertia_weight = 0.4 + 0.5 * ((self.budget - self.func_evals) / self.budget)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced Adaptive Quantum-inspired Particle Swarm Optimization (EAQPSO) incorporating dynamic inertia weight adjustment for improved exploration-exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "129e3e18-b538-424a-96b3-2e82a06300f7", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "188137ad-8b44-4322-96a0-284dbaaffe0a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n        self.w = 0.9  # Added adaptive inertia weight\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component  # Modified to include inertia weight\n                \n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n            # Adjust inertia weight for better balance\n            self.w = 0.4 + (0.5 * (self.budget - self.func_evals) / self.budget)  # New line for inertia adjustment\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced AQPSO with adaptive inertia weight for improved balance between exploration and exploitation.", "configspace": "", "generation": 3, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "129e3e18-b538-424a-96b3-2e82a06300f7", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "def0b34f-b36e-45c0-b1e5-d0cf7f681c18", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n        self.inertia_weight = 0.9  # Initial inertia weight\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] + \n                                      cognitive_component + social_component)\n                \n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Update inertia weight\n            self.inertia_weight *= 0.99\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced AQPSO with dynamic inertia weight for improved balance between exploration and exploitation.", "configspace": "", "generation": 4, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "129e3e18-b538-424a-96b3-2e82a06300f7", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "e26f55d5-8082-4908-b9d0-52b4237c3f33", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                self.velocities[i] = self.velocities[i] + cognitive_component + social_component\n                \n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Dynamic swarm size adjustment\n            if self.func_evals >= self.budget / 2:\n                self.swarm_size = max(10, self.swarm_size // 2)\n\n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced AQPSO with dynamic swarm size adjustment to improve exploration and exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "129e3e18-b538-424a-96b3-2e82a06300f7", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "dfad9f34-de0e-48f8-b03f-8b1e3ebb484b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n        self.inertia_weight = 0.9  # Add dynamic inertia weight\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                # Introduce inertia weight for velocity update\n                self.velocities[i] = self.inertia_weight * self.velocities[i] + cognitive_component + social_component\n                \n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n                \n                # Introduce Gaussian mutation for diversification\n                if np.random.rand() < 0.1:\n                    self.positions[i] += np.random.normal(0, 0.1, self.dim)\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced AQPSO with dynamic inertia weight and Gaussian mutation for improved convergence and exploration.", "configspace": "", "generation": 6, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "129e3e18-b538-424a-96b3-2e82a06300f7", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "741efd15-a844-4db8-accc-c19dbe80295c", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        momentum = 0.9  # Introduced momentum factor\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                self.velocities[i] = momentum * self.velocities[i] + cognitive_component + social_component\n                \n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce a momentum factor in velocity update to enhance convergence speed and stability in AQPSO.", "configspace": "", "generation": 7, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "129e3e18-b538-424a-96b3-2e82a06300f7", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "f3eecc9b-0ca7-431c-b47e-8af3697d8561", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                self.velocities[i] = self.velocities[i] + cognitive_component + social_component\n                \n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Dynamic swarm size adjustment\n            if self.func_evals % 10 == 0:\n                self.swarm_size = int(self.swarm_size * 1.05)  # increase swarm size by 5%\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced AQPSO with dynamic swarm size adjustment for improved convergence and exploration.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 30 is out of bounds for axis 0 with size 30').", "error": "IndexError('index 30 is out of bounds for axis 0 with size 30')", "parent_id": "129e3e18-b538-424a-96b3-2e82a06300f7", "metadata": {}, "mutation_prompt": null}
{"id": "b5767fa2-c947-4ee2-8c1a-aae1372c709b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                self.velocities[i] = self.velocities[i] + cognitive_component + social_component\n                \n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.3: # Changed from 0.5 to 0.3\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced AQPSO by adjusting quantum behavior's probabilistic exploration condition for better balance between exploration and exploitation.", "configspace": "", "generation": 9, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "129e3e18-b538-424a-96b3-2e82a06300f7", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "31c87104-0faf-49b0-8dc8-56df26a481fb", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n        self.inertia_weight = 0.9  # New: Added adaptive inertia weight\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                self.velocities[i] = (self.inertia_weight * self.velocities[i] +  # Changed: Added inertia component\n                                      cognitive_component + social_component)\n                \n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced AQPSO with adaptive inertia weight for improved convergence speed and accuracy.", "configspace": "", "generation": 10, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "129e3e18-b538-424a-96b3-2e82a06300f7", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "0cc61832-85cc-4f37-b8e3-7d4f2b11171f", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                # Line changed: Applied adaptive velocity clamping\n                self.velocities[i] = 0.5 * self.velocities[i] + cognitive_component + social_component \n                \n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced AQPSO with adaptive velocity clamping for improved exploration-exploitation balance.", "configspace": "", "generation": 11, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "129e3e18-b538-424a-96b3-2e82a06300f7", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "65a82d0e-f5cf-43a2-8f08-94993df85146", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.9 - 0.5 * (self.func_evals / self.budget)  # Dynamic inertia weight\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                self.velocities[i] = inertia_weight * self.velocities[i] + cognitive_component + social_component\n                \n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced Adaptive Quantum-inspired Particle Swarm Optimization (EAQPSO) with dynamic inertia weight for improved balance between exploration and exploitation.", "configspace": "", "generation": 12, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "129e3e18-b538-424a-96b3-2e82a06300f7", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "616a4139-2f6d-4766-80b0-35a7efa5a0f7", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2  # adaptive inertia\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced Adaptive Quantum-inspired Particle Swarm Optimization (EAQPSO) introducing adaptive inertia and boundary-aware velocity for improved exploration and convergence.", "configspace": "", "generation": 13, "fitness": 0.7722039504582466, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.772 with standard deviation 0.026. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "129e3e18-b538-424a-96b3-2e82a06300f7", "metadata": {"aucs": [0.7645111623416935, 0.8071771147230691, 0.7449235743099774], "final_y": [0.15495179805340753, 0.12734945283422505, 0.16030684463193423]}, "mutation_prompt": null}
{"id": "eae53aa7-da77-439d-920c-8e89ee642c07", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.5, 0.5, (self.swarm_size, self.dim))  # Adjusted velocity range\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2  # adaptive inertia\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply quantum behavior for exploration\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced Adaptive Quantum-inspired Particle Swarm Optimization (EAQPSO) with randomized velocity initialization for improved exploration and convergence.", "configspace": "", "generation": 14, "fitness": 0.7722039504582466, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.772 with standard deviation 0.026. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "616a4139-2f6d-4766-80b0-35a7efa5a0f7", "metadata": {"aucs": [0.7645111623416935, 0.8071771147230691, 0.7449235743099774], "final_y": [0.15495179805340753, 0.12734945283422505, 0.16030684463193423]}, "mutation_prompt": null}
{"id": "aceccfe1-6863-492b-8c58-d23782b27097", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2  # adaptive inertia\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.5 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Improved AQPSO by introducing adaptive quantum behavior based on convergence feedback for enhanced exploration-exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.7977541699230147, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.008. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "616a4139-2f6d-4766-80b0-35a7efa5a0f7", "metadata": {"aucs": [0.7869969651080853, 0.8014494633175149, 0.8048160813434442], "final_y": [0.14553946981743393, 0.12874010750071752, 0.12443623842379892]}, "mutation_prompt": null}
{"id": "b4f68e20-40bf-4327-b8ae-8761a4b9dc58", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2  # adaptive inertia\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.5 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Incorporate a dynamic velocity update threshold linked to the global best score to enhance solution refinement.", "configspace": "", "generation": 16, "fitness": 0.7977541699230147, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.008. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "aceccfe1-6863-492b-8c58-d23782b27097", "metadata": {"aucs": [0.7869969651080853, 0.8014494633175149, 0.8048160813434442], "final_y": [0.14553946981743393, 0.12874010750071752, 0.12443623842379892]}, "mutation_prompt": null}
{"id": "b608a7e4-4e7c-4eaf-bf26-e627e3052a00", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2  # adaptive inertia\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.5 * (1 - np.tanh(5 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))))\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced quantum behavior by dynamically adjusting the quantum probability using a nonlinear transformation of the global best score.", "configspace": "", "generation": 17, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aceccfe1-6863-492b-8c58-d23782b27097", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "9d4c876e-f9bb-4789-8e27-837e4ec1bcbe", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.5 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced an inertia weight decay mechanism for improved exploitation as iterations progress.", "configspace": "", "generation": 18, "fitness": 0.8037320682093307, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.017. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "aceccfe1-6863-492b-8c58-d23782b27097", "metadata": {"aucs": [0.8245641412713165, 0.783337861503364, 0.8032942018533118], "final_y": [0.13438411672152506, 0.12637078190519213, 0.12305090522360362]}, "mutation_prompt": null}
{"id": "7740b46a-dc01-4a2b-8b4b-ec49187c12df", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.7 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # changed\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Extended the quantum behavior probability adjustment for enhanced global exploration.", "configspace": "", "generation": 19, "fitness": 0.8096372604219382, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.011. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "9d4c876e-f9bb-4789-8e27-837e4ec1bcbe", "metadata": {"aucs": [0.8245641412713165, 0.8049647894084333, 0.7993828505860647], "final_y": [0.13438411672152506, 0.1372658253717668, 0.1250569989284045]}, "mutation_prompt": null}
{"id": "379fcdbc-c779-41e7-b971-818a5f0462f5", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.7 * ((self.gbest_score - score) / max(1, np.std(self.pbest_scores)))**2  # changed\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "AQPSO with improved quantum probability calculation based on dynamic feedback for exploration refinement.", "configspace": "", "generation": 20, "fitness": 0.7952862422706227, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.795 with standard deviation 0.021. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "7740b46a-dc01-4a2b-8b4b-ec49187c12df", "metadata": {"aucs": [0.8245641412713165, 0.7743634679491329, 0.7869311175914189], "final_y": [0.13438411672152506, 0.13345650958960187, 0.1310829072310794]}, "mutation_prompt": null}
{"id": "3486f45c-f425-4b7e-86d4-d8f7f109aaa2", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.7 * (1 / (self.gbest_score + 1e-9)) / (self.func_evals / self.budget)  # changed\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced global exploration by adjusting quantum probability based on the inverse of the best score normalized by evaluations.", "configspace": "", "generation": 21, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7740b46a-dc01-4a2b-8b4b-ec49187c12df", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "f641ab71-1e37-4625-9acd-531d4c3f75c9", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i]) * (1 + np.sin(self.func_evals/self.budget * np.pi))\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.7 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # changed\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Improved diversity by adjusting the social component scaling dynamically.", "configspace": "", "generation": 22, "fitness": 0.8087815381526015, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.007. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "7740b46a-dc01-4a2b-8b4b-ec49187c12df", "metadata": {"aucs": [0.8189991623529214, 0.804650076371902, 0.8026953757329811], "final_y": [0.13683040458691842, 0.1374481261251672, 0.12621433315751263]}, "mutation_prompt": null}
{"id": "83ea8813-2c3e-44d2-8cef-97cbab9485f5", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Minor refinement in quantum probability calculation enables better adaptability in quantum behavior.", "configspace": "", "generation": 23, "fitness": 0.8109410441776873, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.010. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "7740b46a-dc01-4a2b-8b4b-ec49187c12df", "metadata": {"aucs": [0.8245641412713165, 0.8049647894084333, 0.8032942018533118], "final_y": [0.13438411672152506, 0.1372658253717668, 0.12305090522360362]}, "mutation_prompt": null}
{"id": "a5a6e75f-4151-4cfe-b23a-d49ea8e14a5f", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced adaptive social component scaling based on global best improvement rate.", "configspace": "", "generation": 24, "fitness": 0.8110318600699706, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.010. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "83ea8813-2c3e-44d2-8cef-97cbab9485f5", "metadata": {"aucs": [0.8245336638684797, 0.8049344209956113, 0.8036274953458209], "final_y": [0.1343982136846883, 0.13728340603992395, 0.1220168023552175]}, "mutation_prompt": null}
{"id": "8e63eaed-5e6c-4183-b816-b0b4292627fd", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced adaptive cognitive component scaling based on the current best improvement rate.", "configspace": "", "generation": 25, "fitness": 0.8110826826801425, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.010. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "a5a6e75f-4151-4cfe-b23a-d49ea8e14a5f", "metadata": {"aucs": [0.8250304362913109, 0.8033265334680929, 0.8048910782810239], "final_y": [0.13417742035016966, 0.1382174558026673, 0.12156797340804626]}, "mutation_prompt": null}
{"id": "9ed47bad-871a-433b-a2c4-ecdcf33c40df", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                diversity = np.mean(np.std(self.pbest_positions, axis=0))  # modified\n                quantum_prob = 0.5 + 0.6 * diversity\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced dynamic quantum probability scaling based on the diversity of pbest positions.", "configspace": "", "generation": 26, "fitness": 0.7130569120439659, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.713 with standard deviation 0.001. And the mean value of best solutions found was 0.189 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8e63eaed-5e6c-4183-b816-b0b4292627fd", "metadata": {"aucs": [0.7139052735748536, 0.712374268070557, 0.7128911944864873], "final_y": [0.1895089112570083, 0.1887057550255754, 0.18950892296539956]}, "mutation_prompt": null}
{"id": "e167bc88-b496-4d6a-8b06-bdbb7a02f1f5", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Improved convergence by refining velocity update using a weighted average of personal and global best positions.", "configspace": "", "generation": 27, "fitness": 0.8110826826801425, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.010. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "8e63eaed-5e6c-4183-b816-b0b4292627fd", "metadata": {"aucs": [0.8250304362913109, 0.8033265334680929, 0.8048910782810239], "final_y": [0.13417742035016966, 0.1382174558026673, 0.12156797340804626]}, "mutation_prompt": null}
{"id": "91faab8f-4bec-4249-8dbf-208cfb5192aa", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                convergence_sensitivity = 0.5 * (1 - self.func_evals / self.budget)\n                quantum_prob = 0.5 + convergence_sensitivity * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced the quantum probability calculation by incorporating dynamic weight for convergence sensitivity.", "configspace": "", "generation": 28, "fitness": 0.7935625751503967, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.026. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "8e63eaed-5e6c-4183-b816-b0b4292627fd", "metadata": {"aucs": [0.8250304362913109, 0.760516119203221, 0.7951411699566582], "final_y": [0.13417742035016966, 0.15182117381910076, 0.13089772054003923]}, "mutation_prompt": null}
{"id": "2b79b9a6-c9be-4a6f-ad02-f48772b25dec", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1.05 + 0.5 * improvement_rate  # Adaptive social scaling with slight increase\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced a slight increase in the social component scaling to enhance convergence speed.", "configspace": "", "generation": 29, "fitness": 0.8109722828903809, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.010. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "8e63eaed-5e6c-4183-b816-b0b4292627fd", "metadata": {"aucs": [0.8246426352869072, 0.803330037246885, 0.8049441761373504], "final_y": [0.13434701592197296, 0.1382154135032606, 0.12113162874007932]}, "mutation_prompt": null}
{"id": "f928e78d-c8ce-44fb-8566-a628ac06f4f4", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * np.tanh(self.gbest_position - self.positions[i])  # Changed line\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced quantum behavior component by using a nonlinear scaling factor based on the standard deviation of personal best scores.", "configspace": "", "generation": 30, "fitness": 0.6612761630456482, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.661 with standard deviation 0.001. And the mean value of best solutions found was 0.213 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "8e63eaed-5e6c-4183-b816-b0b4292627fd", "metadata": {"aucs": [0.6619830489481212, 0.6606637148198149, 0.6611817253690084], "final_y": [0.20821371561144109, 0.21492214813813892, 0.21490366900691182]}, "mutation_prompt": null}
{"id": "82804c0d-63e8-48dc-bf44-a30d3db8e625", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.7 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced AQPSO with adaptive quantum probability scaling for improved convergence.", "configspace": "", "generation": 31, "fitness": 0.8097887059779901, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.011. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "8e63eaed-5e6c-4183-b816-b0b4292627fd", "metadata": {"aucs": [0.8250304362913109, 0.8033265334680929, 0.8010091481745668], "final_y": [0.13417742035016966, 0.1382174558026673, 0.1283621036646273]}, "mutation_prompt": null}
{"id": "3ae708aa-0efa-4843-a1a0-8a89b779e887", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.4 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i]\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced adaptive quantum behavior by fine-tuning the quantum probability scaling for better exploration-exploitation balance.", "configspace": "", "generation": 32, "fitness": 0.7968160487461011, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.027. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "8e63eaed-5e6c-4183-b816-b0b4292627fd", "metadata": {"aucs": [0.8250304362913109, 0.7605266316659685, 0.8048910782810239], "final_y": [0.13417742035016966, 0.15182117381910076, 0.12156797340804626]}, "mutation_prompt": null}
{"id": "33e7c4c8-b993-43f7-b9f9-3054c431e7ee", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.1 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced adaptive velocity shift based on global best improvement rate.", "configspace": "", "generation": 33, "fitness": 0.8114946796119686, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.010. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "8e63eaed-5e6c-4183-b816-b0b4292627fd", "metadata": {"aucs": [0.8258379795022995, 0.8036949675465387, 0.8049510917870676], "final_y": [0.13388687854056713, 0.1379125413793878, 0.12109111528450078]}, "mutation_prompt": null}
{"id": "36f61457-24c7-4444-9cfa-8a4092a25e13", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2) \n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.1 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n            # New adaptive mutation based on fitness diversity\n            fitness_diversity = np.std(self.pbest_scores)\n            if fitness_diversity < 0.01:\n                mutation_rate = 0.1\n                self.positions += mutation_rate * np.random.uniform(-0.5, 0.5, self.positions.shape)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced adaptive mutation based on fitness diversity to enhance exploration.", "configspace": "", "generation": 34, "fitness": 0.8113091870119536, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.019. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "33e7c4c8-b993-43f7-b9f9-3054c431e7ee", "metadata": {"aucs": [0.8270125762964242, 0.8215932406250416, 0.7853217441143953], "final_y": [0.13248488119381197, 0.1309338295456205, 0.14251956885479444]}, "mutation_prompt": null}
{"id": "63f13786-6664-4ff6-b69a-cbe40d14e42c", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate\n            cognitive_scale = 0.5 + 0.5 * improvement_rate\n            fitness_variance = np.std(self.pbest_scores)  # Added line for variance calculation\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, fitness_variance)  # Modified line for fitness variance\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.1 * improvement_rate)\n            \n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced adaptive neighborhood exploration based on fitness variance to enhance global exploration.", "configspace": "", "generation": 35, "fitness": 0.8114946796119686, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.010. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "33e7c4c8-b993-43f7-b9f9-3054c431e7ee", "metadata": {"aucs": [0.8258379795022995, 0.8036949675465387, 0.8049510917870676], "final_y": [0.13388687854056713, 0.1379125413793878, 0.12109111528450078]}, "mutation_prompt": null}
{"id": "e8254186-e66e-41af-a243-99f3f4b15a49", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.15 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced adaptive velocity adjustment by refining the quantum behavior impact.", "configspace": "", "generation": 36, "fitness": 0.8116617144468905, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.010. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "33e7c4c8-b993-43f7-b9f9-3054c431e7ee", "metadata": {"aucs": [0.8262150595707853, 0.8038489211540042, 0.8049211626158819], "final_y": [0.13375316451369812, 0.13777379923152622, 0.12165143490835639]}, "mutation_prompt": null}
{"id": "5bfc3c5f-3991-45cf-8589-25a2b6f3318b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.15 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n            \n            self.swarm_size = max(5, int(self.swarm_size * (1 - self.func_evals / self.budget)))  # dynamic adjustment\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced a dynamic swarm size adaptation mechanism based on function evaluation progress.", "configspace": "", "generation": 37, "fitness": 0.809338086882771, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.028. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "e8254186-e66e-41af-a243-99f3f4b15a49", "metadata": {"aucs": [0.8117214481637517, 0.842359879267033, 0.773932933217528], "final_y": [0.1430502697644569, 0.12378602291456897, 0.1567515455923253]}, "mutation_prompt": null}
{"id": "7fbf1788-59a1-4884-8467-4aaa14cdb753", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        momentum_factor = 0.9  # New line: add momentum factor\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = momentum_factor * self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.15 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce a momentum factor in velocity update to enhance convergence stability.", "configspace": "", "generation": 38, "fitness": 0.8115032098256864, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.010. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "e8254186-e66e-41af-a243-99f3f4b15a49", "metadata": {"aucs": [0.8257685847144061, 0.8038549106294072, 0.8048861341332458], "final_y": [0.1339508095166494, 0.13777031928171157, 0.1217101877833957]}, "mutation_prompt": null}
{"id": "13ce689e-15d5-4ede-90fc-0dade353d4bd", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                dynamic_limit = 1 + improvement_rate  # Introduced dynamic velocity limits\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb / dynamic_limit, func.bounds.ub / dynamic_limit)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.15 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced dynamic velocity limits based on feedback from convergence rate to enhance exploration-exploitation balance.", "configspace": "", "generation": 39, "fitness": 0.7791291522050122, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.779 with standard deviation 0.029. And the mean value of best solutions found was 0.156 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "e8254186-e66e-41af-a243-99f3f4b15a49", "metadata": {"aucs": [0.7386973387831662, 0.7924793058717959, 0.8062108119600745], "final_y": [0.17401155105187238, 0.1512410782053517, 0.14140065521270562]}, "mutation_prompt": null}
{"id": "84fd5a17-cba6-4185-b783-02e6c0b3102b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.15 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n            # Dynamic swarm size adjustment\n            self.swarm_size = max(10, int(self.swarm_size * (1 + 0.01 * (self.gbest_score - np.mean(self.pbest_scores)))))\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce dynamic swarm size adjustment based on convergence feedback to enhance exploration and exploitation.", "configspace": "", "generation": 40, "fitness": 0.7920739804316668, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.792 with standard deviation 0.030. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "e8254186-e66e-41af-a243-99f3f4b15a49", "metadata": {"aucs": [0.8139503838091698, 0.7502682681401456, 0.8120032893456848], "final_y": [0.1376906864805022, 0.16164688920604298, 0.12356848559821598]}, "mutation_prompt": null}
{"id": "bafd30cc-bb2e-4810-ac6e-a98db974c518", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/3, func.bounds.ub/3)  # adjusted\n\n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    velocity_adaptation = 1.2 + 0.15 * improvement_rate  # modified\n                    self.positions[i] = self.positions[i] + self.velocities[i] * velocity_adaptation  # modified\n                \n            # Bound check\n            dynamic_limit = 0.1 + 0.9 * (self.func_evals / self.budget)  # modified\n            self.positions = np.clip(self.positions, func.bounds.lb * dynamic_limit, func.bounds.ub * dynamic_limit)  # modified\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced adaptive boundary adjustments and strategic velocity scaling for enhanced convergence.", "configspace": "", "generation": 41, "fitness": 0.788613809354446, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.789 with standard deviation 0.033. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "e8254186-e66e-41af-a243-99f3f4b15a49", "metadata": {"aucs": [0.7453214110586378, 0.7958580558818048, 0.8246619611228956], "final_y": [0.13503818328077877, 0.12269203189366651, 0.12538707592796572]}, "mutation_prompt": null}
{"id": "8ea82ae1-7a15-406e-9b2a-caea619e7156", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.15 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced dynamic quantum probability adjustment based on variance of personal best scores to enhance swarm adaptability.", "configspace": "", "generation": 42, "fitness": 0.8116617144468905, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.010. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "e8254186-e66e-41af-a243-99f3f4b15a49", "metadata": {"aucs": [0.8262150595707853, 0.8038489211540042, 0.8049211626158819], "final_y": [0.13375316451369812, 0.13777379923152622, 0.12165143490835639]}, "mutation_prompt": null}
{"id": "1f0f2b43-6e75-435f-8414-d206113f92b8", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - 0.3 * (self.func_evals / self.budget)  # dynamic scaling\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.15 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced dynamic inertia weight scaling to improve convergence stability and performance.", "configspace": "", "generation": 43, "fitness": 0.8106558548263326, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.011. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "e8254186-e66e-41af-a243-99f3f4b15a49", "metadata": {"aucs": [0.8174021545727861, 0.819009656611094, 0.795555753295118], "final_y": [0.13712209926599916, 0.12754023674899972, 0.13676425763267075]}, "mutation_prompt": null}
{"id": "8625c661-53af-4df9-becb-97526b15b8f8", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.15 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n            \n            # Adaptive velocity randomization\n            if np.random.rand() < 0.1:\n                self.velocities += np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce adaptive velocity randomization for improved exploration and convergence.", "configspace": "", "generation": 44, "fitness": 0.8033327171904485, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.008. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "e8254186-e66e-41af-a243-99f3f4b15a49", "metadata": {"aucs": [0.7925099545641402, 0.8053915396695337, 0.8120966573376717], "final_y": [0.1352732765671526, 0.14264214727967706, 0.11999613881836246]}, "mutation_prompt": null}
{"id": "f5f75675-21b3-49fc-8120-00ae9788608b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            swarm_diversity = np.std(self.positions, axis=0).mean()  # calculate swarm diversity\n            adaptive_inertia = inertia_weight * np.tanh(swarm_diversity)  # one-line change\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = adaptive_inertia * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.15 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Further adapt the velocity update by incorporating dynamic inertia influenced by swarm diversity.", "configspace": "", "generation": 45, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e8254186-e66e-41af-a243-99f3f4b15a49", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "6e6815b4-f093-49b8-809d-eb9420a003eb", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhanced velocity update by incorporating dynamic exploration-exploitation balancing.", "configspace": "", "generation": 46, "fitness": 0.8120108916221972, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.011. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "e8254186-e66e-41af-a243-99f3f4b15a49", "metadata": {"aucs": [0.8270890670870994, 0.8041837415481246, 0.8047598662313675], "final_y": [0.13346372350463642, 0.13741539707969985, 0.121777568061325]}, "mutation_prompt": null}
{"id": "bd6024f1-6632-4e7b-980f-58988f055658", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores)) * (1 - self.func_evals/self.budget)  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Improved adaptive quantum behavior by adjusting quantum probability based on iteration count.", "configspace": "", "generation": 47, "fitness": 0.8020405626808907, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.021. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "6e6815b4-f093-49b8-809d-eb9420a003eb", "metadata": {"aucs": [0.8270890670870994, 0.7746208819902654, 0.8044117389653077], "final_y": [0.13346372350463642, 0.1319799438955781, 0.12165897422354555]}, "mutation_prompt": null}
{"id": "3ccebb39-f69f-4315-bf9e-d09d3c82e44a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores)) * np.exp(-self.func_evals/self.budget)  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introducing a dynamic adjustment to the quantum probability based on the convergence rate for enhanced adaptability.", "configspace": "", "generation": 48, "fitness": 0.8020040361129369, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.022. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "6e6815b4-f093-49b8-809d-eb9420a003eb", "metadata": {"aucs": [0.8270890670870994, 0.7741666745233234, 0.8047563667283879], "final_y": [0.13346372350463642, 0.13227647932647457, 0.12173603300691849]}, "mutation_prompt": null}
{"id": "35a42d93-9494-4212-bc05-4daa16a35d82", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component * (1 + 0.2 * np.std(self.pbest_scores))  # updated line\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Improve performance by adjusting social component influence in velocity update dynamically based on the swarm's average performance variance.", "configspace": "", "generation": 49, "fitness": 0.8120067770878938, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.011. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "6e6815b4-f093-49b8-809d-eb9420a003eb", "metadata": {"aucs": [0.8270766794698952, 0.804184847805874, 0.8047588039879123], "final_y": [0.1334690840362186, 0.1374147559974479, 0.121776140062402]}, "mutation_prompt": null}
{"id": "b969fd17-7cf6-4eb8-aa44-ab20a366f592", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, -func.bounds.ub/5, func.bounds.ub/5)  # modified\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Improved adaptive quantum behavior and velocity clipping for enhanced convergence precision.", "configspace": "", "generation": 50, "fitness": 0.660056797745749, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6e6815b4-f093-49b8-809d-eb9420a003eb", "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "de147a9e-5ab3-40a1-a37e-537d026afb07", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.45 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce adaptive cognitive and social scaling to balance exploration and exploitation.", "configspace": "", "generation": 51, "fitness": 0.8108921740227221, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.013. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "6e6815b4-f093-49b8-809d-eb9420a003eb", "metadata": {"aucs": [0.8284460565866858, 0.8043172554038206, 0.7999132100776597], "final_y": [0.1329596185073414, 0.13714925868894878, 0.1282728906867967]}, "mutation_prompt": null}
{"id": "a6d4b418-53e7-4ec9-a630-5075d7d16ba2", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.5 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.5 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                dynamic_scale = 0.7 + 0.3 * improvement_rate  # New line\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * dynamic_scale\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced a dynamic scaling factor to enhance quantum behavior adaptiveness.", "configspace": "", "generation": 52, "fitness": 0.7895258506730833, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.790 with standard deviation 0.011. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "6e6815b4-f093-49b8-809d-eb9420a003eb", "metadata": {"aucs": [0.7836213468400341, 0.8044921990627995, 0.7804640061164165], "final_y": [0.13773672811522264, 0.1333721254258522, 0.13063253301916378]}, "mutation_prompt": null}
{"id": "727073c4-d22d-44a1-b84c-f77dfc98d912", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.6 * improvement_rate  # Adaptive social scaling\n            cognitive_scale = 0.5 + 0.6 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce dynamic cognitive and social scaling to further enhance adaptability in AQPSO.", "configspace": "", "generation": 53, "fitness": 0.8120109269661975, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.011. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "6e6815b4-f093-49b8-809d-eb9420a003eb", "metadata": {"aucs": [0.8271044674800825, 0.804181506080741, 0.8047468073377688], "final_y": [0.13345707248257566, 0.13741669255407962, 0.12179964694371181]}, "mutation_prompt": null}
{"id": "da69357b-df97-4006-bfd8-0c5b32b3fa3a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate  # Improved adaptive social scaling\n            cognitive_scale = 0.5 + 0.6 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Implement improved dynamic social scaling for enhanced convergence in AQPSO.", "configspace": "", "generation": 54, "fitness": 0.8120229820709527, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.011. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "727073c4-d22d-44a1-b84c-f77dfc98d912", "metadata": {"aucs": [0.8271347857388188, 0.8041804945995445, 0.8047536658744945], "final_y": [0.13344402593646953, 0.1374172787209592, 0.12180981631245247]}, "mutation_prompt": null}
{"id": "e887ff59-600e-4a48-8cb2-d9fafb8a897d", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # adaptive inertia with decay\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate  # Improved adaptive social scaling\n            cognitive_scale = 0.5 + 0.6 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.normal(0, 1, self.dim) * (self.gbest_position - self.positions[i])  # modified\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)  # modified\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance AQPSO by refining the calculation of quantum behavior to improve solution exploration.", "configspace": "", "generation": 55, "fitness": 0.7826814613311593, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.045. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "da69357b-df97-4006-bfd8-0c5b32b3fa3a", "metadata": {"aucs": [0.7196243778719866, 0.8084856310626394, 0.8199343750588521], "final_y": [0.16237514120891616, 0.12617743665817716, 0.13377205854142726]}, "mutation_prompt": null}
{"id": "5004f3d9-67ef-4518-bc93-536dd0d48028", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate\n            cognitive_scale = 0.5 + 0.6 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                neighborhood_index = np.random.randint(0, self.swarm_size)  # select random neighbor\n                social_component = r2 * social_scale * (self.pbest_positions[neighborhood_index] - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores)) \n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance AQPSO by incorporating neighborhood-based adaptive velocity control for improved exploration and exploitation balance.", "configspace": "", "generation": 56, "fitness": 0.7957132223685361, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.796 with standard deviation 0.028. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "da69357b-df97-4006-bfd8-0c5b32b3fa3a", "metadata": {"aucs": [0.7563842690390394, 0.8175732326050824, 0.8131821654614864], "final_y": [0.14966457098232588, 0.12992640863205807, 0.13360447671652365]}, "mutation_prompt": null}
{"id": "60699a53-903a-4725-ba25-75e67885e028", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate  # Improved adaptive social scaling\n            cognitive_scale = 0.5 + 0.6 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.3 * improvement_rate)  # modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Refine AQPSO convergence by modifying inertia weight decay and quantum behavior adaptability.", "configspace": "", "generation": 57, "fitness": 0.8174652857413656, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.014. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "da69357b-df97-4006-bfd8-0c5b32b3fa3a", "metadata": {"aucs": [0.8365700102086202, 0.8130329660927116, 0.8027928809227649], "final_y": [0.12942544996056027, 0.13285154378307118, 0.12585671435546475]}, "mutation_prompt": null}
{"id": "ea857eff-be99-44ab-926d-68d012f5a61d", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate  # Improved adaptive social scaling\n            cognitive_scale = 0.5 + 0.6 * improvement_rate  # Adaptive cognitive scaling\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                swarm_diversity = np.mean(np.std(self.positions, axis=0))  # calculate relative swarm diversity\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, swarm_diversity)  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.3 * improvement_rate)  # modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce a dynamic adjustment to the quantum probability based on relative swarm diversity.", "configspace": "", "generation": 58, "fitness": 0.8020722740800833, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.026. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "60699a53-903a-4725-ba25-75e67885e028", "metadata": {"aucs": [0.8367400922774348, 0.7753466943724122, 0.7941300355904024], "final_y": [0.12934845597029776, 0.13619381865264346, 0.1214618242114962]}, "mutation_prompt": null}
{"id": "c8f17923-3002-4c32-a233-db4e64d1ce3e", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate  # Improved adaptive social scaling\n            cognitive_scale = 0.5 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.3 * improvement_rate)  # modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance the balance between exploration and exploitation by fine-tuning the cognitive scaling factor.", "configspace": "", "generation": 59, "fitness": 0.8174712022874046, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.014. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "60699a53-903a-4725-ba25-75e67885e028", "metadata": {"aucs": [0.836570596571341, 0.8130333609973698, 0.8028096492935026], "final_y": [0.1294251755386573, 0.13285134114437913, 0.1258499671516008]}, "mutation_prompt": null}
{"id": "852e0bf8-7577-4891-8014-c48be3a51bee", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate  # Improved adaptive social scaling\n            cognitive_scale = 0.5 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.3 * improvement_rate)  # modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Adaptive mutation based on velocity magnitude\n            if np.linalg.norm(self.velocities[i]) > 0.5:  \n                mutation = np.random.normal(0, 0.1, self.dim)  # small Gaussian perturbation\n                self.positions[i] += mutation\n\n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce an adaptive mutation mechanism based on velocity magnitude to enhance exploration.", "configspace": "", "generation": 60, "fitness": 0.816660576061217, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.013. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "c8f17923-3002-4c32-a233-db4e64d1ce3e", "metadata": {"aucs": [0.8348901349617823, 0.8119818798110373, 0.8031097134108315], "final_y": [0.12909114536617594, 0.14107265184698492, 0.1321416441066654]}, "mutation_prompt": null}
{"id": "f4139670-8cb6-4b19-bb11-c31de1ffc8c8", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate  # Improved adaptive social scaling\n            cognitive_scale = 0.7 + 0.35 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.3 * improvement_rate)  # modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introducing a dynamic adjustment to the cognitive scaling for enhanced convergence precision.", "configspace": "", "generation": 61, "fitness": 0.8171705915866752, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.014. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "c8f17923-3002-4c32-a233-db4e64d1ce3e", "metadata": {"aucs": [0.8365646198308923, 0.8130155836755677, 0.8019315712535655], "final_y": [0.12942766749259293, 0.13286047758339647, 0.12623872188608765]}, "mutation_prompt": null}
{"id": "ba30a973-7a70-4961-a7a1-b57b1e907697", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate  # Improved adaptive social scaling\n            cognitive_scale = 0.5 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.3 * improvement_rate)  # modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Integrate adaptive particle boundary contraction to tighten search space during convergence.", "configspace": "", "generation": 62, "fitness": 0.8175047669538292, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.014. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "c8f17923-3002-4c32-a233-db4e64d1ce3e", "metadata": {"aucs": [0.836605555139576, 0.8130980633150288, 0.802810682406883], "final_y": [0.12942155463245808, 0.1328308576048386, 0.12583751677615684]}, "mutation_prompt": null}
{"id": "b781329f-1b86-4857-9ea1-1b675fcdbe42", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate  # Improved adaptive social scaling\n            cognitive_scale = 0.5 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.55 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified the quantum_prob\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.3 * improvement_rate)  # modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance adaptive quantum behavior by fine-tuning probability and scale for improved exploration-exploitation balance.", "configspace": "", "generation": 63, "fitness": 0.7579279660867021, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.758 with standard deviation 0.023. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "ba30a973-7a70-4961-a7a1-b57b1e907697", "metadata": {"aucs": [0.7731231823755503, 0.7750641362480484, 0.7255965796365074], "final_y": [0.13590146160259553, 0.13420297613988852, 0.15250873135500986]}, "mutation_prompt": null}
{"id": "9cdbcfca-f285-4952-9509-d14e4916d68e", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 1.0 * improvement_rate  # Increased adaptive social scaling\n            cognitive_scale = 0.5 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.3 * improvement_rate)  # modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Integrate adaptive exploration by adjusting social scaling dynamically based on global improvement trends.", "configspace": "", "generation": 64, "fitness": 0.8174648659341752, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.014. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "ba30a973-7a70-4961-a7a1-b57b1e907697", "metadata": {"aucs": [0.8365931722835991, 0.8130739644897353, 0.8027274610291915], "final_y": [0.12942766144288886, 0.13284326422473203, 0.12587687810143877]}, "mutation_prompt": null}
{"id": "25c62215-026a-4196-acf4-0cbd6e2212af", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.7 - (self.func_evals / self.budget) * 0.5  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate  # Improved adaptive social scaling\n            cognitive_scale = 0.5 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.3 * improvement_rate)  # modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance velocity update by refining the inertia weight decay for improved exploration-exploitation balance.", "configspace": "", "generation": 65, "fitness": 0.794513016302921, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.795 with standard deviation 0.028. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "ba30a973-7a70-4961-a7a1-b57b1e907697", "metadata": {"aucs": [0.8117042677368169, 0.7545997830407228, 0.8172349981312235], "final_y": [0.1363903306822415, 0.13664132186361022, 0.12909404837667826]}, "mutation_prompt": null}
{"id": "056ac324-72fa-4d9e-91db-e578dd6fe71d", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate  # Improved adaptive social scaling\n            cognitive_scale = 0.5 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.3 * improvement_rate)  # modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Adjust swarm size dynamically\n            self.swarm_size = int(self.swarm_size * (1 - 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))))\n            self.swarm_size = max(10, min(self.swarm_size, 30))  # Ensures swarm size remains practical\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance convergence by dynamically adjusting swarm size based on performance metrics.", "configspace": "", "generation": 66, "fitness": 0.7965058273816399, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.006. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "ba30a973-7a70-4961-a7a1-b57b1e907697", "metadata": {"aucs": [0.8035492441275616, 0.7887937946790676, 0.7971744433382905], "final_y": [0.13876951580234653, 0.1454639471703575, 0.14085974446744676]}, "mutation_prompt": null}
{"id": "e794718a-7e46-4c8c-8505-7cefd60558fa", "solution": "import numpy as np\n\nclass ImprovedAQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.9 * improvement_rate  # Adjusted scaling factor\n            cognitive_scale = 0.6 + 0.6 * improvement_rate  # Adjusted scaling factor\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.3 * improvement_rate)\n                else:\n                    mutation = np.random.uniform(-0.1, 0.1, self.dim)  # New random walk mutation\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate) + mutation\n\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "ImprovedAQPSO", "description": "Enhance exploration-exploitation balance by introducing random walk mutation and adjusting velocity update.", "configspace": "", "generation": 67, "fitness": 0.7926823616589528, "feedback": "The algorithm ImprovedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.793 with standard deviation 0.023. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "ba30a973-7a70-4961-a7a1-b57b1e907697", "metadata": {"aucs": [0.8087031082397279, 0.7595098373699223, 0.8098341393672084], "final_y": [0.1420346959470491, 0.13212937636903033, 0.12169041713576889]}, "mutation_prompt": null}
{"id": "c8e33673-531a-4657-84fe-aa01e2e10bdc", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate  # Improved adaptive social scaling\n            cognitive_scale = 0.5 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.3 * improvement_rate)  # modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            anomaly_correction = np.sign(self.positions - self.gbest_position) * np.minimum(np.abs(self.positions - self.gbest_position), 1)  # new line\n            self.positions = np.clip(self.positions + anomaly_correction, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance adaptive control in AQPSO by introducing position anomaly correction.", "configspace": "", "generation": 68, "fitness": 0.7844020048288157, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.031. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "ba30a973-7a70-4961-a7a1-b57b1e907697", "metadata": {"aucs": [0.8054942860687117, 0.8074378333755631, 0.7402738950421724], "final_y": [0.1260436573398197, 0.12488273705977637, 0.13338878127826648]}, "mutation_prompt": null}
{"id": "cdfad0c2-46ec-4ea4-a892-826089a82df1", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate  # Improved adaptive social scaling\n            cognitive_scale = 0.5 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-0.5, 0.5, self.dim) * (self.gbest_position - self.positions[i])  # modified\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.3 * improvement_rate)  # modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance the quantum behavior by adjusting the randomness for better exploration-exploitation balance.", "configspace": "", "generation": 69, "fitness": 0.7153597122674435, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.715 with standard deviation 0.032. And the mean value of best solutions found was 0.162 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "ba30a973-7a70-4961-a7a1-b57b1e907697", "metadata": {"aucs": [0.6965118024546355, 0.7605786903745255, 0.6889886439731698], "final_y": [0.19741104088878347, 0.1355071273099122, 0.15334683876178146]}, "mutation_prompt": null}
{"id": "33775452-bcd1-4eb0-9b27-503e9f2a025b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.9 - (self.func_evals / self.budget) * 0.4  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.8 * improvement_rate  # Improved adaptive social scaling\n            cognitive_scale = 0.5 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.3 * improvement_rate)  # modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce dynamic inertia weight scaling for improved exploration-exploitation balance.", "configspace": "", "generation": 70, "fitness": 0.8135678261577213, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.814 with standard deviation 0.034. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "ba30a973-7a70-4961-a7a1-b57b1e907697", "metadata": {"aucs": [0.8547102676819746, 0.7715802394174858, 0.8144129713737037], "final_y": [0.12445496552244462, 0.15119875073206634, 0.1389461900669956]}, "mutation_prompt": null}
{"id": "e9913d4d-797c-4bef-a6f4-8b600a677701", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate  # Enhanced adaptive social scaling\n            cognitive_scale = 0.55 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)  # Enhanced\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance quantum behavior and adaptive scales to improve exploration-exploitation balance in AQPSO.", "configspace": "", "generation": 71, "fitness": 0.8215502291193927, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.014. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "ba30a973-7a70-4961-a7a1-b57b1e907697", "metadata": {"aucs": [0.8370013507676188, 0.8252916403092997, 0.8023576962812597], "final_y": [0.12923679728708792, 0.12375323631449986, 0.12610440842172677]}, "mutation_prompt": null}
{"id": "4730fc90-84bc-4a67-ae24-484530d2e0f6", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate  # Enhanced adaptive social scaling\n            cognitive_scale = 0.55 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, abs(self.gbest_score - np.min(self.pbest_scores)))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)  # Enhanced\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Refine the adaptive quantum behavior by dynamically adjusting the quantum probability based on the rate of improvement.", "configspace": "", "generation": 72, "fitness": 0.8215502291193927, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.014. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "e9913d4d-797c-4bef-a6f4-8b600a677701", "metadata": {"aucs": [0.8370013507676188, 0.8252916403092997, 0.8023576962812597], "final_y": [0.12923679728708792, 0.12375323631449986, 0.12610440842172677]}, "mutation_prompt": null}
{"id": "1ae8bcc2-4837-4a82-a4e0-77eefb115229", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate  # Enhanced adaptive social scaling\n            cognitive_scale = 0.55 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)  # Enhanced\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * improvement_rate  # modified line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduced a dynamic contraction factor that adjusts based on the improvement rate to enhance exploration and exploitation balance.", "configspace": "", "generation": 73, "fitness": 0.8184138776797315, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.025. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "e9913d4d-797c-4bef-a6f4-8b600a677701", "metadata": {"aucs": [0.8433648970966525, 0.8275120685199976, 0.7843646674225442], "final_y": [0.12766778086730435, 0.12204578641660624, 0.1380163600490909]}, "mutation_prompt": null}
{"id": "d5715681-4ac6-4bc1-a687-a6fb687e0dab", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate  # Enhanced adaptive social scaling\n            cognitive_scale = 0.55 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)  # Enhanced\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = np.ptp(self.positions, axis=0).mean() / (func.bounds.ub - func.bounds.lb).mean()  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce adaptive velocity scaling based on population diversity in AQPSO to enhance exploration.", "configspace": "", "generation": 74, "fitness": 0.707614602637911, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.708 with standard deviation 0.041. And the mean value of best solutions found was 0.192 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "e9913d4d-797c-4bef-a6f4-8b600a677701", "metadata": {"aucs": [0.7659817210925866, 0.6817600440858087, 0.6751020427353374], "final_y": [0.15905569190521374, 0.2068354299576527, 0.21025588373878545]}, "mutation_prompt": null}
{"id": "7a4f1b28-f170-493e-a245-af1f11631e0b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate  # Enhanced adaptive social scaling\n            cognitive_scale = 0.55 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)  # Enhanced\n                else:\n                    mutation_factor = np.std(self.pbest_scores) / (1 + abs(self.gbest_score))  # New adaptive mutation\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate) * (1 + mutation_factor)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce adaptive mutation based on fitness variance to enhance exploration in AQPSO.", "configspace": "", "generation": 75, "fitness": 0.8172361064882111, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.014. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "e9913d4d-797c-4bef-a6f4-8b600a677701", "metadata": {"aucs": [0.8365476404412706, 0.8127952441990487, 0.8023654348243139], "final_y": [0.12937202603700815, 0.1329500132931033, 0.12607199633326538]}, "mutation_prompt": null}
{"id": "a215ef4b-80c6-4a73-a323-99f4a88763b8", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.9  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate  # Enhanced adaptive social scaling\n            cognitive_scale = 0.55 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)  # Enhanced\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce a time-varying factor to the inertia weight for better convergence dynamics in AQPSO.", "configspace": "", "generation": 76, "fitness": 0.7685586031534187, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.769 with standard deviation 0.054. And the mean value of best solutions found was 0.156 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "e9913d4d-797c-4bef-a6f4-8b600a677701", "metadata": {"aucs": [0.840357267364988, 0.7105444173165175, 0.7547741247787506], "final_y": [0.12752461450731722, 0.18615158304314094, 0.15473286934041808]}, "mutation_prompt": null}
{"id": "816b7cc7-e0c2-4401-ad49-860804b7ca5d", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate  # Enhanced adaptive social scaling\n            cognitive_scale = 0.55 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.35 * improvement_rate)  # Minor enhancement\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce a subtle enhancement to the quantum behavior calculation for improved convergence in AQPSO.", "configspace": "", "generation": 77, "fitness": 0.8173687339133796, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.014. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "e9913d4d-797c-4bef-a6f4-8b600a677701", "metadata": {"aucs": [0.8368057431217838, 0.8127940038121534, 0.8025064548062018], "final_y": [0.1293282868227904, 0.13296232778577077, 0.1260029678952561]}, "mutation_prompt": null}
{"id": "52430f1b-4647-49ca-a0ce-470243e9b807", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + (self.func_evals / self.budget) / 2  # modified\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate  # Enhanced adaptive social scaling\n            cognitive_scale = 0.55 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)  # Enhanced\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce a dynamic inertia weight that increases with iterations to better balance exploration and exploitation.", "configspace": "", "generation": 78, "fitness": 0.7670281920998013, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.767 with standard deviation 0.012. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "e9913d4d-797c-4bef-a6f4-8b600a677701", "metadata": {"aucs": [0.7786814849009045, 0.7719537253997425, 0.7504493659987574], "final_y": [0.1541004777981344, 0.14164882101725462, 0.14815616654263164]}, "mutation_prompt": null}
{"id": "32e5f237-f92f-485b-82cf-abe40e97254f", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate  # Enhanced adaptive social scaling\n            cognitive_scale = 0.55 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)  # Enhanced\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n            # Adaptive mutation with Lvy flights\n            if np.random.rand() < 0.1:  # Newly added line\n                levy_step = np.random.normal(0, 1, self.dim) * np.power(np.random.rand(self.dim), -1.0)  # Newly added line\n                self.positions += levy_step * 0.5 * (self.positions - self.gbest_position)  # Newly added line\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce adaptive mutation with Lvy flights to AQPSO for enhanced exploration capabilities.", "configspace": "", "generation": 79, "fitness": 0.7683298572810461, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.768 with standard deviation 0.017. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "e9913d4d-797c-4bef-a6f4-8b600a677701", "metadata": {"aucs": [0.7452096995900831, 0.7841413413101731, 0.7756385309428817], "final_y": [0.15684298861622448, 0.15173380321184038, 0.15185172211032794]}, "mutation_prompt": null}
{"id": "e3ab5b64-1d80-45e1-8e78-d89b2df488c1", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                swarm_dynamic_factor = 1 + 0.1 * np.sin(self.func_evals/self.budget * np.pi)  # new line\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n            \n            # Dynamic swarm size adjustment\n            if self.func_evals / self.budget > 0.5:\n                self.swarm_size = int(swarm_dynamic_factor * self.swarm_size)  # new line\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "AQPSO with dynamic swarm size adjustment and enhanced quantum adaptation for better exploration-exploitation balance.", "configspace": "", "generation": 80, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 30 is out of bounds for axis 0 with size 30').", "error": "IndexError('index 30 is out of bounds for axis 0 with size 30')", "parent_id": "e9913d4d-797c-4bef-a6f4-8b600a677701", "metadata": {}, "mutation_prompt": null}
{"id": "f6a78f74-8b4a-4aa6-9e3f-6ac1f10d584a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.4 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.5  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 0.95 + 0.9 * improvement_rate  # Enhanced adaptive social scaling\n            cognitive_scale = 0.5 + 0.75 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)  # Enhanced\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * (self.gbest_score / (self.gbest_score + np.std(self.pbest_scores)))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce dynamic stochastic inertia and improved adaptive cognitive-social scaling for enhanced exploration-exploitation in AQPSO.", "configspace": "", "generation": 81, "fitness": 0.8042636550840873, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.028. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "e9913d4d-797c-4bef-a6f4-8b600a677701", "metadata": {"aucs": [0.8435313046778175, 0.7883349494450781, 0.7809247111293662], "final_y": [0.1263585837794503, 0.1348136409473295, 0.12751072663439722]}, "mutation_prompt": null}
{"id": "b352e679-9615-4030-838a-f014f176a6d4", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate  # Enhanced adaptive social scaling\n            cognitive_scale = 0.55 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)  # Enhanced\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce a dynamic contraction factor for adaptive boundary handling in AQPSO to improve convergence.", "configspace": "", "generation": 82, "fitness": 0.8217273006822919, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.014. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "e9913d4d-797c-4bef-a6f4-8b600a677701", "metadata": {"aucs": [0.8369689969500922, 0.8258617745851716, 0.8023511305116121], "final_y": [0.1292390998635582, 0.12285847369478575, 0.1261211378404562]}, "mutation_prompt": null}
{"id": "f1bffcd4-3422-4091-91a2-2d737c1bf18c", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate  # Enhanced adaptive social scaling\n            cognitive_scale = 0.55 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)  # Enhanced\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce velocity direction adjustment based on iteration to enhance exploratory capabilities in AQPSO.", "configspace": "", "generation": 83, "fitness": 0.8217273006822919, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.014. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "b352e679-9615-4030-838a-f014f176a6d4", "metadata": {"aucs": [0.8369689969500922, 0.8258617745851716, 0.8023511305116121], "final_y": [0.1292390998635582, 0.12285847369478575, 0.1261211378404562]}, "mutation_prompt": null}
{"id": "dc5ea131-b184-4900-ab42-c9e24c637fc5", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate  # Enhanced adaptive social scaling\n            cognitive_scale = 0.55 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = 0.9 * self.velocities[i] + inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)  # Momentum factor added\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)  # Enhanced\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce momentum-based velocity update to enhance exploration and exploitation balance in AQPSO.", "configspace": "", "generation": 84, "fitness": 0.7680851941104728, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.768 with standard deviation 0.031. And the mean value of best solutions found was 0.164 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "b352e679-9615-4030-838a-f014f176a6d4", "metadata": {"aucs": [0.7246852390545819, 0.7975157061921132, 0.7820546370847233], "final_y": [0.18386890616531837, 0.1496519630649843, 0.1573810337434428]}, "mutation_prompt": null}
{"id": "37c78650-99e0-4827-8c72-cd3f2592a95b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            inertia_weight = 0.9 - (self.func_evals / self.budget) * 0.8  # modified decay rate\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate  # Enhanced adaptive social scaling\n            cognitive_scale = 0.55 + 0.65 * improvement_rate  # Adjusted cognitive scaling for better balance\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)  # Enhanced\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))  # new line\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce a dynamic inertia weight decay to balance exploration and exploitation in AQPSO.", "configspace": "", "generation": 85, "fitness": 0.802786817947387, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.021. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "b352e679-9615-4030-838a-f014f176a6d4", "metadata": {"aucs": [0.8265099598760761, 0.7747816454577319, 0.8070688485083534], "final_y": [0.12875979817787886, 0.14976489952886474, 0.13899680680490412]}, "mutation_prompt": null}
{"id": "82915b12-a2cc-4f14-a2de-bf77d435819d", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            dynamic_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / (np.std(self.pbest_scores) + 1e-10))\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6 * dynamic_factor\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce a dynamic inertia weight scaling based on convergence velocity to enhance exploration and exploitation balance in AQPSO.", "configspace": "", "generation": 86, "fitness": 0.821727303453538, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.014. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "b352e679-9615-4030-838a-f014f176a6d4", "metadata": {"aucs": [0.8369689980523112, 0.825861783948305, 0.8023511283599974], "final_y": [0.1292390993665825, 0.12285846264692979, 0.12612113932186397]}, "mutation_prompt": null}
{"id": "aeedb922-eeb1-4e38-bade-01284b150bb4", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            dynamic_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / (np.std(self.pbest_scores) + 1e-10))\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6 * dynamic_factor\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 1 / (1 + np.exp(-(0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores)))))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance quantum behavior by introducing a sigmoid function for better convergence feedback.", "configspace": "", "generation": 87, "fitness": 0.7080432991390379, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.708 with standard deviation 0.007. And the mean value of best solutions found was 0.192 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "82915b12-a2cc-4f14-a2de-bf77d435819d", "metadata": {"aucs": [0.7130439033028658, 0.6983929646002258, 0.7126930295140216], "final_y": [0.18953057582263422, 0.1972042007054482, 0.1898642913083034]}, "mutation_prompt": null}
{"id": "6f20d032-2ca5-43ad-86ad-54c3894e1a86", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            dynamic_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / (np.std(self.pbest_scores) + 1e-10))\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6 * dynamic_factor\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1.2 + 0.85 * improvement_rate  # modified\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce a dynamic social scale adjustment based on global best score variance to enhance adaptability in AQPSO.", "configspace": "", "generation": 88, "fitness": 0.8176138072442818, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.015. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "82915b12-a2cc-4f14-a2de-bf77d435819d", "metadata": {"aucs": [0.8374155454907561, 0.8126543067247464, 0.8027715695173432], "final_y": [0.1290155532286622, 0.13299554011275938, 0.12595014817831207]}, "mutation_prompt": null}
{"id": "9ea85866-aa2c-4ec5-92a0-64882a3f4913", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            dynamic_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / (np.std(self.pbest_scores) + 1e-10))\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6 * dynamic_factor\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10)) * (1 - self.func_evals / self.budget)\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce a time-varying contraction factor for more adaptive position bounding in AQPSO.", "configspace": "", "generation": 89, "fitness": 0.8080039830955634, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.022. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "82915b12-a2cc-4f14-a2de-bf77d435819d", "metadata": {"aucs": [0.8377404156211666, 0.7838556940046828, 0.8024158396608407], "final_y": [0.12914420763500412, 0.13468241820264126, 0.12609771737353626]}, "mutation_prompt": null}
{"id": "5d09cefc-0657-4e9b-be47-a90bae5bbf51", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            dynamic_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / (np.std(self.pbest_scores) + 1e-10))\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6 * dynamic_factor\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1.1 + 0.85 * improvement_rate  # modified\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance convergence by adapting the social scale with dynamic inertia for improved balance in AQPSO.", "configspace": "", "generation": 90, "fitness": 0.8174692182314393, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.015. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "82915b12-a2cc-4f14-a2de-bf77d435819d", "metadata": {"aucs": [0.837199677427827, 0.8125418991957727, 0.8026660780707183], "final_y": [0.1291258194246061, 0.13305346047950262, 0.12597599504954005]}, "mutation_prompt": null}
{"id": "a50cc813-b37d-4545-b0b3-8f74a3abafa7", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            dynamic_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / (np.std(self.pbest_scores) + 1e-10))\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6 * dynamic_factor\n            inertia_weight *= 1 / (1 + np.exp(-0.1 * (self.func_evals - self.budget / 2)))  # modified\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce a decay factor in inertia weight based on a sigmoid function to enhance the balance between exploration and exploitation in AQPSO.", "configspace": "", "generation": 91, "fitness": 0.687501075106734, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.688 with standard deviation 0.020. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.037.", "error": "", "parent_id": "82915b12-a2cc-4f14-a2de-bf77d435819d", "metadata": {"aucs": [0.7161652569138255, 0.6746666886742763, 0.6716712797321001], "final_y": [0.1248063589147822, 0.1979680809366383, 0.20664217014172814]}, "mutation_prompt": null}
{"id": "9054c3d5-d4f2-473c-b9dd-3229fcaa6939", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            dynamic_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / (np.std(self.pbest_scores) + 1e-10))\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6 * dynamic_factor\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Incorporate an adaptive inertia weight influenced by the global best's variance to dynamically adjust exploration and exploitation.", "configspace": "", "generation": 92, "fitness": 0.821727303453538, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.014. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "82915b12-a2cc-4f14-a2de-bf77d435819d", "metadata": {"aucs": [0.8369689980523112, 0.825861783948305, 0.8023511283599974], "final_y": [0.1292390993665825, 0.12285846264692979, 0.12612113932186397]}, "mutation_prompt": null}
{"id": "2ecc2ce1-3f0b-473d-8bd2-db3431ad4a6e", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            dynamic_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / (np.std(self.pbest_scores) + 1e-10))\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.55 * dynamic_factor\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Introduce a slight adjustment to the inertia weight calculation to improve balance in exploration and exploitation phases.", "configspace": "", "generation": 93, "fitness": 0.8130556638239744, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.019. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "82915b12-a2cc-4f14-a2de-bf77d435819d", "metadata": {"aucs": [0.8358383958733366, 0.8136268467607699, 0.7897017488378169], "final_y": [0.12959506176614954, 0.13256025663438997, 0.13878826813736145]}, "mutation_prompt": null}
{"id": "c87b63e1-6a5e-448c-95fe-bf962499b90e", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            dynamic_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / (np.std(self.pbest_scores) + 1e-10))\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6 * dynamic_factor\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                swarm_diversity = np.std(self.positions)  # added\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, swarm_diversity)  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Adjust the exploration-exploitation balance by modifying the quantum behavior parameter to consider swarm diversity, enhancing convergence.", "configspace": "", "generation": 94, "fitness": 0.8016786449912864, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.026. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "82915b12-a2cc-4f14-a2de-bf77d435819d", "metadata": {"aucs": [0.8369918561296885, 0.7749302423238995, 0.7931138365202709], "final_y": [0.12922886254644128, 0.1364462138997964, 0.12463104055438934]}, "mutation_prompt": null}
{"id": "f8714c04-021f-486b-99dc-ade59c845d82", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            dynamic_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / (np.std(self.pbest_scores) + 1e-10))\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6 * dynamic_factor\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + 0.6 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.5 * improvement_rate)  # Modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance AQPSO's exploratory capabilities by adjusting the quantum behavior scaling factor for improved convergence.", "configspace": "", "generation": 95, "fitness": 0.8215229714407156, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.015. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "82915b12-a2cc-4f14-a2de-bf77d435819d", "metadata": {"aucs": [0.8373166474796097, 0.8252961614930548, 0.8019561053494819], "final_y": [0.1290754210698577, 0.12300385821959825, 0.12637428652965554]}, "mutation_prompt": null}
{"id": "0acb7984-f3fc-4c1f-8d98-4062b91a31cb", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            dynamic_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / (np.std(self.pbest_scores) + 1e-10))\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6 * dynamic_factor\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.6 + 0.4 * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance the balance between exploration and exploitation by refining the adaptive quantum probability calculation in AQPSO.", "configspace": "", "generation": 96, "fitness": 0.75553045616515, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.756 with standard deviation 0.044. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.030.", "error": "", "parent_id": "82915b12-a2cc-4f14-a2de-bf77d435819d", "metadata": {"aucs": [0.8047497735590026, 0.7642403995454818, 0.697601195390966], "final_y": [0.1281640822471275, 0.14643547981257854, 0.1981693746373665]}, "mutation_prompt": null}
{"id": "b187c2f8-9726-4272-ada2-30685417c6e1", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            dynamic_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / (np.std(self.pbest_scores) + 1e-10))\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6 * dynamic_factor\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + dynamic_factor * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Refine the quantum probability for enhanced exploration by adjusting it with the dynamic factor.", "configspace": "", "generation": 97, "fitness": 0.8228694241606096, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.823 with standard deviation 0.006. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "82915b12-a2cc-4f14-a2de-bf77d435819d", "metadata": {"aucs": [0.8306765876877928, 0.8224421245470551, 0.8154895602469808], "final_y": [0.1320867388762771, 0.12350632554123675, 0.12192497047217754]}, "mutation_prompt": null}
{"id": "58418ae1-ed6d-40a5-9e8f-33a223bf4aeb", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            dynamic_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / (np.std(self.pbest_scores) + 1e-10))\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6 * dynamic_factor\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + dynamic_factor * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate)\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance exploration by adjusting velocity update with a novel adaptive inertia mechanism.", "configspace": "", "generation": 98, "fitness": 0.8228694241606096, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.823 with standard deviation 0.006. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "b187c2f8-9726-4272-ada2-30685417c6e1", "metadata": {"aucs": [0.8306765876877928, 0.8224421245470551, 0.8154895602469808], "final_y": [0.1320867388762771, 0.12350632554123675, 0.12192497047217754]}, "mutation_prompt": null}
{"id": "f400c5ac-b2fa-4101-83da-a20d19ff7892", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim, swarm_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.positions = np.random.uniform(0, 1, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.pbest_positions = np.copy(self.positions)\n        self.pbest_scores = np.full(self.swarm_size, np.inf)\n        self.gbest_position = np.zeros(self.dim)\n        self.gbest_score = np.inf\n        self.func_evals = 0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            # Evaluate current position\n            for i in range(self.swarm_size):\n                if self.func_evals >= self.budget:\n                    break\n                score = func(self.positions[i])\n                self.func_evals += 1\n                \n                # Update personal best\n                if score < self.pbest_scores[i]:\n                    self.pbest_scores[i] = score\n                    self.pbest_positions[i] = self.positions[i]\n                    \n                # Update global best\n                if score < self.gbest_score:\n                    self.gbest_score = score\n                    self.gbest_position = self.positions[i]\n\n            # Update velocity and position\n            dynamic_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / (np.std(self.pbest_scores) + 1e-10))\n            inertia_weight = 0.5 + np.random.rand() / 2 - (self.func_evals / self.budget) * 0.6 * dynamic_factor\n            improvement_rate = (self.gbest_score - score) / max(1, abs(self.gbest_score))\n            social_scale = 1 + 0.85 * improvement_rate\n            cognitive_scale = 0.55 + 0.65 * improvement_rate\n            for i in range(self.swarm_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = cognitive_scale * r1 * (self.pbest_positions[i] - self.positions[i])\n                social_component = r2 * social_scale * (self.gbest_position - self.positions[i])\n                velocity = self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = inertia_weight * np.clip(velocity, func.bounds.lb/2, func.bounds.ub/2)\n                \n                # Apply adaptive quantum behavior based on convergence feedback\n                quantum_prob = 0.5 + dynamic_factor * (self.gbest_score - score) / max(1, np.std(self.pbest_scores))  # modified\n                quantum_behavior = np.random.uniform(-1, 1, self.dim) * (self.gbest_position - self.positions[i])\n                if np.random.rand() < quantum_prob:\n                    perturbation = np.random.uniform(-0.1, 0.1, self.dim)  # modified\n                    self.positions[i] = self.gbest_position + quantum_behavior * (1 + 0.4 * improvement_rate) + perturbation  # modified\n                else:\n                    self.positions[i] = self.positions[i] + self.velocities[i] * (1 + 0.3 * improvement_rate)\n            \n            # Bound check\n            contraction_factor = 0.9 + 0.1 * np.tanh(self.gbest_score / np.std(self.pbest_scores + 1e-10))\n            self.positions = np.clip(self.positions, func.bounds.lb * contraction_factor, func.bounds.ub * contraction_factor)\n\n        return self.gbest_position, self.gbest_score", "name": "AQPSO", "description": "Enhance AQPSO by refining the quantum behavior with adaptive perturbation for better local exploration.", "configspace": "", "generation": 99, "fitness": 0.7914007202049788, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.791 with standard deviation 0.018. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "b187c2f8-9726-4272-ada2-30685417c6e1", "metadata": {"aucs": [0.8164839743199079, 0.7752033417038062, 0.7825148445912221], "final_y": [0.12877967141217805, 0.14533188582150802, 0.12444163030905375]}, "mutation_prompt": null}
