{"id": "323daf06-eca5-4964-8089-f00c350bf1bf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling to ensure good coverage\n        samples = np.random.uniform(func.bounds.lb, func.bounds.ub, (self.budget // 2, self.dim))\n        evaluations = [func(sample) for sample in samples]\n        self.budget -= len(samples)\n        \n        # Select the best initial sample\n        best_sample_idx = np.argmin(evaluations)\n        best_sample = samples[best_sample_idx]\n        best_value = evaluations[best_sample_idx]\n        \n        # Bounds for the optimizer\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n        \n        # BFGS optimization from the best initial sample\n        def wrapped_func(x):\n            nonlocal best_value\n            if self.budget <= 0:\n                return best_value + 1e6  # Large penalty to stop further evaluation\n            result = func(x)\n            self.budget -= 1\n            if result < best_value:\n                best_value = result\n            return result\n        \n        result = minimize(wrapped_func, best_sample, method='L-BFGS-B', bounds=bounds)\n        \n        return result.x if result.success else best_sample", "name": "HybridOptimizer", "description": "A hybrid algorithm combining uniform sampling for initialization with BFGS local optimization for fast convergence on smooth, low-dimensional landscapes.", "configspace": "", "generation": 0, "fitness": 0.4739293563209759, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.474 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.49770959914796775, 0.46399169066118573, 0.4600867791537743], "final_y": [5.759904076423014e-08, 1.723365491834888e-07, 2.1008146289999495e-07]}, "mutation_prompt": null}
{"id": "a01cef36-c734-4e0a-a3c2-b3510846eb5a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial Gaussian sampling to refine initial guesses\n        samples = np.random.normal((func.bounds.lb + func.bounds.ub) / 2, \n                                   (func.bounds.ub - func.bounds.lb) / 6, \n                                   (self.budget // 2, self.dim))\n        samples = np.clip(samples, func.bounds.lb, func.bounds.ub)\n        evaluations = [func(sample) for sample in samples]\n        self.budget -= len(samples)\n        \n        # Select the best initial sample\n        best_sample_idx = np.argmin(evaluations)\n        best_sample = samples[best_sample_idx]\n        best_value = evaluations[best_sample_idx]\n        \n        # Adaptive bounds based on the best sample and previous samples\n        adaptive_bounds = [(max(func.bounds.lb[i], min(samples[:, i])), \n                            min(func.bounds.ub[i], max(samples[:, i]))) for i in range(self.dim)]\n        \n        # BFGS optimization from the best initial sample\n        def wrapped_func(x):\n            nonlocal best_value\n            if self.budget <= 0:\n                return best_value + 1e6  # Large penalty to stop further evaluation\n            result = func(x)\n            self.budget -= 1\n            if result < best_value:\n                best_value = result\n            return result\n        \n        result = minimize(wrapped_func, best_sample, method='L-BFGS-B', bounds=adaptive_bounds)\n        \n        return result.x if result.success else best_sample", "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid algorithm using Gaussian sampling for initialization with adaptive BFGS optimization that dynamically adjusts bounds based on solution history to improve convergence in smooth, low-dimensional landscapes.", "configspace": "", "generation": 1, "fitness": 0.47045254396035324, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.470 with standard deviation 0.036. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "323daf06-eca5-4964-8089-f00c350bf1bf", "metadata": {"aucs": [0.42622845983743696, 0.5142912939426432, 0.47083787810097966], "final_y": [7.612997361111019e-08, 4.6926344049307445e-09, 2.0027957698280285e-08]}, "mutation_prompt": null}
{"id": "b1347862-00b2-47e9-9698-89afe37a202f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass MultiStartNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        samples_per_restart = max(self.budget // 10, 1)\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n        \n        best_value = float('inf')\n        best_solution = None\n        \n        while self.budget > 0:\n            # Generate random samples\n            samples = np.random.uniform(func.bounds.lb, func.bounds.ub, (samples_per_restart, self.dim))\n            for sample in samples:\n                if self.budget <= 0:\n                    break\n                # Nelder-Mead Optimization\n                result = minimize(func, sample, method='Nelder-Mead', bounds=bounds, options={'maxfev': self.budget, 'adaptive': True})\n                \n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    # Reduce remaining budget\n                    self.budget -= result.nfev\n\n            # Adaptive restart: Adjust sample size based on remaining budget\n            samples_per_restart = max(self.budget // 10, 1)\n        \n        return best_solution if best_solution is not None else np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)", "name": "MultiStartNelderMead", "description": "Multi-Start Nelder-Mead with Adaptive Restarts - a novel metaheuristic using multiple starting points with adaptive restarts to enhance exploration and exploitation in low-dimensional, smooth landscapes.", "configspace": "", "generation": 2, "fitness": 0.6749850005660262, "feedback": "The algorithm MultiStartNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.675 with standard deviation 0.006. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "323daf06-eca5-4964-8089-f00c350bf1bf", "metadata": {"aucs": [0.682250156926566, 0.6685167447281648, 0.6741881000433476], "final_y": [5.753491867799688e-06, 6.326676251107994e-06, 4.179533908493494e-06]}, "mutation_prompt": null}
{"id": "d7c2232d-8a1a-499b-a0b4-94ec7907641e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass MultiStartNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        samples_per_restart = max(self.budget // 10, 1)\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n        \n        best_value = float('inf')\n        best_solution = None\n        \n        while self.budget > 0:\n            # Generate random samples\n            samples = np.random.uniform(func.bounds.lb, func.bounds.ub, (samples_per_restart, self.dim))\n            for sample in samples:\n                if self.budget <= 0:\n                    break\n                # Nelder-Mead Optimization\n                result = minimize(func, sample, method='Nelder-Mead', bounds=bounds, options={'maxfev': self.budget, 'adaptive': True})\n                \n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                # Reduce remaining budget\n                self.budget -= result.nfev\n                # Dynamic Budget Reallocation: Increase samples per restart if unused budget remains\n                if self.budget > 0 and result.nfev < self.budget // 10:\n                    samples_per_restart += 1 \n\n            # Adaptive restart: Adjust sample size based on remaining budget\n            samples_per_restart = max(self.budget // 10, 1)\n        \n        return best_solution if best_solution is not None else np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)", "name": "MultiStartNelderMead", "description": "Enhanced Multi-Start Nelder-Mead with Dynamic Budgeting - improves adaptive sampling size and restarts by reallocating unused budget dynamically for optimal exploration and exploitation.", "configspace": "", "generation": 3, "fitness": 0.643985362052998, "feedback": "The algorithm MultiStartNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.644 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1347862-00b2-47e9-9698-89afe37a202f", "metadata": {"aucs": [0.6264990352806207, 0.6344944352459829, 0.6709626156323901], "final_y": [1.385374645842295e-05, 1.0387477906566008e-05, 7.114466091838023e-06]}, "mutation_prompt": null}
{"id": "a36d72af-6416-4553-8fef-06a68ea6ec13", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMultiStartNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        samples_per_restart = max(self.budget // 10, 1)\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n        \n        best_value = float('inf')\n        best_solution = None\n        \n        # Initial dynamic bounds setup\n        dynamic_bounds = np.array(bounds)\n        \n        while self.budget > 0:\n            # Narrow down search space dynamically by reducing bounds\n            dynamic_bounds[:, 0] = np.maximum(dynamic_bounds[:, 0], best_solution - 0.1 * (dynamic_bounds[:, 1] - dynamic_bounds[:, 0]))\n            dynamic_bounds[:, 1] = np.minimum(dynamic_bounds[:, 1], best_solution + 0.1 * (dynamic_bounds[:, 1] - dynamic_bounds[:, 0]))\n            \n            # Generate random samples within dynamic bounds\n            samples = np.random.uniform(dynamic_bounds[:, 0], dynamic_bounds[:, 1], (samples_per_restart, self.dim))\n            for sample in samples:\n                if self.budget <= 0:\n                    break\n                # Nelder-Mead Optimization\n                result = minimize(func, sample, method='Nelder-Mead', bounds=bounds, options={'maxfev': self.budget, 'adaptive': True})\n                \n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    # Reduce remaining budget\n                    self.budget -= result.nfev\n\n            # Adaptive restart: Adjust sample size based on remaining budget\n            samples_per_restart = max(self.budget // 10, 1)\n        \n        return best_solution if best_solution is not None else np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)", "name": "EnhancedMultiStartNelderMead", "description": "Enhanced Multi-Start Nelder-Mead with Dynamic Search Space Adjustment for Improved Exploration in Smooth Landscapes.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\").", "error": "TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\")", "parent_id": "b1347862-00b2-47e9-9698-89afe37a202f", "metadata": {}, "mutation_prompt": null}
{"id": "ecc3dc6e-868f-4769-ae54-55662a953d7b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass MultiStartNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        samples_per_restart = max(int(np.sqrt(self.budget)), 1)  # dynamic sampling adjustment\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n        \n        best_value = float('inf')\n        best_solution = None\n        \n        while self.budget > 0:\n            # Generate random samples\n            samples = np.random.uniform(func.bounds.lb, func.bounds.ub, (samples_per_restart, self.dim))\n            for sample in samples:\n                if self.budget <= 0:\n                    break\n                # Nelder-Mead Optimization\n                result = minimize(func, sample, method='Nelder-Mead', bounds=bounds, options={'maxfev': self.budget, 'adaptive': True})\n                \n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    # Reduce remaining budget\n                    self.budget -= result.nfev\n\n            # Adaptive restart: Adjust sample size based on remaining budget\n            samples_per_restart = max(int(np.sqrt(self.budget)), 1)  # dynamic sampling adjustment\n        \n        return best_solution if best_solution is not None else np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)", "name": "MultiStartNelderMead", "description": "Enhanced Multi-Start Nelder-Mead with Adaptive Sampling - introduces dynamic sampling for better exploration, reducing premature convergence in smooth landscapes.", "configspace": "", "generation": 5, "fitness": 0.669922856872503, "feedback": "The algorithm MultiStartNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.670 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1347862-00b2-47e9-9698-89afe37a202f", "metadata": {"aucs": [0.6703431261325217, 0.6791365143726029, 0.6602889301123842], "final_y": [6.787858947689165e-06, 6.316336858871152e-06, 5.60276795059221e-06]}, "mutation_prompt": null}
{"id": "79ba093b-0b85-49ac-b373-db3d8e1cd0e5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveMultiStartNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_samples = 5  # Initial number of random samples\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n        \n        best_value = float('inf')\n        best_solution = None\n        utilized_budget = 0\n        \n        while self.budget > utilized_budget:\n            # Determine number of samples based on remaining budget\n            samples_per_restart = max((self.budget - utilized_budget) // initial_samples, 1)\n            samples = np.random.uniform(func.bounds.lb, func.bounds.ub, (samples_per_restart, self.dim))\n            \n            for sample in samples:\n                if utilized_budget >= self.budget:\n                    break\n\n                # Nelder-Mead Optimization with adaptive budget allocation\n                remaining_budget = self.budget - utilized_budget\n                options = {'maxfev': remaining_budget, 'adaptive': True}\n                result = minimize(func, sample, method='Nelder-Mead', bounds=bounds, options=options)\n                \n                # Budget utilization logging\n                utilized_budget += result.nfev\n\n                # Update best solution found\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                \n                # Dynamic budget reallocation based on performance\n                if result.success and result.fun < best_value:\n                    initial_samples = max(initial_samples - 1, 1)\n                else:\n                    initial_samples += 1\n        \n        return best_solution if best_solution is not None else np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)", "name": "AdaptiveMultiStartNelderMead", "description": "Adaptive Multi-Start Nelder-Mead with Dynamic Budget Allocation - an enhanced metaheuristic optimizing convergence by dynamically reallocating budget based on solution performance improvements.", "configspace": "", "generation": 6, "fitness": 0.6367312049515114, "feedback": "The algorithm AdaptiveMultiStartNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.637 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1347862-00b2-47e9-9698-89afe37a202f", "metadata": {"aucs": [0.6188443122239051, 0.6316246459932775, 0.6597246566373512], "final_y": [7.941678698606414e-06, 6.027866729741698e-06, 7.180311036532002e-06]}, "mutation_prompt": null}
{"id": "5e6f29b5-1e20-418f-9176-888262acecb7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass MultiStartNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        samples_per_restart = max(self.budget // 10, 1)\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n        \n        best_value = float('inf')\n        best_solution = None\n        \n        while self.budget > 0:\n            # Hybrid sampling: Random and midpoint sampling\n            samples = np.vstack((np.random.uniform(func.bounds.lb, func.bounds.ub, (samples_per_restart // 2, self.dim)),\n                                 np.array([(func.bounds.lb[i] + func.bounds.ub[i]) / 2 for i in range(self.dim)])))\n            for sample in samples:\n                if self.budget <= 0:\n                    break\n                # Nelder-Mead Optimization\n                result = minimize(func, sample, method='Nelder-Mead', bounds=bounds, options={'maxfev': self.budget, 'adaptive': True})\n                \n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    # Reduce remaining budget\n                    self.budget -= result.nfev\n\n            # Adaptive restart: Adjust sample size based on remaining budget\n            samples_per_restart = max(self.budget // 10, 1)\n        \n        return best_solution if best_solution is not None else np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)", "name": "MultiStartNelderMead", "description": "Enhanced MultiStartNelderMead with hybrid sampling strategy for better initial guesses and budget management.", "configspace": "", "generation": 7, "fitness": 0.6715615580811553, "feedback": "The algorithm MultiStartNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.672 with standard deviation 0.007. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1347862-00b2-47e9-9698-89afe37a202f", "metadata": {"aucs": [0.6701618924936745, 0.6803405559615248, 0.6641822257882666], "final_y": [6.795418569214081e-06, 5.8611054273040425e-06, 6.9280702709966195e-06]}, "mutation_prompt": null}
{"id": "16794b9f-7817-4cb8-ac30-45e74c221ac6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridNelderMeadRandomDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        descent_steps = max(self.budget // 20, 1)\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n        \n        best_value = float('inf')\n        best_solution = None\n        remaining_budget = self.budget\n        \n        while remaining_budget > 0:\n            # Random starting point\n            start_point = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n            \n            # Nelder-Mead Optimization\n            result = minimize(func, start_point, method='Nelder-Mead', bounds=bounds, options={'maxfev': remaining_budget, 'adaptive': True})\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n            remaining_budget -= result.nfev\n            \n            # Random descent to escape local optima\n            for _ in range(descent_steps):\n                if remaining_budget <= 0:\n                    break\n                random_direction = np.random.uniform(-1, 1, self.dim)\n                step_size = np.random.uniform(0.01, 0.1)\n                new_point = result.x + step_size * random_direction\n                \n                # Ensure new point is within bounds\n                new_point = np.clip(new_point, func.bounds.lb, func.bounds.ub)\n                new_value = func(new_point)\n                remaining_budget -= 1\n                \n                if new_value < best_value:\n                    best_value = new_value\n                    best_solution = new_point\n\n        return best_solution if best_solution is not None else np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)", "name": "HybridNelderMeadRandomDescent", "description": "Hybrid Nelder-Mead with Random Descent - Combines Nelder-Mead with random descent steps to escape local optima and enhance exploration in smooth landscapes.", "configspace": "", "generation": 8, "fitness": 0.6747371799471703, "feedback": "The algorithm HybridNelderMeadRandomDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.675 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1347862-00b2-47e9-9698-89afe37a202f", "metadata": {"aucs": [0.6759809244559498, 0.6895442686320095, 0.6586863467535515], "final_y": [3.4752342160600455e-06, 4.588519767594301e-06, 2.9031992233333844e-06]}, "mutation_prompt": null}
{"id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation and Local Search - a novel metaheuristic combining differential evolution for global exploration with adaptive mutation rates and local search using BFGS to refine solutions in smooth landscapes.", "configspace": "", "generation": 9, "fitness": 0.8475266983832584, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.025. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1347862-00b2-47e9-9698-89afe37a202f", "metadata": {"aucs": [0.8578965332232799, 0.8130114721177032, 0.871672089808792], "final_y": [3.038669302357965e-08, 8.690203116095181e-08, 6.402229489798229e-10]}, "mutation_prompt": null}
{"id": "e9775d41-87fd-4354-9fcf-f8b622181e48", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - np.std(fitness) / np.mean(fitness))  # Changed line\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(self.budget, 100)})  # Changed line\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Enhanced Local Search - integrates differential evolution with adaptive strategies and a refined local search step using L-BFGS-B for improved exploitation in smooth landscapes.", "configspace": "", "generation": 10, "fitness": 0.7988914440664008, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.799 with standard deviation 0.032. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7994019729514839, 0.7595330273015833, 0.8377393319461353], "final_y": [3.75595258948522e-08, 2.7036170286579417e-07, 2.5502162436939095e-08]}, "mutation_prompt": null}
{"id": "2e4a1c3b-76a3-4fc7-be09-23e7b8f8b60f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                CR = 0.5 + 0.4 * (1 - fitness.std() / fitness.mean())  # Dynamic crossover rate adjustment\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution Local Search with dynamic crossover rate adjustment to improve convergence in smooth landscapes.", "configspace": "", "generation": 11, "fitness": 0.835945892274116, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.836 with standard deviation 0.033. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.817651291717362, 0.8077808026809676, 0.8824055824240182], "final_y": [6.979810617823614e-08, 1.2819782255398206e-07, 5.390197817853858e-09]}, "mutation_prompt": null}
{"id": "d4fb60fd-992c-41eb-bfd5-e5d946a4249a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / np.max(fitness))  # Change made here\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Local Search - improved global search by optimizing mutation factor based on fitness distribution, preserving exploitation capability.", "configspace": "", "generation": 12, "fitness": 0.7953785656626583, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.795 with standard deviation 0.018. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7764027039677114, 0.7899111278461876, 0.8198218651740758], "final_y": [1.9020621045001643e-07, 1.2045113018286948e-07, 8.737878380764644e-08]}, "mutation_prompt": null}
{"id": "5aa47665-60bd-4da8-8988-ad223e707610", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Dynamic crossover rate based on convergence\n            CR = 0.6 + 0.3 * (fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation and Local Search - Improved crossover strategy by dynamically adjusting the crossover rate based on convergence speed for enhanced exploration and exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.7859723343558254, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8065966776286986, 0.7616622710844686, 0.7896580543543089], "final_y": [1.1621786329749604e-07, 3.245014026255481e-07, 2.241709667724815e-07]}, "mutation_prompt": null}
{"id": "96adb558-a91f-4ba0-8ac6-fd2f1864c1b1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget//2})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n                # Second BFGS run for further refinement\n                if self.budget > 0:\n                    result2 = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                    if result2.fun < best_value:\n                        best_value = result2.fun\n                        best_solution = result2.x\n                        self.budget -= result2.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhancing local search effectiveness in smooth landscapes by incorporating dual BFGS runs for refined solution tuning.", "configspace": "", "generation": 14, "fitness": 0.8044569077295508, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.032. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7808271884796613, 0.8501556261531374, 0.7823879085558538], "final_y": [1.737906904318078e-07, 3.965158691356531e-08, 2.194195099008192e-07]}, "mutation_prompt": null}
{"id": "43b6e076-478f-4141-a60b-cb72bb1b3b72", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n            CR = 0.9 * (1 - fitness.std() / fitness.mean())  # Adaptive crossover rate\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Hybrid Differential Evolution with Adaptive Mutation and BFGS Local Search, enhanced with adaptive crossover rate for better balance between exploration and exploitation.", "configspace": "", "generation": 15, "fitness": 0.8005342357722082, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.801 with standard deviation 0.055. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8429314766319259, 0.835108336346631, 0.7235628943380672], "final_y": [6.167475548522015e-08, 3.1043251926954065e-08, 8.968538681527359e-07]}, "mutation_prompt": null}
{"id": "564834ed-350d-4b17-9882-669acd8ab409", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Local Search - Adjusts mutation factor dynamically to improve convergence in smooth landscapes.", "configspace": "", "generation": 16, "fitness": 0.8254877049537779, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.825 with standard deviation 0.024. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8433045653015897, 0.7917298434639161, 0.8414287060958281], "final_y": [3.226789973630558e-08, 1.1500992570029014e-07, 2.9502999413064986e-08]}, "mutation_prompt": null}
{"id": "d2812525-14ac-4375-bc65-ff1c502c9665", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                gradient = np.gradient(fitness)  # Use gradient for more informed mutation\n                mutant = np.clip(x1 + F * (x2 - x3) - 0.1 * gradient[i], bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Gradient Boosted Mutation and Local Search - this algorithm introduces gradient information to modify mutation strategies in differential evolution, improving convergence speed in smooth landscapes.", "configspace": "", "generation": 17, "fitness": 0.8014706086980498, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.801 with standard deviation 0.025. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.817651291717362, 0.8208261951291294, 0.7659343392476576], "final_y": [6.979810617823614e-08, 6.085501507852816e-08, 1.917519335555568e-07]}, "mutation_prompt": null}
{"id": "5481a15a-a98b-41f3-945a-377b4f1b7b77", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / np.ptp(fitness))  # Changed standard deviation to population range\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Self-Adapting Mutation Factor based on Population Diversity for Improved Convergence.", "configspace": "", "generation": 18, "fitness": 0.7899264633454357, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.790 with standard deviation 0.030. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7644474686388079, 0.8322659174710467, 0.7730660039264525], "final_y": [3.2443732492504774e-07, 3.2593362611056353e-08, 2.0835578785990595e-07]}, "mutation_prompt": null}
{"id": "e0aa2abd-0a72-416d-ad6a-5c915867817a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n        patience = 5  # Restart mechanism\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Adaptive Crossover\n                CR = 0.9 * (1 - fitness.std() / (1e-8 + fitness.mean()))\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n                        patience = 5  # Reset patience on improvement\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n\n            # Restart mechanism\n            patience -= 1\n            if patience == 0:\n                population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                self.budget -= pop_size\n                patience = 5\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Local Search - incorporating adaptive crossover and restart mechanisms to boost global exploration and convergence efficiency.", "configspace": "", "generation": 19, "fitness": 0.8380887019930152, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.838 with standard deviation 0.033. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8843037619430301, 0.8190472105692194, 0.8109151334667963], "final_y": [6.469976567073302e-10, 6.773069489975982e-08, 8.691496370834191e-08]}, "mutation_prompt": null}
{"id": "f43c4a80-a0f3-4b5e-acbc-2eb585dbc036", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.7 + 0.2 * np.random.rand()  # Dynamic Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Dynamic Crossover and Local Search - a refined algorithm with dynamic crossover rates and improved BFGS integration.", "configspace": "", "generation": 20, "fitness": 0.8202026448498129, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.820 with standard deviation 0.051. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8914469605399509, 0.7743335135092699, 0.7948274605002179], "final_y": [3.2912547072952495e-09, 1.4821358380114153e-07, 1.5733813840963344e-07]}, "mutation_prompt": null}
{"id": "b419f490-d497-4f7a-a51d-a2f1f04eee1d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedDifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.8  # Start with higher mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation factor based on diversity\n            diversity = np.std(population, axis=0).mean()\n            F = 0.5 + 0.3 * (diversity / np.ptp(bounds))\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "EnhancedDifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Control and Local Search - integrates adaptive control of mutation factors with local search refinement for efficient convergence in smooth landscapes.", "configspace": "", "generation": 21, "fitness": 0.8310982574055182, "feedback": "The algorithm EnhancedDifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.831 with standard deviation 0.045. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7808818412373222, 0.8910410056915183, 0.8213719252877143], "final_y": [1.5984794926217016e-07, 6.362068497297357e-09, 6.135440901306662e-08]}, "mutation_prompt": null}
{"id": "531fda2d-8182-4648-9f95-ca6ab2ae1193", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            # Adjust the crossover rate based on population diversity\n            CR = 0.8 + 0.2 * (1 - fitness.std() / fitness.mean())\n            \n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Dynamic Crossover Rate Adjustments - refining Differential Evolution by dynamically adjusting the crossover rate based on population diversity, while combining adaptive mutation and local search to optimize smooth landscapes.", "configspace": "", "generation": 22, "fitness": 0.835945892274116, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.836 with standard deviation 0.033. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.817651291717362, 0.8077808026809676, 0.8824055824240182], "final_y": [6.979810617823614e-08, 1.2819782255398206e-07, 5.390197817853858e-09]}, "mutation_prompt": null}
{"id": "36a6348e-0a19-4eec-858e-ab60a1ca07b5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget * 2})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Enhanced Local Search - a refined metaheuristic combining differential evolution for global exploration with adaptive mutation rates and improved local search using L-BFGS-B for precision in smooth landscapes.", "configspace": "", "generation": 23, "fitness": 0.8089093856691791, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.022. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8181552795535874, 0.7781001010245405, 0.8304727764294095], "final_y": [1.107866401497891e-07, 1.5391071708986433e-07, 4.6905116897427675e-08]}, "mutation_prompt": null}
{"id": "ea476376-3287-480c-b14a-30e1ec11c28b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 7)  # Increased population size for better exploration\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n\n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.6  # Tuned mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Reset mechanism to prevent stagnation\n            if fitness.std() < 1e-6:\n                population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                self.budget -= pop_size\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n\n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation, Restart Mechanism, and Improved Local Search utilizing BFGS for precise refinement in smooth landscapes.", "configspace": "", "generation": 24, "fitness": 0.7739083062160513, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.023. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7454120510870453, 0.7751186781090724, 0.8011941894520366], "final_y": [2.7426751352637503e-07, 1.4724158839633122e-07, 8.012854695561648e-08]}, "mutation_prompt": null}
{"id": "b373535d-316a-4be5-9351-4607187fb579", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Dynamic crossover rate based on fitness variance\n                CR = 0.5 + 0.4 * (1 - fitness.std() / fitness.mean())\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Reduced local search frequency for improved exploration\n            if self.budget > 0 and self.budget % (pop_size//2) == 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Dynamic Population and Adaptive Crossover for improved convergence in smooth landscapes.", "configspace": "", "generation": 25, "fitness": 0.7834059471225238, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.044. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7388163256696664, 0.7675386975317336, 0.8438628181661715], "final_y": [3.7157000069417853e-07, 1.688234867706795e-07, 4.590304143441761e-08]}, "mutation_prompt": null}
{"id": "994e7524-8901-4015-b394-578c771ec522", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.3 + 0.4 * (1 - fitness.std() / fitness.mean())  # Adjusted mutation factor\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Improved Differential Evolution by dynamic adjustment of mutation factors and integrating local search intermittently to enhance convergence.", "configspace": "", "generation": 26, "fitness": 0.7964955001403663, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.796 with standard deviation 0.025. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7824839445049988, 0.7749872149265172, 0.8320153409895827], "final_y": [1.1999548722837888e-07, 1.655392153029314e-07, 3.7927053977361016e-08]}, "mutation_prompt": null}
{"id": "e69ce46c-6d58-4430-b077-197b0c55fe59", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        adaptive_interval = 5  # New: frequency of dynamic adaptation\n        generation_count = 0\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # New: Dynamic population adjustment\n            if generation_count % adaptive_interval == 0 and pop_size > 10:\n                pop_size = int(pop_size * 0.9)\n                population = population[:pop_size]\n                fitness = fitness[:pop_size]\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n            \n            generation_count += 1\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation, Local Search, and Dynamic Population Sizing - an improved metaheuristic integrating dynamic population sizing and periodic local search to balance exploration and exploitation effectively.", "configspace": "", "generation": 27, "fitness": 0.7775180617637437, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.002. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7748490736317036, 0.778249021654915, 0.7794560900046127], "final_y": [1.6569227670024195e-07, 2.6231672151878677e-07, 3.763398968311562e-08]}, "mutation_prompt": null}
{"id": "aa5d6e66-8884-477c-80da-985dc461c646", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover with dynamic rate\n                diversity = np.std(population, axis=0).mean()\n                CR = 0.9 * (1 - diversity / bounds[:, 1].ptp().sum())\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Improved Differential Evolution with Adaptive Mutation and Local Search by dynamically adjusting the crossover rate based on population diversity.", "configspace": "", "generation": 28, "fitness": 0.835945892274116, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.836 with standard deviation 0.033. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.817651291717362, 0.8077808026809676, 0.8824055824240182], "final_y": [6.979810617823614e-08, 1.2819782255398206e-07, 5.390197817853858e-09]}, "mutation_prompt": null}
{"id": "36ae760e-9347-46ff-94bb-bf0356c7da1d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation and Local Search - a novel metaheuristic combining differential evolution for global exploration with adaptive mutation rates and local search using BFGS to refine solutions in smooth landscapes.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8578965332232799, 0.8130114721177032, 0.871672089808792], "final_y": [3.038669302357965e-08, 8.690203116095181e-08, 6.402229489798229e-10]}, "mutation_prompt": null}
{"id": "b65b406e-87eb-47ac-afda-9fea6826f281", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n\n            # Dynamic population size adjustment\n            pop_size = max(10, int(pop_size * 0.95))  # Reduce population size over iterations for efficiency\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Local Search using dynamic population size adjustment.", "configspace": "", "generation": 30, "fitness": 0.7648488899366255, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.765 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7457803045404104, 0.7870005013194243, 0.7617658639500419], "final_y": [2.0071999668556274e-07, 1.4587597933385415e-07, 2.1096622009032607e-07]}, "mutation_prompt": null}
{"id": "468dd321-b595-427e-8caa-206b015d9ad7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n\n            # Improve exploration with dynamic mutation factor adjustment\n            F = 0.5 + 0.5 * (self.budget / (self.budget + pop_size))  # Change made here\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation, Local Search, and Improved Exploration - enhances exploration by dynamically adjusting mutation factors based on iteration progress.", "configspace": "", "generation": 31, "fitness": 0.8131648933884456, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.018. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.803613454848728, 0.8388585781258405, 0.7970226471907683], "final_y": [8.672721069997949e-08, 5.189775415658707e-08, 6.577549353390733e-08]}, "mutation_prompt": null}
{"id": "721ac1fe-ca3c-4e49-962a-3b81a510bbce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation with dynamic boundary control\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n                lb, ub = bounds[:, 0], bounds[:, 1]\n                mutant = np.clip(mutant, lb + 0.1 * (ub - lb), ub - 0.1 * (ub - lb))\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Boundary Control and Local Search - improved mutation strategy with dynamic boundary adjustment for better convergence in constrained smooth landscapes.", "configspace": "", "generation": 32, "fitness": 0.7615745087318583, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.762 with standard deviation 0.044. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8163728224596641, 0.758607163372347, 0.7097435403635638], "final_y": [5.929766596165807e-08, 1.890668186671312e-07, 3.0943095982002776e-07]}, "mutation_prompt": null}
{"id": "9412ae0c-663d-4a98-9f57-250e592bf0e7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean()) * np.clip(fitness.std() / fitness.mean(), 0.1, 1.0)\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Local Search, optimizing adaptive mutation factor based on convergence and diversity.", "configspace": "", "generation": 33, "fitness": 0.8032863579151447, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.033. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8360564916531746, 0.7583552732035879, 0.8154473088886718], "final_y": [2.5295009665663843e-08, 1.8960973905997395e-07, 7.868407012754652e-08]}, "mutation_prompt": null}
{"id": "65c1b653-c681-47fc-9e5c-c7c35fb2ff67", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - np.var(fitness) / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with improved adaptive mutation factor calculation to boost convergence in smooth landscapes.", "configspace": "", "generation": 34, "fitness": 0.7740380244926359, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7731760166189543, 0.7508259809198115, 0.798112075939142], "final_y": [2.2152121517498086e-07, 3.774638294024164e-07, 1.0473376225345125e-07]}, "mutation_prompt": null}
{"id": "9d954394-67f6-4c30-bb4f-9d36ff6ff155", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.8  # Dynamic mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on fitness variance\n            F = 0.5 + 0.3 * fitness.std() / fitness.mean()\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Improved Differential Evolution with Adaptive Mutation and Local Search - enhanced by introducing dynamic mutation factor F and additional local search based on the fitness variance for improved convergence.", "configspace": "", "generation": 35, "fitness": 0.7768518548910593, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.777 with standard deviation 0.015. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7752588973864066, 0.7598446201764184, 0.7954520471103531], "final_y": [2.681394162956418e-07, 3.576930089429085e-07, 1.0354660208641788e-07]}, "mutation_prompt": null}
{"id": "f93649bc-31f5-406c-a0ae-5ea481b29145", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.95  # Increased Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation and Local Search - improved by increasing crossover rate for enhanced diversity and exploration.", "configspace": "", "generation": 36, "fitness": 0.8098633503972518, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.047. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7931088801198964, 0.7630535103090332, 0.8734276607628256], "final_y": [9.319891588322491e-08, 2.5035065568980993e-07, 1.1103755996253944e-08]}, "mutation_prompt": null}
{"id": "96dbaec8-83f6-4de0-b1e1-b262767d115a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n\n            # Dynamic population size adjustment based on convergence\n            pop_size = max(10, int(0.9 * pop_size + 0.1 * (self.budget // 10)))\n\n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation, Local Search, and Dynamic Population Size - a refined metaheuristic that adjusts population size dynamically based on convergence, enhancing exploration and exploitation balance.", "configspace": "", "generation": 37, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 14 is out of bounds for axis 0 with size 10').", "error": "IndexError('index 14 is out of bounds for axis 0 with size 10')", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {}, "mutation_prompt": null}
{"id": "e4f305b4-5ab0-4a08-bda4-fc3eca772197", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 5:  # Reduced threshold for more frequent local search\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(self.budget, 20)})  # Limit function evaluations for BFGS\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "An enhanced differential evolution algorithm with adaptive mutation and a more frequent local search to refine solutions faster in smooth landscapes.", "configspace": "", "generation": 38, "fitness": 0.4804911410207029, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.480 with standard deviation 0.086. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.44996438132359307, 0.5971836436048343, 0.3943253981336814], "final_y": [4.145425247914817e-07, 4.072615030114088e-06, 0.0003906526332856334]}, "mutation_prompt": null}
{"id": "90a00eef-4115-4ae4-9ff2-8dd39f4a1f04", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - np.std(population) / np.mean(population))  # Adjusted line\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Improved exploration by adjusting mutation factor based on population diversity for better solution coverage.", "configspace": "", "generation": 39, "fitness": 0.7408201812475644, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.741 with standard deviation 0.026. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7761666374716969, 0.7153899196054383, 0.730903986665558], "final_y": [3.3170587792195033e-07, 8.282807216181812e-07, 7.806788396143898e-07]}, "mutation_prompt": null}
{"id": "18f62526-6574-4f27-bfbd-3a1e44751181", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            local_search_frequency = 0.1 * (fitness.std() / fitness.mean()) + 0.1  # Adjust local search frequency\n            if self.budget > 0 and np.random.rand() < local_search_frequency:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Introduce a dynamic local search frequency based on fitness variance to improve convergence efficiency.", "configspace": "", "generation": 40, "fitness": 0.7505613926626925, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.751 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7315075295806285, 0.7762622693774887, 0.7439143790299605], "final_y": [1.6098403567436857e-07, 1.0603435183564888e-07, 2.092734513264616e-07]}, "mutation_prompt": null}
{"id": "dbb58957-557a-4712-8cd6-96d76121436f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n            CR = 0.9 * (1 - fitness.std() / fitness.mean())  # Line modified for dynamic crossover rate\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Local Search, incorporating dynamic crossover rate adjustment based on population diversity.", "configspace": "", "generation": 41, "fitness": 0.8340083343945341, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.834 with standard deviation 0.037. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8091469164682626, 0.806505046685899, 0.8863730400294408], "final_y": [8.757720586560215e-08, 8.988101604607994e-08, 9.843224793403868e-09]}, "mutation_prompt": null}
{"id": "9b275175-9c23-4d20-887c-5a56ce5a74fa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Initial Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Adaptive crossover adjustment based on diversity\n            CR = 0.9 * (1 - np.std(fitness) / np.mean(fitness))  # Updated line\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with strategic CR adjustment - introducing a dynamic crossover rate adjustment based on solution diversity to improve the convergence speed.", "configspace": "", "generation": 42, "fitness": 0.8064867899835301, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.043. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8672001440237076, 0.7844555154576528, 0.7678047104692294], "final_y": [1.68131534857425e-08, 1.336985185340606e-07, 1.8197085673325013e-07]}, "mutation_prompt": null}
{"id": "7e6c1986-1514-4460-ab73-501975c37010", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Dynamic mutation factor adjustment based on fitness variance\n            F = max(0.1, 0.9 * (1 - fitness.std() / (fitness.mean() + 1e-10)))\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Improved Differential Evolution by dynamically adjusting the mutation factor based on fitness variance.", "configspace": "", "generation": 43, "fitness": 0.8045053805787585, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.805 with standard deviation 0.023. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7801435213760937, 0.7978400621205833, 0.8355325582395987], "final_y": [2.8315405121789745e-07, 7.771761791067656e-08, 5.0981083113167514e-08]}, "mutation_prompt": null}
{"id": "8210ae29-bf43-48a4-ad4c-7639e9c50547", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Reset mutation factor F after each full iteration\n            F = 0.5  # Resetting to initial value\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Strategic Mutation Factor Reset - combines differential evolution with adaptive mutation and strategic mutation factor reset to improve convergence on smooth landscapes.", "configspace": "", "generation": 44, "fitness": 0.7859723343558254, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8065966776286986, 0.7616622710844686, 0.7896580543543089], "final_y": [1.1621786329749604e-07, 3.245014026255481e-07, 2.241709667724815e-07]}, "mutation_prompt": null}
{"id": "7903d65c-eca0-434f-be33-097cf4ee37aa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        stagnation_counter = 0  # Line changed\n\n        while self.budget > 0:\n            previous_best_value = best_value  # Line changed\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n\n            # Restart mechanism to escape local optima\n            if best_value == previous_best_value:  # Line changed\n                stagnation_counter += 1  # Line changed\n            else:  # Line changed\n                stagnation_counter = 0  # Line changed\n            if stagnation_counter >= 5:  # Line changed\n                population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))  # Line changed\n                fitness = np.array([func(ind) for ind in population])  # Line changed\n                self.budget -= pop_size  # Line changed\n                stagnation_counter = 0  # Line changed\n\n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation, Local Search, and Restart Mechanism to improve exploration and convergence in smooth landscapes.", "configspace": "", "generation": 45, "fitness": 0.8281549729546575, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.044. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8311284606831306, 0.8800383086502864, 0.7732981495305555], "final_y": [5.534154422635136e-08, 1.6410798035506912e-08, 2.2039416732407788e-07]}, "mutation_prompt": null}
{"id": "9b20918a-fe36-41c3-bfca-a541bedb9fac", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedDifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.6  # Mutation factor\n        CR_init = 0.9  # Initial Crossover rate\n        CR_final = 0.6  # Final Crossover rate\n\n        while self.budget > 0:\n            CR = CR_init - (CR_init - CR_final) * (1 - self.budget / 1000)\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover with progressive rate adjustment\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.6 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "EnhancedDifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation, Progressive Crossover, and Local Search - utilizes progressive crossover and adaptive mutation rates with BFGS local search to efficiently refine solutions in smooth landscapes.", "configspace": "", "generation": 46, "fitness": 0.8394542885958977, "feedback": "The algorithm EnhancedDifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.839 with standard deviation 0.028. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8465195951214178, 0.8701262085131408, 0.801717062153134], "final_y": [2.8340927307368092e-08, 1.049467819233977e-08, 6.404347191716802e-08]}, "mutation_prompt": null}
{"id": "32002906-ec0a-428e-99b3-5c02e34b76e4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n            \n            # Iterative adjustment of bounds\n            bounds[:, 0] = np.maximum(bounds[:, 0], best_solution - (bounds[:, 1] - bounds[:, 0]) * 0.1)\n            bounds[:, 1] = np.minimum(bounds[:, 1], best_solution + (bounds[:, 1] - bounds[:, 0]) * 0.1)\n\n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation, Local Search, and Iterative Bound Adjustment to refine solutions by narrowing search space as convergence improves.", "configspace": "", "generation": 47, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('An upper bound is less than the corresponding lower bound.').", "error": "ValueError('An upper bound is less than the corresponding lower bound.')", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {}, "mutation_prompt": null}
{"id": "605555ce-6e70-4a68-9c26-e3193e8776b0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, jac='2-point', options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Gradient-Informed Local Search - integrates gradient information for more efficient local search to refine solutions in smooth landscapes.", "configspace": "", "generation": 48, "fitness": 0.7962246954685989, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.796 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8130667576148425, 0.7861455946866858, 0.7894617341042685], "final_y": [7.224372767375368e-08, 1.1873529059207904e-07, 1.43387553793936e-07]}, "mutation_prompt": null}
{"id": "51a740ab-7c61-494d-9be5-e30ed2480653", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearchElite:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation with elite strategy\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearchElite", "description": "Enhanced Differential Evolution with Adaptive Mutation, Local Search, and Elite Strategy - integrates elite preservation to enhance convergence and solution quality.", "configspace": "", "generation": 49, "fitness": 0.7859723343558254, "feedback": "The algorithm DifferentialEvolutionLocalSearchElite got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8065966776286986, 0.7616622710844686, 0.7896580543543089], "final_y": [1.1621786329749604e-07, 3.245014026255481e-07, 2.241709667724815e-07]}, "mutation_prompt": null}
{"id": "69a61ae7-3c67-4b61-a214-e1dcbb61c8e2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n            \n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n            CR = 0.1 + 0.8 * np.exp(-fitness.std())  # Dynamic crossover rate adjustment\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation and Local Search, enhanced with dynamic crossover rate adjustment based on search space exploration.", "configspace": "", "generation": 50, "fitness": 0.8281549729546575, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.044. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8311284606831306, 0.8800383086502864, 0.7732981495305555], "final_y": [5.534154422635136e-08, 1.6410798035506912e-08, 2.2039416732407788e-07]}, "mutation_prompt": null}
{"id": "6584e382-e46e-4c59-ae8f-6a297f0b3b35", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population with uniform sampling and inclusion of boundary values\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        for i in range(self.dim):  # Ensure boundary values are included in population\n            population[i % pop_size, i] = bounds[i, 0]\n            population[(i + 1) % pop_size, i] = bounds[i, 1]\n        \n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on population diversity\n            F = 0.5 * (1 - np.var(fitness) / np.mean(fitness))\n\n            # Local search using L-BFGS-B on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation, Crossover and Local Search using L-BFGS-B for faster convergence in smooth, low-dimensional landscapes.", "configspace": "", "generation": 51, "fitness": 0.7717007361059993, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.772 with standard deviation 0.023. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7468265893565553, 0.7664081253790775, 0.8018674935823651], "final_y": [3.871477001296777e-07, 1.9333052506660466e-07, 1.5713872943527608e-07]}, "mutation_prompt": null}
{"id": "54cc321d-35d4-44f2-8df2-67fc2ad0766e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation and population size adjustment\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n            if fitness.std() < 0.01:\n                pop_size = max(5, int(pop_size * 0.9))\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(self.budget, 50)})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Population Size and Convergence-based Search Intensification, leveraging adaptive population and a focused local search to improve solution refinement.", "configspace": "", "generation": 52, "fitness": 0.5793104747293607, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.579 with standard deviation 0.068. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.4978494010707334, 0.6646325730685088, 0.5754494500488397], "final_y": [8.874792296586012e-07, 1.074829389750333e-07, 3.135163627779679e-07]}, "mutation_prompt": null}
{"id": "ef7ad947-302a-44bc-81be-3c48567deaab", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n            \n            # Dynamic population size adjustment\n            pop_size = max(10, int(pop_size * (1 + 0.01 * (fitness.std() / fitness.mean()))))\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Improved Differential Evolution with Adaptive Mutation and Local Search - enhanced with dynamic population size adjustment based on convergence to optimize solutions efficiently.", "configspace": "", "generation": 53, "fitness": 0.8183913564958751, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.053. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7708131648768893, 0.792194189567152, 0.892166715043584], "final_y": [2.78427620944532e-07, 1.2325840772002703e-07, 1.6230090813014578e-08]}, "mutation_prompt": null}
{"id": "2c666b2d-043b-4a1b-b32e-32be5178cfab", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Adaptive Crossover Rate based on population diversity\n            CR = 0.9 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Local Search using Adaptive Crossover Rate to improve convergence stability and solution refinement.", "configspace": "", "generation": 54, "fitness": 0.7859723343558254, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8065966776286986, 0.7616622710844686, 0.7896580543543089], "final_y": [1.1621786329749604e-07, 3.245014026255481e-07, 2.241709667724815e-07]}, "mutation_prompt": null}
{"id": "8ef860f4-2721-4f9c-b730-8de34656df20", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Hybrid local search using BFGS and Nelder-Mead on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='BFGS', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n                elif self.budget > 0:  # Try Nelder-Mead if BFGS didn't improve\n                    result = minimize(func, best_solution, method='Nelder-Mead', bounds=bounds, options={'maxiter': self.budget})\n                    if result.fun < best_value:\n                        best_value = result.fun\n                        best_solution = result.x\n                        self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Hybrid Local Search - an improved metaheuristic that combines differential evolution for global exploration with adaptive mutation rates and a hybrid local search strategy using BFGS and Nelder-Mead to refine solutions in smooth landscapes.", "configspace": "", "generation": 55, "fitness": 0.8277507463006982, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.066. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.9205614035846192, 0.7690618377059378, 0.793628997611538], "final_y": [3.664563344374113e-09, 3.03275799011149e-07, 2.035426839048114e-07]}, "mutation_prompt": null}
{"id": "ba61c6c0-4c80-4e25-b25d-01d50345e64f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n\n            # Dynamic adjustment of population size\n            pop_size = min(max(10, pop_size + int(self.budget * 0.02)), self.budget)\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Dynamic Population Size Adjustment for Improved Local Search Application.", "configspace": "", "generation": 56, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 12 is out of bounds for axis 0 with size 10').", "error": "IndexError('index 12 is out of bounds for axis 0 with size 10')", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {}, "mutation_prompt": null}
{"id": "a96d3649-30c0-47a5-b013-a558d7f393be", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            CR = 0.9 * (1 - fitness.std() / fitness.mean())  # Dynamic crossover rate adjustment\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(self.budget, 50)})  # Termination criterion\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution Local Search with Dynamic Crossover Rate - An improvement over the existing strategy by dynamically adjusting the crossover rate based on population diversity and integrating a termination criterion for local search efficiency.", "configspace": "", "generation": 57, "fitness": 0.6023361726496096, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.602 with standard deviation 0.024. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.6291744661909607, 0.570555343652537, 0.6072787081053312], "final_y": [8.584775049696837e-07, 3.7472116703445524e-07, 2.5958642592327296e-06]}, "mutation_prompt": null}
{"id": "88b50a4d-61df-4f0b-9305-69951e4818d0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridGeneticNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n\n        def select_parents():\n            # Tournament selection\n            indices = np.random.choice(pop_size, 3, replace=False)\n            fit_values = fitness[indices]\n            return indices[np.argmin(fit_values)]\n\n        def crossover(parent1, parent2):\n            alpha = np.random.rand()\n            return alpha * parent1 + (1 - alpha) * parent2\n\n        mutation_rate = 0.1\n        \n        while self.budget > 0:\n            # Create offspring population\n            offspring = []\n            for _ in range(pop_size):\n                if self.budget <= 0:\n                    break\n                parent1 = population[select_parents()]\n                parent2 = population[select_parents()]\n                child = crossover(parent1, parent2)\n                \n                # Mutation\n                if np.random.rand() < mutation_rate:\n                    mutation_vector = np.random.normal(0, 0.1, self.dim)\n                    child = np.clip(child + mutation_vector, bounds[:, 0], bounds[:, 1])\n                \n                offspring.append(child)\n                trial_fitness = func(child)\n                self.budget -= 1\n                \n                if trial_fitness < best_value:\n                    best_value = trial_fitness\n                    best_solution = child\n            \n            # Replace population with offspring\n            population = np.array(offspring)\n            fitness = np.array([func(ind) for ind in population])\n            self.budget -= pop_size\n\n            # Local search using Nelder-Mead on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='Nelder-Mead', options={'maxfev': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "HybridGeneticNelderMead", "description": "Hybrid Genetic Algorithm with Nelder-Mead Local Search - combines genetic algorithms for diverse exploration with Nelder-Mead method for fast convergence in smooth optimization landscapes.", "configspace": "", "generation": 58, "fitness": 0.5272361698088913, "feedback": "The algorithm HybridGeneticNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.527 with standard deviation 0.174. And the mean value of best solutions found was 0.022 (0. is the best) with standard deviation 0.031.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.6583825647701419, 0.2815857597681978, 0.6417401848883342], "final_y": [6.931972739917492e-06, 0.06523365190315193, 8.503472820684687e-06]}, "mutation_prompt": null}
{"id": "1a33e00b-a06c-4d6b-b696-2b4461eeab75", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            if self.budget < 0.3 * self.budget:  # Dynamic population resizing\n                pop_size = max(5, int(pop_size * 0.8))\n            \n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation and Local Search featuring dynamic population resizing for improved exploration-exploitation balance.", "configspace": "", "generation": 59, "fitness": 0.7837637155633527, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.060. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7727584490478576, 0.8625742794212402, 0.7159584182209602], "final_y": [1.7710054941102382e-07, 2.4813157007545468e-08, 1.2959223865958744e-06]}, "mutation_prompt": null}
{"id": "3e09c260-60ef-449e-995c-9beacf70d604", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n        temperature = 1.0  # Initial temperature for acceptance\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                # With simulated annealing acceptance criterion\n                if trial_fitness < fitness[i] or np.random.rand() < np.exp((fitness[i] - trial_fitness) / temperature):\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n            temperature *= 0.99  # Cooling schedule\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation and BFGS Local Search with temperature-based acceptance criterion - incorporates a simulated annealing-like strategy to occasionally accept worse solutions for escaping local optima.", "configspace": "", "generation": 60, "fitness": 0.8056355731283511, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.062. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7708131648768893, 0.7539268394645802, 0.892166715043584], "final_y": [2.78427620944532e-07, 2.9298086011782373e-07, 1.6230090813014578e-08]}, "mutation_prompt": null}
{"id": "b301336a-ec85-475a-811f-bdd62de0d28f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n            CR = 0.9 * (1 - fitness.std() / fitness.mean())  # Adaptive CR\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Crossover Rate and Local Search using BFGS for refined solutions.", "configspace": "", "generation": 61, "fitness": 0.8204342280522239, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.820 with standard deviation 0.045. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7663066740961407, 0.8767202387544254, 0.8182757713061054], "final_y": [2.2891386689126747e-07, 4.156000693593138e-09, 1.171314196992499e-07]}, "mutation_prompt": null}
{"id": "c0ec220f-c620-4a52-b52f-fcfd7f6d73e7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n            CR = 0.9 * (1 - fitness.std() / fitness.max())  # Dynamic crossover rate adjustment\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Introduced dynamic crossover rate adjustment based on fitness diversity to enhance exploration and exploitation balance.", "configspace": "", "generation": 62, "fitness": 0.7771815521526255, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.777 with standard deviation 0.003. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7727367828041684, 0.7802997191907933, 0.7785081544629147], "final_y": [2.4246224612598234e-07, 1.8205787434374412e-07, 1.557794052549362e-07]}, "mutation_prompt": null}
{"id": "72421b8e-3dbe-4007-82d4-e586a0778888", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                CR = 0.5 + 0.4 * (1 - fitness.std() / fitness.mean())  # Adaptive crossover rate\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Dynamic Crossover Rate and Local Search - leveraging adaptive crossover rates and local search refinement for improved performance in smooth landscapes.", "configspace": "", "generation": 63, "fitness": 0.8329274001213715, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.833 with standard deviation 0.042. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7745849120178567, 0.856286296225879, 0.8679109921203786], "final_y": [1.990792672889444e-07, 2.3406331530976553e-08, 2.533678880556396e-08]}, "mutation_prompt": null}
{"id": "35f8a496-c071-4e2d-abf8-db62395343c6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS with gradient information\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, jac='2-point', options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Local Search using Gradient Information - improves solution refinement in smooth landscapes by incorporating gradient information into the local search step.  ", "configspace": "", "generation": 64, "fitness": 0.7719112874225352, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.772 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7548995328766696, 0.7790375469870916, 0.7817967824038444], "final_y": [2.7833654388140035e-07, 2.2096011941957536e-07, 1.25833668139409e-07]}, "mutation_prompt": null}
{"id": "7999094a-56b3-4567-9f70-0597a693cf4a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n            CR = 0.5 + 0.5 * (fitness.std() / fitness.max())  # New adaptive CR\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Crossover Rate Local Search - incorporates an adaptive crossover rate to balance exploration and exploitation dynamically based on population diversity.", "configspace": "", "generation": 65, "fitness": 0.8089093856691791, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.022. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8181552795535874, 0.7781001010245405, 0.8304727764294095], "final_y": [1.107866401497891e-07, 1.5391071708986433e-07, 4.6905116897427675e-08]}, "mutation_prompt": null}
{"id": "410b4cb1-adb0-4676-aad0-21ad41728e43", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Enhanced adaptive mutation adjustment strategy\n            F = 0.5 * (1 - fitness.std() / (fitness.mean() + 1e-9))  \n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Adaptive Mutation Strategy in Differential Evolution with Local BFGS for Improved Convergence.", "configspace": "", "generation": 66, "fitness": 0.7953785656626583, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.795 with standard deviation 0.018. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7764027039677114, 0.7899111278461876, 0.8198218651740758], "final_y": [1.9020621045001643e-07, 1.2045113018286948e-07, 8.737878380764644e-08]}, "mutation_prompt": null}
{"id": "734ab348-cad0-44c1-9644-18a05fd050d0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean()) * np.random.uniform(0.8, 1.2)\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Improved Differential Evolution with Adaptive Control and Local Search - an enhanced metaheuristic using dynamic control of mutation factor based on the fitness landscape variance and BFGS for local refinement in smooth regions.", "configspace": "", "generation": 67, "fitness": 0.7859723343558254, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8065966776286986, 0.7616622710844686, 0.7896580543543089], "final_y": [1.1621786329749604e-07, 3.245014026255481e-07, 2.241709667724815e-07]}, "mutation_prompt": null}
{"id": "b99fd3d8-1f13-40cc-b88b-16b05e00e631", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Self-adaptive crossover rate adjustment\n            CR = 0.1 + 0.9 * (fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation, Local Search, and Self-adaptive Crossover Rate - Refinement of original method by introducing self-adaptive crossover rate for better exploration-exploitation balance.", "configspace": "", "generation": 68, "fitness": 0.759045719219135, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.759 with standard deviation 0.031. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7171205755302064, 0.7904386570270594, 0.7695779251001393], "final_y": [2.810299210255894e-07, 1.603381466011672e-07, 1.954048269418179e-07]}, "mutation_prompt": null}
{"id": "2c16d122-65f2-46d7-af11-b47c00bab0ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            # New dynamic population size based on fitness variance\n            pop_size = max(10, int(self.dim * (1 + fitness.std() / fitness.mean())))\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Adaptive Crossover\n                CR = 0.9 * (1 - fitness.std() / fitness.mean())\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Local Search - includes adaptive crossover rate and dynamic population size based on fitness variance for improved exploration and exploitation balance.", "configspace": "", "generation": 69, "fitness": 0.8174380883201602, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.046. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8812768959453613, 0.7991458347947303, 0.7718915342203889], "final_y": [1.1058048060701494e-08, 8.707188126813337e-08, 2.0035226099643475e-07]}, "mutation_prompt": null}
{"id": "572a9125-4055-44f3-9a60-a001f9c224fc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass SimulatedAnnealingGradient:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def _acceptance_probability(self, old_cost, new_cost, temperature):\n        if new_cost < old_cost:\n            return 1.0\n        else:\n            return np.exp((old_cost - new_cost) / temperature)\n        \n    def __call__(self, func):\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize solution\n        current_solution = np.random.uniform(bounds[:, 0], bounds[:, 1], self.dim)\n        current_cost = func(current_solution)\n        self.budget -= 1\n        \n        best_solution = np.copy(current_solution)\n        best_cost = current_cost\n        \n        temperature = 1.0\n        cooling_rate = 0.95\n        \n        while self.budget > 0:\n            # Generate new candidate solution\n            candidate_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n            candidate_solution = np.clip(candidate_solution, bounds[:, 0], bounds[:, 1])\n            candidate_cost = func(candidate_solution)\n            self.budget -= 1\n            \n            # Decide acceptance\n            if self._acceptance_probability(current_cost, candidate_cost, temperature) > np.random.rand():\n                current_solution, current_cost = candidate_solution, candidate_cost\n                \n                # Update best solution\n                if current_cost < best_cost:\n                    best_solution, best_cost = current_solution, current_cost\n            \n            # Anneal\n            temperature *= cooling_rate\n            \n            # Optional gradient-based refinement\n            if self.budget > 0 and np.random.rand() < 0.2: # 20% chance to perform gradient-based refinement\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': max(self.budget, 1)})\n                if result.fun < best_cost:\n                    best_solution, best_cost = result.x, result.fun\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "SimulatedAnnealingGradient", "description": "Hybrid Simulated Annealing with Gradient Assistance - a novel metaheuristic combining global exploration of Simulated Annealing with optional gradient-based refinement to enhance solution quality in smooth landscapes.", "configspace": "", "generation": 70, "fitness": 0.7736201996305739, "feedback": "The algorithm SimulatedAnnealingGradient got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7585786520899402, 0.7875591307354376, 0.7747228160663442], "final_y": [2.047355194319411e-07, 1.255362622481834e-07, 2.8837740495837553e-07]}, "mutation_prompt": null}
{"id": "c9562f9b-6988-48f8-8bfb-8b5e8aff4ad7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionDynamicBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Initial mutation factor\n        CR = 0.9  # Initial crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation with dynamic adjustment\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover with dynamic adjustment\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Dynamic mutation and crossover adjustment based on fitness variance\n            fitness_std = fitness.std()\n            fitness_mean = fitness.mean()\n            F = max(0.1, 0.5 * (1 - fitness_std / fitness_mean))\n            CR = min(0.9, 0.5 + fitness_std / (fitness_mean + 1e-5))\n\n            # Adaptive BFGS local search on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(self.budget, 100)})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionDynamicBFGS", "description": "Differential Evolution with Dynamic Convergence and Adaptive BFGS - an enhanced metaheuristic that employs dynamic mutation and crossover based on fitness variance and adaptive BFGS local search to refine solutions efficiently in smooth landscapes.", "configspace": "", "generation": 71, "fitness": 0.7618353349358236, "feedback": "The algorithm DifferentialEvolutionDynamicBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.762 with standard deviation 0.034. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7150692251147865, 0.774998394695812, 0.7954383849968718], "final_y": [1.6408352253187093e-07, 1.0171292890529102e-07, 1.22746185102524e-07]}, "mutation_prompt": null}
{"id": "18ba2ad4-7887-4948-a05a-19ec22afa556", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMeadDynamicRestart:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        initial_simplex = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.dim + 1, self.dim))\n        \n        def wrapped_func(x):\n            return func(np.clip(x, bounds[:, 0], bounds[:, 1]))\n        \n        best_value = float('inf')\n        best_solution = None\n        \n        while self.budget > 0:\n            # Perform Nelder-Mead optimization with the current budget\n            result = minimize(wrapped_func, initial_simplex[0], method='Nelder-Mead', \n                              options={'maxfev': self.budget, 'initial_simplex': initial_simplex})\n            \n            self.budget -= result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = np.clip(result.x, bounds[:, 0], bounds[:, 1])\n            \n            # Detect convergence and decide if a restart is needed\n            if result.success or self.budget <= 0:\n                break\n            \n            # Dynamic restart by generating a new simplex around the best solution found\n            initial_simplex = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.dim + 1, self.dim))\n            initial_simplex[0] = best_solution\n        \n        return best_solution", "name": "AdaptiveNelderMeadDynamicRestart", "description": "Adaptive Nelder-Mead with Dynamic Restart - a metaheuristic leveraging Nelder-Mead for local optimization with adaptive simplex resizing and dynamic restarts based on convergence detection to explore and exploit smooth landscapes efficiently.", "configspace": "", "generation": 72, "fitness": 0.6644441850042919, "feedback": "The algorithm AdaptiveNelderMeadDynamicRestart got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.664 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.6781011556742025, 0.6471375369449385, 0.6680938623937349], "final_y": [6.723720579923895e-06, 1.2874448650046055e-05, 7.82458002078778e-06]}, "mutation_prompt": null}
{"id": "88b33ca3-db9a-4c82-b314-131e883a73cf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation and crossover adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n            CR = min(1.0, 0.9 + 0.1 * (fitness.std() / fitness.mean()))\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation, Crossover, and Local Search using BFGS for faster convergence in smooth landscapes.", "configspace": "", "generation": 73, "fitness": 0.8006537732377254, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.801 with standard deviation 0.041. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7557837223667959, 0.792145852417618, 0.8540317449287621], "final_y": [3.088606499295794e-07, 1.0875788496948032e-07, 2.6440930531507238e-08]}, "mutation_prompt": null}
{"id": "ce7af5fd-d9b3-4793-a3db-1f62b8a7c847", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0 and fitness.std() > 1e-5:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Accelerated Convergence - incorporates DE for global search with adaptive mutation rates and introduces accelerated convergence via BFGS dynamic refinement based on solution diversity.", "configspace": "", "generation": 74, "fitness": 0.8473003122153026, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.847 with standard deviation 0.051. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8893640806697081, 0.7756522021963541, 0.8768846537798451], "final_y": [1.616666930000071e-08, 1.7046780261118076e-07, 1.234449702861188e-08]}, "mutation_prompt": null}
{"id": "de90430d-a526-461f-ad9a-65036f81b029", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover with adaptive rate\n                cross_points = np.random.rand(self.dim) < (CR * (1 - fitness.std() / fitness.mean()))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Enhanced local search using L-BFGS-B with adjusted bounds\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'gtol': 1e-8})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Improved Differential Evolution with adaptive crossover and enhanced local search using L-BFGS-B to refine solutions in smooth landscapes.", "configspace": "", "generation": 75, "fitness": 0.7941782102704265, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8010639967253389, 0.7836858946328601, 0.7977847394530801], "final_y": [1.5640136875824636e-07, 1.6497297338886842e-07, 1.1165546170761007e-07]}, "mutation_prompt": null}
{"id": "d8cca0a2-c189-4d86-9a79-a709485d8f1b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.3 + 0.4 * (1 - fitness.std() / fitness.mean())  # Adjusted mutation factor\n\n            # More frequent local search using BFGS on the best solution found so far\n            if self.budget > 0 and self.budget % 2 == 0:  # Added condition for more frequent local searches\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Local Search - introduces dynamic mutation factors and more frequent local searches to improve convergence on smooth landscapes.", "configspace": "", "generation": 76, "fitness": 0.8105778817777054, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.054. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7906454763064446, 0.8841919736469004, 0.756896195379771], "final_y": [1.4706492605385165e-07, 1.313348082459992e-09, 2.0395713380863068e-07]}, "mutation_prompt": null}
{"id": "db30a6f4-0f59-43eb-99cb-5594353d9504", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            CR = 0.9 * (1 - fitness.std() / fitness.mean())  # Change here: dynamic CR adjustment\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Introduce a dynamic crossover rate adjustment to enhance exploration and exploitation balance during optimization.", "configspace": "", "generation": 77, "fitness": 0.7867882505388488, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.787 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8007749639482351, 0.7750806820496103, 0.7845091056187011], "final_y": [9.518860128743458e-08, 2.2179652632460972e-07, 1.6426262289662723e-07]}, "mutation_prompt": null}
{"id": "0d5eb330-2548-4c67-b4f8-0c4aae653bc5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.8  # Reduced initial crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution by incorporating a dynamic crossover rate for improved adaptability in convergence.", "configspace": "", "generation": 78, "fitness": 0.8058533920283913, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.032. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8098126233340878, 0.8426027457472359, 0.7651448070038501], "final_y": [1.3545331575556847e-07, 3.314713512148438e-08, 1.909950873672376e-07]}, "mutation_prompt": null}
{"id": "a7dd4a29-e50f-40de-957b-38d5f72758d3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                CR = 0.9 * (1 - fitness.std() / fitness.mean())  # Self-adaptive crossover rate\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation, Local Search, and Self-Adaptive Crossover - an enhanced metaheuristic incorporating self-adaptive crossover rates to improve convergence in diverse landscapes.", "configspace": "", "generation": 79, "fitness": 0.8376842063379643, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.838 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8150922232683859, 0.8361338974416477, 0.861826498303859], "final_y": [3.765737876484794e-08, 5.665885037300922e-08, 1.648692557025936e-08]}, "mutation_prompt": null}
{"id": "784cdb07-ec5d-45fd-ad90-543ae5c92971", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.95  # Crossover rate improved to enhance exploration\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation and Local Search using Enhanced Crossover Strategy - this variant incorporates an improved crossover mechanism to enhance diversity and solution refinement in smooth landscapes.", "configspace": "", "generation": 80, "fitness": 0.7775180617637437, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.002. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7748490736317036, 0.778249021654915, 0.7794560900046127], "final_y": [1.6569227670024195e-07, 2.6231672151878677e-07, 3.763398968311562e-08]}, "mutation_prompt": null}
{"id": "8aad3b1d-52df-4aee-9c86-4be3a044f3b7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedDifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Initial mutation factor\n        CR = 0.9  # Initial crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation with dynamic adjustment\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation and crossover adjustment\n            fitness_std = fitness.std()\n            fitness_mean = fitness.mean()\n            if fitness_mean > 0:\n                F = 0.5 * (1 - fitness_std / fitness_mean)\n                CR = 0.9 * (1 - fitness_std / fitness_mean)\n\n            # Hybrid local search using BFGS and Nelder-Mead\n            if self.budget > 0:\n                result_bfgs = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                self.budget -= result_bfgs.nfev\n                \n                if result_bfgs.fun < best_value:\n                    best_value = result_bfgs.fun\n                    best_solution = result_bfgs.x\n\n                if self.budget > 0:\n                    result_nm = minimize(func, best_solution, method='Nelder-Mead', options={'maxfev': self.budget})\n                    self.budget -= result_nm.nfev\n\n                    if result_nm.fun < best_value:\n                        best_value = result_nm.fun\n                        best_solution = result_nm.x\n        \n        return best_solution", "name": "EnhancedDifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Parameters and Hybrid Local Search - combines adaptive parameter tuning in Differential Evolution with hybrid local search utilizing both BFGS and Nelder-Mead, aimed at refining solutions in smooth landscapes more effectively.", "configspace": "", "generation": 81, "fitness": 0.835944561660778, "feedback": "The algorithm EnhancedDifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.836 with standard deviation 0.033. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.817651291717362, 0.8077768108409541, 0.8824055824240182], "final_y": [6.979810617823614e-08, 1.2819782255398206e-07, 5.390197817853858e-09]}, "mutation_prompt": null}
{"id": "9fe0658e-03dd-46ed-ab6a-85ee506f6da8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * np.tanh(1 - fitness.std() / fitness.mean()) # Changed line\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Local Search - improved by refining adaptive mutation strategy using a nonlinear scaling factor.", "configspace": "", "generation": 82, "fitness": 0.8445104023180479, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.034. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8781851064209021, 0.857645983290847, 0.7977001172423945], "final_y": [2.4863663901190208e-09, 1.636035766011632e-08, 1.0820359533239062e-07]}, "mutation_prompt": null}
{"id": "e5a3bb51-d3e4-4999-8d38-e5a9a03775ad", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n            \n            # Adaptive mutation adjustment based on convergence\n            F = np.clip(0.5 + 0.1 * (fitness.std() / fitness.mean()), 0.1, 0.9)\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Improved Differential Evolution with adaptive step size and local search - integrates an adaptive mutation factor update to maintain diversity and enhance convergence.", "configspace": "", "generation": 83, "fitness": 0.7859723343558254, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8065966776286986, 0.7616622710844686, 0.7896580543543089], "final_y": [1.1621786329749604e-07, 3.245014026255481e-07, 2.241709667724815e-07]}, "mutation_prompt": null}
{"id": "4d38c9ff-56d8-48b0-a82d-7595eaa2e0f2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i] or (np.linalg.norm(trial - best_solution) < 0.1):\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Improved selection strategy by preferring individuals close to the best solution, enhancing exploration around promising areas.", "configspace": "", "generation": 84, "fitness": 0.8281549729546575, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.044. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8311284606831306, 0.8800383086502864, 0.7732981495305555], "final_y": [5.534154422635136e-08, 1.6410798035506912e-08, 2.2039416732407788e-07]}, "mutation_prompt": null}
{"id": "93c7f91a-4617-4171-96b2-85f05ff93a30", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Modify the crossover rate based on population diversity\n                CR = 0.9 * (1 - fitness.std() / fitness.mean())\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Local Search - fine-tunes crossover rate based on population diversity to improve convergence efficiency.", "configspace": "", "generation": 85, "fitness": 0.7939242560938989, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.028. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7644474686388079, 0.8322659174710467, 0.7850593821718421], "final_y": [3.2443732492504774e-07, 3.2593362611056353e-08, 1.400487520857853e-07]}, "mutation_prompt": null}
{"id": "cf7af25e-9a17-4109-a8b6-f56d553cb875", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n            \n            # Dynamic crossover rate adjustment based on diversity\n            CR = 0.9 * (1 - np.var(fitness) / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Local Search by adding dynamic crossover rate adjustment based on solution diversity.", "configspace": "", "generation": 86, "fitness": 0.8055405038901156, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.046. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7655499917176607, 0.8699300289224422, 0.7811414910302439], "final_y": [2.3702994206165643e-07, 1.2088743978608215e-09, 1.6839423979609253e-07]}, "mutation_prompt": null}
{"id": "36771929-797a-4c43-9493-0085d09b0fe3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Enhanced Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i] and trial_fitness < best_value:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    best_value = trial_fitness\n                    best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Enhanced Selection Criteria and Local Search- an improved metaheuristic incorporating a stricter condition for trial selection to enhance convergence in smooth landscapes.", "configspace": "", "generation": 87, "fitness": 0.8350790940835043, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.835 with standard deviation 0.026. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8697792352141224, 0.8291315125822399, 0.8063265344541508], "final_y": [2.1213530029268558e-08, 5.6868701301561357e-08, 8.725171019878092e-08]}, "mutation_prompt": null}
{"id": "df888461-c7fb-4dc7-b5e3-925d6a503f99", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                F_dynamic = 0.5 + 0.2 * np.std(fitness) / np.mean(fitness)\n                mutant = np.clip(x1 + F_dynamic * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS and downhill simplex on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n                elif self.budget > 0:\n                    result_simplex = minimize(func, best_solution, method='Nelder-Mead', options={'maxfev': self.budget})\n                    if result_simplex.fun < best_value:\n                        best_value = result_simplex.fun\n                        best_solution = result_simplex.x\n                        self.budget -= result_simplex.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation and Local Search - improved mutation strategy using a dynamic adjustment of the mutation factor based on fitness diversity and inclusion of downhill simplex method for complementing local search.", "configspace": "", "generation": 88, "fitness": 0.7743883644397617, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.025. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7708131648768893, 0.7454665528546494, 0.8068853755877466], "final_y": [2.78427620944532e-07, 3.2294749035265435e-07, 9.673220490715922e-08]}, "mutation_prompt": null}
{"id": "caef9633-7568-4c63-b83f-2d625ed1bea9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n\n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Improved Differential Evolution with Adaptive Mutation and Local Search by dynamically adjusting the population size based on available budget.", "configspace": "", "generation": 89, "fitness": 0.8106406076995317, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.041. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8141491302772443, 0.7589238213492814, 0.8588488714720692], "final_y": [7.807138172491751e-08, 3.0425639936532755e-07, 1.6856563599090488e-08]}, "mutation_prompt": null}
{"id": "82a15759-9f84-4d8d-aa0e-d954370a21be", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using Simulated Annealing on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Stochastic Local Search - integrates stochastic local search using Simulated Annealing to improve local exploration and balance exploitation/exploration.", "configspace": "", "generation": 90, "fitness": 0.835945892274116, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.836 with standard deviation 0.033. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.817651291717362, 0.8077808026809676, 0.8824055824240182], "final_y": [6.979810617823614e-08, 1.2819782255398206e-07, 5.390197817853858e-09]}, "mutation_prompt": null}
{"id": "e9c6425d-ea5c-47ff-80b6-8761044d5797", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover with dynamic adjustment\n                CR = 0.9 * (1 - fitness.std() / fitness.mean())\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation, Dynamic Crossover, and Local Search - a refined metaheuristic combining differential evolution with adaptive mutation, dynamic crossover rates, and local search using BFGS for improved refinement and convergence in smooth landscapes.", "configspace": "", "generation": 91, "fitness": 0.7927801823859211, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.793 with standard deviation 0.030. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7627046118913058, 0.781588974302317, 0.8340469609641403], "final_y": [2.906614667151882e-07, 1.4724158839633122e-07, 3.410036737734302e-08]}, "mutation_prompt": null}
{"id": "f91e58f1-1d32-4dfe-96fd-f50fd97e642e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Dynamic adjustment of population size\n            if self.budget > pop_size:\n                new_pop_size = min(pop_size * 2, self.budget)\n                pop_size = new_pop_size\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation, Local Search, and Dynamic Population Size Control to improve convergence in smooth landscapes.", "configspace": "", "generation": 92, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 18 is out of bounds for axis 0 with size 10').", "error": "IndexError('index 18 is out of bounds for axis 0 with size 10')", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {}, "mutation_prompt": null}
{"id": "e1293e7e-ea1b-41d1-902f-1fb6f3ff8120", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n\n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n        stagnation_counter = 0\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    stagnation_counter = 0  # Reset stagnation counter\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n                else:\n                    stagnation_counter += 1\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Reinitialize population if stagnation is detected\n            if stagnation_counter > 5:\n                population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                self.budget -= pop_size\n                stagnation_counter = 0\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation, Local Search, and Strategic Reinitialization - improving convergence and diversity by incorporating reinitialization when stagnation is detected.", "configspace": "", "generation": 93, "fitness": 0.8178333850513934, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.038. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8720280447647379, 0.7887019773664236, 0.7927701330230186], "final_y": [1.6944669065158776e-09, 1.4595965392965644e-07, 1.1531142563505238e-07]}, "mutation_prompt": null}
{"id": "998cbf0d-e5eb-4b3e-bf83-cd73895cb683", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, jac='2-point', method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Mutation, BFGS, and Gradient-based Local Search for improved convergence in smooth landscapes.", "configspace": "", "generation": 94, "fitness": 0.7995398331176018, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.020. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7839666722495195, 0.8277346046901263, 0.7869182224131597], "final_y": [1.735438362512616e-07, 7.617433789938388e-08, 1.4091828311719242e-07]}, "mutation_prompt": null}
{"id": "8d0625b7-99a6-4b5b-94a4-546b82957a64", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - np.median(fitness) / fitness.mean())  # Changed from std to median\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 10:  # Adjusted from 0 to 10\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(10, self.budget)})  # Limit maxfun to min(10, budget)\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Improved Adaptive Differential Evolution with Local Search optimizes exploration and exploitation for faster convergence in low-dimensional smooth landscapes.", "configspace": "", "generation": 95, "fitness": 0.21872867230925255, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.219 with standard deviation 0.069. And the mean value of best solutions found was 0.573 (0. is the best) with standard deviation 0.384.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.3164269541080875, 0.16617990308397446, 0.1735791597356957], "final_y": [0.030615258136876714, 0.8740760601591525, 0.8141837177598741]}, "mutation_prompt": null}
{"id": "6f7ad1e7-00f3-4bb5-bb10-f46cc2eba751", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on population diversity\n            F = 0.5 * (1 - np.std(population) / np.mean(population))\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhance the adaptive mutation factor to use a more flexible strategy based on the population's diversity.", "configspace": "", "generation": 96, "fitness": 0.829288074941819, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.829 with standard deviation 0.059. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.7794248000400981, 0.7967480530263478, 0.9116913717590113], "final_y": [5.5235635345327655e-08, 8.081133710714469e-08, 4.506551323738913e-09]}, "mutation_prompt": null}
{"id": "c041cc08-7af1-40a0-a930-1a18ff2e272d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.4 + 0.1 * (fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Enhanced Differential Evolution with Adaptive Learning Rate - an improved metaheuristic integrating dynamic mutation factor adjustment based on fitness changes and adaptive local search to enhance convergence in smooth landscapes.", "configspace": "", "generation": 97, "fitness": 0.8089093856691791, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.022. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8181552795535874, 0.7781001010245405, 0.8304727764294095], "final_y": [1.107866401497891e-07, 1.5391071708986433e-07, 4.6905116897427675e-08]}, "mutation_prompt": null}
{"id": "22a8629f-d376-4469-9591-d6c6c9400597", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Improved Crossover\n                cross_points = np.random.rand(self.dim) < np.random.rand() * CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation, Local Search, and Improved Crossover - integrates differential evolution for exploration with adaptive mutation rates and a refined crossover mechanism to enhance solution discovery, along with BFGS-based local search.", "configspace": "", "generation": 98, "fitness": 0.7980655734253942, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.028. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8016000561621809, 0.762123887684592, 0.8304727764294095], "final_y": [1.0607730154784225e-07, 2.5025085463002764e-07, 4.6905116897427675e-08]}, "mutation_prompt": null}
{"id": "399bb5af-daca-48ae-abff-e8d3e69be338", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = max(10, self.dim * 5)\n        bounds = np.array([(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)])\n        \n        # Initialize population\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= pop_size\n        \n        best_idx = np.argmin(fitness)\n        best_value = fitness[best_idx]\n        best_solution = population[best_idx]\n        \n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        while self.budget > 0:\n            for i in range(pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n\n                # Crossover\n                CR = 0.8 + 0.1 * np.cos(np.pi * (self.budget / 1000))  # Dynamic crossover rate\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                trial_fitness = func(trial)\n                self.budget -= 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_value:\n                        best_value = trial_fitness\n                        best_solution = trial\n\n            # Adaptive mutation adjustment based on convergence\n            F = 0.5 * (1 - fitness.std() / fitness.mean())\n\n            # Local search using BFGS on the best solution found so far\n            if self.budget > 0:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n                    self.budget -= result.nfev\n        \n        return best_solution", "name": "DifferentialEvolutionLocalSearch", "description": "Differential Evolution with Adaptive Mutation and Local Search enhanced by dynamic crossover rate for improved convergence in smooth landscapes.", "configspace": "", "generation": 99, "fitness": 0.7990857048380519, "feedback": "The algorithm DifferentialEvolutionLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.799 with standard deviation 0.035. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fb2c03b8-4aa1-4c13-848c-2162b682a49a", "metadata": {"aucs": [0.8456257609468459, 0.7595425901510994, 0.7920887634162103], "final_y": [3.4366002014719115e-08, 9.279517585395872e-08, 1.0835145621927861e-07]}, "mutation_prompt": null}
