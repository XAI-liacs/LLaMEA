{"id": "7bc3f1c4-c656-4b37-8529-699820832d0c", "solution": "import numpy as np\n\nclass AdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_score = float('inf')\n        evaluations = 0\n        step_size = (ub - lb) / 10\n\n        # Initial random solution\n        current_solution = np.random.uniform(lb, ub, self.dim)\n        current_score = func(current_solution)\n        evaluations += 1\n\n        while evaluations < self.budget:\n            # Generate candidate solution by random sampling within step size bounds\n            candidate_solution = current_solution + np.random.uniform(-step_size, step_size, self.dim)\n            candidate_solution = np.clip(candidate_solution, lb, ub)\n            candidate_score = func(candidate_solution)\n            evaluations += 1\n\n            # If candidate solution is better, adopt it and reduce step size\n            if candidate_score < current_score:\n                current_solution = candidate_solution\n                current_score = candidate_score\n                step_size *= 0.9  # Reduce step size\n            else:\n                # If no improvement, increase step size\n                step_size *= 1.1\n\n            # Update best solution found so far\n            if current_score < best_score:\n                best_solution = current_solution\n                best_score = current_score\n\n        return best_solution", "name": "AdaptiveRandomSearch", "description": "The algorithm explores the search space using adaptive random sampling and dynamic step size adjustments to efficiently converge to high-performing solutions.", "configspace": "", "generation": 0, "fitness": 0.11743483409714983, "feedback": "The algorithm AdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.117 with standard deviation 0.033. And the mean value of best solutions found was 3.792 (0. is the best) with standard deviation 2.989.", "error": "", "parent_id": null, "metadata": {"aucs": [0.15383754936115457, 0.07344115901878212, 0.1250257939115128], "final_y": [1.265309889269938, 7.9899663647764285, 2.1204551922747408]}, "mutation_prompt": null}
{"id": "c9fe04a3-f953-471f-b0c9-2eaa91f0c80a", "solution": "import numpy as np\n\nclass AdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_score = float('inf')\n        evaluations = 0\n        step_size = (ub - lb) / 10\n\n        # Initial random solution\n        current_solution = np.random.uniform(lb, ub, self.dim)\n        current_score = func(current_solution)\n        evaluations += 1\n\n        while evaluations < self.budget:\n            # Generate candidate solution by random sampling within step size bounds\n            candidate_solution = current_solution + np.random.uniform(-step_size, step_size, self.dim)\n            candidate_solution = np.clip(candidate_solution, lb, ub)\n            candidate_score = func(candidate_solution)\n            evaluations += 1\n\n            # If candidate solution is better, adopt it and reduce step size\n            if candidate_score < current_score:\n                current_solution = candidate_solution\n                current_score = candidate_score\n                step_size *= 0.85  # Reduce step size more aggressively\n            else:\n                # If no improvement, increase step size\n                step_size *= 1.15  # Increase step size slightly more\n\n            # Update best solution found so far\n            if current_score < best_score:\n                best_solution = current_solution\n                best_score = current_score\n\n        return best_solution", "name": "AdaptiveRandomSearch", "description": "The algorithm explores the search space using adaptive random sampling and employs a dynamic learning rate adjustment to efficiently converge to high-performing solutions.", "configspace": "", "generation": 1, "fitness": 0.1279016847601254, "feedback": "The algorithm AdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.128 with standard deviation 0.018. And the mean value of best solutions found was 2.493 (0. is the best) with standard deviation 0.942.", "error": "", "parent_id": "7bc3f1c4-c656-4b37-8529-699820832d0c", "metadata": {"aucs": [0.1166988188461946, 0.1140760855935099, 0.15293014984067166], "final_y": [3.04378551607855, 3.2677871041692472, 1.1677548288548487]}, "mutation_prompt": null}
{"id": "564b5ee2-f8ec-435c-ab3f-454df36d0772", "solution": "import numpy as np\n\nclass AdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_score = float('inf')\n        evaluations = 0\n        step_size = (ub - lb) / 10\n        momentum = np.zeros(self.dim)  # Initialize momentum\n\n        # Initial random solution\n        current_solution = np.random.uniform(lb, ub, self.dim)\n        current_score = func(current_solution)\n        evaluations += 1\n\n        while evaluations < self.budget:\n            # Generate candidate solution by random sampling with momentum\n            candidate_solution = current_solution + momentum + np.random.uniform(-step_size, step_size, self.dim)\n            candidate_solution = np.clip(candidate_solution, lb, ub)\n            candidate_score = func(candidate_solution)\n            evaluations += 1\n\n            # If candidate solution is better, adopt it and reduce step size\n            if candidate_score < current_score:\n                momentum = 0.2 * (candidate_solution - current_solution)  # Update momentum\n                current_solution = candidate_solution\n                current_score = candidate_score\n                step_size *= 0.85  # Reduce step size more aggressively\n            else:\n                # If no improvement, increase step size\n                step_size *= 1.15  # Increase step size slightly more\n\n            # Update best solution found so far\n            if current_score < best_score:\n                best_solution = current_solution\n                best_score = current_score\n\n        return best_solution", "name": "AdaptiveRandomSearch", "description": "The algorithm adds a momentum-like mechanism to guide the search direction based on previous successful moves, facilitating faster convergence.", "configspace": "", "generation": 2, "fitness": 0.12630551495944653, "feedback": "The algorithm AdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.126 with standard deviation 0.019. And the mean value of best solutions found was 2.632 (0. is the best) with standard deviation 1.308.", "error": "", "parent_id": "c9fe04a3-f953-471f-b0c9-2eaa91f0c80a", "metadata": {"aucs": [0.13803856199122466, 0.09998311280800709, 0.14089487007910784], "final_y": [1.8880859372016696, 4.471079130658889, 1.5364843733408948]}, "mutation_prompt": null}
{"id": "4109547e-97f7-49f3-b5b9-2c653c4bc40f", "solution": "import numpy as np\n\nclass AdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_score = float('inf')\n        evaluations = 0\n        step_size = (ub - lb) / 10\n\n        # Initial random solution\n        current_solution = np.random.uniform(lb, ub, self.dim)\n        current_score = func(current_solution)\n        evaluations += 1\n\n        while evaluations < self.budget:\n            # Generate candidate solution using Gaussian perturbation\n            candidate_solution = current_solution + np.random.normal(0, step_size, self.dim)\n            candidate_solution = np.clip(candidate_solution, lb, ub)\n            candidate_score = func(candidate_solution)\n            evaluations += 1\n\n            # If candidate solution is better, adopt it and reduce step size\n            if candidate_score < current_score:\n                current_solution = candidate_solution\n                current_score = candidate_score\n                step_size *= 0.90  # Adjust step size (less aggressive)\n            else:\n                # If no improvement, increase step size\n                step_size *= 1.10  # Adjust step size (less aggressive)\n\n            # Update best solution found so far\n            if current_score < best_score:\n                best_solution = current_solution\n                best_score = current_score\n\n        return best_solution", "name": "AdaptiveRandomSearch", "description": "The algorithm improves local exploration by incorporating Gaussian perturbations and balance in step size adjustment for optimized convergence.", "configspace": "", "generation": 3, "fitness": 0.11675188870609088, "feedback": "The algorithm AdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.117 with standard deviation 0.026. And the mean value of best solutions found was 3.562 (0. is the best) with standard deviation 2.019.", "error": "", "parent_id": "c9fe04a3-f953-471f-b0c9-2eaa91f0c80a", "metadata": {"aucs": [0.0842518309208572, 0.11855212441458629, 0.14745171078282915], "final_y": [6.282968227801534, 2.9508475007117534, 1.4513845854756884]}, "mutation_prompt": null}
{"id": "0967aec2-5fda-4958-880b-d852e8ad4329", "solution": "import numpy as np\n\nclass AdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_score = float('inf')\n        evaluations = 0\n        step_size = (ub - lb) / 10\n\n        while evaluations < self.budget:\n            # Random restart\n            current_solution = np.random.uniform(lb, ub, self.dim) if evaluations % 10 == 0 else current_solution\n            current_score = func(current_solution)\n            evaluations += 1\n\n            # Generate candidate solution by random sampling within step size bounds\n            candidate_solution = current_solution + np.random.uniform(-step_size, step_size, self.dim)\n            candidate_solution = np.clip(candidate_solution, lb, ub)\n            candidate_score = func(candidate_solution)\n            evaluations += 1\n\n            # If candidate solution is better, adopt it and reduce step size\n            if candidate_score < current_score:\n                current_solution = candidate_solution\n                current_score = candidate_score\n                step_size *= 0.85  # Reduce step size more aggressively\n            else:\n                # If no improvement, increase step size\n                step_size *= 1.15  # Increase step size slightly more\n\n            # Update best solution found so far\n            if current_score < best_score:\n                best_solution = current_solution\n                best_score = current_score\n\n        return best_solution", "name": "AdaptiveRandomSearch", "description": "The algorithm employs adaptive random sampling and dynamic learning rate adjustment, with additional random restarts to escape local optima efficiently.", "configspace": "", "generation": 4, "fitness": 0.17185006343012052, "feedback": "The algorithm AdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.172 with standard deviation 0.016. And the mean value of best solutions found was 0.773 (0. is the best) with standard deviation 0.290.", "error": "", "parent_id": "c9fe04a3-f953-471f-b0c9-2eaa91f0c80a", "metadata": {"aucs": [0.19389214057019621, 0.15773331709323046, 0.16392473262693485], "final_y": [0.47874656705784996, 1.1672698170855234, 0.6744491162810997]}, "mutation_prompt": null}
{"id": "eb6a86cf-0538-4e9b-aa26-fa4dd19ed00c", "solution": "import numpy as np\n\nclass AdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_score = float('inf')\n        evaluations = 0\n        step_size = (ub - lb) / 10\n        restart_interval = 10\n        annealing_rate = 0.95\n\n        while evaluations < self.budget:\n            # Random restart with adaptive interval\n            if evaluations % restart_interval == 0:\n                current_solution = np.random.uniform(lb, ub, self.dim)\n                step_size = (ub - lb) / 10  # Reset step size\n            current_score = func(current_solution)\n            evaluations += 1\n\n            # Generate candidate solution by random sampling within step size bounds\n            candidate_solution = current_solution + np.random.uniform(-step_size, step_size, self.dim)\n            candidate_solution = np.clip(candidate_solution, lb, ub)\n            candidate_score = func(candidate_solution)\n            evaluations += 1\n\n            # If candidate solution is better, adopt it\n            if candidate_score < current_score:\n                current_solution = candidate_solution\n                current_score = candidate_score\n                step_size *= annealing_rate  # Apply annealing to reduce step size\n                restart_interval = max(5, restart_interval - 1)  # Decrease restart interval\n            else:\n                # If no improvement, increase step size\n                step_size *= 1.15\n\n            # Update best solution found so far\n            if current_score < best_score:\n                best_solution = current_solution\n                best_score = current_score\n\n        return best_solution", "name": "AdaptiveRandomSearch", "description": "The algorithm incorporates a dynamic step size adjustment mechanism with annealing-inspired exploration and adaptive restart intervals for enhanced convergence.", "configspace": "", "generation": 5, "fitness": 0.1735959236800685, "feedback": "The algorithm AdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.174 with standard deviation 0.023. And the mean value of best solutions found was 0.324 (0. is the best) with standard deviation 0.143.", "error": "", "parent_id": "0967aec2-5fda-4958-880b-d852e8ad4329", "metadata": {"aucs": [0.14125663738663252, 0.19225871578491716, 0.18727241786865578], "final_y": [0.5068950931527885, 0.3091915719854101, 0.15640709896357732]}, "mutation_prompt": null}
{"id": "64993709-1b06-426c-a733-ca9e350162cd", "solution": "import numpy as np\n\nclass AdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_score = float('inf')\n        evaluations = 0\n        step_size = (ub - lb) / 10\n        restart_interval = 10\n        annealing_rate = 0.95\n\n        while evaluations < self.budget:\n            if evaluations % restart_interval == 0:\n                current_solution = np.random.uniform(lb, ub, self.dim)\n                step_size = (ub - lb) / 10\n\n            current_score = func(current_solution)\n            evaluations += 1\n\n            candidate_solution = current_solution + np.random.uniform(-step_size, step_size, self.dim)\n            candidate_solution = np.clip(candidate_solution, lb, ub)\n            candidate_score = func(candidate_solution)\n            evaluations += 1\n\n            acceptance_probability = np.exp((current_score - candidate_score) / step_size)  # New line for stochastic acceptance\n            if candidate_score < current_score or np.random.rand() < acceptance_probability:  # Changed line for acceptance criteria\n                current_solution = candidate_solution\n                current_score = candidate_score\n                step_size *= annealing_rate\n                restart_interval = max(5, restart_interval - 1)\n            else:\n                step_size *= 1.15\n\n            if current_score < best_score:\n                best_solution = current_solution\n                best_score = current_score\n\n        return best_solution", "name": "AdaptiveRandomSearch", "description": "The algorithm utilizes dynamic step size tuning with annealing-inspired exploration, adaptive restart intervals, and a novel stochastic acceptance criterion for enhanced convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_id": "eb6a86cf-0538-4e9b-aa26-fa4dd19ed00c", "metadata": {}, "mutation_prompt": null}
{"id": "932d1940-4b45-45f3-ba3e-47330ec0643d", "solution": "import numpy as np\n\nclass HarmonySearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 5\n        self.harmony_memory = []\n        self.harmony_memory_consideration_rate = 0.9\n        self.pitch_adjustment_rate = 0.3\n        self.bandwidth = 0.1\n\n    def initialize_harmony_memory(self, lb, ub):\n        for _ in range(self.harmony_memory_size):\n            harmony = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.harmony_memory.append((harmony, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony_memory(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n\n            for i in range(self.dim):\n                if np.random.rand() < self.harmony_memory_consideration_rate:\n                    selected_harmony = self.harmony_memory[np.random.randint(self.harmony_memory_size)][0]\n                    new_harmony[i] = selected_harmony[i]\n\n                    if np.random.rand() < self.pitch_adjustment_rate:\n                        new_harmony[i] += self.bandwidth * (np.random.rand() - 0.5) * (ub[i] - lb[i])\n                else:\n                    new_harmony[i] = np.random.uniform(lb[i], ub[i])\n\n            new_harmony = np.clip(new_harmony, lb, ub)\n            new_score = func(new_harmony)\n            evaluations += 1\n\n            worst_index = np.argmax([score for _, score in self.harmony_memory])\n            if new_score < self.harmony_memory[worst_index][1]:\n                self.harmony_memory[worst_index] = (new_harmony, new_score)\n\n            if new_score < best_score:\n                best_solution = new_harmony\n                best_score = new_score\n\n        return best_solution", "name": "HarmonySearchOptimizer", "description": "Harmony Search Inspired Optimization uses a harmony memory mechanism and dynamic pitch adjustment to balance exploration and exploitation for black box optimization.", "configspace": "", "generation": 7, "fitness": 0.22002884826481237, "feedback": "The algorithm HarmonySearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.220 with standard deviation 0.033. And the mean value of best solutions found was 0.085 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "eb6a86cf-0538-4e9b-aa26-fa4dd19ed00c", "metadata": {"aucs": [0.24811602246485653, 0.23784144672244512, 0.17412907560713542], "final_y": [0.06562283911147887, 0.08476895294587558, 0.10554429085871066]}, "mutation_prompt": null}
{"id": "e99b5228-ac5a-49d4-b85a-28e22ef754c1", "solution": "import numpy as np\n\nclass HarmonySearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 5\n        self.harmony_memory = []\n        self.harmony_memory_consideration_rate = 0.9\n        self.pitch_adjustment_rate = 0.2  # Changed from 0.3\n        self.bandwidth = 0.1\n\n    def initialize_harmony_memory(self, lb, ub):\n        for _ in range(self.harmony_memory_size):\n            harmony = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.harmony_memory.append((harmony, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony_memory(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n\n            for i in range(self.dim):\n                if np.random.rand() < self.harmony_memory_consideration_rate:\n                    selected_harmony = self.harmony_memory[np.random.randint(self.harmony_memory_size)][0]\n                    new_harmony[i] = selected_harmony[i]\n\n                    if np.random.rand() < self.pitch_adjustment_rate:\n                        diversity = np.std([h[0][i] for h in self.harmony_memory])  # New line\n                        new_harmony[i] += self.bandwidth * (np.random.rand() - 0.5) * diversity  # Changed from (ub[i] - lb[i])\n                else:\n                    new_harmony[i] = np.random.uniform(lb[i], ub[i])\n\n            new_harmony = np.clip(new_harmony, lb, ub)\n            new_score = func(new_harmony)\n            evaluations += 1\n\n            worst_index = np.argmax([score for _, score in self.harmony_memory])\n            if new_score < self.harmony_memory[worst_index][1]:\n                self.harmony_memory[worst_index] = (new_harmony, new_score)\n\n            if new_score < best_score:\n                best_solution = new_harmony\n                best_score = new_score\n\n        return best_solution", "name": "HarmonySearchOptimizer", "description": "Enhanced Harmony Search Optimization by adjusting pitch dynamically based on solution diversity to improve convergence.", "configspace": "", "generation": 8, "fitness": 0.12724230297294106, "feedback": "The algorithm HarmonySearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.127 with standard deviation 0.025. And the mean value of best solutions found was 2.447 (0. is the best) with standard deviation 1.503.", "error": "", "parent_id": "932d1940-4b45-45f3-ba3e-47330ec0643d", "metadata": {"aucs": [0.10666508261401697, 0.11217694067486894, 0.1628848856299373], "final_y": [3.7949534334747788, 3.196045743400899, 0.3492580962629183]}, "mutation_prompt": null}
{"id": "42ff4d09-599c-4901-9281-544cae27cccf", "solution": "import numpy as np\n\nclass HarmonySearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 5\n        self.harmony_memory = []\n        self.harmony_memory_consideration_rate = 0.9\n        self.pitch_adjustment_rate = 0.3\n        self.bandwidth = 0.1\n\n    def initialize_harmony_memory(self, lb, ub):\n        for _ in range(self.harmony_memory_size):\n            harmony = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.harmony_memory.append((harmony, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony_memory(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n\n            for i in range(self.dim):\n                if np.random.rand() < self.harmony_memory_consideration_rate:\n                    selected_harmony = self.harmony_memory[np.random.randint(self.harmony_memory_size)][0]\n                    new_harmony[i] = selected_harmony[i]\n\n                    if np.random.rand() < self.pitch_adjustment_rate:\n                        self.bandwidth *= 0.99  # Adaptive bandwidth adjustment\n                        new_harmony[i] += self.bandwidth * (np.random.rand() - 0.5) * (ub[i] - lb[i])\n                else:\n                    new_harmony[i] = np.random.uniform(lb[i], ub[i])\n\n            new_harmony = np.clip(new_harmony, lb, ub)\n            new_score = func(new_harmony)\n            evaluations += 1\n\n            worst_index = np.argmax([score for _, score in self.harmony_memory])\n            if new_score < self.harmony_memory[worst_index][1]:\n                self.harmony_memory[worst_index] = (new_harmony, new_score)\n\n            if new_score < best_score:\n                best_solution = new_harmony\n                best_score = new_score\n\n        return best_solution", "name": "HarmonySearchOptimizer", "description": "Enhanced Harmony Search Optimizer using adaptive bandwidth adjustment for improved exploration-exploitation balance.", "configspace": "", "generation": 9, "fitness": 0.39636904660269456, "feedback": "The algorithm HarmonySearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.396 with standard deviation 0.041. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "932d1940-4b45-45f3-ba3e-47330ec0643d", "metadata": {"aucs": [0.3488283211128371, 0.44849674145574936, 0.3917820772394971], "final_y": [0.0003383587187065238, 0.0001413506243396507, 0.0006709900063198252]}, "mutation_prompt": null}
{"id": "0ff4c98e-9dc7-4ef4-9b82-4a790c4459ce", "solution": "import numpy as np\n\nclass HarmonySearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 5\n        self.harmony_memory = []\n        self.harmony_memory_consideration_rate = 0.9\n        self.pitch_adjustment_rate = 0.4  # Increased pitch adjustment rate\n        self.bandwidth = 0.1\n\n    def initialize_harmony_memory(self, lb, ub):\n        for _ in range(self.harmony_memory_size):\n            harmony = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.harmony_memory.append((harmony, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony_memory(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n\n            for i in range(self.dim):\n                if np.random.rand() < self.harmony_memory_consideration_rate:\n                    selected_harmony = self.harmony_memory[np.random.randint(self.harmony_memory_size)][0]\n                    new_harmony[i] = selected_harmony[i]\n\n                    if np.random.rand() < self.pitch_adjustment_rate:\n                        self.bandwidth *= 0.98  # More aggressive adaptive bandwidth adjustment\n                        new_harmony[i] += self.bandwidth * (np.random.rand() - 0.5) * (ub[i] - lb[i])\n                else:\n                    new_harmony[i] = np.random.uniform(lb[i], ub[i])\n\n            new_harmony = np.clip(new_harmony, lb, ub)\n            new_score = func(new_harmony)\n            evaluations += 1\n\n            worst_index = np.argmax([score for _, score in self.harmony_memory])\n            if new_score < self.harmony_memory[worst_index][1]:\n                self.harmony_memory[worst_index] = (new_harmony, new_score)\n\n            if new_score < best_score:\n                best_solution = new_harmony\n                best_score = new_score\n\n        return best_solution", "name": "HarmonySearchOptimizer", "description": "Improved Harmony Search Optimizer with dynamic pitch adjustment for enhanced balance.", "configspace": "", "generation": 10, "fitness": 0.13045014799718438, "feedback": "The algorithm HarmonySearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.130 with standard deviation 0.023. And the mean value of best solutions found was 2.357 (0. is the best) with standard deviation 0.988.", "error": "", "parent_id": "42ff4d09-599c-4901-9281-544cae27cccf", "metadata": {"aucs": [0.10961114883608236, 0.11984206935661756, 0.16189722579885324], "final_y": [3.4294870542061613, 2.596282362131717, 1.0443135949019202]}, "mutation_prompt": null}
{"id": "7cea0963-8a22-48c4-a79d-cef5aa35f8a9", "solution": "import numpy as np\n\nclass ImprovedHarmonySearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 5\n        self.harmony_memory = []\n        self.harmony_memory_consideration_rate = 0.9\n        self.pitch_adjustment_rate = 0.3\n        self.bandwidth = 0.1\n        self.diversity_factor = 0.5\n\n    def initialize_harmony_memory(self, lb, ub):\n        for _ in range(self.harmony_memory_size):\n            harmony = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.harmony_memory.append((harmony, score))\n\n    def update_memory_size(self, evaluations, budget):\n        # Dynamically modify memory size based on progress\n        progress = evaluations / budget\n        self.harmony_memory_size = max(3, int(5 + 5 * progress))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony_memory(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            self.update_memory_size(evaluations, self.budget)\n            new_harmony = np.zeros(self.dim)\n\n            for i in range(self.dim):\n                if np.random.rand() < self.harmony_memory_consideration_rate:\n                    selected_harmony = self.harmony_memory[np.random.randint(self.harmony_memory_size)][0]\n                    new_harmony[i] = selected_harmony[i]\n\n                    if np.random.rand() < self.pitch_adjustment_rate:\n                        self.bandwidth *= 0.99\n                        new_harmony[i] += self.bandwidth * (np.random.rand() - 0.5) * (ub[i] - lb[i])\n                else:\n                    new_harmony[i] = np.random.uniform(lb[i], ub[i])\n\n            # Promote diversity by adding a small random shift\n            if np.random.rand() < self.diversity_factor:\n                new_harmony += np.random.normal(0, 0.01 * (ub - lb), self.dim)\n\n            new_harmony = np.clip(new_harmony, lb, ub)\n            new_score = func(new_harmony)\n            evaluations += 1\n\n            worst_index = np.argmax([score for _, score in self.harmony_memory])\n            if new_score < self.harmony_memory[worst_index][1]:\n                self.harmony_memory[worst_index] = (new_harmony, new_score)\n\n            if new_score < best_score:\n                best_solution = new_harmony\n                best_score = new_score\n\n        return best_solution", "name": "ImprovedHarmonySearchOptimizer", "description": "Improved Harmony Search Optimizer with dynamic memory size and diversity promotion for enhanced global search capability.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "42ff4d09-599c-4901-9281-544cae27cccf", "metadata": {}, "mutation_prompt": null}
{"id": "2173c8b0-4d6c-488d-b88f-3658c9d32f24", "solution": "import numpy as np\n\nclass HarmonySearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 5\n        self.harmony_memory = []\n        self.harmony_memory_consideration_rate = 0.9\n        self.pitch_adjustment_rate = 0.3\n        self.bandwidth = 0.1\n\n    def initialize_harmony_memory(self, lb, ub):\n        for _ in range(self.harmony_memory_size):\n            harmony = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.harmony_memory.append((harmony, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony_memory(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n\n            for i in range(self.dim):\n                if np.random.rand() < self.harmony_memory_consideration_rate:\n                    selected_harmony = self.harmony_memory[np.random.randint(self.harmony_memory_size)][0]\n                    new_harmony[i] = selected_harmony[i]\n\n                    if np.random.rand() < self.pitch_adjustment_rate:\n                        self.bandwidth *= 0.99  # Adaptive bandwidth adjustment\n                        new_harmony[i] += self.bandwidth * (np.random.rand() - 0.5) * (ub[i] - lb[i])\n                else:\n                    new_harmony[i] = np.random.uniform(lb[i], ub[i])\n\n            new_harmony = np.clip(new_harmony, lb, ub)\n            new_score = func(new_harmony)\n            evaluations += 1\n\n            worst_index = np.argmax([score for _, score in self.harmony_memory])\n            if new_score < self.harmony_memory[worst_index][1]:\n                self.harmony_memory_size = min(self.harmony_memory_size + 1, 10) # Adaptive memory size\n                self.harmony_memory[worst_index] = (new_harmony, new_score)\n\n            if new_score < best_score:\n                best_solution = new_harmony\n                best_score = new_score\n\n        return best_solution", "name": "HarmonySearchOptimizer", "description": "Enhanced Harmony Search Optimizer with adaptive harmony memory size for improved dynamic exploration-exploitation balance.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "42ff4d09-599c-4901-9281-544cae27cccf", "metadata": {}, "mutation_prompt": null}
{"id": "d827cf36-a3a6-4c36-a627-3362c5d98f2c", "solution": "import numpy as np\n\nclass HarmonySearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 5\n        self.harmony_memory = []\n        self.harmony_memory_consideration_rate = 0.9\n        self.pitch_adjustment_rate = 0.3\n        self.bandwidth = 0.1\n\n    def initialize_harmony_memory(self, lb, ub):\n        for _ in range(self.harmony_memory_size):\n            harmony = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.harmony_memory.append((harmony, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony_memory(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n\n            for i in range(self.dim):\n                if np.random.rand() < self.harmony_memory_consideration_rate:\n                    selected_harmony = self.harmony_memory[np.random.randint(self.harmony_memory_size)][0]\n                    new_harmony[i] = selected_harmony[i]\n\n                    if np.random.rand() < self.pitch_adjustment_rate:\n                        self.bandwidth *= 0.98  # Slightly more aggressive adaptive bandwidth adjustment\n                        new_harmony[i] += self.bandwidth * (np.random.rand() - 0.5) * (ub[i] - lb[i])\n                else:\n                    new_harmony[i] = np.random.uniform(lb[i], ub[i])\n\n            new_harmony = np.clip(new_harmony, lb, ub)\n            new_score = func(new_harmony)\n            evaluations += 1\n\n            worst_index = np.argmax([score for _, score in self.harmony_memory])\n            if new_score < self.harmony_memory[worst_index][1]:\n                self.harmony_memory[worst_index] = (new_harmony, new_score)\n\n            if new_score < best_score:\n                best_solution = new_harmony\n                best_score = new_score\n\n        return best_solution", "name": "HarmonySearchOptimizer", "description": "Slightly enhanced adaptive bandwidth adjustment to boost exploration-exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.20940638137212617, "feedback": "The algorithm HarmonySearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.209 with standard deviation 0.082. And the mean value of best solutions found was 1.024 (0. is the best) with standard deviation 1.120.", "error": "", "parent_id": "42ff4d09-599c-4901-9281-544cae27cccf", "metadata": {"aucs": [0.31851852390633695, 0.12035872446230211, 0.1893418957477394], "final_y": [0.01127314986634951, 2.585282783753228, 0.4755978129655737]}, "mutation_prompt": null}
{"id": "77f652de-1ac1-443f-9582-9b2aa5c5e8a5", "solution": "import numpy as np\n\nclass HarmonySearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 5\n        self.harmony_memory = []\n        self.harmony_memory_consideration_rate = 0.9\n        self.pitch_adjustment_rate = 0.3\n        self.bandwidth = 0.1\n\n    def initialize_harmony_memory(self, lb, ub):\n        for _ in range(self.harmony_memory_size):\n            harmony = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.harmony_memory.append((harmony, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony_memory(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n\n            for i in range(self.dim):\n                if np.random.rand() < self.harmony_memory_consideration_rate:\n                    selected_harmony = self.harmony_memory[np.random.randint(self.harmony_memory_size)][0]\n                    new_harmony[i] = selected_harmony[i]\n\n                    if np.random.rand() < self.pitch_adjustment_rate:\n                        self.bandwidth *= 0.99  # Adaptive bandwidth adjustment\n                        new_harmony[i] += self.bandwidth * (np.random.rand() - 0.5) * (ub[i] - lb[i])\n                else:\n                    new_harmony[i] = np.random.uniform(lb[i], ub[i])\n                    \n            if np.random.rand() < 0.01:  # Global random search chance\n                new_harmony = np.random.uniform(lb, ub, self.dim)\n\n            new_harmony = np.clip(new_harmony, lb, ub)\n            new_score = func(new_harmony)\n            evaluations += 1\n\n            worst_index = np.argmax([score for _, score in self.harmony_memory])\n            if new_score < self.harmony_memory[worst_index][1]:\n                self.harmony_memory[worst_index] = (new_harmony, new_score)\n\n            if new_score < best_score:\n                best_solution = new_harmony\n                best_score = new_score\n\n        return best_solution", "name": "HarmonySearchOptimizer", "description": "Improved exploration by introducing a small probability of global random search addition to the Harmony Search.", "configspace": "", "generation": 14, "fitness": 0.1479649179943531, "feedback": "The algorithm HarmonySearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.148 with standard deviation 0.013. And the mean value of best solutions found was 1.125 (0. is the best) with standard deviation 0.430.", "error": "", "parent_id": "42ff4d09-599c-4901-9281-544cae27cccf", "metadata": {"aucs": [0.16381208820722626, 0.1310276928434465, 0.14905497293238656], "final_y": [0.6469326887214379, 1.6887000039307254, 1.040052878268798]}, "mutation_prompt": null}
{"id": "ad85660c-b27f-4658-bf88-7e39c5894c2c", "solution": "import numpy as np\n\nclass HarmonySearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 5\n        self.harmony_memory = []\n        self.harmony_memory_consideration_rate = 0.9\n        self.pitch_adjustment_rate = 0.3\n        self.initial_bandwidth = 0.1\n        self.bandwidth = self.initial_bandwidth\n\n    def initialize_harmony_memory(self, lb, ub):\n        for _ in range(self.harmony_memory_size + np.random.randint(-2, 3)):  # Dynamic memory size\n            harmony = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.harmony_memory.append((harmony, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony_memory(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n\n            for i in range(self.dim):\n                if np.random.rand() < self.harmony_memory_consideration_rate:\n                    selected_harmony = self.harmony_memory[np.random.randint(self.harmony_memory_size)][0]\n                    new_harmony[i] = selected_harmony[i]\n\n                    if np.random.rand() < self.pitch_adjustment_rate:\n                        self.bandwidth = max(self.bandwidth * 0.98, 0.01)  # Improved adaptive bandwidth strategy\n                        new_harmony[i] += self.bandwidth * (np.random.randn()) * (ub[i] - lb[i])  # Gaussian noise\n                else:\n                    new_harmony[i] = np.random.uniform(lb[i], ub[i])\n\n            new_harmony = np.clip(new_harmony, lb, ub)\n            new_score = func(new_harmony)\n            evaluations += 1\n\n            worst_index = np.argmax([score for _, score in self.harmony_memory])\n            if new_score < self.harmony_memory[worst_index][1]:\n                self.harmony_memory[worst_index] = (new_harmony, new_score)\n\n            if new_score < best_score:\n                best_solution = new_harmony\n                best_score = new_score\n\n        return best_solution", "name": "HarmonySearchOptimizer", "description": "Modified Harmony Search Optimizer with dynamic memory size and improved adaptive bandwidth strategy.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "42ff4d09-599c-4901-9281-544cae27cccf", "metadata": {}, "mutation_prompt": null}
{"id": "7094c113-67fb-40bc-878c-adca3226787e", "solution": "import numpy as np\n\nclass HarmonySearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 5\n        self.harmony_memory = []\n        self.harmony_memory_consideration_rate = 0.9\n        self.pitch_adjustment_rate = 0.3\n        self.bandwidth = 0.1\n        self.de_mutation_factor = 0.8  # Added for Differential Evolution\n\n    def initialize_harmony_memory(self, lb, ub):\n        for _ in range(self.harmony_memory_size):\n            harmony = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.harmony_memory.append((harmony, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony_memory(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n\n            for i in range(self.dim):\n                if np.random.rand() < self.harmony_memory_consideration_rate:\n                    selected_harmony = self.harmony_memory[np.random.randint(self.harmony_memory_size)][0]\n                    new_harmony[i] = selected_harmony[i]\n\n                    if np.random.rand() < self.pitch_adjustment_rate:\n                        self.bandwidth *= 0.99\n                        new_harmony[i] += self.bandwidth * (np.random.rand() - 0.5) * (ub[i] - lb[i])\n                else:\n                    indices = np.random.choice(self.harmony_memory_size, 3, replace=False)\n                    a, b, c = [self.harmony_memory[idx][0] for idx in indices]\n                    mutant = a + self.de_mutation_factor * (b - c)  # Differential Evolution mutation\n                    new_harmony[i] = np.clip(mutant[i], lb[i], ub[i])  # Apply DE mutation\n\n            new_harmony = np.clip(new_harmony, lb, ub)\n            new_score = func(new_harmony)\n            evaluations += 1\n\n            worst_index = np.argmax([score for _, score in self.harmony_memory])\n            if new_score < self.harmony_memory[worst_index][1]:\n                self.harmony_memory[worst_index] = (new_harmony, new_score)\n\n            if new_score < best_score:\n                best_solution = new_harmony\n                best_score = new_score\n\n        return best_solution", "name": "HarmonySearchOptimizer", "description": "Hybrid Harmony Search optimizer enhanced with Differential Evolution mutation for better exploration.", "configspace": "", "generation": 16, "fitness": 0.24841165236500942, "feedback": "The algorithm HarmonySearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.077. And the mean value of best solutions found was 0.215 (0. is the best) with standard deviation 0.292.", "error": "", "parent_id": "42ff4d09-599c-4901-9281-544cae27cccf", "metadata": {"aucs": [0.2570128984659996, 0.1500169253789816, 0.3382051332500471], "final_y": [0.015410368785424953, 0.6281283859485157, 0.0015958272712825042]}, "mutation_prompt": null}
{"id": "e96417dd-06b0-4ee2-ab0b-27d5ebbb4bcf", "solution": "import numpy as np\n\nclass HarmonySearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 5\n        self.harmony_memory = []\n        self.harmony_memory_consideration_rate = 0.9\n        self.pitch_adjustment_rate = 0.3\n        self.bandwidth = 0.1\n\n    def initialize_harmony_memory(self, lb, ub):\n        for _ in range(self.harmony_memory_size):\n            harmony = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.harmony_memory.append((harmony, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony_memory(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n\n            for i in range(self.dim):\n                if np.random.rand() < self.harmony_memory_consideration_rate:\n                    selected_harmony = self.harmony_memory[np.random.randint(self.harmony_memory_size)][0]\n                    new_harmony[i] = selected_harmony[i]\n\n                    if np.random.rand() < self.pitch_adjustment_rate:\n                        self.bandwidth *= 0.99  # Adaptive bandwidth adjustment\n                        new_harmony[i] += self.bandwidth * (np.random.rand() - 0.5) * (ub[i] - lb[i])\n                else:\n                    new_harmony[i] = np.random.uniform(lb[i], ub[i])\n\n            new_harmony = np.clip(new_harmony, lb, ub)\n            new_score = func(new_harmony)\n            evaluations += 1\n\n            # Adjust consideration rate based on performance\n            if new_score < best_score:\n                self.harmony_memory_consideration_rate = min(self.harmony_memory_consideration_rate + 0.01, 1.0)\n\n            worst_index = np.argmax([score for _, score in self.harmony_memory])\n            if new_score < self.harmony_memory[worst_index][1]:\n                self.harmony_memory[worst_index] = (new_harmony, new_score)\n\n            if new_score < best_score:\n                best_solution = new_harmony\n                best_score = new_score\n\n        return best_solution", "name": "HarmonySearchOptimizer", "description": "Enhanced Harmony Search Optimizer with dynamic memory consideration rate based on performance feedback.", "configspace": "", "generation": 17, "fitness": 0.26542518307768875, "feedback": "The algorithm HarmonySearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.265 with standard deviation 0.125. And the mean value of best solutions found was 1.494 (0. is the best) with standard deviation 2.111.", "error": "", "parent_id": "42ff4d09-599c-4901-9281-544cae27cccf", "metadata": {"aucs": [0.08858523052841227, 0.3631671547953893, 0.34452316390926474], "final_y": [4.4792281452573155, 0.0007638673524289712, 0.0008417987521861125]}, "mutation_prompt": null}
{"id": "2adad702-6af0-45e6-8edb-0757704ef83d", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                mutant_vector = x1 + self.f * (x2 - x3)\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n\n                # Adaptive parameter control\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Differential Evolution with Adaptive Parameter Control for robust convergence on diverse optimization landscapes.", "configspace": "", "generation": 18, "fitness": 0.44266648803003905, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.443 with standard deviation 0.237. And the mean value of best solutions found was 0.759 (0. is the best) with standard deviation 1.073.", "error": "", "parent_id": "42ff4d09-599c-4901-9281-544cae27cccf", "metadata": {"aucs": [0.5508525757622809, 0.11344533745450092, 0.6637015508733355], "final_y": [4.3221010002264477e-10, 2.276773588211683, 3.048797729781263e-11]}, "mutation_prompt": null}
{"id": "7eb726a5-7006-4e7e-8d92-3f3355bfd672", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def levy_flight(self, lam=1.5):\n        sigma = (np.random.gamma(1 + lam) * np.sin(np.pi * lam / 2) /\n                 (np.random.gamma((1 + lam) / 2) * lam *\n                 2 ** ((lam - 1) / 2))) ** (1 / lam)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        return u / abs(v) ** (1 / lam)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                mutant_vector = x1 + self.f * (x2 - x3) + self.levy_flight()\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n\n                # Adaptive parameter control\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a Lévy flight mechanism for mutation and selection strategies to enhance exploration and avoid local optima.", "configspace": "", "generation": 19, "fitness": 0.19496596580670447, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.195 with standard deviation 0.026. And the mean value of best solutions found was 0.239 (0. is the best) with standard deviation 0.195.", "error": "", "parent_id": "2adad702-6af0-45e6-8edb-0757704ef83d", "metadata": {"aucs": [0.17119939746437296, 0.23078650790267685, 0.18291199205306363], "final_y": [0.4931312886353471, 0.019236789891402588, 0.20559342235065778]}, "mutation_prompt": null}
{"id": "c901c454-5a3b-4c8a-ab00-e2595ee5a6ce", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Changed from 10 to 20 for better exploration\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                mutant_vector = x1 + self.f * (x2 - x3)\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n\n                # Adaptive parameter control\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    self.population_size = int(self.population_size * 0.9)  # Adjusting population size\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with dynamic population size adjustment for improved exploration-exploitation balance.", "configspace": "", "generation": 20, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('list.remove(x): x not in list').", "error": "ValueError('list.remove(x): x not in list')", "parent_id": "2adad702-6af0-45e6-8edb-0757704ef83d", "metadata": {}, "mutation_prompt": null}
{"id": "6dcdbf15-ad15-4a8d-a216-e63f3845f0b6", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                mutant_vector = x1 + self.f * (x2 - x3)\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n\n                # Adaptive parameter control\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    self.population_size = max(5, self.population_size + int(np.random.choice([-1, 1])))\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhanced exploration by adjusting population size dynamically based on convergence.", "configspace": "", "generation": 21, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('list.remove(x): x not in list').", "error": "ValueError('list.remove(x): x not in list')", "parent_id": "2adad702-6af0-45e6-8edb-0757704ef83d", "metadata": {}, "mutation_prompt": null}
{"id": "dc5bfda9-cbb0-4dc1-b57c-3e2f50fddb02", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                mutant_vector = x1 + self.f * (x2 - x3)\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n\n                # Adaptive parameter control\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.8)  # Adjusted range for f\n                    self.cr = np.random.uniform(0.5, 1.0)\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with improved parameter adaptation based on convergence behavior.", "configspace": "", "generation": 22, "fitness": 0.26788967524823903, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.268 with standard deviation 0.258. And the mean value of best solutions found was 3.923 (0. is the best) with standard deviation 2.985.", "error": "", "parent_id": "2adad702-6af0-45e6-8edb-0757704ef83d", "metadata": {"aucs": [0.09566235575048776, 0.07596999423001527, 0.6320366757642141], "final_y": [4.53664934495913, 7.232900178696749, 1.7884862939542972e-10]}, "mutation_prompt": null}
{"id": "27511363-9830-40e2-970a-734724264847", "solution": "import numpy as np\n\nclass HybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.omega = 0.5  # Inertia weight for PSO\n        self.c1 = 1.5  # Cognitive component for PSO\n        self.c2 = 1.5  # Social component for PSO\n        self.population = []\n        self.velocities = []\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            velocity = np.random.uniform(-abs(ub-lb), abs(ub-lb), self.dim)\n            self.population.append((individual, score))\n            self.velocities.append(velocity)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n        global_best = np.random.uniform(lb, ub, self.dim)\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Differential Evolution step\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                mutant_vector = x1 + self.f * (x2 - x3)\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n\n                # Particle Swarm Optimization step\n                new_velocity = (self.omega * self.velocities[i] + \n                                self.c1 * np.random.rand(self.dim) * (target_vector - trial_vector) + \n                                self.c2 * np.random.rand(self.dim) * (global_best - trial_vector))\n                \n                new_position = trial_vector + new_velocity\n                new_position = np.clip(new_position, lb, ub)\n                new_score = func(new_position)\n                evaluations += 1\n\n                if new_score < trial_score:\n                    self.population[i] = (new_position, new_score)\n                    self.velocities[i] = new_velocity\n\n                    if new_score < best_score:\n                        best_solution = new_position\n                        best_score = new_score\n                        global_best = new_position\n\n                # Adaptive parameter control\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    self.omega = np.random.uniform(0.4, 0.9)\n\n        return best_solution", "name": "HybridDEPSO", "description": "Hybrid Differential Evolution and Particle Swarm Optimization with Adaptive Parameter control for enhanced exploration and exploitation balance.", "configspace": "", "generation": 23, "fitness": 0.3478416002096276, "feedback": "The algorithm HybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.348 with standard deviation 0.056. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2adad702-6af0-45e6-8edb-0757704ef83d", "metadata": {"aucs": [0.27108177782379483, 0.4023631229118837, 0.37007989989320444], "final_y": [0.0003897318596004742, 5.702264774696849e-05, 0.0002325150110274886]}, "mutation_prompt": null}
{"id": "4378a521-9ac2-4d40-bfb5-87541ea4c0ad", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n                \n                # Modify the mutation strategy by introducing weighted probability\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < 0.8 else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n\n                # Adaptive parameter control\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution is enhanced by introducing weighted probability for mutation to improve convergence.", "configspace": "", "generation": 24, "fitness": 0.5172024132546921, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.517 with standard deviation 0.101. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2adad702-6af0-45e6-8edb-0757704ef83d", "metadata": {"aucs": [0.38410565613944425, 0.6277808360225965, 0.5397207476020354], "final_y": [0.001003442688939312, 6.845003853741748e-10, 6.685690814385758e-10]}, "mutation_prompt": null}
{"id": "3d094c69-3ea3-4311-bb92-e6b452e69ebf", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n\n                # Adaptive parameter control\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Improved mutation strategy in Adaptive Differential Evolution by allowing dynamic adjustment of mutation direction based on evolving population diversity.", "configspace": "", "generation": 25, "fitness": 0.6716348536418546, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.672 with standard deviation 0.044. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4378a521-9ac2-4d40-bfb5-87541ea4c0ad", "metadata": {"aucs": [0.6412920226297236, 0.7336402118037724, 0.6399723264920679], "final_y": [4.26271801525823e-11, 6.27684789358761e-14, 1.7271625060231327e-11]}, "mutation_prompt": null}
{"id": "438ba3d6-4d64-48ca-8d19-29c7172a6802", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector * diversity  # Adjust mutation with diversity\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n\n                # Adaptive parameter control\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhanced diversity control in Differential Evolution by integrating population variance into mutation strength adaptation.", "configspace": "", "generation": 26, "fitness": 0.12573535725445958, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.126 with standard deviation 0.006. And the mean value of best solutions found was 2.542 (0. is the best) with standard deviation 0.342.", "error": "", "parent_id": "3d094c69-3ea3-4311-bb92-e6b452e69ebf", "metadata": {"aucs": [0.1312684156554874, 0.12824390356546977, 0.11769375254242154], "final_y": [2.2257550974736615, 2.3835744139328323, 3.016178718022608]}, "mutation_prompt": null}
{"id": "68b80283-448b-4770-997e-a38e568188fb", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                self.f = 0.5 + 0.3 * (1 - diversity)  # Adjust f based on diversity\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = 0.5 + 0.5 * diversity  # Adjust cr based on diversity\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhanced exploration through adaptive scaling and crossover parameters based on population convergence dynamics.", "configspace": "", "generation": 27, "fitness": 0.1255940236204193, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.126 with standard deviation 0.006. And the mean value of best solutions found was 2.550 (0. is the best) with standard deviation 0.338.", "error": "", "parent_id": "3d094c69-3ea3-4311-bb92-e6b452e69ebf", "metadata": {"aucs": [0.13128905321248086, 0.11752498934538846, 0.1279680283033886], "final_y": [2.2257550974736615, 3.016178718022608, 2.4081580768583843]}, "mutation_prompt": null}
{"id": "28015403-cb97-4e16-bf05-b895dec88edc", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n\n                # Change: Non-uniform mutation strategy based on diversity\n                weighted_vector = (1 - diversity) * self.f * (x2 - x3) + diversity * self.f * (x3 - x2)\n                \n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n\n                # Adaptive parameter control\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a non-uniform mutation strategy to enhance exploration and exploitation balance.", "configspace": "", "generation": 28, "fitness": 0.15792402993154875, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.158 with standard deviation 0.000. And the mean value of best solutions found was 1.234 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3d094c69-3ea3-4311-bb92-e6b452e69ebf", "metadata": {"aucs": [0.1577192313048471, 0.15816536560006944, 0.1578874928897297], "final_y": [1.2343493718926426, 1.2343493718926426, 1.2343493718926426]}, "mutation_prompt": null}
{"id": "467bb659-2bda-487c-9492-762b9dedec14", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhancing diversity and adaptive control in Adaptive Differential Evolution by including a memory-based adaptation mechanism for weights and crossover probabilities.", "configspace": "", "generation": 29, "fitness": 0.6945837959803179, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.695 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3d094c69-3ea3-4311-bb92-e6b452e69ebf", "metadata": {"aucs": [0.6850200378191251, 0.6911686700272754, 0.7075626800945535], "final_y": [2.0466566808037656e-08, 1.747156082218427e-07, 4.073616357329258e-14]}, "mutation_prompt": null}
{"id": "7cb9e0ce-1b8b-410d-b480-81b1cb7468e2", "solution": "import numpy as np\n\nclass SelfAdaptiveParticleSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0  # Cognitive component\n        self.c2 = 2.0  # Social component\n        self.w = 0.7   # Inertia weight\n        self.population = []\n        self.velocities = []\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            position = np.random.uniform(lb, ub, self.dim)\n            velocity = np.random.uniform(-1, 1, self.dim)\n            score = float('inf')\n            self.population.append((position, score, position))  # (position, score, best_position)\n            self.velocities.append(velocity)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        global_best_position = None\n        global_best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                current_position, current_score, personal_best_position = self.population[i]\n\n                # Evaluate current position\n                score = func(current_position)\n                evaluations += 1\n\n                if score < current_score:\n                    self.population[i] = (current_position, score, current_position)\n\n                    if score < global_best_score:\n                        global_best_position = current_position\n                        global_best_score = score\n\n                # Update particle velocity and position\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                personal_best_position = self.population[i][2]\n                inertia = self.w * self.velocities[i]\n                cognitive = self.c1 * r1 * (personal_best_position - current_position)\n                social = self.c2 * r2 * (global_best_position - current_position)\n                velocity = inertia + cognitive + social\n\n                # Update position and clip to bounds\n                new_position = current_position + velocity\n                new_position = np.clip(new_position, lb, ub)\n\n                self.velocities[i] = velocity\n                self.population[i] = (new_position, score, personal_best_position)\n\n            # Adapt inertia weight and coefficients based on diversity\n            positions = np.array([ind[0] for ind in self.population])\n            diversity = np.std(positions, axis=0).mean()\n\n            self.w = 0.5 + 0.2 * np.exp(-diversity)\n            self.c1 = 2.5 - 0.5 * diversity\n            self.c2 = 2.5 - 0.5 * (1 - diversity)\n\n        return global_best_position", "name": "SelfAdaptiveParticleSwarm", "description": "Introducing a Self-Adaptive Particle Swarm Optimization (SAPSO) algorithm that dynamically adjusts the inertia weight and acceleration coefficients based on the swarm's diversity to enhance exploration and exploitation.", "configspace": "", "generation": 30, "fitness": 0.16862788544406945, "feedback": "The algorithm SelfAdaptiveParticleSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.169 with standard deviation 0.024. And the mean value of best solutions found was 1.022 (0. is the best) with standard deviation 0.638.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.1361318976863053, 0.19458994494677018, 0.17516181369913286], "final_y": [1.9218451083922166, 0.5189055679463345, 0.6242671323884066]}, "mutation_prompt": null}
{"id": "71218520-7fec-4a61-a40b-f2ea7f4579e8", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n                # Refresh diversity by introducing new random individuals\n                if evaluations % (self.budget // 5) == 0:  \n                    self.initialize_population(lb, ub)\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhance exploration by periodically refreshing population diversity with new random individuals.", "configspace": "", "generation": 31, "fitness": 0.6071098636438457, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.607 with standard deviation 0.133. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.4249460434670541, 0.6570113913262405, 0.7393721561382421], "final_y": [0.0006715192998229853, 7.661679092181453e-07, 2.8754610699217895e-14]}, "mutation_prompt": null}
{"id": "3eebccd8-d2f7-4bab-ad40-0307d35b9ff5", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def chaotic_sequence(self, size):\n        sequence = np.zeros(size)\n        sequence[0] = np.random.rand()\n        for i in range(1, size):\n            sequence[i] = 4 * sequence[i-1] * (1 - sequence[i-1])\n        return sequence\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        chaos_sequence = self.chaotic_sequence(self.population_size)\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = chaos_sequence[i] * (x2 - x3) if p < diversity else chaos_sequence[i] * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing a chaotic-driven mutation mechanism in Adaptive Differential Evolution to enhance exploration and exploitation balance.", "configspace": "", "generation": 32, "fitness": 0.08222126539564974, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.082 with standard deviation 0.034. And the mean value of best solutions found was 8.127 (0. is the best) with standard deviation 4.977.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.07195508618597335, 0.12746280453389092, 0.04724590546708496], "final_y": [7.990161075596667, 2.100840710576352, 14.28959177008251]}, "mutation_prompt": null}
{"id": "b9ee7a42-ca59-4093-a0c0-0c53eaa4784b", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n            # Tournament selection for next generation\n            tournament_size = 3\n            new_population = []\n            for _ in range(self.population_size):\n                tournament = np.random.choice(self.population, tournament_size, replace=False)\n                winner = min(tournament, key=lambda ind: ind[1])\n                new_population.append(winner)\n            self.population = new_population\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Incorporating tournament selection to improve diversity and convergence rates in Adaptive Differential Evolution.", "configspace": "", "generation": 33, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (10, 2) + inhomogeneous part.').", "error": "ValueError('setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (10, 2) + inhomogeneous part.')", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {}, "mutation_prompt": null}
{"id": "bb430706-a0b3-463d-a314-16b1d45bf702", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            if evaluations % (self.budget // 5) == 0:  # Dynamic resizing\n                self.population_size = int(np.random.uniform(8, 12))\n                if len(self.population) > self.population_size:\n                    self.population = sorted(self.population, key=lambda x: x[1])[:self.population_size]\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing dynamic population resizing and diversity-enhanced selection to improve convergence in Adaptive Differential Evolution.", "configspace": "", "generation": 34, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {}, "mutation_prompt": null}
{"id": "82a71e49-4699-435d-b92d-1d019e386da5", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else 0.3 * self.f * (x3 - x2)  # Adjusted mutation\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing adaptive mutation strategy for improved convergence by dynamically adjusting the mutation factor based on the diversity metric.", "configspace": "", "generation": 35, "fitness": 0.1681183815895124, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.168 with standard deviation 0.119. And the mean value of best solutions found was 4.140 (0. is the best) with standard deviation 3.072.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.33679142301635234, 0.07587955237668198, 0.09168416937550283], "final_y": [0.011795513335952893, 7.37570798094793, 5.0313522125784385]}, "mutation_prompt": null}
{"id": "e6d31c69-18c7-4803-bbdf-93a12766770f", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing tournament selection and adaptive scaling factor adjustments to enhance convergence in Adaptive Differential Evolution.", "configspace": "", "generation": 36, "fitness": 0.6945837959803179, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.695 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.6850200378191251, 0.6911686700272754, 0.7075626800945535], "final_y": [2.0466566808037656e-08, 1.747156082218427e-07, 4.073616357329258e-14]}, "mutation_prompt": null}
{"id": "0c5cffab-da4d-4bfd-b829-ba1f0ad91a8d", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                self.f = np.clip(0.4 + diversity, 0.4, 0.9)  # Dynamic adjustment of f\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Integrating adaptive mutation scaling dynamically adjusted based on current population diversity in Adaptive Differential Evolution.", "configspace": "", "generation": 37, "fitness": 0.43205895257077964, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.432 with standard deviation 0.145. And the mean value of best solutions found was 0.070 (0. is the best) with standard deviation 0.100.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.2266401940061783, 0.5339462926216278, 0.5355903710845329], "final_y": [0.21135343147533572, 1.1696741204014363e-05, 2.2705799652895607e-05]}, "mutation_prompt": null}
{"id": "2ad799a5-aae9-47c8-be8f-076c06a14d22", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with chaotic maps\n                z = 0.7 + 0.3 * np.cos(2 * np.pi * evaluations / self.budget)\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = z * np.random.uniform(0.4, 0.9)\n                    self.cr = z * np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing chaotic maps for parameter adaptation in Adaptive Differential Evolution to enhance exploration and exploitation balance.", "configspace": "", "generation": 38, "fitness": 0.3575157471501413, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.358 with standard deviation 0.103. And the mean value of best solutions found was 0.068 (0. is the best) with standard deviation 0.088.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.2346558396749474, 0.35221388336273673, 0.48567751841273976], "final_y": [0.19223198482105697, 0.011149941294744175, 0.00024266450378765083]}, "mutation_prompt": null}
{"id": "57e49260-40ac-4282-b1cb-eaeb683ff388", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.6, 1.0)  # Slightly adjusted range\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Improved convergence by recalibrating crossover probability adaptation in Adaptive Differential Evolution.", "configspace": "", "generation": 39, "fitness": 0.6945837959803179, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.695 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.6850200378191251, 0.6911686700272754, 0.7075626800945535], "final_y": [2.0466566808037656e-08, 1.747156082218427e-07, 4.073616357329258e-14]}, "mutation_prompt": null}
{"id": "f1596a6a-f635-4059-a41e-a351aac62029", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr * (1 + diversity / 10) or j == j_rand:  # Modified line\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing an adaptive dynamic adjustment to the crossover probability based on population diversity in Adaptive Differential Evolution.", "configspace": "", "generation": 40, "fitness": 0.3541608547995929, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.354 with standard deviation 0.265. And the mean value of best solutions found was 0.906 (0. is the best) with standard deviation 1.051.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.1281706584530108, 0.20760496828648745, 0.7267069376592805], "final_y": [2.3796854830461545, 0.3392169457377539, 6.48309443773522e-14]}, "mutation_prompt": null}
{"id": "f3b474b2-f230-449e-b11f-996a10aa40f9", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                \n                # Selective mutation strategy\n                if np.random.rand() < 0.5:\n                    x1 = target_vector\n\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing selective mutation strategy to enhance convergence in Adaptive Differential Evolution by modifying the random target vector selection.", "configspace": "", "generation": 41, "fitness": 0.6172099963359933, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.617 with standard deviation 0.029. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.5786119288057662, 0.647036015930964, 0.6259820442712496], "final_y": [1.7711474674400817e-09, 6.890287445878859e-10, 4.768048906330659e-10]}, "mutation_prompt": null}
{"id": "6566ba45-b4ca-401a-a16a-27b52b2e4181", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = (self.f + diversity) * (x2 - x3) if p < diversity else self.f * (x3 - x2)  # Changed line\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce diversity-based adaptive scaling for the mutation vector in Adaptive Differential Evolution.", "configspace": "", "generation": 42, "fitness": 0.04785011837789033, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.048 with standard deviation 0.015. And the mean value of best solutions found was 14.873 (0. is the best) with standard deviation 4.282.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.037179334256532304, 0.03726588895289351, 0.06910513192424517], "final_y": [17.901191238544744, 17.901191238544744, 8.816830893489069]}, "mutation_prompt": null}
{"id": "667aa69e-bb61-48cb-80dc-2ed5e9343e0b", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                self.f = 0.4 + 0.5 * np.random.rand() * diversity  # Adaptive mutation strategy\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce an adaptive mutation strategy to balance exploration and exploitation by dynamically adjusting the differential weight.", "configspace": "", "generation": 43, "fitness": 0.12408164821091143, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.124 with standard deviation 0.009. And the mean value of best solutions found was 2.665 (0. is the best) with standard deviation 0.497.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.1368974967473131, 0.11765369534299963, 0.11769375254242154], "final_y": [1.961529572581947, 3.016178718022608, 3.016178718022608]}, "mutation_prompt": null}
{"id": "7144571c-8683-45fa-b06d-a666f4403370", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else (0.5 + np.random.rand() / 2) * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n                    if np.random.rand() < 0.1:  # Re-evaluation strategy\n                        trial_score = func(trial_vector)\n                        evaluations += 1\n                        if trial_score < target_score:\n                            self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Incorporating a self-adaptive mutation strategy and re-evaluation mechanism to improve convergence in Adaptive Differential Evolution.", "configspace": "", "generation": 44, "fitness": 0.5599266537119237, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.560 with standard deviation 0.082. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.5991121303791651, 0.6350281003608365, 0.44563973039576954], "final_y": [1.5209638192943777e-09, 1.2153219496562858e-10, 4.850154631581533e-08]}, "mutation_prompt": null}
{"id": "86c765ce-61cc-433d-aa6f-8eeb1108fb13", "solution": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = []\n        self.memory_f = []\n        self.memory_cr = []\n        self.success_rates = []\n        self.success_rate_window = 5\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def update_parameters(self):\n        if len(self.success_rates) >= self.success_rate_window:\n            recent_success_rate = np.mean(self.success_rates[-self.success_rate_window:])\n            self.f = np.clip(self.f * (1 + 0.1 * (recent_success_rate - 0.5)), 0.4, 0.9)\n            self.cr = np.clip(self.cr * (1 + 0.1 * (recent_success_rate - 0.5)), 0.5, 1.0)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            successful_trials = 0\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n                    successful_trials += 1\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)\n                        self.memory_cr.append(self.cr)\n\n            # Record success rate\n            self.success_rates.append(successful_trials / self.population_size)\n            # Adjust parameters based on recent success rate\n            self.update_parameters()\n\n        return best_solution", "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Introducing a dynamic feedback mechanism in Adaptive Differential Evolution that leverages historical success rates to fine-tune differential and crossover parameters, enhancing convergence speed and diversity retention.", "configspace": "", "generation": 45, "fitness": 0.38324274663322094, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.383 with standard deviation 0.255. And the mean value of best solutions found was 0.537 (0. is the best) with standard deviation 0.725.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.7352748268411486, 0.1407029949196924, 0.27375041813882195], "final_y": [0.0, 1.5614432095990625, 0.04943683615092347]}, "mutation_prompt": null}
{"id": "59093217-a847-49c9-9286-2e79ebcf1467", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                # Adjust crossover probability based on diversity\n                self.cr = 0.9 - 0.4 * diversity\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a diversity-based adjustment to the crossover probability to enhance convergence.", "configspace": "", "generation": 46, "fitness": 0.1982078461930971, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.198 with standard deviation 0.037. And the mean value of best solutions found was 0.451 (0. is the best) with standard deviation 0.521.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.21185422247874464, 0.23511147724608417, 0.1476578388544625], "final_y": [0.17220822741912123, 0.0001003770867547713, 1.182187841306994]}, "mutation_prompt": null}
{"id": "585f853f-953c-4f3d-8ebc-a82c7026af60", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector.copy()  # Ensure the best solution is retained\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n            # Elitism strategy: ensure the best solution survives\n            if best_solution is not None:\n                worst_idx = np.argmax([ind[1] for ind in self.population])\n                self.population[worst_idx] = (best_solution, best_score)\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Incorporate an elitism strategy to retain the best solution across generations in Adaptive Differential Evolution.", "configspace": "", "generation": 47, "fitness": 0.25299077394283426, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.253 with standard deviation 0.082. And the mean value of best solutions found was 0.358 (0. is the best) with standard deviation 0.248.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.19513073667851843, 0.3687042361731966, 0.1951373489767878], "final_y": [0.5329949809454918, 0.007783822008981894, 0.5329956382610108]}, "mutation_prompt": null}
{"id": "32593e5d-c7ba-4c08-8197-5345f0f0dfb9", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory and exponential decay\n                decay = np.exp(-evaluations / self.budget)\n                self.f = decay * self.f + (1 - decay) * 0.5\n                self.cr = decay * self.cr + (1 - decay) * 0.9\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce an exponential decay mechanism for dynamic adjustment of differential weight and crossover probability in Adaptive Differential Evolution.", "configspace": "", "generation": 48, "fitness": 0.525534308985041, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.526 with standard deviation 0.219. And the mean value of best solutions found was 0.080 (0. is the best) with standard deviation 0.113.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.607563877974886, 0.743409639371627, 0.22562940960861022], "final_y": [3.9703093033369433e-07, 7.935361698796144e-12, 0.23950543501019242]}, "mutation_prompt": null}
{"id": "82d1afd2-7f77-4baf-b301-7ecd458fc1a9", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = (1 + diversity) * self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    success_rate = len(self.memory_f[-10:]) / 10 if len(self.memory_f) >= 10 else 0.5\n                    self.f = np.random.uniform(0.4, 0.9 * (1 + success_rate))\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Improving adaptive parameter control by incorporating success rate and diversity-based scaling in Adaptive Differential Evolution.", "configspace": "", "generation": 49, "fitness": 0.05907867759185007, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.059 with standard deviation 0.023. And the mean value of best solutions found was 12.352 (0. is the best) with standard deviation 4.985.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.042563582548341694, 0.042722122642182, 0.09195032758502653], "final_y": [15.876808010952455, 15.876808010952455, 5.302800395286793]}, "mutation_prompt": null}
{"id": "b691e596-dc5c-4d80-84e9-28ffede4e68b", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                self.f = 0.4 + 0.5 * diversity  # Adjust f based on diversity\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.cr = np.mean(self.memory_cr[-5:]) if self.memory_cr else np.random.uniform(0.5, 1.0)\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce dynamic crossover and mutation adjustments by leveraging the diversity and historical success of parameters.", "configspace": "", "generation": 50, "fitness": 0.1257338178240878, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.126 with standard deviation 0.006. And the mean value of best solutions found was 2.542 (0. is the best) with standard deviation 0.342.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.131264489489474, 0.12824321144036788, 0.11769375254242154], "final_y": [2.2257550974736615, 2.3835744139328323, 3.016178718022608]}, "mutation_prompt": null}
{"id": "1912cea7-34d2-4ac7-b2c7-aa36c5c4a708", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                self.f = 0.4 + 0.5 * diversity  # Adjust scaling factor based on diversity\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing diversity-driven mutation scaling for enhanced exploration in Adaptive Differential Evolution.", "configspace": "", "generation": 51, "fitness": 0.05907929098313458, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.059 with standard deviation 0.023. And the mean value of best solutions found was 12.352 (0. is the best) with standard deviation 4.985.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.04256370678416832, 0.042723927122056304, 0.09195023904317912], "final_y": [15.876808010952455, 15.876808010952455, 5.302800395286793]}, "mutation_prompt": null}
{"id": "1849e532-038f-4a40-b186-b60e65cdb69c", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                # Local search step (introducing a random small perturbation)\n                trial_vector = trial_vector + np.random.normal(0, 0.01, self.dim) * (ub - lb)\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduced a local search step to refine trial vectors, improving convergence precision.", "configspace": "", "generation": 52, "fitness": 0.336859170687642, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.337 with standard deviation 0.014. And the mean value of best solutions found was 0.007 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.3408735243349681, 0.3175888216870967, 0.352115166040861], "final_y": [0.0033707858178192883, 0.009100859728796443, 0.008208101223220195]}, "mutation_prompt": null}
{"id": "0a47cdbe-6705-4e59-aeef-d5a7f9e28890", "solution": "import numpy as np\n\nclass EnhancedDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5\n        self.cr = 0.9\n        self.population = []\n        self.memory_f = []\n        self.memory_cr = []\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Dynamic diversity-adaptive strategy\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                dynamic_weight = self.f * (1 + 0.1 * diversity)\n                weighted_vector = dynamic_weight * (x2 - x3)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)\n                        self.memory_cr.append(self.cr)\n\n                # Adaptive parameter control with memory and decay\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9) * (1 - 0.01 * (evaluations / self.budget))\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])\n\n        return best_solution", "name": "EnhancedDifferentialEvolution", "description": "Introducing a dynamic diversity-adaptive mutation strategy and differential weight decay in Adaptive Differential Evolution to improve convergence stability and solution quality.  ", "configspace": "", "generation": 53, "fitness": 0.39607866930916996, "feedback": "The algorithm EnhancedDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.396 with standard deviation 0.220. And the mean value of best solutions found was 0.383 (0. is the best) with standard deviation 0.537.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.6860552505481443, 0.15470045643040653, 0.347480300948959], "final_y": [6.678969279272996e-13, 1.1417965237247434, 0.006655128960520661]}, "mutation_prompt": null}
{"id": "fd110109-ccda-43f0-a25b-8d1e9c7b8788", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.median(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.median(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhance parameter adaptation by introducing a dynamic adjustment mechanism based on the median of past successful parameters for improved convergence.", "configspace": "", "generation": 54, "fitness": 0.39877883804330394, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.399 with standard deviation 0.175. And the mean value of best solutions found was 0.415 (0. is the best) with standard deviation 0.586.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.558255877260265, 0.48275219755054277, 0.1553284393191039], "final_y": [1.0899627246573487e-05, 0.00020042758693896502, 1.244219049838911]}, "mutation_prompt": null}
{"id": "45f23ed6-2e55-4265-9080-0d55a48115eb", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []\n        self.memory_cr = []\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Adjust the differential weight based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                self.f = 0.4 + 0.5 * diversity  # Line modified\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)\n                        self.memory_cr.append(self.cr)\n\n                # More frequent adaptive parameter control\n                if evaluations % (self.budget // 20) == 0:  # Line modified\n                    if self.memory_f:\n                        self.f = np.clip(np.mean(self.memory_f[-5:]), 0.4, 0.9)  # Line modified\n                    if self.memory_cr:\n                        self.cr = np.clip(np.mean(self.memory_cr[-5:]), 0.5, 1.0)  # Line modified\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Improved Adaptive Differential Evolution by integrating a diversity-based mutation scheme and refining parameter adaptation.", "configspace": "", "generation": 55, "fitness": 0.1257338178240878, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.126 with standard deviation 0.006. And the mean value of best solutions found was 2.542 (0. is the best) with standard deviation 0.342.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.131264489489474, 0.12824321144036788, 0.11769375254242154], "final_y": [2.2257550974736615, 2.3835744139328323, 3.016178718022608]}, "mutation_prompt": null}
{"id": "545010df-f640-49e0-b211-40c8daeeb01e", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 1.0 - (evaluations / self.budget) * 0.3)  # Adjust f based on progress\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing a recombination step with higher exploration potential by modifying differential weight adaptively based on evaluation progress.", "configspace": "", "generation": 56, "fitness": 0.28376994680732653, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.284 with standard deviation 0.113. And the mean value of best solutions found was 0.565 (0. is the best) with standard deviation 0.760.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.41905474211476024, 0.2905680435584267, 0.14168705474879273], "final_y": [0.001145580504307669, 0.05335540689849744, 1.6393457745589608]}, "mutation_prompt": null}
{"id": "4452aa31-db5a-4eb4-ac2c-1969c4888bf3", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = 0.7 * self.f if len(self.memory_f) / max(1, evaluations) > 0.5 else np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing a self-adaptive mechanism for dynamically adjusting the differential weight `f` based on the success rate of trial vectors.", "configspace": "", "generation": 57, "fitness": 0.6945837959803179, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.695 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.6850200378191251, 0.6911686700272754, 0.7075626800945535], "final_y": [2.0466566808037656e-08, 1.747156082218427e-07, 4.073616357329258e-14]}, "mutation_prompt": null}
{"id": "836e7df3-95e2-4e6f-9fe9-20b3b656d83f", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Adjust mutation strategy based on progress\n                progress_rate = 1 - best_score / (abs(best_score) + 1e-9)\n                self.f = 0.4 + 0.5 * progress_rate\n                weighted_vector = self.f * (x2 - x3)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.population_size = min(20, int(10 + 10 * progress_rate))\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhance Adaptive Differential Evolution by introducing dynamic population sizing and adaptive mutation based on progress rate.", "configspace": "", "generation": 58, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('cannot convert float NaN to integer').", "error": "ValueError('cannot convert float NaN to integer')", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {}, "mutation_prompt": null}
{"id": "0a30334e-5fea-4211-a596-6b1caec8a3e6", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr * diversity or j == j_rand:  # Changed line\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing a diversity-based rule to dynamically adapt the crossover probability.", "configspace": "", "generation": 59, "fitness": 0.4633342771214423, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.463 with standard deviation 0.224. And the mean value of best solutions found was 0.431 (0. is the best) with standard deviation 0.609.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.6067598732360151, 0.6368489880237727, 0.14639397010453914], "final_y": [3.4966845049791957e-08, 2.2668373792226917e-09, 1.291961244631492]}, "mutation_prompt": null}
{"id": "7e7a2485-dddc-4674-9fed-4ccb9304c5dd", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n            # Adjust population size based on remaining budget\n            self.population_size = max(5, int(self.budget / 100) + 3)\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing dynamic adjustment of population size to enhance exploration and exploitation balance.", "configspace": "", "generation": 60, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {}, "mutation_prompt": null}
{"id": "9450bc87-cc11-49a4-bc70-912d55bd753b", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                self.f = 0.5 + 0.5 * diversity  # Adjusted differential weight\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Improved the mutation strategy by dynamically adjusting the differential weight based on the diversity of the population to enhance exploration capabilities.", "configspace": "", "generation": 61, "fitness": 0.04845340985590622, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.048 with standard deviation 0.003. And the mean value of best solutions found was 13.658 (0. is the best) with standard deviation 1.141.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.05100401887500805, 0.04963716616264613, 0.04471904453006448], "final_y": [12.341219640686608, 13.506774710857552, 15.12488915751401]}, "mutation_prompt": null}
{"id": "c438af5b-9729-4fa5-ab1c-5ed9c2358882", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = (1 - diversity) * self.f * (x2 - x3) + diversity * (x3 - x2)  # Modified line\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Integrating weighted memory adaptation to enhance diversity and convergence control in adaptive differential evolution.", "configspace": "", "generation": 62, "fitness": 0.12225559859188617, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.122 with standard deviation 0.006. And the mean value of best solutions found was 2.753 (0. is the best) with standard deviation 0.373.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.13130100198930883, 0.1177958040414292, 0.11766998974492049], "final_y": [2.2257550974736615, 3.016178718022608, 3.016178718022608]}, "mutation_prompt": null}
{"id": "2328996d-f831-4b47-8dbc-891022dc4cba", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Self-adaptive mutation factor\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                self.f = np.random.uniform(0.4, 0.9) * diversity\n\n                weighted_vector = self.f * (x2 - x3)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduces self-adaptive mutation factor and probability for improving convergence in Adaptive Differential Evolution.", "configspace": "", "generation": 63, "fitness": 0.12572755518982293, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.126 with standard deviation 0.006. And the mean value of best solutions found was 2.542 (0. is the best) with standard deviation 0.342.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.13124370646167705, 0.1282452065653702, 0.11769375254242154], "final_y": [2.2257550974736615, 2.3835744139328323, 3.016178718022608]}, "mutation_prompt": null}
{"id": "c8466c1a-be52-40ea-a529-bcb89ee0d3e0", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n                        self.cr = min(1.0, max(0.1, np.random.normal(self.cr, 0.1)))  # Self-adaptive cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing self-adaptive crossover probability in Adaptive Differential Evolution to enhance exploration-exploitation balance.", "configspace": "", "generation": 64, "fitness": 0.4688699230919425, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.469 with standard deviation 0.216. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.164.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.21120748534559586, 0.45564501541212943, 0.7397572685181022], "final_y": [0.34754805224294805, 0.00032124731345939617, 1.5451287004334347e-13]}, "mutation_prompt": null}
{"id": "eb68eaa8-c774-468b-a8ba-8ecf85e1a45c", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, dim)  # Dynamic population size\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = np.random.uniform(0.5, self.f) * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Improve the convergence speed and accuracy by incorporating a dynamic adjustment for population size and enhancing diversity exploitation using a stochastic scaling factor.  ", "configspace": "", "generation": 65, "fitness": 0.3456257231856821, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.346 with standard deviation 0.270. And the mean value of best solutions found was 0.876 (0. is the best) with standard deviation 0.642.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.1626304329516277, 0.14725464841611258, 0.7269920881893059], "final_y": [1.1053980367261536, 1.522680484570849, 7.054716846788686e-14]}, "mutation_prompt": null}
{"id": "9dd27aca-fffa-47d2-bce0-16be3a42846a", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            # Adjust population size dynamically\n            self.population_size = max(5, int(self.budget / (evaluations + 1)))\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduced a dynamic population size adjustment mechanism to enhance exploration and exploitation in Adaptive Differential Evolution.", "configspace": "", "generation": 66, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {}, "mutation_prompt": null}
{"id": "1d87918e-2612-4dd0-a7a7-98e96746bd7d", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0 * (len(self.memory_cr) / (1 + len(self.memory_cr))))  # Modified crossover probability\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Improving convergence by adjusting crossover probability based on successful trials.", "configspace": "", "generation": 67, "fitness": 0.38895414751781154, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.389 with standard deviation 0.269. And the mean value of best solutions found was 0.404 (0. is the best) with standard deviation 0.458.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.7670701007293576, 0.2393309624444543, 0.1604613793796228], "final_y": [0.0, 0.16808758991830286, 1.044919787793013]}, "mutation_prompt": null}
{"id": "620f3119-f7a2-4867-a674-074a14efe2f1", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n                # Periodic reinitialization to enhance diversity\n                if evaluations % (self.budget // 5) == 0:\n                    self.population[i] = (np.random.uniform(lb, ub, self.dim), float('inf'))\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhancing diversity and adaptive control in Adaptive Differential Evolution with periodic reinitialization of population members.", "configspace": "", "generation": 68, "fitness": 0.579173506643127, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.579 with standard deviation 0.079. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.5395793030606144, 0.508265217397261, 0.6896759994715055], "final_y": [1.029517378316915e-05, 0.00010489448582579846, 5.930733003443829e-13]}, "mutation_prompt": null}
{"id": "56862931-f9c5-4773-b1ae-014572965881", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.population_size = min(self.population_size + 1, 20)  # Increment population size as improvement\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Dynamic adjustment of population size in Adaptive Differential Evolution to enhance exploration and exploitation balance.", "configspace": "", "generation": 69, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {}, "mutation_prompt": null}
{"id": "bbc2b4d4-a750-4fdb-80ad-2fdca642e362", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:]) * 1.1  # Slightly increase f based on recent success\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhanced adaptive control by incorporating a self-adjusting scaling factor based on feedback from recent successful mutations.", "configspace": "", "generation": 70, "fitness": 0.5572339940599768, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.557 with standard deviation 0.248. And the mean value of best solutions found was 0.006 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.7030212202116293, 0.7606799145641503, 0.2080008474041506], "final_y": [1.1653073888199118e-09, 7.671465633032555e-11, 0.016903020838739683]}, "mutation_prompt": null}
{"id": "f45170ae-00c1-4b87-9629-2df4cad7edce", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:]) + np.std(self.memory_f[-5:]) * 0.1 # Adjust f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:]) + np.std(self.memory_cr[-5:]) * 0.1 # Adjust cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Optimized Adaptive Differential Evolution by introducing fitness-based parameter adjustment and enhanced diversity management.", "configspace": "", "generation": 71, "fitness": 0.6945837959803179, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.695 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.6850200378191251, 0.6911686700272754, 0.7075626800945535], "final_y": [2.0466566808037656e-08, 1.747156082218427e-07, 4.073616357329258e-14]}, "mutation_prompt": null}
{"id": "c9a53b08-ee31-4ac8-927c-2b8bf1ec12cd", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:]) * 0.95  # Recent successful f, with slight decay\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhancing diversity and adaptive control in Adaptive Differential Evolution by including a memory-based adaptation mechanism for weights and crossover probabilities, with a small enhancement to update strategy.", "configspace": "", "generation": 72, "fitness": 0.4472548660747022, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.447 with standard deviation 0.208. And the mean value of best solutions found was 0.070 (0. is the best) with standard deviation 0.096.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.2287427823411955, 0.7263690781680461, 0.3866527377148652], "final_y": [0.2055417504915634, 4.294572432717521e-08, 0.003077091235028825]}, "mutation_prompt": null}
{"id": "80ef7324-8564-44f8-ac9a-0891b5c51cee", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr * (1 + diversity):  # Changed line\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Improving convergence rate and accuracy by dynamically adjusting the crossover probability based on population diversity.", "configspace": "", "generation": 73, "fitness": 0.23343969069475376, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.233 with standard deviation 0.106. And the mean value of best solutions found was 0.968 (0. is the best) with standard deviation 1.046.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.12744509339467136, 0.19479505540925468, 0.3780789232803352], "final_y": [2.4211464176046213, 0.4803079269955447, 0.003461367247343149]}, "mutation_prompt": null}
{"id": "d5e61ee7-32ef-42f0-b390-de091adc8c85", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if len(self.memory_f) > 0 and np.random.rand() < 0.3:  # Success-based adaptation\n                    self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                if len(self.memory_cr) > 0 and np.random.rand() < 0.3:  # Success-based adaptation\n                    self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with Success-Based Parameter Adaptation to Enhance Exploration and Exploitation Balance.", "configspace": "", "generation": 74, "fitness": 0.3242277013871242, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.324 with standard deviation 0.275. And the mean value of best solutions found was 1.487 (0. is the best) with standard deviation 1.081.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.7124418188701438, 0.12249541716858858, 0.13774586812264022], "final_y": [1.1939470694390813e-13, 2.539694541996067, 1.9217835874696132]}, "mutation_prompt": null}
{"id": "ba6608fc-ae57-48fb-b354-db492fb86641", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.2, 0.7)  # Adjusted range for f\n                    self.cr = np.random.uniform(0.6, 1.0)  # Adjusted range for cr\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Refining dynamic adaptation mechanisms in Adaptive Differential Evolution for enhanced convergence efficiency.", "configspace": "", "generation": 75, "fitness": 0.4310539726681862, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.431 with standard deviation 0.229. And the mean value of best solutions found was 0.101 (0. is the best) with standard deviation 0.134.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.7469909371270467, 0.21256194336645062, 0.3336090375110612], "final_y": [4.764375120274477e-11, 0.2907906370286861, 0.011719473149895082]}, "mutation_prompt": null}
{"id": "17b3fceb-e53e-4444-95cf-8cf911ed4696", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                # Slightly updated line for diversity-aware weighting\n                weighted_vector = self.f * (x2 - x3) if p < (diversity / 2) else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution by adjusting the mutation strategy to improve convergence speed through diversity-aware weighting.", "configspace": "", "generation": 76, "fitness": 0.539220617152998, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.539 with standard deviation 0.231. And the mean value of best solutions found was 0.098 (0. is the best) with standard deviation 0.138.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.6630732067077454, 0.7387348885007015, 0.21585375625054692], "final_y": [8.159165850207516e-12, 1.0649308549664338e-13, 0.29316863434079204]}, "mutation_prompt": null}
{"id": "18704b77-a701-4c5d-b68d-82be66c2f7b4", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f *= np.mean(self.memory_f[-5:])  # Scale f with recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhance diversity control in Adaptive Differential Evolution by dynamically scaling the mutation factor based on historical performance.", "configspace": "", "generation": 77, "fitness": 0.2735169484802028, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.274 with standard deviation 0.054. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.090.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.2248332239916595, 0.3492184890716382, 0.2464991323773107], "final_y": [0.22864965905073856, 0.009588264930044802, 0.138482387024619]}, "mutation_prompt": null}
{"id": "c10a8485-e6ec-4a66-b875-2831c1634e79", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) * diversity if p < diversity else self.f * (x3 - x2) * (1-diversity)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Improved parameter adaptation and dynamic diversity control in Adaptive Differential Evolution for enhanced optimization performance.", "configspace": "", "generation": 78, "fitness": 0.12573535725445958, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.126 with standard deviation 0.006. And the mean value of best solutions found was 2.542 (0. is the best) with standard deviation 0.342.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.1312684156554874, 0.12824390356546977, 0.11769375254242154], "final_y": [2.2257550974736615, 2.3835744139328323, 3.016178718022608]}, "mutation_prompt": null}
{"id": "79f46f62-d60d-429f-ab2f-d4823bb33345", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                if diversity < 0.1: self.f *= 0.9  # Adjust f based on diversity\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a condition to adaptively reduce f when diversity is low for better convergence.", "configspace": "", "generation": 79, "fitness": 0.40465949215260033, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.405 with standard deviation 0.033. And the mean value of best solutions found was 0.003 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.4164574174347274, 0.4382623856680967, 0.35925867335497685], "final_y": [0.0011733335980888375, 0.0010169393793259933, 0.00712329146118782]}, "mutation_prompt": null}
{"id": "1c5f5946-5163-40da-9e48-1848dfb2fa7a", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = 0.5 + 0.5 * (1 - evaluations / self.budget)  # Time-varying cr\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing a time-varying crossover probability for enhancing global search capabilities in Adaptive Differential Evolution.", "configspace": "", "generation": 80, "fitness": 0.4829467343872402, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.483 with standard deviation 0.186. And the mean value of best solutions found was 0.080 (0. is the best) with standard deviation 0.113.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.6585932292036818, 0.5647370184288498, 0.22550995552918873], "final_y": [4.413208658819955e-13, 1.4429387069541592e-05, 0.24035214189550091]}, "mutation_prompt": null}
{"id": "f2eb8309-7d65-4fb1-9077-b0e33d288ac5", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n            \n            self.population.sort(key=lambda x: x[1])  # Sort by score to retain best\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce an elitism strategy to retain the best individual in each generation and enhance convergence speed.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('unexpected indent', ('<string>', 71, 16, '                if evaluations % (self.budget // 10) == 0:\\n', 71, -1)).", "error": "IndentationError('unexpected indent', ('<string>', 71, 16, '                if evaluations % (self.budget // 10) == 0:\\n', 71, -1))", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {}, "mutation_prompt": null}
{"id": "cc170d20-6d74-49bd-b7b0-ec8a69805297", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n        self.crowding_distance = [0] * self.population_size  # Crowding distance for diversity\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def calculate_crowding_distance(self):\n        distances = np.linalg.norm([ind[0] for ind in self.population], axis=1)\n        self.crowding_distance = distances / np.sum(distances)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            self.calculate_crowding_distance()  # Update crowding distance for diversity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Incorporating success-based parameter adaptation and diversity enhancement using crowding distance in Adaptive Differential Evolution.", "configspace": "", "generation": 82, "fitness": 0.3894638006705318, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.389 with standard deviation 0.155. And the mean value of best solutions found was 0.034 (0. is the best) with standard deviation 0.032.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.6058735612638858, 0.3141849764239816, 0.24833286432372814], "final_y": [2.679224628018516e-07, 0.024711799503247894, 0.07771993580898658]}, "mutation_prompt": null}
{"id": "ab27f11c-58e7-48d3-86eb-adf35ac8e8f6", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                speculative_jump = np.random.rand() < 0.1\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + (2.0 * weighted_vector if speculative_jump else weighted_vector)\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 10) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 1.0)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce speculative jumps and self-adaptive mutation strategies to improve convergence efficiency and solution quality.", "configspace": "", "generation": 83, "fitness": 0.4215500307589051, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.422 with standard deviation 0.252. And the mean value of best solutions found was 0.642 (0. is the best) with standard deviation 0.905.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.7499298854395515, 0.13782698572132734, 0.37689322111583656], "final_y": [3.894330385672422e-14, 1.9217835874696136, 0.005668653835947529]}, "mutation_prompt": null}
{"id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 5) == 0:  # Adjusted frequency for adaptation\n                    self.f = np.random.uniform(0.6, 0.8)  # Fine-tuned range\n                    self.cr = np.random.uniform(0.6, 0.9)  # Fine-tuned range\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Incorporate adaptive control for both differential weight and crossover probability using dynamic diversity monitoring in an enhanced Differential Evolution algorithm.", "configspace": "", "generation": 84, "fitness": 0.7424315989667832, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.742 with standard deviation 0.007. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "467bb659-2bda-487c-9492-762b9dedec14", "metadata": {"aucs": [0.7523003707391666, 0.7390154667321686, 0.7359789594290145], "final_y": [1.8892033169359147e-12, 1.3041833645620322e-13, 4.807265151128014e-09]}, "mutation_prompt": null}
{"id": "fccef860-d666-4ed6-8557-be192826b2c3", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector * (1.0 + diversity)  # Enhance diversity impact\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                if evaluations % (self.budget // 5) == 0:\n                    self.f = np.random.uniform(0.6, 0.8)\n                    self.cr = np.random.uniform(0.6, 0.9)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])\n                    self.population_size = max(5, int(len(self.population) * 0.9))  # Dynamic resizing\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhance exploration by introducing a diversity-based mutation strategy and dynamic population resizing.", "configspace": "", "generation": 85, "fitness": 0.07049534958092951, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.070 with standard deviation 0.023. And the mean value of best solutions found was 9.542 (0. is the best) with standard deviation 3.785.", "error": "", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {"aucs": [0.054373973339127746, 0.05450627229449856, 0.10260580310916223], "final_y": [12.218182460350059, 12.218182460350059, 4.188676856764739]}, "mutation_prompt": null}
{"id": "1c9c664c-af40-4536-9f93-efe85a62106a", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                self.f = 0.2 + 0.6 * diversity  # Changing line to introduce diversity-based mutation scaling\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 5) == 0:  # Adjusted frequency for adaptation\n                    self.f = np.random.uniform(0.6, 0.8)  # Fine-tuned range\n                    self.cr = np.random.uniform(0.6, 0.9)  # Fine-tuned range\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introducing a diversity-based mutation scaling to enhance the exploration-exploitation balance in Adaptive Differential Evolution.", "configspace": "", "generation": 86, "fitness": 0.1579775101617008, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.158 with standard deviation 0.014. And the mean value of best solutions found was 1.253 (0. is the best) with standard deviation 0.458.", "error": "", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {"aucs": [0.1383889773168695, 0.17079245137381205, 0.16475110179442087], "final_y": [1.9017527433964236, 0.9292891864349516, 0.9292891864349548]}, "mutation_prompt": null}
{"id": "7b54d02c-444e-4c20-aee3-27480f7cb488", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                # Elite-guided mutation\n                mutant_vector = x1 + weighted_vector + 0.01 * (best_solution - x1) if best_solution is not None else weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 5) == 0:  # Adjusted frequency for adaptation\n                    self.f = np.random.uniform(0.6, 0.8)  # Fine-tuned range\n                    self.cr = np.random.uniform(0.6, 0.9)  # Fine-tuned range\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Refine the mutation strategy by incorporating elite-guided perturbation to enhance convergence speed and solution quality.", "configspace": "", "generation": 87, "fitness": 0.36723974004377674, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.367 with standard deviation 0.247. And the mean value of best solutions found was 1.217 (0. is the best) with standard deviation 1.695.", "error": "", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {"aucs": [0.29213740360844276, 0.7006312945942634, 0.10895052192862409], "final_y": [0.03637637852167316, 6.654126188283378e-13, 3.6141821971530126]}, "mutation_prompt": null}
{"id": "fee766df-21b3-461a-899f-8b4cacb4aadf", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                # Guided mutation strategy\n                if np.random.rand() < 0.2:  # 20% chance to guide mutation\n                    guide_vector = np.mean([ind[0] for ind in self.population], axis=0)\n                    mutant_vector = 0.5 * (mutant_vector + guide_vector)\n                \n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 5) == 0:  # Adjusted frequency for adaptation\n                    self.f = np.random.uniform(0.6, 0.8)  # Fine-tuned range\n                    self.cr = np.random.uniform(0.6, 0.9)  # Fine-tuned range\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution using a guided mutation strategy and improved adaptive control.", "configspace": "", "generation": 88, "fitness": 0.1897524921790019, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.190 with standard deviation 0.038. And the mean value of best solutions found was 0.783 (0. is the best) with standard deviation 0.617.", "error": "", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {"aucs": [0.23548717889159954, 0.19105136681878732, 0.1427189308266188], "final_y": [0.18918656758225044, 0.5275998633357695, 1.6330043776600387]}, "mutation_prompt": null}
{"id": "5a75bc23-de1c-47c9-b1d9-6e6e365bbd2b", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 5) == 0:  # Adjusted frequency for adaptation\n                    self.f = np.random.uniform(0.6, 0.8)  # Fine-tuned range\n                    self.cr = np.random.uniform(0.6, 0.9)  # Fine-tuned range\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:]) ** 1.1  # Non-linear scaling\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce non-linear scaling for adaptive parameter control to enhance exploration and exploitation balance.", "configspace": "", "generation": 89, "fitness": 0.46510490228237095, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.465 with standard deviation 0.270. And the mean value of best solutions found was 1.577 (0. is the best) with standard deviation 2.231.", "error": "", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {"aucs": [0.5629204664008794, 0.09600389815349342, 0.7363903422927398], "final_y": [4.893742940827714e-06, 4.732079880075474, 6.541615871019602e-09]}, "mutation_prompt": null}
{"id": "4e23c5b1-0bb5-409d-834c-33746e0a6f23", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            if evaluations % (self.budget // 10) == 0:  # Dynamic population resize every 10% of budget\n                self.population_size = min(self.population_size + 1, 20)  # Increase up to 20\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                self.f = np.random.uniform(0.4, 0.9)  # Adaptive mutation strategy\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                if evaluations % (self.budget // 5) == 0:\n                    self.f = np.random.uniform(0.6, 0.8)\n                    self.cr = np.random.uniform(0.6, 0.9)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Integrate a self-adaptive mutation strategy and dynamic population resizing to enhance convergence in Adaptive Differential Evolution.", "configspace": "", "generation": 90, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {}, "mutation_prompt": null}
{"id": "7bffd905-6812-445c-9aee-88f76a9f5e14", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.base_population_size = 10\n        self.population_size = self.base_population_size\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)\n                        self.memory_cr.append(self.cr)\n\n                if evaluations % (self.budget // 5) == 0:\n                    self.f = np.random.uniform(0.4, 0.9)\n                    self.cr = np.random.uniform(0.5, 0.95)\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-5:])\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-5:])\n                    self.population_size = min(self.base_population_size + evaluations // (self.budget // 10), 30)\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Implemented dynamic population size and improved adaptive parameter tuning to enhance exploration and exploitation balance.", "configspace": "", "generation": 91, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {}, "mutation_prompt": null}
{"id": "4a60d028-5ac7-4d01-bae9-86532dd0b1a4", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 5) == 0:  # Adjusted frequency for adaptation\n                    self.f = np.random.uniform(0.6, 0.8 + 0.1 * diversity)  # Fine-tuned range\n                    self.cr = np.random.uniform(0.6, 0.9)  # Fine-tuned range\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a diversity-based scaling factor to enhance exploration in Adaptive Differential Evolution.", "configspace": "", "generation": 92, "fitness": 0.6694800706666416, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.669 with standard deviation 0.119. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {"aucs": [0.7962231016258082, 0.510117543483692, 0.702099566890425], "final_y": [2.10708941206615e-14, 0.00014068759896429184, 2.9571220664738333e-08]}, "mutation_prompt": null}
{"id": "1cf2f441-f653-4e65-a4f0-0a050683cef8", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                mutation_choice = np.random.choice([1, -1]) # Select mutation direction randomly\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * mutation_choice * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 5) == 0:  # Adjusted frequency for adaptation\n                    self.f = np.random.uniform(0.6, 0.8)  # Fine-tuned range\n                    self.cr = np.random.uniform(0.6, 0.9)  # Fine-tuned range\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Improve exploration by introducing randomization in mutation strategy selection and dynamic population size adjustment based on diversity. ", "configspace": "", "generation": 93, "fitness": 0.5748335708869977, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.575 with standard deviation 0.179. And the mean value of best solutions found was 0.007 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {"aucs": [0.322282592505409, 0.7119722142872039, 0.6902459058683803], "final_y": [0.019701058842807657, 6.629837169228378e-11, 1.0870710067267898e-12]}, "mutation_prompt": null}
{"id": "bf5e9e2a-b191-4edd-933a-29a28203bbc9", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n                \n                # Elite retention strategy\n                self.population.append((best_solution, best_score))\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 5) == 0:  # Adjusted frequency for adaptation\n                    self.f = np.random.uniform(0.6, 0.8)  # Fine-tuned range\n                    self.cr = np.random.uniform(0.6, 0.9)  # Fine-tuned range\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Incorporate an elite retention strategy to preserve the best individuals across generations in Adaptive Differential Evolution.", "configspace": "", "generation": 94, "fitness": 0.4513428170005011, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.451 with standard deviation 0.236. And the mean value of best solutions found was 0.303 (0. is the best) with standard deviation 0.429.", "error": "", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {"aucs": [0.7418965838851763, 0.4479137317666304, 0.16421813534969643], "final_y": [9.577340311572715e-14, 0.0004260543023638183, 0.9099202105561571]}, "mutation_prompt": null}
{"id": "3e9df9e2-21dd-47a4-bd5f-f45bfd057e16", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 5) == 0:  # Adjusted frequency for adaptation\n                    self.f = np.random.uniform(0.6, 0.8)  # Fine-tuned range\n                    self.cr = np.random.uniform(0.6, 0.9)  # Fine-tuned range\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])  # Recent successful cr\n\n            # Ensure the best solution is always in the population\n            self.population.sort(key=lambda x: x[1])\n            self.population[-1] = (best_solution, best_score)\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce elitism to ensure the best solution is always retained in the population.", "configspace": "", "generation": 95, "fitness": 0.38174501209745976, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.382 with standard deviation 0.258. And the mean value of best solutions found was 0.270 (0. is the best) with standard deviation 0.193.", "error": "", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {"aucs": [0.20223592723420425, 0.7459057961772305, 0.19709331288094467], "final_y": [0.37603129158260484, 2.5093207345178777e-07, 0.4351601291994217]}, "mutation_prompt": null}
{"id": "8666724a-264c-4f0e-82e7-e3cd1ca52947", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 5) == 0:  # Adjusted frequency for adaptation\n                    self.f = np.random.uniform(0.6, 0.8) * np.exp(-evaluations/self.budget)  # Exponential decay\n                    self.cr = np.random.uniform(0.6, 0.9)  # Fine-tuned range\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Fine-tune adaptive control by introducing an exponential decay factor for differential weight and crossover probability within the Adaptive Differential Evolution algorithm.", "configspace": "", "generation": 96, "fitness": 0.33118926099403045, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.331 with standard deviation 0.050. And the mean value of best solutions found was 0.019 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {"aucs": [0.2860934304165812, 0.3063543679689379, 0.4011199845965723], "final_y": [0.02505125235878743, 0.03138376023207851, 0.0006796596203353643]}, "mutation_prompt": null}
{"id": "1f00966f-007e-4805-ab3a-529486f9ad97", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 5) == 0:  # Adjusted frequency for adaptation\n                    self.f = np.random.uniform(0.6, 0.8)  # Fine-tuned range\n                    self.cr = np.random.uniform(0.6, 0.9)  # Fine-tuned range\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce diversity-guided dynamic adaptation of differential weight and crossover probability to improve convergence.", "configspace": "", "generation": 97, "fitness": 0.33118926099403045, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.331 with standard deviation 0.050. And the mean value of best solutions found was 0.019 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {"aucs": [0.2860934304165812, 0.3063543679689379, 0.4011199845965723], "final_y": [0.02505125235878743, 0.03138376023207851, 0.0006796596203353643]}, "mutation_prompt": null}
{"id": "5f9bb860-49c9-4b19-85d2-a8681eb36295", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n        self.success_rate_f = 1.0\n        self.success_rate_cr = 1.0\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity else self.f * (x3 - x2)\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n                        self.success_rate_f = min(1.0, self.success_rate_f + 0.1)\n                        self.success_rate_cr = min(1.0, self.success_rate_cr + 0.1)\n                else:\n                    self.success_rate_f = max(0.0, self.success_rate_f - 0.1)\n                    self.success_rate_cr = max(0.0, self.success_rate_cr - 0.1)\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 5) == 0:  # Adjusted frequency for adaptation\n                    self.f = np.random.uniform(0.5, 0.7 * self.success_rate_f + 0.1)  # Fine-tuned range\n                    self.cr = np.random.uniform(0.5, 0.8 * self.success_rate_cr + 0.1)  # Fine-tuned range\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Enhance Adaptive Differential Evolution by incorporating an elitism strategy and improved parameter adaptation using historical success rates.", "configspace": "", "generation": 98, "fitness": 0.33118926099403045, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.331 with standard deviation 0.050. And the mean value of best solutions found was 0.019 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {"aucs": [0.2860934304165812, 0.3063543679689379, 0.4011199845965723], "final_y": [0.02505125235878743, 0.03138376023207851, 0.0006796596203353643]}, "mutation_prompt": null}
{"id": "e85e0205-2db1-44bc-a4d9-69c1ecf7b2e4", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.population = []\n        self.memory_f = []  # Memory for f values\n        self.memory_cr = []  # Memory for cr values\n\n    def initialize_population(self, lb, ub):\n        for _ in range(self.population_size):\n            individual = np.random.uniform(lb, ub, self.dim)\n            score = float('inf')\n            self.population.append((individual, score))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_solution = None\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target_vector, target_score = self.population[i]\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                x1, _ = self.population[a]\n                x2, _ = self.population[b]\n                x3, _ = self.population[c]\n\n                # Introduce dynamic adjustment based on diversity\n                diversity = np.std([ind[0] for ind in self.population], axis=0).mean()\n                p = np.random.uniform()\n                weighted_vector = self.f * (x2 - x3) if p < diversity/2 else self.f * (x3 - x2)  # Adjusted probability\n                mutant_vector = x1 + weighted_vector\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                trial_vector = np.empty(self.dim)\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = target_vector[j]\n\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score < target_score:\n                    self.population[i] = (trial_vector, trial_score)\n\n                    if trial_score < best_score:\n                        best_solution = trial_vector\n                        best_score = trial_score\n                        self.memory_f.append(self.f)  # Store successful f\n                        self.memory_cr.append(self.cr)  # Store successful cr\n\n                # Adaptive parameter control with memory\n                if evaluations % (self.budget // 5) == 0:  # Adjusted frequency for adaptation\n                    self.f = np.random.uniform(0.6, 0.8)  # Fine-tuned range\n                    self.cr = np.random.uniform(0.6, 0.9)  # Fine-tuned range\n                    if self.memory_f:\n                        self.f = np.mean(self.memory_f[-3:])  # Recent successful f\n                    if self.memory_cr:\n                        self.cr = np.mean(self.memory_cr[-3:])  # Recent successful cr\n\n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Adaptive parameter control improvement with strategic probability adjustment for diversity in Differential Evolution.", "configspace": "", "generation": 99, "fitness": 0.38206784081050876, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.382 with standard deviation 0.233. And the mean value of best solutions found was 0.515 (0. is the best) with standard deviation 0.715.", "error": "", "parent_id": "f9caa68b-c164-4ad3-998e-d0f1f9fa9370", "metadata": {"aucs": [0.30233022341435767, 0.6986056433736216, 0.14526765564354704], "final_y": [0.019186878406084282, 2.8322054508930436e-07, 1.526924572295974]}, "mutation_prompt": null}
