{"id": "96b9a4e7-23b3-4482-98d8-15ca148b2f2a", "solution": "import numpy as np\n\nclass AntColonyOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pheromones = None\n        self.alpha = 1.0  # pheromone importance\n        self.beta = 2.0   # heuristic importance\n        self.evaporation_rate = 0.5\n        self.num_ants = 10\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_pheromones(self, bounds):\n        self.pheromones = np.ones((self.num_ants, self.dim))\n        self.lb, self.ub = bounds.lb, bounds.ub\n    \n    def update_pheromones(self, ants_positions, ants_obj):\n        for i in range(self.num_ants):\n            contribution = 1.0 / (1.0 + ants_obj[i])  # better solutions contribute more pheromone\n            self.pheromones[i] = (1 - self.evaporation_rate) * self.pheromones[i] + contribution\n\n    def construct_solution(self):\n        solution = []\n        for d in range(self.dim):\n            probabilities = (self.pheromones[:, d] ** self.alpha) * ((1.0 / (1.0 + np.abs(np.random.uniform(-1, 1, self.num_ants)))) ** self.beta)\n            probabilities /= np.sum(probabilities)\n            chosen_ant = np.random.choice(range(self.num_ants), p=probabilities)\n            step = np.random.uniform(self.lb[d], self.ub[d]) * self.pheromones[chosen_ant, d]\n            solution.append(np.clip(step, self.lb[d], self.ub[d]))\n        return np.array(solution)\n    \n    def local_search(self, solution):\n        # Simple local perturbation: random small change\n        perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n        new_solution = solution + perturbation\n        return np.clip(new_solution, self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_pheromones(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            ants_positions = [self.construct_solution() for _ in range(self.num_ants)]\n            ants_positions = [self.local_search(sol) for sol in ants_positions]\n            ants_obj = [func(pos) for pos in ants_positions]\n            evaluations += self.num_ants\n            \n            self.update_pheromones(ants_positions, ants_obj)\n            \n            min_idx = np.argmin(ants_obj)\n            if ants_obj[min_idx] < self.best_obj:\n                self.best_obj = ants_obj[min_idx]\n                self.best_solution = ants_positions[min_idx]\n        \n        return self.best_solution", "name": "AntColonyOptimization", "description": "A novel metaheuristic algorithm inspired by the collaborative behavior of ant colonies, where agents (ants) explore the search space and share information about promising solutions through pheromones, enhanced by a dynamic local search for refinement.", "configspace": "", "generation": 0, "fitness": 0.22399296186552323, "feedback": "The algorithm AntColonyOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.224 with standard deviation 0.002. And the mean value of best solutions found was 0.272 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": null, "metadata": {"aucs": [0.2270660472977415, 0.22273938796566195, 0.22217345033316627], "final_y": [0.2583225742184502, 0.27164897100841434, 0.2850040933959693]}, "mutation_prompt": null}
{"id": "d3fc433b-ea5f-4646-aee7-4d104896b460", "solution": "import numpy as np\n\nclass DolphinEcholocationOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_dolphins = 15\n        self.best_solution = None\n        self.best_obj = float('inf')\n        self.learning_rate = 0.05  # social learning rate\n        self.echolocate_radius = 0.1  # initial echolocation radius\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.num_dolphins, self.dim))\n        self.lb, self.ub = bounds.lb, bounds.ub\n\n    def echolocate(self, dolphin):\n        direction = np.random.uniform(-1, 1, self.dim)\n        distance = np.random.uniform(0, self.echolocate_radius)\n        return np.clip(dolphin + direction * distance, self.lb, self.ub)\n\n    def social_learning(self, dolphin, global_best):\n        movement = self.learning_rate * (global_best - dolphin)\n        return np.clip(dolphin + movement, self.lb, self.ub)\n\n    def __call__(self, func):\n        self.initialize_population(func.bounds)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            new_population = []\n            for dolphin in self.population:\n                echolocation_solution = self.echolocate(dolphin)\n                social_solution = self.social_learning(dolphin, self.best_solution if self.best_solution is not None else dolphin)\n                solutions = [echolocation_solution, social_solution]\n                \n                # Evaluate both solutions\n                objs = [func(sol) for sol in solutions]\n                evaluations += len(solutions)\n                \n                # Choose the better solution\n                best_idx = np.argmin(objs)\n                new_population.append(solutions[best_idx])\n                \n                # Update global best\n                if objs[best_idx] < self.best_obj:\n                    self.best_obj = objs[best_idx]\n                    self.best_solution = solutions[best_idx]\n                \n                # Early stopping if budget exceeded\n                if evaluations >= self.budget:\n                    break\n\n            # Update the population\n            self.population = np.array(new_population)\n            # Dynamic adjustment of echolocation radius\n            self.echolocate_radius *= 0.99\n\n        return self.best_solution", "name": "DolphinEcholocationOptimization", "description": "A novel metaheuristic algorithm inspired by the dynamic social and cognitive behavior of dolphins, which employs echolocation-inspired search strategies and social learning for enhanced exploration and exploitation of the search space.", "configspace": "", "generation": 1, "fitness": 0.2208848398106101, "feedback": "The algorithm DolphinEcholocationOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.221 with standard deviation 0.013. And the mean value of best solutions found was 0.310 (0. is the best) with standard deviation 0.100.", "error": "", "parent_id": "96b9a4e7-23b3-4482-98d8-15ca148b2f2a", "metadata": {"aucs": [0.23384445172370816, 0.20265368615452672, 0.2261563815535954], "final_y": [0.22060226454309073, 0.44887640137657203, 0.2604078941893747]}, "mutation_prompt": null}
{"id": "913af302-81f8-4ffe-9dec-458dc037cd8c", "solution": "import numpy as np\n\nclass EnhancedAntColonyOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pheromones = None\n        self.alpha = 1.0  # pheromone importance\n        self.beta = 2.0   # heuristic importance\n        self.evaporation_rate = 0.5\n        self.num_ants = 10\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_pheromones(self, bounds):\n        self.pheromones = np.ones((self.num_ants, self.dim))\n        self.lb, self.ub = bounds.lb, bounds.ub\n    \n    def update_pheromones(self, ants_positions, ants_obj):\n        avg_obj = np.mean(ants_obj)\n        dynamic_evaporation = self.evaporation_rate * (avg_obj / (self.best_obj + 1e-9))\n        for i in range(self.num_ants):\n            contribution = 1.0 / (1.0 + ants_obj[i])\n            self.pheromones[i] = (1 - dynamic_evaporation) * self.pheromones[i] + contribution\n    \n    def construct_solution(self):\n        solution = []\n        for d in range(self.dim):\n            probabilities = (self.pheromones[:, d] ** self.alpha) * ((1.0 / (1.0 + np.abs(np.random.uniform(-1, 1, self.num_ants)))) ** self.beta)\n            probabilities /= np.sum(probabilities)\n            chosen_ant = np.random.choice(range(self.num_ants), p=probabilities)\n            diversity_factor = np.random.uniform(0.9, 1.1)  # Introduce diversity\n            step = np.random.uniform(self.lb[d], self.ub[d]) * self.pheromones[chosen_ant, d] * diversity_factor\n            solution.append(np.clip(step, self.lb[d], self.ub[d]))\n        return np.array(solution)\n    \n    def local_search(self, solution):\n        perturbation = np.random.uniform(-0.05, 0.05, self.dim)  # Smaller local search range\n        new_solution = solution + perturbation\n        return np.clip(new_solution, self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_pheromones(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            ants_positions = [self.construct_solution() for _ in range(self.num_ants)]\n            ants_positions = [self.local_search(sol) for sol in ants_positions]\n            ants_obj = [func(pos) for pos in ants_positions]\n            evaluations += self.num_ants\n            \n            self.update_pheromones(ants_positions, ants_obj)\n            \n            min_idx = np.argmin(ants_obj)\n            if ants_obj[min_idx] < self.best_obj:\n                self.best_obj = ants_obj[min_idx]\n                self.best_solution = ants_positions[min_idx]\n        \n        return self.best_solution", "name": "EnhancedAntColonyOptimization", "description": "Enhanced Ant Colony Optimization incorporating pheromone diversity and adaptive pheromone evaporation, leveraging exploration-exploitation balance for improved convergence.", "configspace": "", "generation": 2, "fitness": 0.22003941406560415, "feedback": "The algorithm EnhancedAntColonyOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.220 with standard deviation 0.003. And the mean value of best solutions found was 0.290 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "96b9a4e7-23b3-4482-98d8-15ca148b2f2a", "metadata": {"aucs": [0.22435385975884115, 0.21726458779430113, 0.2184997946436702], "final_y": [0.279048701085438, 0.2727822966563166, 0.31873560174033233]}, "mutation_prompt": null}
{"id": "747e4b20-f429-4881-b140-494a792b1546", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.7   # inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        for i in range(self.num_particles):\n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -1, 1)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "An innovative Particle Swarm Optimization (PSO) variant leveraging dynamic particle interaction and adaptive velocity scaling to enhance exploration and exploitation in diverse optimization landscapes.", "configspace": "", "generation": 3, "fitness": 0.24352841435695782, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "96b9a4e7-23b3-4482-98d8-15ca148b2f2a", "metadata": {"aucs": [0.2452775283203904, 0.24191257778080422, 0.24339513696967885], "final_y": [0.1648557719047642, 0.16485577627006165, 0.16485577190473388]}, "mutation_prompt": null}
{"id": "cd05d24f-e3b6-449d-9a6d-5b050e4cea72", "solution": "import numpy as np\n\nclass FireflyAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_fireflies = 10\n        self.alpha = 0.2  # randomization parameter\n        self.beta0 = 1.0  # base attractiveness\n        self.gamma = 1.0  # absorption coefficient\n        self.best_solution = None\n        self.best_obj = float('inf')\n\n    def initialize_fireflies(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_fireflies, self.dim))\n        self.intensities = np.array([float('inf')] * self.num_fireflies)\n\n    def attractiveness(self, distance):\n        return self.beta0 * np.exp(-self.gamma * distance ** 2)\n\n    def update_positions(self):\n        for i in range(self.num_fireflies):\n            for j in range(self.num_fireflies):\n                if self.intensities[i] > self.intensities[j]:  # Move firefly i towards j\n                    distance = np.linalg.norm(self.positions[i] - self.positions[j])\n                    beta = self.attractiveness(distance)\n                    self.positions[i] += beta * (self.positions[j] - self.positions[i]) \\\n                                         + self.alpha * (np.random.rand(self.dim) - 0.5)\n                    self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n\n    def __call__(self, func):\n        self.initialize_fireflies(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_fireflies\n\n            for i in range(self.num_fireflies):\n                self.intensities[i] = objectives[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n\n            self.update_positions()\n        \n        return self.best_solution", "name": "FireflyAlgorithm", "description": "A novel Firefly Algorithm variant that employs adaptive attractiveness and dynamic randomization to enhance convergence and escape local optima in complex optimization landscapes.", "configspace": "", "generation": 4, "fitness": 0.19925835844557263, "feedback": "The algorithm FireflyAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.199 with standard deviation 0.002. And the mean value of best solutions found was 0.490 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "747e4b20-f429-4881-b140-494a792b1546", "metadata": {"aucs": [0.19963658436853804, 0.19656906121536488, 0.201569429752815], "final_y": [0.4851222111652522, 0.5192011663143732, 0.46469205749927356]}, "mutation_prompt": null}
{"id": "499d6c3c-7dd9-452e-870b-d08ddbf7a942", "solution": "import numpy as np\n\nclass CooperativeHybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_population(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, self.dim))\n        self.population_obj = np.array([float('inf')] * self.population_size)\n    \n    def evaluate_population(self, func):\n        objectives = [func(ind) for ind in self.population]\n        return np.array(objectives)\n    \n    def select_parents(self):\n        indices = np.random.choice(self.population_size, 3, replace=False)\n        return self.population[indices]\n    \n    def differential_evolution_step(self, target_idx):\n        a, b, c = self.select_parents()\n        mutant = np.clip(a + self.mutation_factor * (b - c), self.lb, self.ub)\n        trial = np.copy(self.population[target_idx])\n        crossover = np.random.rand(self.dim) < self.crossover_rate\n        trial[crossover] = mutant[crossover]\n        return trial\n    \n    def update_population(self, func):\n        new_population = []\n        for i in range(self.population_size):\n            trial = self.differential_evolution_step(i)\n            trial_obj = func(trial)\n            if trial_obj < self.population_obj[i]:\n                new_population.append(trial)\n                self.population_obj[i] = trial_obj\n            else:\n                new_population.append(self.population[i])\n        self.population = np.array(new_population)\n\n    def genetic_algorithm_step(self):\n        new_population = []\n        for _ in range(self.population_size):\n            parents = self.select_parents()\n            child = np.mean(parents[:2], axis=0) + np.random.randn(self.dim) * 0.05\n            child = np.clip(child, self.lb, self.ub)\n            new_population.append(child)\n        self.population = np.array(new_population)\n    \n    def diversity_adaptive(self):\n        diversity = np.std(self.population, axis=0).mean()\n        self.crossover_rate = np.clip(0.5 + 0.3 * (1 - diversity), 0.1, 0.9)\n        self.mutation_factor = np.clip(0.5 + 0.3 * diversity, 0.1, 1.0)\n    \n    def __call__(self, func):\n        self.initialize_population(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            self.population_obj = self.evaluate_population(func)\n            evaluations += self.population_size\n            \n            best_idx = np.argmin(self.population_obj)\n            if self.population_obj[best_idx] < self.best_obj:\n                self.best_obj = self.population_obj[best_idx]\n                self.best_solution = self.population[best_idx]\n            \n            if evaluations % (2 * self.population_size) < self.population_size:\n                self.update_population(func)\n            else:\n                self.genetic_algorithm_step()\n            \n            self.diversity_adaptive()\n        \n        return self.best_solution", "name": "CooperativeHybridOptimization", "description": "A novel Cooperative Hybrid Optimization (CHO) algorithm that synergistically combines Genetic Algorithms and Differential Evolution to exploit diverse solution strategies, dynamically adapting crossover and mutation rates based on population diversity to balance exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.2311789877501067, "feedback": "The algorithm CooperativeHybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.231 with standard deviation 0.004. And the mean value of best solutions found was 0.240 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "747e4b20-f429-4881-b140-494a792b1546", "metadata": {"aucs": [0.23047650373378858, 0.22696020714229104, 0.23610025237424048], "final_y": [0.24371363992584882, 0.26287746257649314, 0.21449956522212277]}, "mutation_prompt": null}
{"id": "1dcfb766-46f6-49ab-800c-334a7ab91551", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        for i in range(self.num_particles):\n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -1, 1)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Enhanced PSO with dynamic inertia weight to balance exploration and exploitation for improved performance.", "configspace": "", "generation": 6, "fitness": 0.24410821672130809, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "747e4b20-f429-4881-b140-494a792b1546", "metadata": {"aucs": [0.24531330178231858, 0.24394940608374927, 0.24306194229785638], "final_y": [0.1648557719625816, 0.16485577729091472, 0.16485577193150103]}, "mutation_prompt": null}
{"id": "bc5e9490-6ef7-4cc7-8552-9477f10db115", "solution": "import numpy as np\n\nclass GradientInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.alpha = 0.01  # gradient influence factor\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def compute_local_gradient(self, func, position):\n        grad = np.zeros(self.dim)\n        epsilon = 1e-8\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            grad[i] = (func(position + perturb) - func(position - perturb)) / (2 * epsilon)\n        return grad\n\n    def update_velocities_and_positions(self, func):\n        for i in range(self.num_particles):\n            local_gradient = self.compute_local_gradient(func, self.positions[i])\n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            gradient_velocity = self.alpha * local_gradient\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity - gradient_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -1, 1)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions(func)\n        \n        return self.best_solution", "name": "GradientInspiredPSO", "description": "Gradient-Inspired Particle Swarm Optimization (GIPSO) integrates local gradient information to enhance convergence while maintaining swarm diversity.", "configspace": "", "generation": 7, "fitness": 0.20644146392233162, "feedback": "The algorithm GradientInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.206 with standard deviation 0.004. And the mean value of best solutions found was 0.343 (0. is the best) with standard deviation 0.047.", "error": "", "parent_id": "1dcfb766-46f6-49ab-800c-334a7ab91551", "metadata": {"aucs": [0.2120732733310069, 0.20194061809075314, 0.2053105003452348], "final_y": [0.2773526495005457, 0.3680164774816411, 0.3835445843584333]}, "mutation_prompt": null}
{"id": "b22b9256-dc87-4a40-96c3-8035321c6e47", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        for i in range(self.num_particles):\n            cognitive_velocity = (self.c1 + 0.5 * (self.best_obj / self.personal_best_obj[i])) * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = (self.c2 + 0.5 * (1 - (self.best_obj / self.personal_best_obj[i]))) * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -1, 1)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Integration of adaptive cognitive and social components in PSO for improved solution convergence.", "configspace": "", "generation": 8, "fitness": 0.24412033707944544, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1dcfb766-46f6-49ab-800c-334a7ab91551", "metadata": {"aucs": [0.24553282597750337, 0.24337795864448286, 0.24345022661635007], "final_y": [0.16485577190469924, 0.1648557719666538, 0.164855771904722]}, "mutation_prompt": null}
{"id": "d8140b28-5b8d-4125-ba25-6ac27cb5403d", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        for i in range(self.num_particles):\n            cognitive_velocity = (self.c1 * np.random.rand() + 0.5 * (self.best_obj / self.personal_best_obj[i])) * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = (self.c2 * np.random.rand() + 0.5 * (1 - (self.best_obj / self.personal_best_obj[i]))) * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -1, 1)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Enhanced velocity update by integrating asynchronous learning rates for cognitive and social components.", "configspace": "", "generation": 9, "fitness": 0.24386296331712878, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b22b9256-dc87-4a40-96c3-8035321c6e47", "metadata": {"aucs": [0.2454896799371491, 0.24272187998328887, 0.2433773300309484], "final_y": [0.16485580735958627, 0.1648861800269701, 0.16485577324559797]}, "mutation_prompt": null}
{"id": "595b2547-26a1-48ca-9831-09195728be2a", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(10, budget // 10)  # Dynamic adjustment of num_particles\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        for i in range(self.num_particles):\n            cognitive_velocity = (self.c1 + 0.5 * (self.best_obj / self.personal_best_obj[i])) * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = (self.c2 + 0.5 * (1 - (self.best_obj / self.personal_best_obj[i]))) * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -1, 1)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a dynamic number of particles based on the budget to improve search efficiency.", "configspace": "", "generation": 10, "fitness": 0.24412033707944544, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b22b9256-dc87-4a40-96c3-8035321c6e47", "metadata": {"aucs": [0.24553282597750337, 0.24337795864448286, 0.24345022661635007], "final_y": [0.16485577190469924, 0.1648557719666538, 0.164855771904722]}, "mutation_prompt": null}
{"id": "58aaa357-1b8f-4e3d-b503-49169927e1a9", "solution": "import numpy as np\n\nclass HybridSwarmGuidedSimulatedAnnealing:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.temp_initial = 100.0\n        self.temp_final = 1.0\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self, temperature):\n        for i in range(self.num_particles):\n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = 0.5 * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -1, 1)\n            \n            new_position = self.positions[i] + self.velocities[i]\n            new_position = np.clip(new_position, self.lb, self.ub)\n            new_obj = func(new_position)\n            \n            # Simulated annealing acceptance\n            delta_obj = new_obj - self.personal_best_obj[i]\n            if delta_obj < 0 or np.exp(-delta_obj / temperature) > np.random.rand():\n                self.positions[i] = new_position\n                self.personal_best_positions[i] = new_position\n                self.personal_best_obj[i] = new_obj\n                if new_obj < self.best_obj:\n                    self.best_obj = new_obj\n                    self.best_solution = new_position\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        temperature = self.temp_initial\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            temperature = self.temp_initial - (self.temp_initial - self.temp_final) * (evaluations / self.budget)\n            self.update_velocities_and_positions(temperature)\n        \n        return self.best_solution", "name": "HybridSwarmGuidedSimulatedAnnealing", "description": "Hybrid Swarm-Guided Simulated Annealing (HSG-SA) combines swarm intelligence with simulated annealing for balanced exploration and exploitation.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "b22b9256-dc87-4a40-96c3-8035321c6e47", "metadata": {}, "mutation_prompt": null}
{"id": "aee460c2-9504-49fa-8b16-f4a83576df1a", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            cognitive_velocity = (self.c1 + 0.5 * (self.best_obj / self.personal_best_obj[i])) * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = (self.c2 + 0.5 * (1 - (self.best_obj / self.personal_best_obj[i]))) * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Enhanced particle diversity through adaptive velocity clipping for improved convergence in PSO.", "configspace": "", "generation": 12, "fitness": 0.2473590155164804, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.247 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b22b9256-dc87-4a40-96c3-8035321c6e47", "metadata": {"aucs": [0.2476921476292242, 0.2471682934090612, 0.24721660551115576], "final_y": [0.16485577190473533, 0.16485577190470224, 0.16485577241197846]}, "mutation_prompt": null}
{"id": "70935a55-c9ad-4d1e-bcfd-f748631efbef", "solution": "import numpy as np\n\nclass AdaptiveGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_rate = 0.1\n        self.elite_fraction = 0.1\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_population(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            self.fitness[i] = func(self.population[i])\n            if self.fitness[i] < self.best_obj:\n                self.best_obj = self.fitness[i]\n                self.best_solution = self.population[i]\n    \n    def select_parents(self):\n        probabilities = 1 / (self.fitness + 1e-9)\n        probabilities /= probabilities.sum()\n        parents_indices = np.random.choice(self.population_size, size=self.population_size, p=probabilities)\n        return self.population[parents_indices]\n    \n    def crossover(self, parent1, parent2):\n        alpha = np.random.rand(self.dim)\n        offspring = alpha * parent1 + (1 - alpha) * parent2\n        return offspring\n    \n    def mutate(self, individual):\n        mutation_prob = np.random.rand(self.dim) < self.mutation_rate\n        mutation_values = np.random.uniform(self.lb, self.ub, self.dim)\n        individual[mutation_prob] = mutation_values[mutation_prob]\n        return individual\n    \n    def create_next_generation(self, parents):\n        next_generation = []\n        num_elites = int(self.population_size * self.elite_fraction)\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        next_generation.extend(self.population[elite_indices])\n        \n        while len(next_generation) < self.population_size:\n            parent1, parent2 = parents[np.random.choice(self.population_size, 2, replace=False)]\n            offspring = self.crossover(parent1, parent2)\n            offspring = self.mutate(offspring)\n            next_generation.append(offspring)\n        \n        return np.array(next_generation)\n    \n    def __call__(self, func):\n        self.initialize_population(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            evaluations += self.population_size\n            \n            parents = self.select_parents()\n            self.population = self.create_next_generation(parents)\n            \n            # Adapt mutation rate based on budget usage\n            self.mutation_rate = 0.1 * (1 - (evaluations / self.budget))\n        \n        return self.best_solution", "name": "AdaptiveGeneticAlgorithm", "description": "\"Adaptive Genetic Algorithm with Dynamic Mutation Rate and Elitism for Robust Global Search\"", "configspace": "", "generation": 13, "fitness": 0.2470813797660335, "feedback": "The algorithm AdaptiveGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.247 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aee460c2-9504-49fa-8b16-f4a83576df1a", "metadata": {"aucs": [0.24747022107272865, 0.24694316835896724, 0.24683074986640463], "final_y": [0.1649130877245194, 0.16522777258263888, 0.16511758881832705]}, "mutation_prompt": null}
{"id": "e792d386-b7bb-4e7c-bb46-894d97566d0a", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Enhanced convergence by dynamically adjusting cognitive and social components in PSO.", "configspace": "", "generation": 14, "fitness": 0.24781122610938378, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aee460c2-9504-49fa-8b16-f4a83576df1a", "metadata": {"aucs": [0.24786869842307568, 0.2477222239724559, 0.24784275593261973], "final_y": [0.16485579479828227, 0.1648561585144631, 0.16485636303795947]}, "mutation_prompt": null}
{"id": "577983e3-ec5d-47d6-8300-38bad330c9b4", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = 0.05 * (self.ub - self.lb)  # Refined adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Improve adaptive velocity control in Particle Swarm Optimization by refining velocity limit factor calculation.", "configspace": "", "generation": 15, "fitness": 0.2472813772980024, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.247 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e792d386-b7bb-4e7c-bb46-894d97566d0a", "metadata": {"aucs": [0.24774559889195247, 0.24732360991342084, 0.2467749230886339], "final_y": [0.16485659360850258, 0.16485577504145232, 0.1648557783448542]}, "mutation_prompt": null}
{"id": "9abb3d38-20b1-412f-8aa7-221367ed3e4a", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.initial_num_particles = 10  # Initial number of particles\n        self.final_num_particles = 20  # Final number of particles\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            velocity_scaling_factor = 0.5 + 0.5 * (1 - progress_ratio)  # Line added\n            self.velocities[i] *= velocity_scaling_factor  # Line added\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Dynamically update inertia weight\n            self.num_particles = self.initial_num_particles + (self.final_num_particles - self.initial_num_particles) * (evaluations / self.budget)  # Line added\n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduced adaptive particle count and velocity scaling based on function evaluation phases for improved convergence.  ", "configspace": "", "generation": 16, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'float' object cannot be interpreted as an integer\").", "error": "TypeError(\"'float' object cannot be interpreted as an integer\")", "parent_id": "e792d386-b7bb-4e7c-bb46-894d97566d0a", "metadata": {}, "mutation_prompt": null}
{"id": "2c28c82e-d265-4a3a-aca4-e3a07d29067a", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.5 * (1 + np.cos(evaluations / self.budget * np.pi)) / 2  # Dynamically update inertia weight using cosine\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Enhanced adaptive mechanism by dynamically adjusting inertia weight and cognitive/social balances using a cosine function.", "configspace": "", "generation": 17, "fitness": 0.24176120369503776, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.242 with standard deviation 0.001. And the mean value of best solutions found was 0.188 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "e792d386-b7bb-4e7c-bb46-894d97566d0a", "metadata": {"aucs": [0.24289981887351575, 0.24095824381704178, 0.24142554839455577], "final_y": [0.18054441294108203, 0.19237132824254255, 0.1912462612834671]}, "mutation_prompt": null}
{"id": "7a27f627-42fc-4e08-92a5-29558d591a53", "solution": "import numpy as np\n\nclass AdaptiveParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1_initial = 2.5  # initial cognitive component\n        self.c2_initial = 0.5  # initial social component\n        self.c1_final = 0.5    # final cognitive component\n        self.c2_final = 2.5    # final social component\n        self.w_initial = 0.9   # initial inertia weight\n        self.w_final = 0.4     # final inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n        \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self, evaluations):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        progress = evaluations / self.budget\n        c1 = self.c1_initial * (1 - progress) + self.c1_final * progress\n        c2 = self.c2_initial * (1 - progress) + self.c2_final * progress\n        w = self.w_initial * (1 - progress) + self.w_final * progress\n        \n        for i in range(self.num_particles):\n            cognitive_velocity = c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.update_velocities_and_positions(evaluations)\n        \n        return self.best_solution", "name": "AdaptiveParticleSwarmOptimization", "description": "Adaptive Particle Swarm Optimization with Time-Varying Acceleration Coefficients for enhanced exploration-exploitation balance.", "configspace": "", "generation": 18, "fitness": 0.24728938077939275, "feedback": "The algorithm AdaptiveParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.247 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e792d386-b7bb-4e7c-bb46-894d97566d0a", "metadata": {"aucs": [0.2475246802064096, 0.24743222734158743, 0.24691123479018118], "final_y": [0.164855771905408, 0.16485577190495004, 0.1648557719047652]}, "mutation_prompt": null}
{"id": "1d3c8394-f836-4dea-97bb-ff3998c1380d", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Enhanced PSO with dynamic population size for improved exploration and exploitation.", "configspace": "", "generation": 19, "fitness": 0.24781122610938378, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e792d386-b7bb-4e7c-bb46-894d97566d0a", "metadata": {"aucs": [0.24786869842307568, 0.2477222239724559, 0.24784275593261973], "final_y": [0.16485579479828227, 0.1648561585144631, 0.16485636303795947]}, "mutation_prompt": null}
{"id": "3a8f071f-95d7-46e1-ba9f-f0754dba4423", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.4 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Particle Swarm Optimization with enhanced adaptive inertia and dynamic velocity control.  ", "configspace": "", "generation": 20, "fitness": 0.24782364171760882, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e792d386-b7bb-4e7c-bb46-894d97566d0a", "metadata": {"aucs": [0.24787932045632355, 0.24775765014606876, 0.2478339545504341], "final_y": [0.16485584389736663, 0.16485586027159926, 0.16485611881354656]}, "mutation_prompt": null}
{"id": "bc66f61f-7c33-4137-a151-cdae7558ac73", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i] + np.random.normal(0, 0.01, self.dim)  # Added mutation\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.4 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Incorporate a mutation strategy by adding a small random perturbation to particle positions to enhance exploration.", "configspace": "", "generation": 21, "fitness": 0.24767441625850475, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3a8f071f-95d7-46e1-ba9f-f0754dba4423", "metadata": {"aucs": [0.2475948097976708, 0.24757758543379027, 0.24785085354405323], "final_y": [0.1648557823261121, 0.1648557840513849, 0.16485577830724463]}, "mutation_prompt": null}
{"id": "9cdabf60-046c-4b04-a7eb-58cac658d6b5", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * (0.1 + 0.2 * (self.best_obj / np.min(self.personal_best_obj)))  # Time-varying velocity limit\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.4 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Particle Swarm Optimization with improved dynamic velocity control using time-varying velocity limits.", "configspace": "", "generation": 22, "fitness": 0.24505978846866525, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3a8f071f-95d7-46e1-ba9f-f0754dba4423", "metadata": {"aucs": [0.24191184265957477, 0.24670677771231309, 0.24656074503410785], "final_y": [0.18187812748971466, 0.16485647400307024, 0.16485578767027365]}, "mutation_prompt": null}
{"id": "9642a2b4-d494-49b1-8c86-6a6f40806b6e", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(5, int(budget / (10 * dim)))  # Adaptive number of particles\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.momentum = 0.2  # Introduce momentum to velocity\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity + self.momentum * self.velocities[i]\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.4 * (evaluations / self.budget)\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "A modified Particle Swarm Optimization algorithm with momentum and adaptive num_particles for enhanced exploration and exploitation balance.", "configspace": "", "generation": 23, "fitness": 0.24506009740205748, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.000. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "3a8f071f-95d7-46e1-ba9f-f0754dba4423", "metadata": {"aucs": [0.24495863305961396, 0.24551634337945571, 0.24470531576710275], "final_y": [0.16730526493452924, 0.16581995029722008, 0.1659514347222737]}, "mutation_prompt": null}
{"id": "92eeff62-ef72-4ab9-9d09-b6271b7342a3", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i] * (1 - progress_ratio)  # Apply progress scaling\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.4 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Particle Swarm Optimization with enhanced adaptive inertia and dynamic velocity control, now with improved velocity adjustment during progress.", "configspace": "", "generation": 24, "fitness": 0.20086294375475547, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.201 with standard deviation 0.004. And the mean value of best solutions found was 0.473 (0. is the best) with standard deviation 0.041.", "error": "", "parent_id": "3a8f071f-95d7-46e1-ba9f-f0754dba4423", "metadata": {"aucs": [0.20013559483119503, 0.19656906128712337, 0.20588417514594803], "final_y": [0.4795248947620613, 0.5192011654580759, 0.42044898739920933]}, "mutation_prompt": null}
{"id": "c3e08e27-78f7-4864-ab3d-69734e7a940b", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(self.lb - self.ub, self.ub - self.lb, (self.num_particles, self.dim))  # Improved velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.4 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Particle Swarm Optimization with enhanced adaptive inertia, dynamic velocity control, and improved velocity initialization.", "configspace": "", "generation": 25, "fitness": 0.239585518126156, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.240 with standard deviation 0.008. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "3a8f071f-95d7-46e1-ba9f-f0754dba4423", "metadata": {"aucs": [0.24336476815733188, 0.24753441992418168, 0.2278573662969544], "final_y": [0.18187809677092304, 0.16485577499871573, 0.2578100863705999]}, "mutation_prompt": null}
{"id": "203c7cd0-9d29-4dee-927c-01c37702652e", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.4 * (evaluations / self.budget) ** 2  # Nonlinear update of inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Particle Swarm Optimization with nonlinear adaptive inertia for improved convergence speed and accuracy.", "configspace": "", "generation": 26, "fitness": 0.24761449337731226, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3a8f071f-95d7-46e1-ba9f-f0754dba4423", "metadata": {"aucs": [0.24771943109468297, 0.24745689778399305, 0.24766715125326078], "final_y": [0.16485577190590128, 0.16485577715058675, 0.16485590945909712]}, "mutation_prompt": null}
{"id": "5ba087fb-087a-4dc6-9826-50918dfadc1e", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.2  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.4 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Particle Swarm Optimization with adaptive velocity clipping and improved inertia update.", "configspace": "", "generation": 27, "fitness": 0.24648291589741056, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3a8f071f-95d7-46e1-ba9f-f0754dba4423", "metadata": {"aucs": [0.2472482679396364, 0.24619455810006707, 0.24600592165252821], "final_y": [0.1648563358023719, 0.16485577408063035, 0.16485577488800174]}, "mutation_prompt": null}
{"id": "28b2a1fb-0aa9-46af-8009-9ed1a2bc7fc6", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * np.mean(np.std(self.positions, axis=0))  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.4 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Adaptive inertia and velocity limits based on diversity measure to enhance exploration and exploitation balance.", "configspace": "", "generation": 28, "fitness": 0.24327882275292143, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.243 with standard deviation 0.001. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3a8f071f-95d7-46e1-ba9f-f0754dba4423", "metadata": {"aucs": [0.24149102811869028, 0.24426190197011055, 0.24408353816996342], "final_y": [0.18187811393749476, 0.1648612431521893, 0.16485609131600776]}, "mutation_prompt": null}
{"id": "7f4dd960-53c1-4eb3-b902-5bdaa9a965e0", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, int(0.1 * budget / dim))  # Adjust number of particles based on budget and dimension\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.4 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Particle Swarm Optimization with enhanced adaptive inertia, dynamic velocity control, and adaptive particle count adjustment.", "configspace": "", "generation": 29, "fitness": 0.24640349234583547, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3a8f071f-95d7-46e1-ba9f-f0754dba4423", "metadata": {"aucs": [0.24613201818208186, 0.24655955526792372, 0.24651890358750084], "final_y": [0.164856116182459, 0.16485592581699027, 0.16485601097573677]}, "mutation_prompt": null}
{"id": "fbd29512-2c89-47ab-8058-a4cd8a4fa5c1", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n            if np.mean(np.abs(self.velocities)) < 0.01:  # Adaptive swarm diversity control\n                self.velocities[i] = np.random.uniform(-1, 1, self.dim)  # Reinitialize velocities if diversity is low\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.4 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Particle Swarm Optimization with adaptive swarm diversity control to enhance exploration-exploitation balance.", "configspace": "", "generation": 30, "fitness": 0.24782363902966745, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3a8f071f-95d7-46e1-ba9f-f0754dba4423", "metadata": {"aucs": [0.24787925076789608, 0.2477576607299723, 0.247834005591134], "final_y": [0.1648562020584753, 0.16485577594137923, 0.1648557844748515]}, "mutation_prompt": null}
{"id": "00bc7cbb-de8d-4f5f-947d-39408a7a8034", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10 + (self.dim // 10)  # Adaptive particle count\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.4 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Enhanced Particle Swarm Optimization with adaptive num_particles for improved convergence.", "configspace": "", "generation": 31, "fitness": 0.24754276960737123, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3a8f071f-95d7-46e1-ba9f-f0754dba4423", "metadata": {"aucs": [0.24789038829864696, 0.24763258146429823, 0.24710533905916854], "final_y": [0.1648558030357934, 0.16485577252768224, 0.1648558004249232]}, "mutation_prompt": null}
{"id": "55d720e8-fe0f-49c3-9f7c-02faac98896e", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            # Dynamically adjust c1 and c2 based on progress\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)  # Limit the velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Dynamically update inertia weight\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Particle Swarm Optimization with enhanced inertia dynamics and adaptive velocity control.", "configspace": "", "generation": 32, "fitness": 0.24781122610938378, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3a8f071f-95d7-46e1-ba9f-f0754dba4423", "metadata": {"aucs": [0.24786869842307568, 0.2477222239724559, 0.24784275593261973], "final_y": [0.16485579479828227, 0.1648561585144631, 0.16485636303795947]}, "mutation_prompt": null}
{"id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a global diversity mechanism and adaptive inertia update to enhance exploration and convergence.", "configspace": "", "generation": 33, "fitness": 0.24787369063832862, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3a8f071f-95d7-46e1-ba9f-f0754dba4423", "metadata": {"aucs": [0.24778370455662158, 0.24794028868727813, 0.24789707867108612], "final_y": [0.16487475960750686, 0.16487555748965244, 0.16487203707805664]}, "mutation_prompt": null}
{"id": "6e28f571-46be-465d-b1a4-e072c8b8e397", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            self.num_particles = max(3, int(10 - 7 * (evaluations / self.budget)))  # Dynamic number of particles\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a dynamic component to adaptively adjust the number of particles based on budget utilization.", "configspace": "", "generation": 34, "fitness": 0.24750910239286908, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24776338202458614, 0.24775692768478608, 0.24700699746923505], "final_y": [0.16487680737945476, 0.16489135043965808, 0.16488454987478918]}, "mutation_prompt": null}
{"id": "09f02892-f2dc-4c46-9502-03d7c22dbf1d", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            noise_factor = 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Noise for diversity\n            self.positions[i] += self.velocities[i] + np.random.normal(0, 0.1) * noise_factor  # Modify line for noise control\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce noise-controlled diversity mechanism to balance exploration and exploitation.", "configspace": "", "generation": 35, "fitness": 0.24771530624885238, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24775285458678487, 0.24750130733937492, 0.24789175682039732], "final_y": [0.16485590690755847, 0.1648559239822942, 0.1648558104813228]}, "mutation_prompt": null}
{"id": "b15ea2b6-39cc-4d3d-8bb0-b296e34b52f7", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, dim)  # Adaptive number of particles based on dimensionality\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce an adaptive number of particles based on the dimensionality of the problem to improve exploration and convergence in Particle Swarm Optimization.", "configspace": "", "generation": 36, "fitness": 0.24787369063832862, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24778370455662158, 0.24794028868727813, 0.24789707867108612], "final_y": [0.16487475960750686, 0.16487555748965244, 0.16487203707805664]}, "mutation_prompt": null}
{"id": "486af446-cb70-4766-bd87-f1bae46c773a", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            self.num_particles = max(5, int(10 * (1 - evaluations / self.budget)))  # Adaptive number of particles\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce an adaptive number of particles based on the progress of optimization to improve exploration and exploitation balance.", "configspace": "", "generation": 37, "fitness": 0.2475220293717081, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24782308128018227, 0.24777175000427198, 0.24697125683067012], "final_y": [0.16490211586841252, 0.16489572610957992, 0.16493163760760055]}, "mutation_prompt": null}
{"id": "c8a3dff3-5473-44e9-802a-a2d188468831", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            self.num_particles = max(5, int(10 * (1 - evaluations / self.budget)))  # Dynamic particle number\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a dynamic particle number to adjust exploration and exploitation balance over iterations.", "configspace": "", "generation": 38, "fitness": 0.24764748280876558, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24765235797555696, 0.2477575097632272, 0.2475325806875126], "final_y": [0.1648951034174294, 0.16489829400570644, 0.16489397277958484]}, "mutation_prompt": null}
{"id": "162ec1af-2932-49fc-9d90-a196c3a94dbb", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.num_particles = min(max(5, self.num_particles + int(0.01 * self.budget)), 50)  # Adjust particle number\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a dynamic particle number adjustment feature to enhance exploration and convergence.", "configspace": "", "generation": 39, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 10 is out of bounds for axis 0 with size 10').", "error": "IndexError('index 10 is out of bounds for axis 0 with size 10')", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {}, "mutation_prompt": null}
{"id": "52bc2f50-0250-43bd-9e6a-19d919848875", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.015 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Increased mutation for diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Integrate an adaptive mutation mechanism to further enhance exploration capabilities.", "configspace": "", "generation": 40, "fitness": 0.24774376525663414, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24773350903116065, 0.24782957062104782, 0.24766821611769396], "final_y": [0.16490322438428573, 0.1649015751157552, 0.1649163606340831]}, "mutation_prompt": null}
{"id": "e5d9d464-32a1-45ae-9d8f-36c3937b2095", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            # Change the expression to adjust dynamically\n            self.c2 = 1.5 - 0.5 * (progress_ratio ** 2) \n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a dynamic strategy for updating cognitive and social components to balance exploration and exploitation.", "configspace": "", "generation": 41, "fitness": 0.24772886984251172, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24738947876075212, 0.24799367386578164, 0.24780345690100136], "final_y": [0.1648708133538842, 0.16487971315069994, 0.16487562114374932]}, "mutation_prompt": null}
{"id": "f3a49485-fdcc-422c-82e0-03b90e27b7c9", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            random_velocity = 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity + random_velocity * np.random.randn(self.dim)\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Improve diversity by introducing random velocity direction adjustment.", "configspace": "", "generation": 42, "fitness": 0.24743256210392447, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.247 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.2474143838567432, 0.24762554063092002, 0.2472577618241102], "final_y": [0.16487265527872175, 0.16489647215345593, 0.16488654652049273]}, "mutation_prompt": null}
{"id": "3c818ff4-6a8b-4ab2-a5a1-449302f021a1", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((evaluations / self.budget) ** 2)  # Nonlinear inertia weight update\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a nonlinear inertia weight update strategy to enhance convergence while maintaining exploration.", "configspace": "", "generation": 43, "fitness": 0.24771428393890418, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24766740728284342, 0.2476830608181645, 0.24779238371570467], "final_y": [0.16486689317320247, 0.164876609698271, 0.1648813744767872]}, "mutation_prompt": null}
{"id": "5d933f34-9700-43d1-9e5e-bc3a02ded054", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget) ** 2  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a non-linear adaptive inertia update to improve convergence.", "configspace": "", "generation": 44, "fitness": 0.24771428393890418, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24766740728284342, 0.2476830608181645, 0.24779238371570467], "final_y": [0.16486689317320247, 0.164876609698271, 0.1648813744767872]}, "mutation_prompt": null}
{"id": "eebc3721-6525-42a4-918b-0871be9eb80e", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.05 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Enhance the diversity mechanism by increasing the randomness in position updates.", "configspace": "", "generation": 45, "fitness": 0.24754612589709393, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24754918910615065, 0.2476000641316597, 0.2474891244534715], "final_y": [0.16531846996181687, 0.16533976573510756, 0.16511803880186415]}, "mutation_prompt": null}
{"id": "04cc6ca1-8bf8-4a41-8e89-02b029ce8366", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n\n            if evaluations < self.budget // 2:  # Change: Dynamically adjust particle count\n                self.num_particles = min(self.num_particles + 1, 20)\n\n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Integrate dynamic particle count adjustment to improve exploration and convergence.", "configspace": "", "generation": 46, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 10 is out of bounds for axis 0 with size 10').", "error": "IndexError('index 10 is out of bounds for axis 0 with size 10')", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {}, "mutation_prompt": null}
{"id": "925b759b-a547-40e5-842c-7d6f395aad1b", "solution": "import numpy as np\n\nclass EnhancedParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 2.0  # initial cognitive component\n        self.c2 = 2.0  # initial social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def chaotic_local_search(self, pos):\n        chaos_factor = 0.1\n        return pos + chaos_factor * np.sin(np.arange(self.dim))\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / (self.personal_best_obj[i] + np.finfo(float).eps)\n            self.c1 = 2.5 - 1.5 * progress_ratio\n            self.c2 = 1.5 + 1.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n            \n            # Apply chaotic local search to enhance exploration\n            if np.random.rand() < 0.1:\n                self.positions[i] = self.chaotic_local_search(self.positions[i])\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget) \n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "EnhancedParticleSwarmOptimization", "description": "Introduce a dynamic learning rate and chaotic local search to improve exploration and exploitation balance in PSO.", "configspace": "", "generation": 47, "fitness": 0.24737452936961654, "feedback": "The algorithm EnhancedParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.247 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24761013626957107, 0.2477493416349661, 0.24676411020431244], "final_y": [0.16485585334327924, 0.1648565644736576, 0.16485603615764544]}, "mutation_prompt": null}
{"id": "5f00b3ba-3519-4fa5-a17c-7f7df8c63f7c", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n            if np.random.rand() < 0.05:  # Mutation mechanism for exploration\n                self.positions[i] += np.random.normal(0, 0.1, self.dim) * (self.ub - self.lb)\n\n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a mutation mechanism to improve exploration and avoid local optima.", "configspace": "", "generation": 48, "fitness": 0.24755250500424927, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.2476518954255541, 0.2477087525572338, 0.2472968670299599], "final_y": [0.16488330328368583, 0.16489521785934624, 0.16489396126695655]}, "mutation_prompt": null}
{"id": "f09faa7c-0f50-4520-9693-f36871a70356", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget) * np.exp(-evaluations/self.budget)  # Introduce temperature-based adaptation\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a temperature-based mechanism in inertia weight adaptation for enhanced convergence and exploration.", "configspace": "", "generation": 49, "fitness": 0.2478636349772317, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24780997826691864, 0.2479315596657965, 0.24784936699897997], "final_y": [0.1649265999716979, 0.16492746224386257, 0.1649015368130775]}, "mutation_prompt": null}
{"id": "20afd07e-c316-4389-a899-183f6d6a734c", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 2.0 - 1.0 * (evaluations / self.budget)  # Time-varying cognitive component\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce time-varying cognitive and social components to enhance adaptive exploration and convergence.", "configspace": "", "generation": 50, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {}, "mutation_prompt": null}
{"id": "d1e5fc67-cedc-4029-ae91-733bd6296bdd", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget))) # Adaptive particle count\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce adaptive particle count to balance exploration and exploitation dynamically.", "configspace": "", "generation": 51, "fitness": 0.24752594041969167, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24778193909342894, 0.2477735391598398, 0.24702234300580628], "final_y": [0.16487840563100653, 0.16488527773520956, 0.16490470138437108]}, "mutation_prompt": null}
{"id": "568f8f4c-a264-4938-851e-183c1fcc0f69", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            # Dynamic population size adjustment\n            self.num_particles = int(10 + (evalutions / self.budget) * 5)  # Change: Update population size\n\n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a dynamic population size adjustment based on optimization progress to enhance convergence rates.", "configspace": "", "generation": 52, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evalutions' is not defined\").", "error": "NameError(\"name 'evalutions' is not defined\")", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {}, "mutation_prompt": null}
{"id": "1ff7eebe-98a6-443d-8ea2-2433f71792ef", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, int(0.1 * dim))  # Adaptive particle count\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            directed_perturbation = 0.01 * (self.best_solution - self.positions[i])  # Directed perturbation\n            self.positions[i] += self.velocities[i] + directed_perturbation\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Enhance exploration and convergence by introducing adaptive particle count and directed perturbations.", "configspace": "", "generation": 53, "fitness": 0.2477600054692012, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24789677172687574, 0.2476337020227929, 0.24774954265793503], "final_y": [0.16485615762035988, 0.16485592263838433, 0.16485587223802078]}, "mutation_prompt": null}
{"id": "131ee918-3d66-4868-9c3c-714b2f551a40", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            # Adaptive update of cognitive and social components\n            self.c1 = 1.5 + 0.5 * (1 - (evaluations / self.budget))\n            self.c2 = 1.5 - 0.5 * (evaluations / self.budget)\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce an adaptive cognitive-social weight update to enhance solution exploration and convergence.", "configspace": "", "generation": 54, "fitness": 0.24787369063832862, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24778370455662158, 0.24794028868727813, 0.24789707867108612], "final_y": [0.16487475960750686, 0.16487555748965244, 0.16487203707805664]}, "mutation_prompt": null}
{"id": "fe579b84-66bd-4920-8124-082b4c8abe20", "solution": "import numpy as np\n\nclass HybridDEAS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.temperature = 100.0\n        self.cooling_rate = 0.99\n        self.best_solution = None\n        self.best_obj = float('inf')\n\n    def initialize_population(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.population = np.random.uniform(self.lb, self.ub, (self.population_size, self.dim))\n        self.objectives = np.array([float('inf')] * self.population_size)\n\n    def differential_evolution_step(self):\n        new_population = np.copy(self.population)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[indices]\n            mutant = a + self.mutation_factor * (b - c)\n            mutant = np.clip(mutant, self.lb, self.ub)\n            trial = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant, self.population[i])\n            trial_obj = func(trial)\n            if trial_obj < self.objectives[i]:\n                new_population[i] = trial\n                self.objectives[i] = trial_obj\n        self.population = new_population\n\n    def simulated_annealing_step(self, func):\n        for i in range(self.population_size):\n            candidate = self.population[i] + np.random.normal(0, 0.1, self.dim)\n            candidate = np.clip(candidate, self.lb, self.ub)\n            candidate_obj = func(candidate)\n            if candidate_obj < self.objectives[i] or np.random.rand() < np.exp((self.objectives[i] - candidate_obj) / self.temperature):\n                self.population[i] = candidate\n                self.objectives[i] = candidate_obj\n        self.temperature *= self.cooling_rate\n\n    def __call__(self, func):\n        self.initialize_population(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            self.differential_evolution_step()\n            evaluations += self.population_size\n\n            self.simulated_annealing_step(func)\n            evaluations += self.population_size\n\n            for i in range(self.population_size):\n                if self.objectives[i] < self.best_obj:\n                    self.best_obj = self.objectives[i]\n                    self.best_solution = self.population[i]\n        \n        return self.best_solution", "name": "HybridDEAS", "description": "Utilize a hybrid of differential evolution and adaptive simulated annealing to balance exploration and exploitation across diverse search spaces.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {}, "mutation_prompt": null}
{"id": "f7908b5b-d07a-4d69-9616-32160584a592", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget) + np.random.uniform(-0.05, 0.05)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce randomness in inertia weight update for enhanced exploration and convergence.", "configspace": "", "generation": 56, "fitness": 0.24756434457795373, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24798252007774968, 0.24740365934036923, 0.24730685431574229], "final_y": [0.16487869575296288, 0.16487716029781596, 0.16487801310103867]}, "mutation_prompt": null}
{"id": "91a781f0-7227-48df-880d-648563107ae8", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(5, int(10 * (1 + np.log(1 + dim)/np.log(50))))  # Self-adaptive particle count\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a self-adaptive particle count mechanism to dynamically balance exploration and exploitation.", "configspace": "", "generation": 57, "fitness": 0.24757802424821276, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24760807086009218, 0.2478170270538218, 0.24730897483072434], "final_y": [0.16489185666991868, 0.1648775380245041, 0.1648751153673117]}, "mutation_prompt": null}
{"id": "546eda1a-3d81-4b7f-acda-03eff95e770d", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 * (1 - progress_ratio) + 0.5 * progress_ratio\n            self.c2 = 1.5 * progress_ratio + 0.5 * (1 - progress_ratio)\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Integrate dynamic cognitive and social coefficients with a diversity boost through random perturbations for a balanced exploration-exploitation trade-off.", "configspace": "", "generation": 58, "fitness": 0.24770679919171015, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24793843120418424, 0.24785316415515468, 0.24732880221579157], "final_y": [0.16494098988288697, 0.16495414680205334, 0.16497175234060435]}, "mutation_prompt": null}
{"id": "30870505-5c59-4750-970f-0f5c3dea94c3", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            diversity_factor = np.std(self.positions, axis=0).mean() / (self.ub - self.lb).mean()  # New line\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio) * diversity_factor  # Modified line\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Incorporate a learning factor adjustment based on swarm diversity to enhance solution quality and convergence rate.", "configspace": "", "generation": 59, "fitness": 0.2477760492871219, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24765754039355903, 0.24782465236407503, 0.24784595510373164], "final_y": [0.16488380275462644, 0.16488286464758628, 0.164874453592321]}, "mutation_prompt": null}
{"id": "d910beb9-d763-4de9-a646-e7a7abf573d6", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget) + np.random.uniform(-0.05, 0.05)  # Introduce randomness in inertia\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a slight randomness in inertia weight for better exploration.", "configspace": "", "generation": 60, "fitness": 0.24756434457795373, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24798252007774968, 0.24740365934036923, 0.24730685431574229], "final_y": [0.16487869575296288, 0.16487716029781596, 0.16487801310103867]}, "mutation_prompt": null}
{"id": "96161895-4d80-44c1-b431-ad19243bf3a8", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n\n            if evaluations % 50 == 0:  # Random restart mechanism\n                self.initialize_particles(func.bounds)\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a random restart mechanism to escape local optima and enhance exploration.", "configspace": "", "generation": 61, "fitness": 0.24380719236149961, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.002. And the mean value of best solutions found was 0.179 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.2465496418917814, 0.2435659175949232, 0.24130601759779424], "final_y": [0.16680747382600758, 0.17865190147931942, 0.19182453895260487]}, "mutation_prompt": null}
{"id": "981c651d-3391-4360-9439-3ab7c92b0cdc", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.65 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Adjust the inertia weight decrease rate to improve convergence.", "configspace": "", "generation": 62, "fitness": 0.2478484207982048, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24783757217692037, 0.2479478414827473, 0.24775984873494672], "final_y": [0.16487801491145848, 0.16487762448535181, 0.16487776296680812]}, "mutation_prompt": null}
{"id": "67f2253d-7a6e-4b13-a4e3-5f0198c39881", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            diversity_scale = 0.01 * (1 - (self.best_obj / float('inf')))  # Dynamic scaling\n            self.positions[i] += self.velocities[i] + diversity_scale * (np.random.uniform(self.lb, self.ub) - self.positions[i])\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Enhance particle diversity by dynamically scaling the random perturbation based on optimization progress.", "configspace": "", "generation": 63, "fitness": 0.24787369063832862, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24778370455662158, 0.24794028868727813, 0.24789707867108612], "final_y": [0.16487475960750686, 0.16487555748965244, 0.16487203707805664]}, "mutation_prompt": null}
{"id": "88d5743d-8f8e-4338-af62-dad4712f13de", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a random inertia factor scaling to improve exploration and convergence adaptivity.", "configspace": "", "generation": 64, "fitness": 0.24804653183076233, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cbde77e-c264-49f7-bbaf-16094e2d3101", "metadata": {"aucs": [0.24809236383775535, 0.24813097762511938, 0.2479162540294123], "final_y": [0.1648720423956076, 0.16487465176430993, 0.16487015291232454]}, "mutation_prompt": null}
{"id": "4d47907f-8102-4bc6-b039-dd7fa007de83", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((evaluations / self.budget) ** 2)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce non-linear damping to inertia weight for better exploration-exploitation balance.", "configspace": "", "generation": 65, "fitness": 0.24804579030187757, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "88d5743d-8f8e-4338-af62-dad4712f13de", "metadata": {"aucs": [0.24808826125434635, 0.2481293608852735, 0.24791974876601286], "final_y": [0.16487590313431055, 0.16487788575165296, 0.1648781877740506]}, "mutation_prompt": null}
{"id": "956ed329-8e3a-4187-9e33-4c3d204c4dd2", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            self.num_particles = min(self.num_particles + 1, int(self.budget / self.dim))  # Dynamic adjustment of particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Implement a dynamic adjustment of the number of particles to enhance diversity and exploration adaptivity.", "configspace": "", "generation": 66, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "88d5743d-8f8e-4338-af62-dad4712f13de", "metadata": {}, "mutation_prompt": null}
{"id": "cd79ad3a-a7be-4b51-b500-f7a307c85b19", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, dim)  # Dynamic number of particles based on dimensionality\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a dynamic number of particles to adaptively balance exploration and exploitation.", "configspace": "", "generation": 67, "fitness": 0.24804653183076233, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "88d5743d-8f8e-4338-af62-dad4712f13de", "metadata": {"aucs": [0.24809236383775535, 0.24813097762511938, 0.2479162540294123], "final_y": [0.1648720423956076, 0.16487465176430993, 0.16487015291232454]}, "mutation_prompt": null}
{"id": "b2331485-057c-491a-9398-7259a9d21af1", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            # Adjust cognitive and social components based on evaluations ratio\n            eval_ratio = evaluations / self.budget\n            self.c1 = 1.5 * (1 - eval_ratio) + 2 * eval_ratio\n            self.c2 = 1.5 * (1 - eval_ratio) + 2 * eval_ratio\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Enhance the exploration-exploitation balance by adjusting the cognitive and social components based on the evaluations ratio.", "configspace": "", "generation": 68, "fitness": 0.24804653183076233, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "88d5743d-8f8e-4338-af62-dad4712f13de", "metadata": {"aucs": [0.24809236383775535, 0.24813097762511938, 0.2479162540294123], "final_y": [0.1648720423956076, 0.16487465176430993, 0.16487015291232454]}, "mutation_prompt": null}
{"id": "34c4040b-34e4-458b-b5d8-7b24282ad20d", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity + 0.01 * np.random.randn(self.dim)  # Enhance exploration\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Diversify the velocity update by adding a time-varying random factor to enhance exploration.", "configspace": "", "generation": 69, "fitness": 0.247959276904228, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "88d5743d-8f8e-4338-af62-dad4712f13de", "metadata": {"aucs": [0.2480968577843048, 0.24782399133199018, 0.24795698159638901], "final_y": [0.16487865251511657, 0.16487624076580254, 0.16487183102121694]}, "mutation_prompt": null}
{"id": "71cf275c-c3bd-4210-99d6-6a4a125725e6", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Enhance diversity by introducing a small random displacement to particle positions each iteration.", "configspace": "", "generation": 70, "fitness": 0.2480225000780852, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "88d5743d-8f8e-4338-af62-dad4712f13de", "metadata": {"aucs": [0.2480879414281243, 0.24791532474169176, 0.24806423406443956], "final_y": [0.16491932698410228, 0.16491637156218264, 0.16489974642794114]}, "mutation_prompt": null}
{"id": "099d7944-92ff-47c3-84df-4248360ba17e", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio) * np.random.rand()  # Changed line\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce random cognitive and social components to improve solution diversity and robustness.", "configspace": "", "generation": 71, "fitness": 0.2479657606659865, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "88d5743d-8f8e-4338-af62-dad4712f13de", "metadata": {"aucs": [0.24806124375464977, 0.24785767320945618, 0.2479783650338535], "final_y": [0.16487861538009108, 0.16486996029597956, 0.16487804218750757]}, "mutation_prompt": null}
{"id": "60b7d050-92dc-42cb-bde9-f3d40b676e0d", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            if evaluations % (self.budget // 10) == 0:  # Adaptive exploration boost\n                self.positions += 0.05 * np.random.randn(self.num_particles, self.dim) * (self.ub - self.lb)\n\n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce an adaptive random exploration boost to enhance particle exploration in stagnation phases.", "configspace": "", "generation": 72, "fitness": 0.24804640749209914, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "88d5743d-8f8e-4338-af62-dad4712f13de", "metadata": {"aucs": [0.24809328262484942, 0.24813018494272332, 0.24791575490872464], "final_y": [0.16487051754795023, 0.16487475409802688, 0.16486386052938506]}, "mutation_prompt": null}
{"id": "dc673f81-b86f-41b6-8a23-f9d939cd89d8", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            iteration_ratio = min(1.0, evaluations / self.budget)  # Dynamic adjustment\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio * iteration_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio * iteration_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a dynamic adjustment of cognitive and social components based on iteration progress to improve convergence speed and diversity.", "configspace": "", "generation": 73, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "88d5743d-8f8e-4338-af62-dad4712f13de", "metadata": {}, "mutation_prompt": null}
{"id": "52040912-b5e8-4f05-9716-aebd87107e8a", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Implement dynamic adjustment of the number of particles based on the current progress to balance exploration and exploitation.", "configspace": "", "generation": 74, "fitness": 0.24807706022267792, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "88d5743d-8f8e-4338-af62-dad4712f13de", "metadata": {"aucs": [0.24811192204593746, 0.24814123786864273, 0.24797802075345354], "final_y": [0.16488544269618255, 0.16487268200409844, 0.16487658195931143]}, "mutation_prompt": null}
{"id": "bf063da3-3275-4704-a0f5-644b0e51b472", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.5 + 0.4 * (1 - (evaluations / self.budget))  # Refined inertia update for better adaptability\n\n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Refine the inertia weight update for better adaptability over iterations to enhance convergence.", "configspace": "", "generation": 75, "fitness": 0.24807164211144817, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "52040912-b5e8-4f05-9716-aebd87107e8a", "metadata": {"aucs": [0.24809977713456388, 0.2481414075509305, 0.2479737416488501], "final_y": [0.1648750712687176, 0.16487956261739056, 0.1648836854770721]}, "mutation_prompt": null}
{"id": "24188052-d7ae-48d2-adad-1ac8f0f7d80a", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Enhance exploration by initializing particles with a wider velocity range.", "configspace": "", "generation": 76, "fitness": 0.2481096623324627, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "52040912-b5e8-4f05-9716-aebd87107e8a", "metadata": {"aucs": [0.24812198787711526, 0.24815942123937706, 0.24804757788089582], "final_y": [0.16486679089969747, 0.16487208619490423, 0.1648739664146216]}, "mutation_prompt": null}
{"id": "97d70358-faf5-4678-b151-c5dc7a1e95e2", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.01 * (np.random.uniform(self.lb, self.ub) - self.positions[i]) + np.random.normal(0, 0.1, self.dim)  # Enhance diversity with Gaussian noise\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Integrate Gaussian perturbation to enhance diversity within the swarm.", "configspace": "", "generation": 77, "fitness": 0.2480498392096053, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "24188052-d7ae-48d2-adad-1ac8f0f7d80a", "metadata": {"aucs": [0.2480772425099358, 0.24798436554307313, 0.24808790957580695], "final_y": [0.164874100444941, 0.16487990153962329, 0.16487594280573992]}, "mutation_prompt": null}
{"id": "85918748-3ac8-4f0b-85ad-a58a32e29a82", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Enhance exploration by increasing the random diversity factor in position updates.", "configspace": "", "generation": 78, "fitness": 0.24811433722039022, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "24188052-d7ae-48d2-adad-1ac8f0f7d80a", "metadata": {"aucs": [0.24812658043560587, 0.24812956034337463, 0.24808687088219017], "final_y": [0.16493701485976864, 0.16493648173572162, 0.16491953024683137]}, "mutation_prompt": null}
{"id": "5d1fcb61-09e6-46cc-b3b2-6b5b6aec63e3", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * (0.5 + 0.5 * (evaluations / self.budget)) * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity  # Velocity scaling change\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Incorporate velocity scaling based on the evaluation progress to improve convergence.", "configspace": "", "generation": 79, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "85918748-3ac8-4f0b-85ad-a58a32e29a82", "metadata": {}, "mutation_prompt": null}
{"id": "351eb70b-f143-4f74-9075-7e5ad8274abd", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio + 0.1 * (evaluations / self.budget)  # Change to improve social component\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Adaptive dynamic adjustment of the social component to improve convergence.", "configspace": "", "generation": 80, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "85918748-3ac8-4f0b-85ad-a58a32e29a82", "metadata": {}, "mutation_prompt": null}
{"id": "875cd824-8bee-4c71-9a62-c854edef76ed", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        diversity_factor = np.std(self.positions)\n        for i in range(self.num_particles):\n            # Adaptive learning rates\n            self.c1 = 1.5 + 0.5 * (1 - diversity_factor)\n            self.c2 = 1.5 - 0.5 * diversity_factor\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Enhance inertia update adaptivity\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Improve convergence by integrating adaptive learning rates for cognitive and social components based on particle diversity.", "configspace": "", "generation": 81, "fitness": 0.20299758827313896, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.203 with standard deviation 0.002. And the mean value of best solutions found was 0.450 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "85918748-3ac8-4f0b-85ad-a58a32e29a82", "metadata": {"aucs": [0.20511833023885329, 0.2023050048277486, 0.201569429752815], "final_y": [0.42953711726379473, 0.4572117062906401, 0.46469205749927356]}, "mutation_prompt": null}
{"id": "d8644925-c11d-4fbf-81e2-0563f3d644c4", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 * (1 - (evaluations / self.budget)**2)  # Nonlinear inertia update for sustained exploration\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a nonlinear inertia weight update to maintain exploration capabilities for a longer period.", "configspace": "", "generation": 82, "fitness": 0.24811202281959646, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "85918748-3ac8-4f0b-85ad-a58a32e29a82", "metadata": {"aucs": [0.24813276782144744, 0.24812393318634685, 0.24807936745099513], "final_y": [0.16492463171658733, 0.1649236962158932, 0.16492587895390043]}, "mutation_prompt": null}
{"id": "d8223bc8-e0da-4100-8cff-06afb4c29544", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((evaluations / self.budget) ** 1.2)  # Adaptive inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce adaptive inertia weight decay to balance exploration and exploitation more effectively.", "configspace": "", "generation": 83, "fitness": 0.2481146102196896, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "85918748-3ac8-4f0b-85ad-a58a32e29a82", "metadata": {"aucs": [0.24813696766634818, 0.24812416048511066, 0.24808270250761], "final_y": [0.16490213871678228, 0.1649419759889318, 0.16492781964171066]}, "mutation_prompt": null}
{"id": "838b1c07-a00f-48eb-9446-02d8327a6b1b", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def variable_neighborhood_search(self):  # New method\n        if np.random.rand() < 0.1:  # Occasionally perturb\n            idx = np.random.choice(self.num_particles, size=2, replace=False)  # Select two random particles\n            self.positions[idx] = np.random.uniform(self.lb, self.ub, (2, self.dim))  # Redefine their positions\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((evaluations / self.budget) ** 1.2)  # Adaptive inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.variable_neighborhood_search()  # Apply new method\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce variable neighborhood search to diversify the particle positions occasionally.", "configspace": "", "generation": 84, "fitness": 0.24804490153156156, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d8223bc8-e0da-4100-8cff-06afb4c29544", "metadata": {"aucs": [0.24815327521356922, 0.24806098619204808, 0.24792044318906736], "final_y": [0.16492090547709803, 0.16493966510830305, 0.1649341147947505]}, "mutation_prompt": null}
{"id": "c06befe4-d0de-492d-a92a-b69aaf91fb78", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.0 + 0.5 * progress_ratio  # Dynamic adjustment of social component\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((evaluations / self.budget) ** 1.2)  # Adaptive inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a dynamic adjustment of the social component to increase robustness in various stages of optimization.", "configspace": "", "generation": 85, "fitness": 0.24811103540763027, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d8223bc8-e0da-4100-8cff-06afb4c29544", "metadata": {"aucs": [0.24810952959056487, 0.24813177374689344, 0.24809180288543253], "final_y": [0.16495011671916515, 0.16497643669178963, 0.1649288115066515]}, "mutation_prompt": null}
{"id": "6d697752-171c-4b08-b86b-2c5ee6c3b161", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.positions += 0.01 * (np.random.rand(self.num_particles, self.dim) - 0.5) * (self.ub - self.lb)  # Mutation-based perturbation\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((evaluations / self.budget) ** 1.2)  # Adaptive inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a mutation-based perturbation to particle positions during initialization for enhanced exploration.", "configspace": "", "generation": 86, "fitness": 0.24801697484315868, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d8223bc8-e0da-4100-8cff-06afb4c29544", "metadata": {"aucs": [0.24815864611515448, 0.24800305722344296, 0.24788922119087864], "final_y": [0.16492367321308676, 0.16494449352535845, 0.1649223564293375]}, "mutation_prompt": null}
{"id": "7e79fd76-d414-4b9e-a668-0b005f9cfcae", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            # New neighborhood-based component\n            neighborhood_velocity = 0.1 * np.random.rand(self.dim) * (self.positions[(i+1) % self.num_particles] - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity + neighborhood_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((evaluations / self.budget) ** 1.2)  # Adaptive inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce neighborhood-based velocity update to enhance convergence by considering nearby particles' positions.", "configspace": "", "generation": 87, "fitness": 0.24812701711531823, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d8223bc8-e0da-4100-8cff-06afb4c29544", "metadata": {"aucs": [0.24814797743084904, 0.2481417644535452, 0.24809130946156044], "final_y": [0.16492599925666218, 0.16494747692200018, 0.1649037979657323]}, "mutation_prompt": null}
{"id": "42643789-2bcf-4991-9ead-314498cb595c", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            # New neighborhood-based component\n            neighborhood_velocity = 0.1 * np.random.rand(self.dim) * (self.positions[(i+1) % self.num_particles] - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity + neighborhood_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.1 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhanced diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((evaluations / self.budget) ** 1.2)  # Adaptive inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a mutation-based random position update to enhance exploration by occasionally resetting particles to a random position.", "configspace": "", "generation": 88, "fitness": 0.24754673627734547, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7e79fd76-d414-4b9e-a668-0b005f9cfcae", "metadata": {"aucs": [0.24745162512557428, 0.2477396106744183, 0.24744897303204383], "final_y": [0.16694914226559787, 0.1660208363503285, 0.16633954189871147]}, "mutation_prompt": null}
{"id": "83279f71-3d3e-4085-a1c7-0f274fbdcb1e", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            neighborhood_velocity = (0.1 + 0.1 * progress_ratio) * np.random.rand(self.dim) * (self.positions[(i+1) % self.num_particles] - self.positions[i])  # Adaptive neighborhood component\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity + neighborhood_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((evaluations / self.budget) ** 1.2)  # Adaptive inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce adaptive neighborhood influence to dynamically balance exploration and exploitation in the search space.", "configspace": "", "generation": 89, "fitness": 0.24811090814243186, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7e79fd76-d414-4b9e-a668-0b005f9cfcae", "metadata": {"aucs": [0.24817821351232006, 0.24814273993268088, 0.24801177098229465], "final_y": [0.1649268725790075, 0.16490189630540508, 0.1649194703267306]}, "mutation_prompt": null}
{"id": "361e0b88-3f19-46e7-ab83-4e46d92421c4", "solution": "import numpy as np\n\nclass HybridPSO_DE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.9\n        self.F = 0.5  # DE mutation factor\n        self.CR = 0.9  # DE crossover rate\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        for i in range(self.num_particles):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_velocity = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * r2 * (self.best_solution - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_velocity + social_velocity\n            \n            # Perform DE mutation and crossover\n            a, b, c = np.random.choice(self.num_particles, 3, replace=False)\n            mutant_vector = self.positions[a] + self.F * (self.positions[b] - self.positions[c])\n            crossover_mask = np.random.rand(self.dim) < self.CR\n            trial_vector = np.where(crossover_mask, mutant_vector, self.positions[i])\n            \n            # Update position\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.where(crossover_mask, trial_vector, self.positions[i])\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((evaluations / self.budget) ** 1.2)\n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "HybridPSO_DE", "description": "Leverage a hybridized approach combining particle swarm exploration with differential evolution mutation strategies to enhance diversity and convergence.", "configspace": "", "generation": 90, "fitness": 0.2165078860006091, "feedback": "The algorithm HybridPSO_DE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.217 with standard deviation 0.002. And the mean value of best solutions found was 0.296 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "7e79fd76-d414-4b9e-a668-0b005f9cfcae", "metadata": {"aucs": [0.2141946494125916, 0.2176645468546713, 0.21766446173456444], "final_y": [0.3301673285296194, 0.2660942820359582, 0.29092090451591224]}, "mutation_prompt": null}
{"id": "56e43f6f-a8fa-4b7b-bf1b-161e06e5e516", "solution": "import numpy as np\n\nclass ImprovedParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # initial cognitive component\n        self.c2 = 1.5  # initial social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.0 + 1.0 * (1 - progress_ratio)  # Adaptive cognitive component\n            self.c2 = 2.0 - 1.0 * progress_ratio  # Adaptive social component\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            # New neighborhood-based component\n            neighborhood_velocity = 0.2 * np.random.rand(self.dim) * (self.positions[(i+1) % self.num_particles] - self.positions[i])\n            \n            # Mutation-like perturbation for diversity\n            mutation = 0.05 * np.random.randn(self.dim) * (self.ub - self.lb)\n            \n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity + neighborhood_velocity + mutation\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = np.array([func(pos) for pos in self.positions])\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = np.copy(self.positions[i])\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = np.copy(self.positions[i])\n            \n            self.w = 0.9 - 0.6 * ((evaluations / self.budget) ** 1.2)  # Adaptive inertia weight decay\n            \n            self.update_velocities_and_positions()\n        \n        return self.best_solution", "name": "ImprovedParticleSwarmOptimization", "description": "Utilize adaptive learning rates in cognitive and social components to enhance exploration and convergence while introducing mutation-like perturbations for diversity.", "configspace": "", "generation": 91, "fitness": 0.24733108957944913, "feedback": "The algorithm ImprovedParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.247 with standard deviation 0.000. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7e79fd76-d414-4b9e-a668-0b005f9cfcae", "metadata": {"aucs": [0.24738531872743952, 0.2474423515491332, 0.24716559846177466], "final_y": [0.1669880188328816, 0.16704237831356084, 0.166406849463549]}, "mutation_prompt": null}
{"id": "cdff6331-b8e8-4ad6-933d-caacc18f08f6", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            # New neighborhood-based component\n            neighborhood_velocity = 0.1 * np.random.rand(self.dim) * (self.positions[(i+1) % self.num_particles] - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity + neighborhood_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((evaluations / self.budget) ** 1.2)  # Adaptive inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n            if evaluations < self.budget:  # Random reinitialization of worst particle if evaluation budget allows\n                worst_index = np.argmax(objectives)\n                self.positions[worst_index] = np.random.uniform(self.lb, self.ub, self.dim)\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Incorporate random reinitialization of worst-performing particles to enhance exploration and avoid local optima.", "configspace": "", "generation": 92, "fitness": 0.24809301107381665, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7e79fd76-d414-4b9e-a668-0b005f9cfcae", "metadata": {"aucs": [0.24812087498962865, 0.24810732077053854, 0.24805083746128276], "final_y": [0.16494373798373674, 0.16492927330042173, 0.1649343331992651]}, "mutation_prompt": null}
{"id": "9ad2cf9d-4839-42f4-b19f-e70bdf8d98fb", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            # New neighborhood-based component\n            neighborhood_velocity = 0.1 * np.random.rand(self.dim) * (self.positions[(i+1) % self.num_particles] - self.positions[i]) * (1 + progress_ratio)\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity + neighborhood_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((evaluations / self.budget) ** 1.2)  # Adaptive inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Add a learning factor to neighborhood velocity for improved adaptability.", "configspace": "", "generation": 93, "fitness": 0.24811090814243186, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7e79fd76-d414-4b9e-a668-0b005f9cfcae", "metadata": {"aucs": [0.24817821351232006, 0.24814273993268088, 0.24801177098229465], "final_y": [0.1649268725790075, 0.16490189630540508, 0.1649194703267306]}, "mutation_prompt": null}
{"id": "06fc8b72-b231-4934-b048-8aacdb78a17b", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            # New neighborhood-based component\n            neighborhood_velocity = 0.1 * np.random.rand(self.dim) * (self.positions[(i+1) % self.num_particles] - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity + neighborhood_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((1 - evaluations / self.budget) ** 1.2)  # Nonlinear inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a nonlinear update for the inertia weight to balance exploration and exploitation more effectively.", "configspace": "", "generation": 94, "fitness": 0.24813590520938303, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7e79fd76-d414-4b9e-a668-0b005f9cfcae", "metadata": {"aucs": [0.24820528706273648, 0.24817781212840806, 0.24802461643700457], "final_y": [0.16489074032647788, 0.16492654008389485, 0.16492825450858806]}, "mutation_prompt": null}
{"id": "c8a62055-d341-473b-91ee-2f7744a40473", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            # New neighborhood-based component\n            neighborhood_velocity = 0.1 * np.random.rand(self.dim) * (self.positions[(i+1) % self.num_particles] - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity + neighborhood_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((1 - evaluations / self.budget) ** 1.2)  # Nonlinear inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce dynamic adjustment to the cognitive component to balance exploration and exploitation more effectively.", "configspace": "", "generation": 95, "fitness": 0.24813590520938303, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "06fc8b72-b231-4934-b048-8aacdb78a17b", "metadata": {"aucs": [0.24820528706273648, 0.24817781212840806, 0.24802461643700457], "final_y": [0.16489074032647788, 0.16492654008389485, 0.16492825450858806]}, "mutation_prompt": null}
{"id": "450b3e47-0c11-4a2b-b59c-30aa54e99192", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            neighborhood_velocity = 0.1 * np.random.rand(self.dim) * (self.positions[(i+1) % self.num_particles] - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity + neighborhood_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((1 - evaluations / self.budget) ** 1.15)  # Adjusted nonlinear inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce adaptive cognitive and social coefficients based on individual particle performance.  ", "configspace": "", "generation": 96, "fitness": 0.24813571275986915, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "06fc8b72-b231-4934-b048-8aacdb78a17b", "metadata": {"aucs": [0.24820385084217267, 0.24817653810184126, 0.24802674933559354], "final_y": [0.16493499174182347, 0.16494319902378518, 0.16493227665624732]}, "mutation_prompt": null}
{"id": "28113cec-c2f5-4904-8421-cd54ef005658", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            # New neighborhood-based component\n            neighborhood_velocity = 0.2 * np.random.rand(self.dim) * (self.positions[(i+1) % self.num_particles] - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity + neighborhood_velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((1 - evaluations / self.budget) ** 1.2)  # Nonlinear inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce a scaling factor to the neighborhood-based component to improve solution diversity and convergence.", "configspace": "", "generation": 97, "fitness": 0.24810335970533912, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "06fc8b72-b231-4934-b048-8aacdb78a17b", "metadata": {"aucs": [0.24818652936526575, 0.2480351010096744, 0.24808844874107716], "final_y": [0.16491706703671283, 0.16490141391681123, 0.1649200151818908]}, "mutation_prompt": null}
{"id": "7b32cba8-69b4-4c44-be5a-e80eeff2d700", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            neighborhood_velocity = 0.1 * np.random.rand(self.dim) * (self.positions[(i+1) % self.num_particles] - self.positions[i])\n            success_factor = 1 if self.personal_best_obj[i] < self.best_obj else 0.5  # Adaptive momentum\n            self.velocities[i] = success_factor * (self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity + neighborhood_velocity)\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((1 - evaluations / self.budget) ** 1.2)  # Nonlinear inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Introduce adaptive momentum to velocities based on a particle's success rate to improve convergence.", "configspace": "", "generation": 98, "fitness": 0.24788056302770758, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "06fc8b72-b231-4934-b048-8aacdb78a17b", "metadata": {"aucs": [0.24794941960495287, 0.24805529393581438, 0.24763697554235553], "final_y": [0.16493202343424251, 0.16494326853422148, 0.1648937273390726]}, "mutation_prompt": null}
{"id": "14f9d357-304c-4de0-a78c-c9ea8a6876b4", "solution": "import numpy as np\n\nclass ParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.c1 = 1.5  # cognitive component\n        self.c2 = 1.5  # social component\n        self.w = 0.9   # initial inertia weight\n        self.best_solution = None\n        self.best_obj = float('inf')\n    \n    def initialize_particles(self, bounds):\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-3, 3, (self.num_particles, self.dim))  # Increased velocity range\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_obj = np.array([float('inf')] * self.num_particles)\n    \n    def update_velocities_and_positions(self):\n        velocity_limit_factor = (self.ub - self.lb) * 0.1  # Adaptive velocity clipping\n        for i in range(self.num_particles):\n            progress_ratio = self.best_obj / self.personal_best_obj[i]\n            self.c1 = 1.5 + 0.5 * (1 - progress_ratio)\n            self.c2 = 1.5 - 0.5 * progress_ratio\n            \n            cognitive_velocity = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.positions[i])\n            social_velocity = self.c2 * np.random.rand(self.dim) * (self.best_solution - self.positions[i])\n            # New neighborhood-based component\n            neighborhood_velocity = 0.1 * np.random.rand(self.dim) * (self.positions[(i+1) % self.num_particles] - self.positions[i])\n            self.velocities[i] = self.w * np.random.rand() * self.velocities[i] + cognitive_velocity + social_velocity + neighborhood_velocity + 0.01 # Minor tweak to velocity\n            self.velocities[i] = np.clip(self.velocities[i], -velocity_limit_factor, velocity_limit_factor)\n            self.positions[i] += self.velocities[i] + 0.02 * (np.random.uniform(self.lb, self.ub) - self.positions[i])  # Enhance diversity\n            self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n    \n    def __call__(self, func):\n        self.initialize_particles(func.bounds)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            objectives = [func(pos) for pos in self.positions]\n            evaluations += self.num_particles\n            \n            for i in range(self.num_particles):\n                if objectives[i] < self.personal_best_obj[i]:\n                    self.personal_best_obj[i] = objectives[i]\n                    self.personal_best_positions[i] = self.positions[i]\n                if objectives[i] < self.best_obj:\n                    self.best_obj = objectives[i]\n                    self.best_solution = self.positions[i]\n            \n            self.w = 0.9 - 0.6 * ((1 - evaluations / self.budget) ** 1.2)  # Nonlinear inertia weight decay\n            \n            self.update_velocities_and_positions()\n            self.num_particles = max(5, int(10 - 5 * (evaluations / self.budget)))  # Dynamic adjustment of num_particles\n        \n        return self.best_solution", "name": "ParticleSwarmOptimization", "description": "Utilize a dynamic inertia weight adjustment based on exploration-exploitation balance with a slight tweak to the velocity computation.", "configspace": "", "generation": 99, "fitness": 0.2481336432686794, "feedback": "The algorithm ParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.248 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "06fc8b72-b231-4934-b048-8aacdb78a17b", "metadata": {"aucs": [0.2482217635445758, 0.24817681910838285, 0.24800234715307956], "final_y": [0.1648882782293456, 0.164940930489909, 0.16493520492571212]}, "mutation_prompt": null}
