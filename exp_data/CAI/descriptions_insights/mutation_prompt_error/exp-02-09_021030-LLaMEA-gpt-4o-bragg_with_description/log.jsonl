{"id": "ae38402c-5d69-4b44-85b7-d625ceb5b508", "solution": "import numpy as np\n\nclass HybridHarmonySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.HMCR = 0.9  # Harmony Memory Consideration Rate\n        self.PAR = 0.3   # Pitch Adjustment Rate\n        self.bandwidth = 0.1\n        self.harmony_memory_size = 10\n        self.harmony_memory = None\n\n    def initialize_harmony_memory(self, bounds):\n        self.harmony_memory = np.random.uniform(bounds.lb, bounds.ub, (self.harmony_memory_size, self.dim))\n\n    def local_search(self, solution, bounds):\n        local_sol = solution + np.random.uniform(-self.bandwidth, self.bandwidth, self.dim)\n        return np.clip(local_sol, bounds.lb, bounds.ub)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_harmony_memory(bounds)\n        best_solution = None\n        best_score = float('-inf')\n        evaluations = 0\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n            for i in range(self.dim):\n                if np.random.rand() < self.HMCR:\n                    new_harmony[i] = self.harmony_memory[np.random.randint(self.harmony_memory_size)][i]\n                    if np.random.rand() < self.PAR:\n                        new_harmony[i] += self.bandwidth * (2 * np.random.rand() - 1)\n                else:\n                    new_harmony[i] = np.random.uniform(bounds.lb[i], bounds.ub[i])\n            \n            new_harmony = np.clip(new_harmony, bounds.lb, bounds.ub)\n            new_harmony_score = func(new_harmony)\n            evaluations += 1\n\n            if new_harmony_score > best_score:\n                best_score = new_harmony_score\n                best_solution = new_harmony\n\n            worst_idx = np.argmin([func(harmony) for harmony in self.harmony_memory])\n            if new_harmony_score > func(self.harmony_memory[worst_idx]):\n                self.harmony_memory[worst_idx] = new_harmony\n\n            # Local search phase\n            if evaluations < self.budget:\n                local_solution = self.local_search(best_solution, bounds)\n                local_score = func(local_solution)\n                evaluations += 1\n                if local_score > best_score:\n                    best_solution = local_solution\n                    best_score = local_score\n\n        return best_solution", "name": "HybridHarmonySearch", "description": "A hybrid harmony search algorithm enhanced with local search to effectively navigate multi-modal optimization landscapes by balancing exploration and exploitation.", "configspace": "", "generation": 0, "fitness": 0.20667444798557855, "feedback": "The algorithm HybridHarmonySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.207 with standard deviation 0.009. And the mean value of best solutions found was 0.422 (0. is the best) with standard deviation 0.075.", "error": "", "parent_id": null, "metadata": {"aucs": [0.19963971266144476, 0.2188161279192501, 0.2015675033760408], "final_y": [0.4851222111652522, 0.31684378595759133, 0.46469205749927356]}, "mutation_prompt": null}
{"id": "14fca994-ecf7-4107-9e3a-86e70d78e428", "solution": "import numpy as np\n\nclass EnhancedHybridHarmonySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.HMCR = 0.9  # Harmony Memory Consideration Rate\n        self.PAR = 0.3   # Initial Pitch Adjustment Rate\n        self.bandwidth = 0.1\n        self.harmony_memory_size = 10\n        self.harmony_memory = None\n        self.diversity_threshold = 0.05  # Threshold for diversity control\n\n    def initialize_harmony_memory(self, bounds):\n        self.harmony_memory = np.random.uniform(bounds.lb, bounds.ub, (self.harmony_memory_size, self.dim))\n\n    def local_search(self, solution, bounds):\n        local_sol = solution + np.random.uniform(-self.bandwidth, self.bandwidth, self.dim)\n        return np.clip(local_sol, bounds.lb, bounds.ub)\n\n    def adaptive_par(self, evaluations):\n        return self.PAR + 0.5 * (evaluations / self.budget)  # Adaptive PAR increases over time\n\n    def diversity_control(self):\n        # Calculate diversity as the variance of the harmony memory\n        diversity = np.var(self.harmony_memory, axis=0).mean()\n        return diversity < self.diversity_threshold\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_harmony_memory(bounds)\n        best_solution = None\n        best_score = float('-inf')\n        evaluations = 0\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n            current_PAR = self.adaptive_par(evaluations)\n\n            for i in range(self.dim):\n                if np.random.rand() < self.HMCR:\n                    new_harmony[i] = self.harmony_memory[np.random.randint(self.harmony_memory_size)][i]\n                    if np.random.rand() < current_PAR:\n                        new_harmony[i] += self.bandwidth * (2 * np.random.rand() - 1)\n                else:\n                    new_harmony[i] = np.random.uniform(bounds.lb[i], bounds.ub[i])\n            \n            new_harmony = np.clip(new_harmony, bounds.lb, bounds.ub)\n            new_harmony_score = func(new_harmony)\n            evaluations += 1\n\n            if new_harmony_score > best_score:\n                best_score = new_harmony_score\n                best_solution = new_harmony\n\n            worst_idx = np.argmin([func(harmony) for harmony in self.harmony_memory])\n            if new_harmony_score > func(self.harmony_memory[worst_idx]):\n                self.harmony_memory[worst_idx] = new_harmony\n\n            # Local Search Phase\n            if evaluations < self.budget:\n                local_solution = self.local_search(best_solution, bounds)\n                local_score = func(local_solution)\n                evaluations += 1\n                if local_score > best_score:\n                    best_solution = local_solution\n                    best_score = local_score\n\n            # Diversity Control: If diversity is low, reinitialize part of the harmony memory\n            if self.diversity_control() and evaluations < self.budget:\n                self.harmony_memory = np.random.uniform(bounds.lb, bounds.ub, (self.harmony_memory_size, self.dim))\n\n        return best_solution", "name": "EnhancedHybridHarmonySearch", "description": "Enhanced Hybrid Harmony Search with Adaptive Pitch Adjustment and Diversity Control for improved exploration and exploitation balance in multi-modal landscapes.", "configspace": "", "generation": 1, "fitness": 0.20667444798557855, "feedback": "The algorithm EnhancedHybridHarmonySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.207 with standard deviation 0.009. And the mean value of best solutions found was 0.422 (0. is the best) with standard deviation 0.075.", "error": "", "parent_id": "ae38402c-5d69-4b44-85b7-d625ceb5b508", "metadata": {"aucs": [0.19963971266144476, 0.2188161279192501, 0.2015675033760408], "final_y": [0.4851222111652522, 0.31684378595759133, 0.46469205749927356]}, "mutation_prompt": null}
{"id": "33d471cc-ee0f-4361-85d4-f74fa38ead12", "solution": "import numpy as np\n\nclass HybridHarmonySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.HMCR = 0.9  # Harmony Memory Consideration Rate\n        self.PAR = 0.3   # Pitch Adjustment Rate\n        self.bandwidth = 0.1\n        self.harmony_memory_size = 10\n        self.harmony_memory = None\n\n    def initialize_harmony_memory(self, bounds):\n        self.harmony_memory = np.random.uniform(bounds.lb, bounds.ub, (self.harmony_memory_size, self.dim))\n\n    def local_search(self, solution, bounds):\n        local_sol = solution + np.random.uniform(-self.bandwidth, self.bandwidth, self.dim)\n        return np.clip(local_sol, bounds.lb, bounds.ub)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_harmony_memory(bounds)\n        best_solution = None\n        best_score = float('-inf')\n        evaluations = 0\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n            for i in range(self.dim):\n                if np.random.rand() < self.HMCR:\n                    new_harmony[i] = self.harmony_memory[np.random.randint(self.harmony_memory_size)][i]\n                    if np.random.rand() < self.PAR:\n                        new_harmony[i] += self.bandwidth * (2 * np.random.rand() - 1)\n                else:\n                    new_harmony[i] = np.random.uniform(bounds.lb[i], bounds.ub[i])\n            \n            new_harmony = np.clip(new_harmony, bounds.lb, bounds.ub)\n            new_harmony_score = func(new_harmony)\n            evaluations += 1\n\n            if new_harmony_score > best_score:\n                best_score = new_harmony_score\n                best_solution = new_harmony\n                self.PAR *= 0.95  # Adaptive tuning\n\n            worst_idx = np.argmin([func(harmony) for harmony in self.harmony_memory])\n            if new_harmony_score > func(self.harmony_memory[worst_idx]):\n                self.harmony_memory[worst_idx] = new_harmony\n\n            # Local search phase\n            if evaluations < self.budget:\n                local_solution = self.local_search(best_solution, bounds)\n                local_score = func(local_solution)\n                evaluations += 1\n                if local_score > best_score:\n                    best_solution = local_solution\n                    best_score = local_score\n                    self.bandwidth *= 0.95  # Adaptive tuning\n\n        return best_solution", "name": "HybridHarmonySearch", "description": "A refined hybrid harmony search algorithm with adaptive parameter tuning to enhance search efficiency and convergence rates.", "configspace": "", "generation": 2, "fitness": 0.2078196478434545, "feedback": "The algorithm HybridHarmonySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.208 with standard deviation 0.008. And the mean value of best solutions found was 0.410 (0. is the best) with standard deviation 0.070.", "error": "", "parent_id": "ae38402c-5d69-4b44-85b7-d625ceb5b508", "metadata": {"aucs": [0.19963971266144476, 0.21881356001477326, 0.20500567085414545], "final_y": [0.4851222111652522, 0.31686184759796143, 0.4268068544322754]}, "mutation_prompt": null}
{"id": "2d0fe2e0-ab38-48b9-bc4d-75cacbada36e", "solution": "import numpy as np\n\nclass AdaptiveDimensionalSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.positions = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_position = None\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def initialize_swarm(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.best_positions = np.copy(self.positions)\n        self.global_best_position = self.positions[np.random.randint(self.swarm_size)]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_swarm(bounds)\n        evaluations = 0\n        best_score = float('-inf')\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                score = func(self.positions[i])\n                evaluations += 1\n\n                if score > func(self.best_positions[i]):\n                    self.best_positions[i] = self.positions[i]\n\n                if score > best_score:\n                    best_score = score\n                    self.global_best_position = self.positions[i]\n\n            self.update_velocities()\n            self.update_positions(bounds)\n\n            # Adaptation: Adjust swarm size and inertia weight dynamically\n            self.swarm_size = max(5, self.swarm_size - int(self.budget / 1000))\n            self.inertia_weight = max(0.1, self.inertia_weight * 0.99)\n\n        return self.global_best_position\n\n    def update_velocities(self):\n        for i in range(self.swarm_size):\n            cognitive_component = self.cognitive_coeff * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i])\n            social_component = self.social_coeff * np.random.rand(self.dim) * (self.global_best_position - self.positions[i])\n            self.velocities[i] = self.inertia_weight * self.velocities[i] + cognitive_component + social_component\n\n    def update_positions(self, bounds):\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)", "name": "AdaptiveDimensionalSwarmOptimization", "description": "An Adaptive Dimensional Swarm Optimization (ADSO) algorithm that dynamically adjusts the search dimensions and swarm parameters to navigate complex landscapes with multiple local minima effectively.", "configspace": "", "generation": 3, "fitness": 0.20880818294107925, "feedback": "The algorithm AdaptiveDimensionalSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.209 with standard deviation 0.011. And the mean value of best solutions found was 0.406 (0. is the best) with standard deviation 0.088.", "error": "", "parent_id": "33d471cc-ee0f-4361-85d4-f74fa38ead12", "metadata": {"aucs": [0.19963086083011705, 0.20302806174071597, 0.2237656262524047], "final_y": [0.4851222111652522, 0.44885741943027735, 0.28377763883518325]}, "mutation_prompt": null}
{"id": "4c24ca02-5a5b-47ff-a810-f4f542f032e5", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.q_population = np.random.rand(self.population_size, self.dim, 2)\n        self.best_solution = None\n\n    def initialize_q_population(self):\n        self.q_population = np.random.rand(self.population_size, self.dim, 2)\n        self.q_population /= np.linalg.norm(self.q_population, axis=2, keepdims=True)\n\n    def q_measure(self):\n        return np.array([np.random.choice([0, 1], size=self.dim, p=[1 - q[0]**2, q[0]**2]) for q in self.q_population])\n\n    def update_q_population(self, classical_population, best_index):\n        for i in range(self.population_size):\n            for j in range(self.dim):\n                theta = 0.01 * (2 * classical_population[best_index, j] - 1) * (2 * classical_population[i, j] - 1)\n                self.q_population[i, j] = self.rotate_quantum_bit(self.q_population[i, j], theta)\n\n    def rotate_quantum_bit(self, q_bit, theta):\n        cos_theta = np.cos(theta)\n        sin_theta = np.sin(theta)\n        new_q_bit = np.array([cos_theta * q_bit[0] - sin_theta * q_bit[1],\n                              sin_theta * q_bit[0] + cos_theta * q_bit[1]])\n        return new_q_bit / np.linalg.norm(new_q_bit)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        evaluations = 0\n        best_score = float('inf')\n        self.initialize_q_population()\n\n        while evaluations < self.budget:\n            classical_population = self.q_measure()\n            classical_population = bounds.lb + (bounds.ub - bounds.lb) * classical_population\n            scores = np.array([func(individual) for individual in classical_population])\n            evaluations += self.population_size\n\n            best_index = np.argmin(scores)\n            if scores[best_index] < best_score:\n                best_score = scores[best_index]\n                self.best_solution = classical_population[best_index]\n\n            self.update_q_population(classical_population, best_index)\n\n        return self.best_solution", "name": "QuantumInspiredEvolutionaryAlgorithm", "description": "Quantum-Inspired Evolutionary Algorithm (QIEA) that utilizes quantum bits and rotation gates to explore the search space efficiently and flexibly navigate complex optimization landscapes.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'p' must be 1-dimensional\").", "error": "ValueError(\"'p' must be 1-dimensional\")", "parent_id": "2d0fe2e0-ab38-48b9-bc4d-75cacbada36e", "metadata": {}, "mutation_prompt": null}
{"id": "dac1db0e-8f5d-420e-8a0d-e347154ed236", "solution": "import numpy as np\n\nclass AdaptiveDimensionalSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.positions = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_position = None\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def initialize_swarm(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.best_positions = np.copy(self.positions)\n        self.global_best_position = self.positions[np.random.randint(self.swarm_size)]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_swarm(bounds)\n        evaluations = 0\n        best_score = float('-inf')\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                score = func(self.positions[i])\n                evaluations += 1\n\n                if score > func(self.best_positions[i]):\n                    self.best_positions[i] = self.positions[i]\n\n                if score > best_score:\n                    best_score = score\n                    self.global_best_position = self.positions[i]\n\n            self.update_velocities()\n            self.update_positions(bounds)\n\n            # Adaptation: Adjust swarm size and inertia weight dynamically\n            self.swarm_size = max(5, self.swarm_size - int(self.budget / 1000))\n            self.inertia_weight = max(0.1, self.inertia_weight * 0.99)\n\n        return self.global_best_position\n\n    def update_velocities(self):\n        for i in range(self.swarm_size):\n            cognitive_component = self.cognitive_coeff * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i])\n            social_component = self.social_coeff * np.random.rand(self.dim) * (self.global_best_position - self.positions[i])\n            learning_rate = np.exp(-evaluations / self.budget)  # Dynamic learning rate\n            self.velocities[i] = learning_rate * (self.inertia_weight * self.velocities[i] + cognitive_component + social_component)\n\n    def update_positions(self, bounds):\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)", "name": "AdaptiveDimensionalSwarmOptimization", "description": "An enhanced version of Adaptive Dimensional Swarm Optimization (ADSO) that introduces dynamic learning rates to balance exploration and exploitation effectively, improving convergence in complex landscapes.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "2d0fe2e0-ab38-48b9-bc4d-75cacbada36e", "metadata": {}, "mutation_prompt": null}
{"id": "987ffffe-4668-4d1e-99c4-f40e1bf12393", "solution": "import numpy as np\n\nclass AdaptiveDimensionalSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.positions = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_position = None\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def initialize_swarm(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.best_positions = np.copy(self.positions)\n        self.global_best_position = self.positions[np.random.randint(self.swarm_size)]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_swarm(bounds)\n        evaluations = 0\n        best_score = float('-inf')\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                score = func(self.positions[i])\n                evaluations += 1\n\n                if score > func(self.best_positions[i]):\n                    self.best_positions[i] = self.positions[i]\n\n                if score > best_score:\n                    best_score = score\n                    self.global_best_position = self.positions[i]\n\n            self.update_velocities()\n            self.update_positions(bounds)\n\n            # Adaptation: Adjust swarm size and inertia weight dynamically\n            self.swarm_size = max(5, self.swarm_size - int(self.budget / 1000))\n            self.inertia_weight = max(0.1, self.inertia_weight * np.exp(-0.01 * evaluations))  # Nonlinear decay\n            self.cognitive_coeff = 2.0 - 1.5 * (evaluations / self.budget)  # Adaptive cognitive coefficient\n\n        return self.global_best_position\n\n    def update_velocities(self):\n        for i in range(self.swarm_size):\n            cognitive_component = self.cognitive_coeff * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i])\n            social_component = self.social_coeff * np.random.rand(self.dim) * (self.global_best_position - self.positions[i])\n            self.velocities[i] = self.inertia_weight * self.velocities[i] + cognitive_component + social_component\n\n    def update_positions(self, bounds):\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)", "name": "AdaptiveDimensionalSwarmOptimization", "description": "Modified ADSO with nonlinear inertia weight decay and adaptive cognitive coefficient adjustment for enhanced convergence.", "configspace": "", "generation": 6, "fitness": 0.2081934454705041, "feedback": "The algorithm AdaptiveDimensionalSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.208 with standard deviation 0.011. And the mean value of best solutions found was 0.412 (0. is the best) with standard deviation 0.091.", "error": "", "parent_id": "2d0fe2e0-ab38-48b9-bc4d-75cacbada36e", "metadata": {"aucs": [0.19963086083011705, 0.20118384932899058, 0.2237656262524047], "final_y": [0.4851222111652522, 0.46833410910020323, 0.28377763883518325]}, "mutation_prompt": null}
{"id": "30277145-cb6f-4c5b-a36f-ee9f724ef5b7", "solution": "import numpy as np\n\nclass HybridAdaptiveParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.positions = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_position = None\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.local_search_prob = 0.1  # Probability of local search\n\n    def initialize_swarm(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.best_positions = np.copy(self.positions)\n        self.global_best_position = self.positions[np.random.randint(self.swarm_size)]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_swarm(bounds)\n        evaluations = 0\n        best_score = float('-inf')\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                score = func(self.positions[i])\n                evaluations += 1\n\n                if score > func(self.best_positions[i]):\n                    self.best_positions[i] = self.positions[i]\n\n                if score > best_score:\n                    best_score = score\n                    self.global_best_position = self.positions[i]\n\n            self.update_velocities()\n            self.update_positions(bounds)\n\n            # Adaptation: Adjust swarm size and inertia weight dynamically\n            self.swarm_size = max(5, self.swarm_size - int(self.budget / 1000))\n            self.inertia_weight = max(0.1, self.inertia_weight * 0.99)\n\n            # Added local search for exploitation\n            if np.random.rand() < self.local_search_prob:\n                self.local_search(func, bounds)\n                \n        return self.global_best_position\n\n    def update_velocities(self):\n        for i in range(self.swarm_size):\n            cognitive_component = self.cognitive_coeff * np.random.rand(self.dim) * (self.best_positions[i] - self.positions[i])\n            social_component = self.social_coeff * np.random.rand(self.dim) * (self.global_best_position - self.positions[i])\n            self.velocities[i] = self.inertia_weight * self.velocities[i] + cognitive_component + social_component\n\n    def update_positions(self, bounds):\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)\n    \n    def local_search(self, func, bounds):\n        for i in range(self.swarm_size):\n            perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n            new_position = self.positions[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score > func(self.best_positions[i]):\n                self.best_positions[i] = new_position\n                if new_score > func(self.global_best_position):\n                    self.global_best_position = new_position", "name": "HybridAdaptiveParticleSwarmOptimization", "description": "A Hybrid Adaptive Particle Swarm Optimization (HAPSO) algorithm that combines swarm intelligence with local search heuristics to effectively balance exploration and exploitation in multimodal landscapes.", "configspace": "", "generation": 7, "fitness": 0.21418816096862014, "feedback": "The algorithm HybridAdaptiveParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.214 with standard deviation 0.010. And the mean value of best solutions found was 0.360 (0. is the best) with standard deviation 0.089.", "error": "", "parent_id": "2d0fe2e0-ab38-48b9-bc4d-75cacbada36e", "metadata": {"aucs": [0.19963086083011705, 0.21916799582333868, 0.2237656262524047], "final_y": [0.4851222111652522, 0.3121723797561542, 0.28377763883518325]}, "mutation_prompt": null}
{"id": "93275ccd-35a9-45c9-9e80-9a4482bd1057", "solution": "import numpy as np\n\nclass QuantumInspiredDifferentialEvolutionAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.mutation_factor = 0.8\n        self.crossover_prob = 0.9\n        self.quantum_prob = 0.1  # Probability of using quantum-inspired principles\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.best_global_position = self.population[np.random.randint(self.population_size)]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('-inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutate\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                donor_vector = self.population[indices[0]] + self.mutation_factor * (self.population[indices[1]] - self.population[indices[2]])\n\n                # Crossover\n                trial_vector = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_prob:\n                        trial_vector[j] = donor_vector[j]\n\n                # Quantum-inspired perturbation\n                if np.random.rand() < self.quantum_prob:\n                    trial_vector = self.quantum_perturbation(trial_vector, bounds)\n\n                # Selection\n                trial_vector = np.clip(trial_vector, bounds.lb, bounds.ub)\n                trial_score = func(trial_vector)\n                evaluations += 1\n\n                if trial_score > func(self.population[i]):\n                    self.population[i] = trial_vector\n\n                if trial_score > best_score:\n                    best_score = trial_score\n                    self.best_global_position = trial_vector\n\n        return self.best_global_position\n\n    def quantum_perturbation(self, vector, bounds):\n        # Use quantum superposition principle for perturbation\n        q_vector = np.random.uniform(-1, 1, self.dim)\n        amplitude = np.linalg.norm(vector - self.best_global_position)\n        q_vector = vector + amplitude * q_vector\n        return np.clip(q_vector, bounds.lb, bounds.ub)", "name": "QuantumInspiredDifferentialEvolutionAlgorithm", "description": "Quantum-inspired Differential Evolution Algorithm (QIDEA) that leverages quantum superposition principles combined with classical differential evolution to enhance exploration and escape local minima.", "configspace": "", "generation": 8, "fitness": 0.20649848170858878, "feedback": "The algorithm QuantumInspiredDifferentialEvolutionAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.206 with standard deviation 0.003. And the mean value of best solutions found was 0.412 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "30277145-cb6f-4c5b-a36f-ee9f724ef5b7", "metadata": {"aucs": [0.20678365580149338, 0.20982512584287238, 0.2028866634814006], "final_y": [0.4095184450232999, 0.3851471919729802, 0.44019069931856025]}, "mutation_prompt": null}
{"id": "1640a08a-6f6e-4fc6-8d97-c80680ddad8a", "solution": "import numpy as np\n\nclass MemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.local_search_prob = 0.2\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutation and Crossover\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant, self.population[i])\n\n                # Evaluate trial\n                trial_score = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n\n            # Adaptive Mutation Factor\n            self.mutation_factor = 0.5 + 0.5 * np.random.rand()\n\n            # Local Search\n            if np.random.rand() < self.local_search_prob:\n                self.local_search(func, bounds)\n\n        return self.best_individual\n\n    def local_search(self, func, bounds):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position", "name": "MemeticDifferentialEvolution", "description": "Memetic Differential Evolution with Adaptive Mutation (MDEAM) combines differential evolution with adaptive mutation and local search to enhance solution diversity and convergence speed.", "configspace": "", "generation": 9, "fitness": 0.23950283894247149, "feedback": "The algorithm MemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.240 with standard deviation 0.004. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "30277145-cb6f-4c5b-a36f-ee9f724ef5b7", "metadata": {"aucs": [0.23399370066625924, 0.24247024328108413, 0.24204457288007108], "final_y": [0.1818781009656153, 0.164855774889216, 0.16485577470141055]}, "mutation_prompt": null}
{"id": "4f472bd0-38ef-42fa-9f82-546cc22008b8", "solution": "import numpy as np\n\nclass MemeticDifferentialEvolutionDynamicPopulation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.local_search_prob = 0.2\n\n    def initialize_population(self, bounds):\n        self.population_size = self.initial_population_size\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def dynamic_population_sizing(self, current_evaluations, max_evaluations):\n        factor = (1 - current_evaluations / max_evaluations)\n        self.population_size = max(10, int(self.initial_population_size * factor))\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            self.dynamic_population_sizing(evaluations, self.budget)\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation and Crossover\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant, self.population[i])\n\n                # Evaluate trial\n                trial_score = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n\n            # Adaptive Mutation Factor\n            self.mutation_factor = 0.5 + 0.5 * np.random.rand()\n\n            # Local Search\n            if np.random.rand() < self.local_search_prob:\n                self.local_search(func, bounds)\n\n        return self.best_individual\n\n    def local_search(self, func, bounds):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position", "name": "MemeticDifferentialEvolutionDynamicPopulation", "description": "Memetic Differential Evolution with Adaptive Mutation and Dynamic Population Sizing (MDEAM-DPS) enhances solution exploration by dynamically adjusting population size based on convergence behaviors.", "configspace": "", "generation": 10, "fitness": 0.23927279590840078, "feedback": "The algorithm MemeticDifferentialEvolutionDynamicPopulation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.239 with standard deviation 0.004. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "1640a08a-6f6e-4fc6-8d97-c80680ddad8a", "metadata": {"aucs": [0.24063355666302488, 0.24384084512194182, 0.23334398594023564], "final_y": [0.1648557774742141, 0.1648557720521191, 0.2004449131656284]}, "mutation_prompt": null}
{"id": "bf766138-4842-47be-8c8b-be0330f829b4", "solution": "import numpy as np\n\nclass MemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.local_search_prob = 0.2\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutation and Crossover\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant, self.population[i])\n\n                # Evaluate trial\n                trial_score = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n\n                # Elitism\n                if trial_score < best_score:\n                    self.best_individual = trial\n\n            # Adaptive Mutation Factor\n            self.mutation_factor = 0.5 + 0.5 * np.random.rand()\n\n            # Adaptive Crossover Rate\n            self.crossover_rate = 0.7 + 0.3 * np.random.rand()\n\n            # Local Search\n            if np.random.rand() < self.local_search_prob:\n                self.local_search(func, bounds)\n\n        return self.best_individual\n\n    def local_search(self, func, bounds):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position", "name": "MemeticDifferentialEvolution", "description": "Enhanced Memetic Differential Evolution adds adaptive crossover probability and elitism, boosting convergence and diversity balance.", "configspace": "", "generation": 11, "fitness": 0.23845124471142756, "feedback": "The algorithm MemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.238 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1640a08a-6f6e-4fc6-8d97-c80680ddad8a", "metadata": {"aucs": [0.2377052768934036, 0.24022317103339175, 0.23742528620748737], "final_y": [0.16487214449306675, 0.16485613574863456, 0.16485834516083098]}, "mutation_prompt": null}
{"id": "779c306b-58e5-42bd-bfed-2e86a0199051", "solution": "import numpy as np\n\nclass MemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            self.mutation_factor = 0.5 + 0.5 * np.random.rand()\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position", "name": "MemeticDifferentialEvolution", "description": "Enhanced Memetic Differential Evolution with Dynamic Population and Adaptive Local Search optimizes diversity and convergence by adjusting population size and local search intensity based on performance feedback.", "configspace": "", "generation": 12, "fitness": 0.24029342762948303, "feedback": "The algorithm MemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.240 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1640a08a-6f6e-4fc6-8d97-c80680ddad8a", "metadata": {"aucs": [0.23636772052200572, 0.2424727026424387, 0.24203985972400466], "final_y": [0.16485613478032946, 0.164855781202073, 0.16485577338128043]}, "mutation_prompt": null}
{"id": "f2bd9a65-306c-493b-b62b-336720882f30", "solution": "import numpy as np\n\nclass MemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            self.mutation_factor = 0.7 + 0.3 * np.random.rand()  # Change 1\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 15:  # Change 2\n                self.population_size = max(5, self.population_size // 2)\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position", "name": "MemeticDifferentialEvolution", "description": "Improved Memetic Differential Evolution with Stagnation-Driven Dynamic Population Adjustment reduces premature convergence by adapting mutation strength based on stagnation.", "configspace": "", "generation": 13, "fitness": 0.23868446861409456, "feedback": "The algorithm MemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.239 with standard deviation 0.002. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "779c306b-58e5-42bd-bfed-2e86a0199051", "metadata": {"aucs": [0.23666322726808164, 0.24095465307786512, 0.23843552549633695], "final_y": [0.18187980242455948, 0.16485986873930114, 0.18187810640253688]}, "mutation_prompt": null}
{"id": "b893442d-9cb8-4eee-b784-c147d5fb6b5a", "solution": "import numpy as np\n\nclass MemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.local_search_prob = 0.3  # Increased local search probability\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            self.mutation_factor = 0.5 + 0.5 * np.random.rand()\n\n            if np.random.rand() < self.local_search_prob:\n                self.multi_phase_local_search(func, bounds, evaluations - last_improvement)  # Modified to multi-phase search\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, int(self.population_size * 0.7))  # Adaptive reduction factor\n\n        return self.best_individual\n\n    def multi_phase_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            step_size = 0.01 if no_improvement_steps < self.budget // 20 else 0.1\n            perturbation = np.random.uniform(-step_size, step_size, self.dim)\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position", "name": "MemeticDifferentialEvolution", "description": "Enhanced Memetic Differential Evolution with Adaptive Population and Multi-Phase Local Search optimizes convergence by introducing multi-phase local search tactics and adaptive population control.", "configspace": "", "generation": 14, "fitness": 0.23912823980354003, "feedback": "The algorithm MemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.239 with standard deviation 0.001. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "779c306b-58e5-42bd-bfed-2e86a0199051", "metadata": {"aucs": [0.24093286246563195, 0.2389180509302905, 0.23753380601469765], "final_y": [0.16485582742700589, 0.18187810180479047, 0.16485584346617332]}, "mutation_prompt": null}
{"id": "053c7f49-4af1-464a-b8a6-1e3dbd652c62", "solution": "import numpy as np\n\nclass MemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor based on population variance\n            self.mutation_factor = 0.5 + 0.5 * np.var(self.fitness) / np.max(self.fitness)\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position", "name": "MemeticDifferentialEvolution", "description": "Improved Memetic Differential Evolution with variance-based adaptive mutation factor enhances exploration by adjusting mutation based on population diversity.", "configspace": "", "generation": 15, "fitness": 0.2416294887906023, "feedback": "The algorithm MemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.242 with standard deviation 0.004. And the mean value of best solutions found was 0.178 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "779c306b-58e5-42bd-bfed-2e86a0199051", "metadata": {"aucs": [0.2360850800791695, 0.24487279134355167, 0.24393059494908576], "final_y": [0.2004449130578928, 0.16657731131272513, 0.16840388920143345]}, "mutation_prompt": null}
{"id": "1ab3170a-31af-48ca-ab76-960f74eae0d4", "solution": "import numpy as np\n\nclass QuantumInspiredDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.amplitude_factor = 0.5\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant, self.population[i])\n\n                # Quantum-inspired probabilistic amplitude adjustment\n                quantum_adjustment = (2 * np.random.rand(self.dim) - 1) * self.amplitude_factor\n                trial = np.clip(trial + quantum_adjustment, bounds.lb, bounds.ub)\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n\n        return self.best_individual", "name": "QuantumInspiredDifferentialEvolution", "description": "Quantum-inspired Differential Evolution leverages quantum superposition principles to enhance exploration, generating trial vectors with probabilistic amplitude-based adjustments.", "configspace": "", "generation": 16, "fitness": 0.2367556667056995, "feedback": "The algorithm QuantumInspiredDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.237 with standard deviation 0.001. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "053c7f49-4af1-464a-b8a6-1e3dbd652c62", "metadata": {"aucs": [0.23555479742398366, 0.23753586991848674, 0.23717633277462813], "final_y": [0.1649658578705805, 0.16486404989917924, 0.18189678290176436]}, "mutation_prompt": null}
{"id": "abb0cd0f-3f7e-46ed-8ed3-a25e59fde083", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor based on fitness diversity\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = 0.5 + 0.3 * (fitness_std / np.mean(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced Memetic Differential Evolution with dynamic mutation and crossover rates based on fitness diversity for improved exploration-exploitation balance.", "configspace": "", "generation": 17, "fitness": 0.24409314425290554, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "053c7f49-4af1-464a-b8a6-1e3dbd652c62", "metadata": {"aucs": [0.24090392868907617, 0.24570458375292048, 0.24567092031672], "final_y": [0.18257555159355332, 0.16485895456105282, 0.1648557889953326]}, "mutation_prompt": null}
{"id": "08d73e84-f53a-489b-9c12-5952cf76f63c", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor based on fitness diversity\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = 0.5 + 0.3 * (fitness_std / np.mean(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2) + int(np.log1p(last_improvement))\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced Memetic Differential Evolution with adaptive dynamic population size based on performance improvement.", "configspace": "", "generation": 18, "fitness": 0.24409314425290554, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "abb0cd0f-3f7e-46ed-8ed3-a25e59fde083", "metadata": {"aucs": [0.24090392868907617, 0.24570458375292048, 0.24567092031672], "final_y": [0.18257555159355332, 0.16485895456105282, 0.1648557889953326]}, "mutation_prompt": null}
{"id": "4006439c-5f01-4a9f-a2f0-8bfd5a97413e", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor based on fitness diversity\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = 0.5 + 0.3 * (fitness_std / np.mean(self.fitness))\n\n            if np.random.rand() < self.local_search_prob * (1.0 - (evaluations - last_improvement) / self.budget):\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced Memetic Differential Evolution with adaptive local search frequency based on recent improvements for optimized exploration-exploitation balance.", "configspace": "", "generation": 19, "fitness": 0.24409314425290554, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "abb0cd0f-3f7e-46ed-8ed3-a25e59fde083", "metadata": {"aucs": [0.24090392868907617, 0.24570458375292048, 0.24567092031672], "final_y": [0.18257555159355332, 0.16485895456105282, 0.1648557889953326]}, "mutation_prompt": null}
{"id": "4c7d754f-3223-4ea1-9500-6e540936ba97", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor based on fitness diversity\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = 0.6 + 0.2 * (fitness_std / np.mean(self.fitness))  # Changed line\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced Memetic Differential Evolution with dynamic mutation factor scaling based on fitness diversity and introduction of elitism to retain top solutions.", "configspace": "", "generation": 20, "fitness": 0.24188057459108062, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.242 with standard deviation 0.002. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "abb0cd0f-3f7e-46ed-8ed3-a25e59fde083", "metadata": {"aucs": [0.24086762247186244, 0.2405757045538679, 0.24419839674751154], "final_y": [0.18187895859920122, 0.18187809676835032, 0.16485577190480682]}, "mutation_prompt": null}
{"id": "b19de728-f5aa-4ab7-8dd6-8740ed9d18bb", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Adaptive Differential Evolution with dynamic strategy selection and crowding distance maintenance for improved diversity and convergence.", "configspace": "", "generation": 21, "fitness": 0.24415849802093217, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "abb0cd0f-3f7e-46ed-8ed3-a25e59fde083", "metadata": {"aucs": [0.24109998999315607, 0.24570458375292048, 0.24567092031672], "final_y": [0.18190742425024686, 0.16485895456105282, 0.1648557889953326]}, "mutation_prompt": null}
{"id": "05a31306-e100-44a4-aa6f-f1d339e4e03b", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness)) + 0.1 * (self.fitness[i] / np.min(self.fitness))  \n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Incorporate fitness-based dynamic crossover rate adjustment to better exploit and explore the search space.", "configspace": "", "generation": 22, "fitness": 0.23826490442305878, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.238 with standard deviation 0.001. And the mean value of best solutions found was 0.198 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "b19de728-f5aa-4ab7-8dd6-8740ed9d18bb", "metadata": {"aucs": [0.23761295174873254, 0.23961441969906394, 0.23756734182137984], "final_y": [0.20274814259147922, 0.19131841045084197, 0.19894280366354744]}, "mutation_prompt": null}
{"id": "7b656eb5-34c0-4b44-9afc-b7f75f843ce8", "solution": "import numpy as np\n\nclass RefinedAdaptiveMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n        self.feedback_factor = 0.1\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n        self.best_score = float('inf')\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < self.best_score:\n                        self.best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            self.adjust_mutation_factor()\n            self.check_and_apply_local_search(func, bounds, evaluations - last_improvement)\n\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adjust_mutation_factor(self):\n        fitness_std = np.std(self.fitness)\n        self.base_mutation_factor = np.clip(\n            0.5 + self.feedback_factor * (fitness_std / np.mean(self.fitness)), 0.5, 1.0\n        )\n\n    def check_and_apply_local_search(self, func, bounds, no_improvement_steps):\n        if np.random.rand() < self.local_search_prob:\n            self.adaptive_local_search(func, bounds, no_improvement_steps)\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < self.best_score:\n                    self.best_individual = new_position\n                    self.best_score = new_score\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.allclose(self.fitness[i], self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "RefinedAdaptiveMemeticDifferentialEvolution", "description": "Adaptive Memetic Differential Evolution with feedback-based mutation factor adjustment and diversity maintenance for robust convergence in multimodal landscapes.", "configspace": "", "generation": 23, "fitness": 0.24354793662860508, "feedback": "The algorithm RefinedAdaptiveMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.001. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b19de728-f5aa-4ab7-8dd6-8740ed9d18bb", "metadata": {"aucs": [0.2426401357444654, 0.2425160199927493, 0.24548765414860052], "final_y": [0.18188011941061877, 0.1818805094153061, 0.16486243045425875]}, "mutation_prompt": null}
{"id": "bb154dff-cff4-4da2-9f2d-c321be6b10db", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced adaptive local search with increased perturbation scaling for improved convergence.", "configspace": "", "generation": 24, "fitness": 0.24420027863811866, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b19de728-f5aa-4ab7-8dd6-8740ed9d18bb", "metadata": {"aucs": [0.24120916464001096, 0.24572974274979287, 0.24566192852455215], "final_y": [0.18187809727318183, 0.16485831720202626, 0.1648557723394577]}, "mutation_prompt": null}
{"id": "294e1d50-f2f5-4c18-bd23-96420f898b53", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.075, 0.075, self.dim)  # Adjusted from -0.07, 0.07\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Slightly increase the perturbation range in the adaptive local search to enhance exploration capabilities.", "configspace": "", "generation": 25, "fitness": 0.24417755810144817, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "bb154dff-cff4-4da2-9f2d-c321be6b10db", "metadata": {"aucs": [0.24121996323577455, 0.24568936163815835, 0.24562334943041164], "final_y": [0.18187810132237747, 0.1648592713810635, 0.16485755435672744]}, "mutation_prompt": null}
{"id": "9246ef51-e527-40ff-ab85-d2b3db0c2328", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.7 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced adaptive local search with dynamic mutation and strategic diversity injection.", "configspace": "", "generation": 26, "fitness": 0.23919283765660695, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.239 with standard deviation 0.002. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "bb154dff-cff4-4da2-9f2d-c321be6b10db", "metadata": {"aucs": [0.24151842896598041, 0.23955793693829164, 0.23650214706554884], "final_y": [0.16485578031284331, 0.18187809690634937, 0.18188671933153444]}, "mutation_prompt": null}
{"id": "5f28797d-b1cc-4d13-81d1-b6dc3220165f", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 25  # Increased for better exploration\n        self.base_mutation_factor = 0.7  # Adjusted for better balance\n        self.base_crossover_rate = 0.85  # Adjusted for better balance\n        self.local_search_prob = 0.25  # Increased for more local refinement\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.4 + 0.4 * (fitness_std / np.mean(self.fitness)), 0.4, 1.0)\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)  # Modified perturbation range\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 1.5  # Slightly adjusted intensification\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Adaptive Multi-Directional Differential Evolution with enhanced mutation and crossover strategies for improved exploration and exploitation balance.", "configspace": "", "generation": 27, "fitness": 0.24239601524720267, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.242 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "bb154dff-cff4-4da2-9f2d-c321be6b10db", "metadata": {"aucs": [0.2434289865497028, 0.23972342459879104, 0.24403563459311417], "final_y": [0.1648557917020218, 0.18187811168790025, 0.1648558672057343]}, "mutation_prompt": null}
{"id": "1f5adcfc-1b94-414b-ac01-1a8c66671a12", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Adjusted population size for better exploration\n        self.base_mutation_factor = 0.7  # Adjusted mutation factor for stability\n        self.base_crossover_rate = 0.85  # Slightly reduced crossover rate\n        self.local_search_prob = 0.25  # Increased probability of local search\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                chaotic_factor = 0.5 * np.sin(evaluations) + 0.5  # Chaotic map-based factor\n                mutant = np.clip(a + chaotic_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor dynamically\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.6 + 0.2 * (fitness_std / np.mean(self.fitness)), 0.4, 0.9)\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 15:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            if no_improvement_steps > self.budget // 25:\n                perturbation *= 3  # Increased perturbation when stuck\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.08, 0.08, self.dim)  # Adjusted diversity maintenance", "name": "EnhancedMemeticDifferentialEvolution", "description": "Incorporates dynamic adaptive strategies and chaotic maps to enhance exploration and convergence.", "configspace": "", "generation": 28, "fitness": 0.24320946651263933, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.243 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "bb154dff-cff4-4da2-9f2d-c321be6b10db", "metadata": {"aucs": [0.2423299931877726, 0.245094610008175, 0.24220379634197042], "final_y": [0.1648558433593169, 0.1648557865847592, 0.1648557728580401]}, "mutation_prompt": null}
{"id": "ef5d3489-923b-4262-8424-f457ffd225e6", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introduced dynamic local search probability to balance exploration and exploitation.", "configspace": "", "generation": 29, "fitness": 0.24478281067152508, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "bb154dff-cff4-4da2-9f2d-c321be6b10db", "metadata": {"aucs": [0.2452997698174988, 0.24517402704907176, 0.24387463514800467], "final_y": [0.1648557771120751, 0.16485583606236587, 0.16485577192062106]}, "mutation_prompt": null}
{"id": "cbe6f800-4651-4b98-99b9-ec4dcabc1d4e", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)  # Perturb duplicates", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced population diversity by random perturbation of duplicate solutions.", "configspace": "", "generation": 30, "fitness": 0.24478281067152508, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ef5d3489-923b-4262-8424-f457ffd225e6", "metadata": {"aucs": [0.2452997698174988, 0.24517402704907176, 0.24387463514800467], "final_y": [0.1648557771120751, 0.16485583606236587, 0.16485577192062106]}, "mutation_prompt": null}
{"id": "da309e21-2c0e-4a16-9fe3-135bf25fb8bd", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        diversity = np.std(self.population, axis=0)\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim) * (1 + diversity.mean())  # Adjusted scaling\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Improved adaptive local search with perturbation scaling based on population diversity to enhance exploration.", "configspace": "", "generation": 31, "fitness": 0.24408116698301172, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "ef5d3489-923b-4262-8424-f457ffd225e6", "metadata": {"aucs": [0.24159140672750345, 0.24594938584197867, 0.244702708379553], "final_y": [0.18259215358787972, 0.16485577524855166, 0.164855780930093]}, "mutation_prompt": null}
{"id": "24faa0ac-a60c-43a5-afec-8e96cf2b7eb1", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            else:\n                self.global_search(func, bounds)\n\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def global_search(self, func, bounds):\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[indices]\n            mutant_vector = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n            trial_vector = np.where(np.random.rand(self.dim) < self.base_crossover_rate, mutant_vector, self.population[i])\n            trial_score = func(trial_vector)\n            if trial_score < self.fitness[i]:\n                self.population[i] = trial_vector\n                self.fitness[i] = trial_score\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introduced adaptive ensemble strategy to dynamically select between global and local search methods based on population diversity.", "configspace": "", "generation": 32, "fitness": 0.24426631950718258, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "ef5d3489-923b-4262-8424-f457ffd225e6", "metadata": {"aucs": [0.24491482605377013, 0.24495504710109361, 0.242929085366684], "final_y": [0.1648557798432062, 0.16485639869502222, 0.1663732771921178]}, "mutation_prompt": null}
{"id": "e7f81fc2-92ac-48c2-9eab-905c2f115b88", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining niche diversity\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introduced maintenance of niche diversity by perturbing similar individuals for enhanced exploration.", "configspace": "", "generation": 33, "fitness": 0.24478281067152508, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ef5d3489-923b-4262-8424-f457ffd225e6", "metadata": {"aucs": [0.2452997698174988, 0.24517402704907176, 0.24387463514800467], "final_y": [0.1648557771120751, 0.16485583606236587, 0.16485577192062106]}, "mutation_prompt": null}
{"id": "5ddd6385-0be0-4f56-afaf-02fb2ab03783", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n            if evaluations % (self.budget // 20) == 0:  # Introduce a random local search step\n                random_index = np.random.randint(0, self.population_size)\n                random_perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n                random_trial = np.clip(self.population[random_index] + random_perturbation, bounds.lb, bounds.ub)\n                random_score = func(random_trial)\n                if random_score < self.fitness[random_index]:\n                    self.population[random_index] = random_trial\n                    self.fitness[random_index] = random_score\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introduced a random local search step to enhance exploration capabilities in stagnation phases.", "configspace": "", "generation": 34, "fitness": 0.24374174016685415, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.001. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "ef5d3489-923b-4262-8424-f457ffd225e6", "metadata": {"aucs": [0.24488596142766605, 0.24195390373668157, 0.24438535533621486], "final_y": [0.1648557955541341, 0.1818781021313879, 0.1648557732902638]}, "mutation_prompt": null}
{"id": "1486251d-1ee0-465e-80f9-e576e331303c", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.6 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)  # Adjusted mutation factor\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Refined the adjustment of the mutation factor to optimize convergence stability.", "configspace": "", "generation": 35, "fitness": 0.23960106169150405, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.240 with standard deviation 0.003. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "ef5d3489-923b-4262-8424-f457ffd225e6", "metadata": {"aucs": [0.24018733613098486, 0.235558007676416, 0.24305784126711127], "final_y": [0.16485579067491685, 0.18187809766945395, 0.16485577194441825]}, "mutation_prompt": null}
{"id": "ee46e145-2a63-4673-94cd-1d1fd68ad439", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 8:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.06, 0.06, self.dim)  # Adjusted from -0.07, 0.07\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Improved dynamic population strategy and local search perturbation based on performance feedback.", "configspace": "", "generation": 36, "fitness": 0.2447489026407542, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ef5d3489-923b-4262-8424-f457ffd225e6", "metadata": {"aucs": [0.24528513351519854, 0.2451165154541045, 0.2438450589529595], "final_y": [0.16485656202430443, 0.16491358089612107, 0.16485578232420606]}, "mutation_prompt": null}
{"id": "12446ed1-5dca-4e6f-ab80-e695974173fd", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.6 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Improved dynamic adjustment of mutation factor for better adaptability.  ", "configspace": "", "generation": 37, "fitness": 0.23960106169150405, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.240 with standard deviation 0.003. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "ef5d3489-923b-4262-8424-f457ffd225e6", "metadata": {"aucs": [0.24018733613098486, 0.235558007676416, 0.24305784126711127], "final_y": [0.16485579067491685, 0.18187809766945395, 0.16485577194441825]}, "mutation_prompt": null}
{"id": "c42a59d5-965a-4e03-9abf-e739e559e36e", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced Memetic Differential Evolution with adaptive crossover rate adjusted based on convergence speed.", "configspace": "", "generation": 38, "fitness": 0.24478281067152508, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ef5d3489-923b-4262-8424-f457ffd225e6", "metadata": {"aucs": [0.2452997698174988, 0.24517402704907176, 0.24387463514800467], "final_y": [0.1648557771120751, 0.16485583606236587, 0.16485577192062106]}, "mutation_prompt": null}
{"id": "8294c3db-bfed-4512-bd70-b5618f4a87da", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                self.base_crossover_rate = 0.9 + 0.1 * (1.0 - np.var(self.fitness) / np.max(self.fitness)) # Modified\n                trial = np.where(np.random.rand(self.dim) < self.base_crossover_rate, mutant, self.population[i]) # Modified\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.3 + 0.2 * (best_score / np.min(self.fitness)) # Modified\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.1, 0.1, self.dim)  # Modified range\n            if no_improvement_steps > self.budget // 15:  # Modified condition\n                perturbation *= 2.5  # Modified multiplier\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introduced adaptive mutation and crossover rates with an enhanced local search strategy for improved exploration and exploitation balance.", "configspace": "", "generation": 39, "fitness": 0.23742917736786265, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.237 with standard deviation 0.004. And the mean value of best solutions found was 0.196 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "ef5d3489-923b-4262-8424-f457ffd225e6", "metadata": {"aucs": [0.24073810956530728, 0.2402229067353887, 0.23132651580289199], "final_y": [0.183529790135726, 0.1824312229345848, 0.2210407336345701]}, "mutation_prompt": null}
{"id": "17442834-3f32-44ef-aadc-ac15743b1be6", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Improved diversity maintenance by increasing perturbation range during diversity step.", "configspace": "", "generation": 40, "fitness": 0.2447828410886639, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ef5d3489-923b-4262-8424-f457ffd225e6", "metadata": {"aucs": [0.2452997698174988, 0.24517402704907176, 0.2438747263994212], "final_y": [0.1648557771120751, 0.16485583606236587, 0.16485577248247918]}, "mutation_prompt": null}
{"id": "73805ed2-4d55-46fa-bbee-e453371eb73b", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - 0.5 * np.var(self.fitness))  # Adjusted crossover rate\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced exploration by adaptively adjusting the crossover rate based on population variance.", "configspace": "", "generation": 41, "fitness": 0.24385629999817446, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.001. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "17442834-3f32-44ef-aadc-ac15743b1be6", "metadata": {"aucs": [0.2420128159174103, 0.2452651227845214, 0.24429096129259165], "final_y": [0.18187809699908986, 0.1648557724031544, 0.16485577735653512]}, "mutation_prompt": null}
{"id": "6687f631-4d39-4bc1-a07b-26dfeec51357", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted from -0.07, 0.07\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 1.5  # Adjusted from 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Refined adaptive local search range and perturbation scale to enhance convergence speed and solution quality.", "configspace": "", "generation": 42, "fitness": 0.24474218904268386, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "17442834-3f32-44ef-aadc-ac15743b1be6", "metadata": {"aucs": [0.2452809706360718, 0.24508709582656896, 0.24385850066541082], "final_y": [0.16485712023502297, 0.16489726419756723, 0.16485578010056223]}, "mutation_prompt": null}
{"id": "98f28121-cf78-4440-81b8-55271c6768a1", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                # Adaptive crossover based on fitness variance\n                crossover_rate = min(1.0, max(0.1, self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.10, 0.10, self.dim)  # Adjusted perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introduced adaptive crossover rate and crowding distance for better diversity maintenance and solution convergence.", "configspace": "", "generation": 43, "fitness": 0.24025427430964116, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.240 with standard deviation 0.001. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "17442834-3f32-44ef-aadc-ac15743b1be6", "metadata": {"aucs": [0.24004225804376433, 0.2391853464625785, 0.24153521842258063], "final_y": [0.1818780967973297, 0.181878100976969, 0.1677775968626285]}, "mutation_prompt": null}
{"id": "ee10b5f9-451d-4c47-b576-cce78239b9d7", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.1, 0.1, self.dim)  # Changed from -0.07, 0.07\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced local search step by increasing perturbation range to improve exploration in adaptive local search.", "configspace": "", "generation": 44, "fitness": 0.24354052965037942, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.001. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "17442834-3f32-44ef-aadc-ac15743b1be6", "metadata": {"aucs": [0.24530450079325172, 0.24177982631141726, 0.2435372618464693], "final_y": [0.16485577315859112, 0.1818780967689123, 0.16499762145351637]}, "mutation_prompt": null}
{"id": "394292c8-d421-4ea0-bd51-8dc67a572416", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.2, 0.2, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced diversity by adding a larger perturbation range during crowding distance maintenance.", "configspace": "", "generation": 45, "fitness": 0.24478258218060314, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "17442834-3f32-44ef-aadc-ac15743b1be6", "metadata": {"aucs": [0.2452997698174988, 0.24517402704907176, 0.2438739496752389], "final_y": [0.1648557771120751, 0.16485583606236587, 0.1648557744365462]}, "mutation_prompt": null}
{"id": "85faa270-fec2-4d83-8755-3b23959b5c17", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate + 0.1 * (np.mean(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "This algorithm refines diversity with adaptive crossover rates and enhanced local search, optimizing efficiency within constraints.", "configspace": "", "generation": 46, "fitness": 0.23394518546732312, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.234 with standard deviation 0.004. And the mean value of best solutions found was 0.212 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "17442834-3f32-44ef-aadc-ac15743b1be6", "metadata": {"aucs": [0.23646168638689669, 0.22880729396721933, 0.23656657604785336], "final_y": [0.20539681832968204, 0.23355610779389302, 0.19806651114954876]}, "mutation_prompt": null}
{"id": "626cbb30-ce12-4e65-af6f-bc08ba6f9c9c", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + (self.base_mutation_factor + 0.1 * np.random.rand()) * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.6 + 0.25 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.15 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, int(self.population_size * 0.75))\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 1.5\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.2, 0.2, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced adaptive strategies and diversity techniques in differential evolution for improved convergence and exploration.", "configspace": "", "generation": 47, "fitness": 0.24012071904128032, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.240 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "17442834-3f32-44ef-aadc-ac15743b1be6", "metadata": {"aucs": [0.2359061810067119, 0.24299659690964848, 0.24145937920748062], "final_y": [0.1648573504117189, 0.16485577202480062, 0.16485577192970624]}, "mutation_prompt": null}
{"id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Improved the calculation of the dynamic crossover rate by considering the average fitness. ", "configspace": "", "generation": 48, "fitness": 0.2456878399184902, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "17442834-3f32-44ef-aadc-ac15743b1be6", "metadata": {"aucs": [0.24644621945149014, 0.244735714966269, 0.24588158533771143], "final_y": [0.1648557719067848, 0.16515217879516864, 0.1648557719404794]}, "mutation_prompt": null}
{"id": "43f8ea1a-e04b-4631-9a12-a73ec842bd5d", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.4 * (fitness_std / np.mean(self.fitness)), 0.4, 1.0)\n\n            self.local_search_prob = 0.3 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.2, 0.2, self.dim)  # Increased perturbation range\n        if np.random.rand() < 0.1:  # Added condition for random resetting\n            indices_to_reset = np.random.choice(self.population_size, 2, replace=False)\n            for idx in indices_to_reset:\n                self.population[idx] = np.random.uniform(bounds.lb, bounds.ub, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhance the diversity maintenance by incorporating dynamic resetting and adaptive mutation, optimizing exploration and exploitation balance.", "configspace": "", "generation": 49, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'bounds' is not defined\").", "error": "NameError(\"name 'bounds' is not defined\")", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {}, "mutation_prompt": null}
{"id": "d0de6c65-adc0-4434-8583-8ee70f014804", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Refined crowding distance maintenance\n            self.adaptive_maintain_diversity(bounds)\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def adaptive_maintain_diversity(self, bounds):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)\n                self.population[i] = np.clip(self.population[i], bounds.lb, bounds.ub)  # Ensure within bounds", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introduced adaptive crowding distance maintenance to refine diversity control and improve solution exploration.", "configspace": "", "generation": 50, "fitness": 0.2456878399184902, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.24644621945149014, 0.244735714966269, 0.24588158533771143], "final_y": [0.1648557719067848, 0.16515217879516864, 0.1648557719404794]}, "mutation_prompt": null}
{"id": "e9975f96-229f-45a9-834c-8d3edb272c80", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.6 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.17, 0.17, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced Memetic Differential Evolution with adaptive mutation scaling and improved diversity management.", "configspace": "", "generation": 51, "fitness": 0.24084549976704908, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.241 with standard deviation 0.002. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.2409561388793262, 0.23867246417630184, 0.24290789624551923], "final_y": [0.18187809676875588, 0.18187809677478184, 0.16485577200708368]}, "mutation_prompt": null}
{"id": "5fd92d8d-4be0-44e7-80fa-1b9aba836298", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (fitness_std / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Incorporates an improved method for calculating the local search probability to enhance exploration.", "configspace": "", "generation": 52, "fitness": 0.24523864920643382, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.24622463363737057, 0.24410226274014035, 0.24538905124179056], "final_y": [0.16485578036720017, 0.1648582395539503, 0.16485577462799117]}, "mutation_prompt": null}
{"id": "eea3faf6-0a66-4fe8-a52b-0b50c3aa56d5", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.3 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced crossover strategy by incorporating fitness variance adjustment and added local search exploitation based on lack of recent improvements.", "configspace": "", "generation": 53, "fitness": 0.24462203379424963, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.2451830671399059, 0.24463124909081746, 0.24405178515202552], "final_y": [0.16485577388639117, 0.1648557760475664, 0.1648557721407028]}, "mutation_prompt": null}
{"id": "632f7699-afc4-438f-8490-2f3e5fbe8cb6", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.6 + 0.3 * (fitness_std / (1 + np.mean(self.fitness))), 0.5, 1.0)  # Modification here\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)  # Modification here", "name": "EnhancedMemeticDifferentialEvolution", "description": "Refine the mutation factor adaptation and enhance diversity maintenance by adjusting perturbation scaling.", "configspace": "", "generation": 54, "fitness": 0.24204778827436127, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.242 with standard deviation 0.001. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.2408713755494607, 0.24344708445458652, 0.24182490481903662], "final_y": [0.18187809682784328, 0.16485577254349926, 0.1648557719292375]}, "mutation_prompt": null}
{"id": "ab379af1-a6dd-4243-bf21-2f99df92052b", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n\n                # Refined crossover rate formula\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.max(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            # Adjusted perturbation range\n            perturbation = np.random.uniform(-0.06, 0.06, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Refined crossover rate formula and adjustment in perturbation range to enhance convergence.", "configspace": "", "generation": 55, "fitness": 0.2447489026407542, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.24528513351519854, 0.2451165154541045, 0.2438450589529595], "final_y": [0.16485656202430443, 0.16491358089612107, 0.16485578232420606]}, "mutation_prompt": null}
{"id": "fdd1a78f-1a40-4e19-b9ef-dd0cc537147c", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness) + np.var(self.population)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhance mutation diversity by adjusting the mutation factor using both population variance and fitness standard deviation. ", "configspace": "", "generation": 56, "fitness": 0.23227564286970107, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.232 with standard deviation 0.005. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.23766428608199286, 0.23433859379769073, 0.22482404872941963], "final_y": [0.16591984885727606, 0.1818781392680897, 0.16591672461236195]}, "mutation_prompt": null}
{"id": "75051bfe-ebf7-46b8-a986-a5c3704c3c1b", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.7  # Changed from 0.8\n        self.base_crossover_rate = 0.85  # Changed from 0.9\n        self.local_search_prob = 0.25  # Changed from 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.4 + 0.4 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)  # Adjusted\n\n            self.local_search_prob = 0.25 + 0.15 * (best_score / np.min(self.fitness))  # Adjusted\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 15:  # Adjusted\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted perturbation range\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 1.5  # Adjusted multiplier\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)  # Adjusted perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced adaptive search with intensified local exploration and diversity control for improved convergence in complex optimization landscapes.", "configspace": "", "generation": 57, "fitness": 0.24396602083154995, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.24638660647975985, 0.2422696184472558, 0.24324183756763418], "final_y": [0.1648557719254693, 0.1818780970120243, 0.16487300021999374]}, "mutation_prompt": null}
{"id": "fe06b100-e1ff-4473-bcd3-d7c5cf78f4be", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.6 + 0.2 * (fitness_std / np.mean(self.fitness)), 0.6, 1.0)  # Changed\n\n            self.local_search_prob = 0.3 + 0.1 * (best_score / np.min(self.fitness))  # Changed\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, int(self.population_size * 0.75))  # Changed\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced selection and mutation strategy to improve local search and global exploration balance. ", "configspace": "", "generation": 58, "fitness": 0.24191190907324903, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.242 with standard deviation 0.001. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.24027773561305976, 0.24197627652389309, 0.24348171508279426], "final_y": [0.18187809809625366, 0.16485578639155862, 0.16485577191316947]}, "mutation_prompt": null}
{"id": "955d363d-1c8a-44d9-93e0-ebe4fbdbffc2", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            improvement_rate = (evaluations - last_improvement) / self.budget\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)) * (1 - improvement_rate), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Adaptive adjustment of mutation factor now considers both fitness variance and improvement rate.", "configspace": "", "generation": 59, "fitness": 0.24461082893994104, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.2464250966704129, 0.24147130266807337, 0.24593608748133688], "final_y": [0.16485578231194797, 0.18187810474192334, 0.16485577263191686]}, "mutation_prompt": null}
{"id": "867f2df4-383b-4e4c-a1a7-f689f34cef25", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                # Change made here for dynamic crossover rate adjustment\n                improvement_rate = (best_score - self.fitness[i]) / best_score if best_score != float('inf') else 1.0\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness)) * improvement_rate\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introduce a dynamic adjustment to crossover rate based on best individual improvement rate.", "configspace": "", "generation": 60, "fitness": 0.20822321633065977, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.208 with standard deviation 0.005. And the mean value of best solutions found was 0.342 (0. is the best) with standard deviation 0.090.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.21402691337104074, 0.20890813265182961, 0.20173460296910894], "final_y": [0.24649031969255508, 0.3152784402543862, 0.46287067057603215]}, "mutation_prompt": null}
{"id": "e9a820c6-981a-497b-ba43-5949e23d61e6", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / (0.1 + np.mean(self.fitness)))  # Modified\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  # Adjusted from -0.05, 0.05\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 1.5  # Modified\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced adaptive local search by improving perturbation scaling and crossover rate adjustment.", "configspace": "", "generation": 61, "fitness": 0.24343868238773395, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.243 with standard deviation 0.001. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.2452997698174988, 0.24205124229617436, 0.2429650350495287], "final_y": [0.1648557771120751, 0.18187809747005035, 0.1648557978715265]}, "mutation_prompt": null}
{"id": "b770fb24-d5ff-491f-a721-b90ff1393e40", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n        \n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                \n                # Adaptive crossover rate based on population diversity\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.population, axis=0).mean() / np.mean(self.fitness))\n                \n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation and crossover strategies adaptively\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.6 + 0.4 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.15 + 0.15 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  \n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 1.5  # Adjusted factor to 1.5\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced Memetic Differential Evolution with Adaptive Layered Mutation for Improved Global and Local Search Balance.", "configspace": "", "generation": 62, "fitness": 0.19393523019624756, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.194 with standard deviation 0.004. And the mean value of best solutions found was 0.538 (0. is the best) with standard deviation 0.059.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.18912637833049706, 0.19824033884811887, 0.19443897341012673], "final_y": [0.6119762475271808, 0.4672808295715819, 0.5342183265631633]}, "mutation_prompt": null}
{"id": "9bbfd501-ca68-44f6-8340-2f887c70c36b", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            diversity = np.mean(np.std(self.population, axis=0))\n            self.base_mutation_factor = 0.7 + 0.3 * (diversity / np.mean(self.fitness))\n            self.base_mutation_factor = np.clip(self.base_mutation_factor, 0.5, 1.2)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.07, 0.07, self.dim)  \n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "EnhancedMemeticDifferentialEvolution with adaptive mutation strategy incorporating population diversity metrics to balance exploration and exploitation.", "configspace": "", "generation": 63, "fitness": 0.224428799629786, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.224 with standard deviation 0.002. And the mean value of best solutions found was 0.198 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.22723189671233301, 0.2223446942081766, 0.22370980796884843], "final_y": [0.18247429299741902, 0.2281560068956453, 0.18189482741808616]}, "mutation_prompt": null}
{"id": "94f7457d-df7f-4663-92c7-3a5616330e2f", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.08, 0.08, self.dim)  # Adjusted from -0.07, 0.07\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced local search by increasing the perturbation range for diversification.", "configspace": "", "generation": 64, "fitness": 0.24519201151523948, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.245 with standard deviation 0.001. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.24644051486602747, 0.2432492786294872, 0.24588624105020374], "final_y": [0.1648557739823071, 0.16977784318109346, 0.16485577197501]}, "mutation_prompt": null}
{"id": "a164711f-e768-485a-a36e-3a4c2f089b67", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.07 * np.var(self.fitness) / np.mean(self.fitness)  # Changed perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Improved local search by adjusting perturbation dynamics based on variance in recent fitness improvements.", "configspace": "", "generation": 65, "fitness": 0.24570626976647605, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6175fc4e-dc0f-4f2b-8043-7b2f329a0506", "metadata": {"aucs": [0.2464232128225765, 0.24476046457253597, 0.2459351319043157], "final_y": [0.16485957880862112, 0.16577311259617455, 0.16485592608511246]}, "mutation_prompt": null}
{"id": "5ad0a59e-a353-4cf9-af0d-5bd41f733aeb", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.3 + 0.1 * (best_score / np.min(self.fitness))  # Adjusted probability\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 8:  # Adjusted condition\n                self.population_size = max(5, int(self.population_size * 0.7))  # Revised population scaling\n\n            # Introduce adaptive crowding distance maintenance\n            self.adaptive_maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.05 * np.var(self.fitness) / np.mean(self.fitness)  # Changed perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 15:  # Adjusted condition\n                perturbation *= 1.5\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def adaptive_maintain_diversity(self):\n        # Ensure diversity by maintaining adaptive crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)  # Adjusted perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introduced adaptive crowding distance maintenance and enhanced dynamic population control to improve convergence and diversity.", "configspace": "", "generation": 66, "fitness": 0.244093618590379, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a164711f-e768-485a-a36e-3a4c2f089b67", "metadata": {"aucs": [0.24378658704283918, 0.24455945340286622, 0.2439348153254316], "final_y": [0.1649776552307185, 0.1648568452723822, 0.16486709098778285]}, "mutation_prompt": null}
{"id": "9ccdbd3e-85b3-4930-b6ea-66ffaaf8c1a1", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.9  # Slightly increased mutation factor for exploration\n        self.base_crossover_rate = 0.85  # Adjusted crossover rate for better diversity\n        self.local_search_prob = 0.25  # Increased probability of local search\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.6 + 0.4 * (fitness_std / np.mean(self.fitness)), 0.6, 1.0)\n\n            self.local_search_prob = 0.25 + 0.15 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.07 * np.var(self.fitness) / np.mean(self.fitness)\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced population diversity and fitness variance-based mutation adaptations to improve convergence speed.", "configspace": "", "generation": 67, "fitness": 0.2412276567970536, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.241 with standard deviation 0.003. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "a164711f-e768-485a-a36e-3a4c2f089b67", "metadata": {"aucs": [0.24080145545048348, 0.2380952390647627, 0.24478627587591462], "final_y": [0.18187809688966083, 0.1648561950699935, 0.16485577471912372]}, "mutation_prompt": null}
{"id": "94f6459b-ac6d-4ded-aaf1-442179088942", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / (np.mean(self.fitness) + 1e-9)) # Changed this line\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.07 * np.var(self.fitness) / np.mean(self.fitness)  # Changed perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introduced dynamic adjustment of crossover rate using fitness variance and mean ratio, enhancing exploration and exploitation balance.", "configspace": "", "generation": 68, "fitness": 0.24570626976647605, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a164711f-e768-485a-a36e-3a4c2f089b67", "metadata": {"aucs": [0.2464232128225765, 0.24476046457253597, 0.2459351319043157], "final_y": [0.16485957880862112, 0.16577311259617455, 0.16485592608511246]}, "mutation_prompt": null}
{"id": "5b13c9bc-3db1-42c5-ae0c-a447214e5e64", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.07 * np.var(self.fitness) / np.mean(self.fitness)  # Changed perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        crowded_distance_factor = np.var(self.fitness) / np.mean(self.fitness)  # New line added\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim) * crowded_distance_factor", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced trial solution selection by incorporating fitness variance in crowding distance calculation.", "configspace": "", "generation": 69, "fitness": 0.2457061235696624, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a164711f-e768-485a-a36e-3a4c2f089b67", "metadata": {"aucs": [0.2464227772066817, 0.24476046457253597, 0.24593512892976954], "final_y": [0.16486545864090263, 0.16577311259617455, 0.16485598438047266]}, "mutation_prompt": null}
{"id": "0367d478-20e9-4fe0-b7f7-91f80e9acb6a", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.07 * np.var(self.fitness) / np.mean(self.fitness)  # Changed perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.2, 0.2, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Improved crowding distance maintenance by expanding perturbation range for increased diversity.", "configspace": "", "generation": 70, "fitness": 0.2457062317073372, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a164711f-e768-485a-a36e-3a4c2f089b67", "metadata": {"aucs": [0.24642309864516, 0.24476046457253597, 0.2459351319043157], "final_y": [0.16486191312404241, 0.16577311259617455, 0.16485592608511246]}, "mutation_prompt": null}
{"id": "c8abbba1-c121-42a2-bfde-ac64ad278c76", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.95  # Changed from 0.9 to 0.95 to enhance convergence\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.07 * np.var(self.fitness) / np.mean(self.fitness)  # Changed perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Adjusted base crossover rate to improve exploration-exploitation balance.", "configspace": "", "generation": 71, "fitness": 0.24287320046956162, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.243 with standard deviation 0.002. And the mean value of best solutions found was 0.172 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "a164711f-e768-485a-a36e-3a4c2f089b67", "metadata": {"aucs": [0.2433756296416666, 0.24017120881114506, 0.2450727629558732], "final_y": [0.16619123368322475, 0.18323282257794482, 0.16514590932221052]}, "mutation_prompt": null}
{"id": "81ef5c93-b507-4348-b2c1-35e14428ed8c", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.07 * np.var(self.fitness) / np.mean(self.fitness)  # Changed perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced adaptive local search by increasing perturbation dynamics based on stagnation period.", "configspace": "", "generation": 72, "fitness": 0.2457143040247194, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a164711f-e768-485a-a36e-3a4c2f089b67", "metadata": {"aucs": [0.2464232128225765, 0.24478456734726595, 0.2459351319043157], "final_y": [0.16485957880862112, 0.16551910614450627, 0.16485592608511246]}, "mutation_prompt": null}
{"id": "7cb46e66-a9fe-4613-bb03-70000bdd318a", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c) * np.random.rand(), bounds.lb, bounds.ub)  # Change 1\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.07 * np.var(self.fitness) / np.mean(self.fitness)  # Changed perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim) * np.random.rand(self.dim)  # Change 2", "name": "EnhancedMemeticDifferentialEvolution", "description": "Refining mutation strategy with adaptive scaling and enhanced diversity maintenance.", "configspace": "", "generation": 73, "fitness": 0.23466958003140828, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.235 with standard deviation 0.008. And the mean value of best solutions found was 0.221 (0. is the best) with standard deviation 0.045.", "error": "", "parent_id": "81ef5c93-b507-4348-b2c1-35e14428ed8c", "metadata": {"aucs": [0.23842678383354354, 0.2422709737474138, 0.22331098251326753], "final_y": [0.20003391022914252, 0.18039503685929292, 0.2839460318129916]}, "mutation_prompt": null}
{"id": "35415828-baf7-4421-8643-97d4a72df0ee", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Refined adaptive local search with increased perturbation dynamics and enhanced crossover strategy.", "configspace": "", "generation": 74, "fitness": 0.2457159499980376, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "81ef5c93-b507-4348-b2c1-35e14428ed8c", "metadata": {"aucs": [0.24641818998286436, 0.24479424980819253, 0.24593541020305598], "final_y": [0.16487522625109663, 0.16553611389713074, 0.164856105788747]}, "mutation_prompt": null}
{"id": "6f008be8-801c-46d7-8861-a2604cdc5708", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.5 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)  # Adjusted\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced exploration with adaptive mutation based on convergence rate and fitness variance.", "configspace": "", "generation": 75, "fitness": 0.24282744488403707, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.243 with standard deviation 0.001. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.241989019304367, 0.24195795799136943, 0.24453535735637477], "final_y": [0.1819719158549845, 0.1722575573737145, 0.16485656183178443]}, "mutation_prompt": null}
{"id": "18ec9030-ea7b-405f-b5cf-1a192565344d", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, int(self.population_size * 0.6))  # Changed reduction rate\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Improved dynamic population adjustment using a new reduction rate for better exploration and convergence balance.", "configspace": "", "generation": 76, "fitness": 0.2457159499980376, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.24641818998286436, 0.24479424980819253, 0.24593541020305598], "final_y": [0.16487522625109663, 0.16553611389713074, 0.164856105788747]}, "mutation_prompt": null}
{"id": "b0ef6a93-febe-4a18-bc30-64bff65958f9", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.3  # Increased local search probability\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.3 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.2 * np.var(self.fitness) / np.mean(self.fitness)  # Increase perturbation base\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 2  # Reducing excessive perturbation\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Maintain balanced diversity", "name": "EnhancedMemeticDifferentialEvolution", "description": "Adaptive Dynamic Differential Evolution with Enhanced Local Search and Diversity Maintenance.", "configspace": "", "generation": 77, "fitness": 0.2440678666631723, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.24379039344918452, 0.2445554756763778, 0.24385773086395457], "final_y": [0.16492059234838008, 0.16486760084021113, 0.16495265871226017]}, "mutation_prompt": null}
{"id": "df34ab0f-688c-4f84-9e30-e6e067a9758b", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.6 + 0.2 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)  # Adjusted mutation factor\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.2, 0.2, self.dim)  # Further increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Improved mutation factor adaptation and diversity maintenance to enhance convergence and exploration balance.", "configspace": "", "generation": 78, "fitness": 0.24066755078680183, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.241 with standard deviation 0.001. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.2400649313399298, 0.2399415371862902, 0.24199618383418553], "final_y": [0.1818780967871355, 0.18187809678888633, 0.16485577191864764]}, "mutation_prompt": null}
{"id": "fe8b09a5-d200-4729-bcd3-cdd073db83ea", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                # Enhancement: Integrating adaptive differential weight adjustment\n                adaptive_factor = 0.5 + np.random.rand() * 0.5\n                mutant = np.clip(a + adaptive_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced mutation strategy by integrating adaptive differential weight adjustment to improve exploration capability.", "configspace": "", "generation": 79, "fitness": 0.2421511935147694, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.242 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.24056091583923966, 0.2429594612606215, 0.24293320344444702], "final_y": [0.16485579355015023, 0.16485578317586946, 0.16485578701037928]}, "mutation_prompt": null}
{"id": "b67a7cea-e2ec-4112-bf67-920466d528ff", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.2), 0.95)  # Adjusted crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.25 + 0.05 * (best_score / np.min(self.fitness))  # Adjusted local search probability\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.15 * np.var(self.fitness) / np.mean(self.fitness)  # Adjusted perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhancing adaptive local search with targeted perturbations and dynamic crossover adjustment.", "configspace": "", "generation": 80, "fitness": 0.24568345789501947, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.24642258313144616, 0.24469220752400167, 0.2459355830296106], "final_y": [0.16486352198758158, 0.16567667759538884, 0.1648557748339322]}, "mutation_prompt": null}
{"id": "45c65a0d-4c40-4856-8381-8d4e1a8db225", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            diversity = np.mean(np.std(self.population, axis=0))  # New diversity measure\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (diversity / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced exploration through adaptive mutation scaling based on population diversity measures.", "configspace": "", "generation": 81, "fitness": 0.22993207746015457, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.230 with standard deviation 0.008. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.23751030618051538, 0.2333681310920288, 0.21891779510791953], "final_y": [0.1670549187714937, 0.1818786938146737, 0.2578247176098112]}, "mutation_prompt": null}
{"id": "52e75ab2-6619-4887-9e5f-66056e40977f", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            improvement_rate = (evaluations - last_improvement) / self.budget\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)) * (1.0 - improvement_rate), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introducing adaptive mutation factor scaling based on improvement dynamics to enhance search efficiency.", "configspace": "", "generation": 82, "fitness": 0.24554224887633427, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.24631237358474012, 0.24437774919131372, 0.245936623852949], "final_y": [0.16486343349260468, 0.16697369579022048, 0.16485650654903383]}, "mutation_prompt": null}
{"id": "5b582e2b-ff5d-40a9-a74e-564add8fce5f", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n        self.inertia_weight = 0.9  # Adaptive inertia weight\n        self.eta = 0.1\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n        self.velocity = np.zeros((self.population_size, self.dim))  # Initialize velocity for individuals\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                self.velocity[i] = self.inertia_weight * self.velocity[i] + self.eta * (mutant - self.population[i])\n                trial += self.velocity[i]  # Apply velocity to trial\n                trial = np.clip(trial, bounds.lb, bounds.ub)\n                \n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (np.std(self.fitness) / np.mean(self.fitness)), 0.5, 1.0)\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.99)  # Reduce inertia weight over time\n\n            if np.random.rand() < self.local_search_prob:\n                self.chaotic_local_search(func, bounds)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def chaotic_local_search(self, func, bounds):\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-0.2, 0.2, self.dim) * np.sin(np.pi * np.random.rand(self.dim))  # Chaotic\n            new_position = np.clip(self.population[i] + perturbation, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Integration of adaptive inertia weight and chaotic local search in Differential Evolution for improved exploration-exploitation balance.", "configspace": "", "generation": 83, "fitness": 0.24148802367682112, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.241 with standard deviation 0.001. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.2417379841230054, 0.23998604008858215, 0.24274004681887584], "final_y": [0.16485578607886697, 0.18187826450998335, 0.16485577387771533]}, "mutation_prompt": null}
{"id": "3afcb203-739b-4da5-a605-b542761b02d3", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Refined adaptive local search with increased perturbation dynamics and enhanced crossover strategy.", "configspace": "", "generation": 75, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.24641818998286436, 0.24479424980819253, 0.24593541020305598], "final_y": [0.16487522625109663, 0.16553611389713074, 0.164856105788747]}, "mutation_prompt": null}
{"id": "c873e9b2-88fd-4f88-846d-b91047279455", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n            if evaluations - last_improvement < self.budget // 20:  # Line changed for dynamic increase\n                self.population_size = min(40, self.population_size * 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introduced a dynamic adjustment of population size based on recent progress to improve convergence.", "configspace": "", "generation": 85, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {}, "mutation_prompt": null}
{"id": "7121cac2-7214-40b0-b763-da615aea00de", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.8)  # Adjusted crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.6 + 0.2 * (fitness_std / np.mean(self.fitness)), 0.5, 0.9)\n\n            self.local_search_prob = 0.25 + 0.05 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 8:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.15 * np.var(self.fitness) / np.mean(self.fitness)\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 15:\n                perturbation *= 2.5\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.1, 0.1, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Integrates dynamic mutation with adaptive local search and diversity-driven population evolution for improved convergence.", "configspace": "", "generation": 86, "fitness": 0.24276326110832291, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.243 with standard deviation 0.001. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.24405448183348055, 0.24089875582864861, 0.24333654566283958], "final_y": [0.16485578882254903, 0.18187809677260025, 0.164855845620556]}, "mutation_prompt": null}
{"id": "9860ba21-613a-4ea9-969a-5715e1332c5e", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced adaptive local search with self-adapting perturbation dynamics and crossover strategy.", "configspace": "", "generation": 87, "fitness": 0.2457159499980376, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.24641818998286436, 0.24479424980819253, 0.24593541020305598], "final_y": [0.16487522625109663, 0.16553611389713074, 0.164856105788747]}, "mutation_prompt": null}
{"id": "1f2a69f7-d27a-4279-95bf-443c1b639fb0", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.6 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.6, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.2, 0.2, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced adaptive local search with increased mutation factor dynamics and improved crowding distance maintenance.", "configspace": "", "generation": 88, "fitness": 0.24136418933213735, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.241 with standard deviation 0.001. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.2407444365181285, 0.24003049265814047, 0.24331763882014312], "final_y": [0.1818780967711996, 0.1648810587419668, 0.16485577190846945]}, "mutation_prompt": null}
{"id": "7790811e-214e-455e-b173-7e0e3bd00d8a", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.85  # Adjusted crossover rate\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Adjusted the crossover rate for tighter distribution to improve solution convergence.", "configspace": "", "generation": 89, "fitness": 0.24337138532488786, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.243 with standard deviation 0.001. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.2416492607839611, 0.24405963381257256, 0.2444052613781299], "final_y": [0.18187809677830258, 0.16485585288380455, 0.16485580197288707]}, "mutation_prompt": null}
{"id": "6d841b2e-2f7b-4040-b0fb-612ad77d25c7", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 8:  # Slightly reduced interval\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "The algorithm has been slightly refined by adjusting the dynamic population reduction condition to enhance adaptability and performance.", "configspace": "", "generation": 90, "fitness": 0.2457159499980376, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.24641818998286436, 0.24479424980819253, 0.24593541020305598], "final_y": [0.16487522625109663, 0.16553611389713074, 0.164856105788747]}, "mutation_prompt": null}
{"id": "38334ba2-92a9-4603-b721-a9a4cac3f3e1", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.4 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)  # Adjusted factor\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "EnhancedMemeticDifferentialEvolution with adaptive mutation based on diversity and dynamic local search intensity.", "configspace": "", "generation": 91, "fitness": 0.24436565759814552, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.24195104606329965, 0.24500929691641538, 0.2461366298147215], "final_y": [0.1818781653726823, 0.1653007422543057, 0.1649199430723599]}, "mutation_prompt": null}
{"id": "2cd63bd7-140c-4f7d-a9ad-9f70cea32cf0", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.7, 1.2)  # Changed mutation factor range\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced diversity through a dynamic mutation factor to escape local optima.", "configspace": "", "generation": 92, "fitness": 0.24167330135124257, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.242 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.24162884787629846, 0.24077678413449155, 0.24261427204293773], "final_y": [0.16485578975522752, 0.16485579332535094, 0.1648557822835406]}, "mutation_prompt": null}
{"id": "b3876a09-7d4f-4a8b-82b8-a3b035146340", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                # Adjust mutation factor based on diversity\n                diversity_factor = np.std(self.population)\n                dynamic_mutation_factor = self.base_mutation_factor + 0.2 * diversity_factor\n                mutant = np.clip(a + dynamic_mutation_factor * (b - c), bounds.lb, bounds.ub)\n\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Adaptive dynamic mutation strategy based on population diversity to enhance convergence speed and solution quality.", "configspace": "", "generation": 93, "fitness": 0.20129324077062008, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.201 with standard deviation 0.002. And the mean value of best solutions found was 0.446 (0. is the best) with standard deviation 0.036.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.201223064845943, 0.19921089682168336, 0.20344576064423392], "final_y": [0.46108278484669807, 0.48034694100322295, 0.39538894006712233]}, "mutation_prompt": null}
{"id": "fd2a03bd-d709-4907-b8da-5391460dca22", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce entropy-based diversity maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3  # Increase perturbation dynamics\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Integrates entropy-based diversity enhancement to balance exploration and exploitation.", "configspace": "", "generation": 94, "fitness": 0.2457159499980376, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.24641818998286436, 0.24479424980819253, 0.24593541020305598], "final_y": [0.16487522625109663, 0.16553611389713074, 0.164856105788747]}, "mutation_prompt": null}
{"id": "1967759f-9c2f-4a68-b240-bb134dfff913", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n        self.step_size = 0.1  # Adaptive step size for local search\n        self.bandit_weights = np.ones(self.population_size)  # Bandit-inspired selection weights\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False, p=self.bandit_weights/self.bandit_weights.sum())\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    self.bandit_weights[i] *= 1.1  # Increase weight for successful trials\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n                else:\n                    self.bandit_weights[i] *= 0.9  # Decrease weight for unsuccessful trials\n\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = self.step_size * np.var(self.fitness) / np.mean(self.fitness)\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introduce adaptive step sizing and a multiarmed bandit-inspired selection mechanism for enhanced exploration and exploitation balance.", "configspace": "", "generation": 95, "fitness": 0.24325310887955612, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.243 with standard deviation 0.002. And the mean value of best solutions found was 0.172 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.24514096476381675, 0.2436284095677398, 0.24098995230711184], "final_y": [0.1648561638898418, 0.16753089032059432, 0.18323275516183435]}, "mutation_prompt": null}
{"id": "ba4b5d80-4b60-462d-8c25-853842c0c7a2", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3.5  # Increased perturbation dynamics further\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Improved perturbation strategy by increasing perturbation dynamics when no improvement is detected for a certain period.", "configspace": "", "generation": 96, "fitness": 0.245739857739354, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35415828-baf7-4421-8643-97d4a72df0ee", "metadata": {"aucs": [0.24641818998286436, 0.24486597303214164, 0.24593541020305598], "final_y": [0.16487522625109663, 0.1649475503688046, 0.164856105788747]}, "mutation_prompt": null}
{"id": "0b0579f3-aaa6-41a4-88cd-7c86139d95ef", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.1), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0) * (0.9 + 0.1 * (best_score / np.min(self.fitness)))\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3.5  # Increased perturbation dynamics further\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introduced adaptive adjustment to the mutation factor based on historical best scores to improve convergence.", "configspace": "", "generation": 97, "fitness": 0.24569015000629288, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "ba4b5d80-4b60-462d-8c25-853842c0c7a2", "metadata": {"aucs": [0.24641820050238816, 0.24471684123092552, 0.24593540828556493], "final_y": [0.16487533553656974, 0.16604724751157574, 0.16485607044885264]}, "mutation_prompt": null}
{"id": "8ac78ff9-174e-4938-8f15-974a4d70e825", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                # Adjusted crossover strategy\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / np.mean(self.fitness))\n                crossover_rate = min(max(crossover_rate, 0.2), 0.9)  # Enhanced crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n                elif np.random.rand() < 0.05:  # Adaptive trial replacement strategy\n                    self.population[i] = trial\n\n            # Adjust mutation factor and strategy selection\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / np.mean(self.fitness)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            # Introduce crowding distance maintenance\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  # Increased perturbation dynamics\n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3.5  # Increased perturbation dynamics further\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        # Ensure diversity by maintaining crowding distance\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  # Increased perturbation range", "name": "EnhancedMemeticDifferentialEvolution", "description": "Enhanced crossover strategy with adaptive trial replacement for improved exploration.", "configspace": "", "generation": 98, "fitness": 0.23186894553723467, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.232 with standard deviation 0.006. And the mean value of best solutions found was 0.220 (0. is the best) with standard deviation 0.032.", "error": "", "parent_id": "ba4b5d80-4b60-462d-8c25-853842c0c7a2", "metadata": {"aucs": [0.2240649394820189, 0.23729750967349694, 0.23424438745618814], "final_y": [0.26301505976668904, 0.18817549952006507, 0.2085449548753633]}, "mutation_prompt": null}
{"id": "fcd3d06b-7b31-4f7c-be88-bd6ca1b295ff", "solution": "import numpy as np\n\nclass EnhancedMemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.base_mutation_factor = 0.8\n        self.base_crossover_rate = 0.9\n        self.local_search_prob = 0.2\n        self.dynamic_population = True\n\n    def initialize_population(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.array([float('inf')] * self.population_size)\n        self.best_individual = None\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize_population(bounds)\n        evaluations = 0\n        best_score = float('inf')\n        last_improvement = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant = np.clip(a + self.base_mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover_rate = self.base_crossover_rate * (1.0 - np.var(self.fitness) / (np.mean(self.fitness)**1.5))\n                crossover_rate = min(max(crossover_rate, 0.2), 0.9)  # Adjusted crossover strategy\n                trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, self.population[i])\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_score\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        self.best_individual = trial\n                        last_improvement = evaluations\n\n            fitness_std = np.std(self.fitness)\n            self.base_mutation_factor = np.clip(0.5 + 0.3 * (fitness_std / (np.mean(self.fitness) + 1e-8)), 0.5, 1.0)\n\n            self.local_search_prob = 0.2 + 0.1 * (best_score / np.min(self.fitness))\n\n            if np.random.rand() < self.local_search_prob:\n                self.adaptive_local_search(func, bounds, evaluations - last_improvement)\n            if self.dynamic_population and evaluations - last_improvement > self.budget // 10:\n                self.population_size = max(5, self.population_size // 2)\n\n            self.maintain_diversity()\n\n        return self.best_individual\n\n    def adaptive_local_search(self, func, bounds, no_improvement_steps):\n        perturbation_base = 0.1 * np.var(self.fitness) / np.mean(self.fitness)  \n        for i in range(self.population_size):\n            perturbation = np.random.uniform(-perturbation_base, perturbation_base, self.dim)\n            if no_improvement_steps > self.budget // 20:\n                perturbation *= 3.5  \n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = func(new_position)\n            if new_score < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_score\n                if new_score < func(self.best_individual):\n                    self.best_individual = new_position\n\n    def maintain_diversity(self):\n        distances = np.sum((self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :]) ** 2, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        closest_indices = np.argmin(distances, axis=1)\n        for i in range(self.population_size):\n            if np.all(self.fitness[i] == self.fitness[closest_indices[i]]):\n                self.population[i] += np.random.uniform(-0.15, 0.15, self.dim)  ", "name": "EnhancedMemeticDifferentialEvolution", "description": "Introduced convergence-aware mutation scaling and adaptive crossover probability to enhance exploration and solution quality.", "configspace": "", "generation": 99, "fitness": 0.2438467126928431, "feedback": "The algorithm EnhancedMemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.244 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "ba4b5d80-4b60-462d-8c25-853842c0c7a2", "metadata": {"aucs": [0.24143423836990718, 0.2445833016351623, 0.24552259807345989], "final_y": [0.1821029512153599, 0.16486552162755297, 0.16486609478115688]}, "mutation_prompt": null}
