{"id": "82ac9dc8-0217-49a5-9877-6de52b234e03", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.scale_f = 0.8\n        self.crossover_prob = 0.9\n        self.used_evaluations = 0\n\n    def _initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(self.population_size)\n        for i in range(self.population_size):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        for i in range(self.population_size):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population, fitness = self._differential_evolution_step(population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "HybridOptimization", "description": "A hybrid global-local optimization algorithm that combines Differential Evolution with periodicity-inspired constraints and local fine-tuning for efficient exploration and exploitation of the search space.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 280, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 128, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 66, in __call__\n  File \"<string>\", line 44, in _differential_evolution_step\nNameError: name 'func' is not defined\n.", "error": "NameError(\"name 'func' is not defined\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 280, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 128, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 66, in __call__\n  File \"<string>\", line 44, in _differential_evolution_step\nNameError: name 'func' is not defined\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "4df35756-376e-456a-8d5b-aa8efff512fb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Multi-stage self-adaptive DE algorithm using localized periodicity insights and adaptive population sizing to efficiently tackle complex optimization landscapes.", "configspace": "", "generation": 1, "fitness": 0.788614305924309, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.789 with standard deviation 0.016. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "82ac9dc8-0217-49a5-9877-6de52b234e03", "metadata": {"aucs": [0.7792210101471845, 0.7759324007611859, 0.810689506864557], "final_y": [0.17987981226446814, 0.19249264268978017, 0.1745646917441639]}, "mutation_prompt": null}
{"id": "f18104a4-b97b-49a3-a3c6-4a361e584d92", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget)  # Change line\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhanced AdaptiveDEWithPeriodicity by integrating a self-adaptive crossover probability mechanism for improved exploration-exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.7982512583144455, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.006. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "4df35756-376e-456a-8d5b-aa8efff512fb", "metadata": {"aucs": [0.7935492980462491, 0.8064127070691299, 0.7947917698279581], "final_y": [0.1801787675835944, 0.174183503258455, 0.1779596896964979]}, "mutation_prompt": null}
{"id": "1538086a-cfd6-41ad-8b2c-fa6dba5c2413", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedAdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            # Adaptive mutation factor based on current best fitness\n            adaptive_scale_f = self.scale_f + 0.2 * (1 - fitness[i] / np.max(fitness))\n            mutant = np.clip(a + adaptive_scale_f * (b - c), lb, ub)\n            # Periodicity-inducing bias\n            perturbation = 0.1 * np.sin(2 * np.pi * np.arange(self.dim) / self.dim)\n            mutant = (mutant + perturbation) % (ub - lb) + lb\n            self.crossover_prob = 0.5 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "EnhancedAdaptiveDEWithPeriodicity", "description": "Introduce a periodicity-inducing bias and adaptive mutation scaling in Enhanced AdaptiveDEWithPeriodicity for improved convergence.", "configspace": "", "generation": 3, "fitness": 0.7702211849756799, "feedback": "The algorithm EnhancedAdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.770 with standard deviation 0.006. And the mean value of best solutions found was 0.190 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "f18104a4-b97b-49a3-a3c6-4a361e584d92", "metadata": {"aucs": [0.7780469899527469, 0.7654261191641386, 0.7671904458101544], "final_y": [0.19699143801942898, 0.18662208085030596, 0.1852843559316606]}, "mutation_prompt": null}
{"id": "bfba9f0d-bbe0-43a8-a382-575918d37a84", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.1 * np.sin(self.used_evaluations * np.pi / self.budget)  # Change line\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget)  # Change line\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Improved AdaptiveDEWithPeriodicity by introducing dynamic scaling factor based on evaluations to enhance solution exploration and exploitation.", "configspace": "", "generation": 4, "fitness": 0.8226979649246001, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.823 with standard deviation 0.036. And the mean value of best solutions found was 0.178 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "f18104a4-b97b-49a3-a3c6-4a361e584d92", "metadata": {"aucs": [0.8476831696445728, 0.7723203289239509, 0.8480903962052768], "final_y": [0.18164519653315403, 0.18330744428308243, 0.16922957033487385]}, "mutation_prompt": null}
{"id": "3c948a79-a507-4175-8b47-d960c2910654", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.1 * np.sin(self.used_evaluations * np.pi / self.budget)  # Change line\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget)  # Change line\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        population = np.vstack([population, np.eye(self.dim)])  # Change line\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhanced AdaptiveDEWithPeriodicity by incorporating orthogonal initialization to improve diversity and convergence rate.", "configspace": "", "generation": 5, "fitness": 0.76472031268309, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.765 with standard deviation 0.036. And the mean value of best solutions found was 0.191 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "bfba9f0d-bbe0-43a8-a382-575918d37a84", "metadata": {"aucs": [0.8112526489599947, 0.7243639011944417, 0.7585443878948337], "final_y": [0.1827143477188342, 0.21475717104263103, 0.1751667021210842]}, "mutation_prompt": null}
{"id": "eff97519-e5ff-4eca-a62b-e43d2eca562f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DEWithSimulatedAnnealing:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n        self.temperature = 1.0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n\n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                # Simulated Annealing acceptance criterion\n                diff = trial_fitness - fitness[i]\n                if diff < 0 or np.random.rand() < np.exp(-diff / self.temperature):\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n                self.temperature *= 0.99  # Decrease temperature\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "DEWithSimulatedAnnealing", "description": "Introduce synergy between Differential Evolution and Simulated Annealing by integrating temperature-based acceptance to escape local minima.", "configspace": "", "generation": 6, "fitness": 0.776330382899212, "feedback": "The algorithm DEWithSimulatedAnnealing got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.776 with standard deviation 0.038. And the mean value of best solutions found was 0.192 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "bfba9f0d-bbe0-43a8-a382-575918d37a84", "metadata": {"aucs": [0.8216245833441157, 0.7779891625792753, 0.7293774027742448], "final_y": [0.19407637880628625, 0.19491430946440402, 0.18625571220705162]}, "mutation_prompt": null}
{"id": "43b2126c-d114-4ae4-97ad-7d08fdca5b86", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            selected = np.random.choice(idxs, 3, replace=False, p=fitness[idxs]/fitness[idxs].sum())  # Change line\n            a, b, c = population[selected]\n            self.scale_f = 0.4 + 0.1 * np.sin(self.used_evaluations * np.pi / self.budget)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhanced AdaptiveDEWithPeriodicity by introducing weighted random selection for mutation vectors to improve the convergence rate.", "configspace": "", "generation": 7, "fitness": 0.7838978834406625, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.025. And the mean value of best solutions found was 0.188 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "bfba9f0d-bbe0-43a8-a382-575918d37a84", "metadata": {"aucs": [0.8191037752529444, 0.7627619077593883, 0.7698279673096549], "final_y": [0.17961511611138736, 0.19994219478085595, 0.18471952723788854]}, "mutation_prompt": null}
{"id": "7eaec36b-8fc5-485c-8e32-6035901bbf39", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i]) + 0.1 * np.sin(2 * np.pi * i / self.dim)  # Change line\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.1 * np.sin(self.used_evaluations * np.pi / self.budget) \n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget)  # Change line\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial) + 0.1 * np.sin(2 * np.pi * i / self.dim)  # Change line\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhanced AdaptiveDEWithPeriodicity by introducing an oscillating periodicity factor in the objective to encourage periodic solutions.", "configspace": "", "generation": 8, "fitness": 0.8226979649246001, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.823 with standard deviation 0.036. And the mean value of best solutions found was 0.178 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "bfba9f0d-bbe0-43a8-a382-575918d37a84", "metadata": {"aucs": [0.8476831696445728, 0.7723203289239509, 0.8480903962052768], "final_y": [0.18164519653315403, 0.18330744428308243, 0.16922957033487385]}, "mutation_prompt": null}
{"id": "e43d548b-e0e5-4cf9-80da-64e0d3069470", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedAdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n        self.restart_threshold = budget // 4  # Restart mechanism threshold\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.1 * np.sin(self.used_evaluations * np.pi / self.budget)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n        evaluations_since_restart = 0\n\n        while self.used_evaluations < self.budget:\n            if evaluations_since_restart >= self.restart_threshold:\n                population = self._initialize_population(bounds, population_size)\n                fitness = self._evaluate_population(func, population)\n                evaluations_since_restart = 0\n\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n            evaluations_since_restart += population_size\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "EnhancedAdaptiveDEWithPeriodicity", "description": "Enhanced AdaptiveDEWithPeriodicity by incorporating a periodic parameter restart mechanism to escape local minima and improve global exploration.", "configspace": "", "generation": 9, "fitness": 0.7503981571874999, "feedback": "The algorithm EnhancedAdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.750 with standard deviation 0.039. And the mean value of best solutions found was 0.245 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "bfba9f0d-bbe0-43a8-a382-575918d37a84", "metadata": {"aucs": [0.8054142934078093, 0.7181946576067566, 0.7275855205479335], "final_y": [0.22684730775431527, 0.25330185463222565, 0.2544595625092122]}, "mutation_prompt": null}
{"id": "f895c1e5-9905-4137-bb58-e9dc7e585845", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            # Introduce chaotic sequence for scale_f\n            self.scale_f = 0.4 + 0.1 * np.sin(self.used_evaluations * np.pi / self.budget) + 0.05 * np.sin(3.6 * self.used_evaluations)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Refined exploration by introducing chaotic sequences in mutation scaling, enhancing search diversity in AdaptiveDEWithPeriodicity.", "configspace": "", "generation": 10, "fitness": 0.8034764135919383, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.033. And the mean value of best solutions found was 0.178 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "bfba9f0d-bbe0-43a8-a382-575918d37a84", "metadata": {"aucs": [0.8264890912628945, 0.7568516007201057, 0.8270885487928147], "final_y": [0.16994262997961707, 0.1885189068140286, 0.1740979974671334]}, "mutation_prompt": null}
{"id": "b9b1f1e5-142f-4482-bae3-5418f0e8b5d9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Change line\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.2 * np.cos(self.used_evaluations * np.pi / self.budget)  # Change line\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhanced dynamic scaling and crossover with cosine function for smoother transitions during evolution.", "configspace": "", "generation": 11, "fitness": 0.8060317430836355, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.015. And the mean value of best solutions found was 0.174 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "bfba9f0d-bbe0-43a8-a382-575918d37a84", "metadata": {"aucs": [0.8173482796639883, 0.7844307948952722, 0.8163161546916456], "final_y": [0.17239609493180896, 0.1797488610316249, 0.16971619082305323]}, "mutation_prompt": null}
{"id": "7fb1a84c-02f3-4d0a-8b2b-f6ee6ffdbab2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.1 * np.sin(self.used_evaluations * np.pi / self.budget)  \n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget)  # Change line\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)  # Change line\n        new_population[0] = population[elite_idx]  # Change line\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhanced AdaptiveDEWithPeriodicity by dynamically adjusting crossover probabilities and introducing elite preservation for heightened convergence speed and solution quality.", "configspace": "", "generation": 12, "fitness": 0.8365085803101477, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.033. And the mean value of best solutions found was 0.169 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "bfba9f0d-bbe0-43a8-a382-575918d37a84", "metadata": {"aucs": [0.8831188062438663, 0.808709799194635, 0.8176971354919422], "final_y": [0.16723394435422267, 0.16991591564582276, 0.170558736079503]}, "mutation_prompt": null}
{"id": "2ed4f465-1f25-4029-b0c0-17bb5dda96f4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 2))  # Change line 1\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.1 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Change line 2\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = trial + (np.mean(population) - trial) * 0.1  # Change line 3\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhanced AdaptiveDEWithPeriodicity by introducing a dynamic periodicity factor and adaptive mutation to better explore the optimization landscape and encourage periodic solutions.", "configspace": "", "generation": 13, "fitness": 0.9359138298163625, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.936 with standard deviation 0.008. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7fb1a84c-02f3-4d0a-8b2b-f6ee6ffdbab2", "metadata": {"aucs": [0.9324736343619171, 0.9475305100054241, 0.9277373450817465], "final_y": [0.16631194040307018, 0.16564958957082498, 0.16617000637336177]}, "mutation_prompt": null}
{"id": "b1e8ef3a-480d-4135-80af-10471a6dd686", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))  # Change line 1\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.15 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Change line 2\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = trial + (np.mean(population) - trial) * 0.1\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Further refined Dynamic Periodicity Factor in Differential Evolution to enhance exploitation and exploration balance.", "configspace": "", "generation": 14, "fitness": 0.93844802858443, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.938 with standard deviation 0.008. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "2ed4f465-1f25-4029-b0c0-17bb5dda96f4", "metadata": {"aucs": [0.936331829996597, 0.9490823180281578, 0.9299299377285352], "final_y": [0.16550166322363835, 0.1652469282776412, 0.16722878686375187]}, "mutation_prompt": null}
{"id": "e2f01a8b-c4ad-48df-90cc-6e1fe13b721b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridPSODynamicPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, 5 * dim)\n        self.used_evaluations = 0\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def _initialize_particles(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        positions = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        velocities = np.zeros((self.population_size, self.dim))\n        return positions, velocities\n\n    def _evaluate_particles(self, func, positions):\n        fitness = np.zeros(len(positions))\n        for i in range(len(positions)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(positions[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _update_velocities_and_positions(self, positions, velocities, personal_best_positions, global_best_position, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (self.inertia_weight * velocities[i] +\n                             self.cognitive_coeff * r1 * (personal_best_positions[i] - positions[i]) +\n                             self.social_coeff * r2 * (global_best_position - positions[i]))\n            velocities[i] = np.clip(velocities[i], lb - positions[i], ub - positions[i])\n            positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n        return positions, velocities\n\n    def _dynamic_periodicity_adjustment(self, positions):\n        periodicity_factor = np.cos(self.used_evaluations * np.pi / self.budget) + 1\n        for i in range(len(positions)):\n            positions[i] += (np.mean(positions) - positions[i]) * periodicity_factor * 0.01\n        return positions\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)),\n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        positions, velocities = self._initialize_particles(bounds)\n        fitness = self._evaluate_particles(func, positions)\n        personal_best_positions = np.copy(positions)\n        personal_best_fitness = np.copy(fitness)\n        global_best_idx = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_idx]\n\n        while self.used_evaluations < self.budget:\n            positions = self._dynamic_periodicity_adjustment(positions)\n            positions, velocities = self._update_velocities_and_positions(\n                positions, velocities, personal_best_positions, global_best_position, bounds)\n            fitness = self._evaluate_particles(func, positions)\n            for i in range(self.population_size):\n                if fitness[i] < personal_best_fitness[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_fitness[i] = fitness[i]\n            new_global_best_idx = np.argmin(personal_best_fitness)\n            if personal_best_fitness[new_global_best_idx] < personal_best_fitness[global_best_idx]:\n                global_best_idx = new_global_best_idx\n                global_best_position = personal_best_positions[global_best_idx]\n\n        best_solution = self._local_fine_tuning(global_best_position, func, bounds)\n        \n        return best_solution", "name": "HybridPSODynamicPeriodicity", "description": "Hybrid Particle Swarm Optimization with Dynamic Periodicity and Local Search to enhance exploration and exploitation in multilayered photonic structure optimization.", "configspace": "", "generation": 15, "fitness": 0.9372280420385374, "feedback": "The algorithm HybridPSODynamicPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.937 with standard deviation 0.024. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b1e8ef3a-480d-4135-80af-10471a6dd686", "metadata": {"aucs": [0.9635864397046557, 0.9426191692907688, 0.9054785171201876], "final_y": [0.16485935216568637, 0.16485962787769115, 0.1818791067597999]}, "mutation_prompt": null}
{"id": "108d3401-b244-4f7b-8ef0-cdc48f4adabe", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))  # Change line 1\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.15 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Change line 2\n            self.crossover_prob = 0.6 + 0.1 * np.cos(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Change line 3\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = trial + (np.mean(population) - trial) * 0.1\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhanced exploration and exploitation by incorporating adaptive crossover probability and refined local tuning.", "configspace": "", "generation": 16, "fitness": 0.9342044291039145, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.934 with standard deviation 0.007. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1e8ef3a-480d-4135-80af-10471a6dd686", "metadata": {"aucs": [0.9411383527753516, 0.9361860931292643, 0.9252888414071279], "final_y": [0.16587067842168912, 0.16528666362677957, 0.16634911009297249]}, "mutation_prompt": null}
{"id": "f4ec77be-4f2b-489b-9cb5-6649f7151a3e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ImprovedAdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # Change line 1\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.15 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Change line 2\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            dynamic_crossover_prob = 0.5 + 0.25 * np.cos(self.used_evaluations * np.pi / (self.budget / 2))  # New line\n            cross_points = np.random.rand(self.dim) < dynamic_crossover_prob  # Change line 3\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = trial + (np.mean(population) - trial) * 0.1\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        periodic_constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x[::2] - x[1::2])}]\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='SLSQP', options={'maxfun': self.budget - self.used_evaluations},\n                          constraints=periodic_constraints)  # Changed local search to SLSQP with constraints\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "ImprovedAdaptiveDEWithPeriodicity", "description": "Incorporate dynamic crossover strategies and periodicity-aware local search to improve convergence and solution quality in Adaptive Differential Evolution.", "configspace": "", "generation": 17, "fitness": 0.9232730796083327, "feedback": "The algorithm ImprovedAdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.923 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1e8ef3a-480d-4135-80af-10471a6dd686", "metadata": {"aucs": [0.9210442523140084, 0.9193755702340682, 0.9293994162769212], "final_y": [0.16601036105335876, 0.16600662736923688, 0.16660297795989976]}, "mutation_prompt": null}
{"id": "687e44d2-752e-43cc-89fe-fd2b53ae7152", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.15 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.6 + 0.3 * np.cos(self.used_evaluations * np.pi / (self.budget / 2))  # Change line\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = trial + (np.mean(population) - trial) * 0.1\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce a dynamically adjusting crossover probability to enhance exploration-exploitation balance in Adaptive Differential Evolution.", "configspace": "", "generation": 18, "fitness": 0.946315221576377, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.946 with standard deviation 0.007. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1e8ef3a-480d-4135-80af-10471a6dd686", "metadata": {"aucs": [0.9458852387591639, 0.9373621442470749, 0.9556982817228926], "final_y": [0.16567105925087222, 0.16569610677450675, 0.16534592954780858]}, "mutation_prompt": null}
{"id": "4c7e16f6-f049-4b40-b736-722a119ab1b2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.15 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.6 + 0.3 * np.cos(self.used_evaluations * np.pi / (self.budget / 2))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = trial + (np.mean(population) - trial) * 0.05  # Changed line\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance convergence by adjusting the trial solution's attraction towards the population mean and fine-tuning weight.", "configspace": "", "generation": 19, "fitness": 0.9165816438954423, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.917 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "687e44d2-752e-43cc-89fe-fd2b53ae7152", "metadata": {"aucs": [0.9147738109805782, 0.9060479258569829, 0.9289231948487656], "final_y": [0.16505772653445538, 0.1651273479269717, 0.16506788350429558]}, "mutation_prompt": null}
{"id": "ba0b54f8-d82d-48fa-bed6-cba070f3d283", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.15 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            # Adjust crossover probability based on fitness\n            self.crossover_prob = 0.6 + 0.3 * (1 - fitness[i] / (np.max(fitness) + 1e-9))  # Change line\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = trial + (np.mean(population) - trial) * 0.1\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance the exploitation of promising solutions by adjusting the crossover probability based on the relative fitness of a trial solution.", "configspace": "", "generation": 20, "fitness": 0.9303972895771263, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.930 with standard deviation 0.003. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "687e44d2-752e-43cc-89fe-fd2b53ae7152", "metadata": {"aucs": [0.933436535531217, 0.9258291410476323, 0.9319261921525295], "final_y": [0.16538888959206643, 0.1656923778962378, 0.16609594744945866]}, "mutation_prompt": null}
{"id": "180e7a33-7698-4b32-9967-a7557d627a16", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.15 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.6 + 0.3 * np.sin(self.used_evaluations * np.pi / (self.budget / 2))  # Change line\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = trial + (np.mean(population) - trial) * 0.1\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduced a variable crossover strategy based on the sine function to balance exploration and exploitation dynamically.", "configspace": "", "generation": 21, "fitness": 0.9339507269164976, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.934 with standard deviation 0.006. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "687e44d2-752e-43cc-89fe-fd2b53ae7152", "metadata": {"aucs": [0.9365498727899997, 0.9390533219608885, 0.9262489859986045], "final_y": [0.16608770241043025, 0.16540296994866233, 0.16628720768532002]}, "mutation_prompt": null}
{"id": "79f596ce-5101-444d-a027-9120ede0c638", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.15 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.6 + 0.3 * np.cos(self.used_evaluations * np.pi / (self.budget / 2))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = trial + (np.mean(population) - trial) * 0.1\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': (self.budget - self.used_evaluations) // 2})  # Change line\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce adaptive scaling to dynamically adjust both the mutation factor and local tuning intensity with remaining evaluations.", "configspace": "", "generation": 22, "fitness": 0.946315221576377, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.946 with standard deviation 0.007. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "687e44d2-752e-43cc-89fe-fd2b53ae7152", "metadata": {"aucs": [0.9458852387591639, 0.9373621442470749, 0.9556982817228926], "final_y": [0.16567105925087222, 0.16569610677450675, 0.16534592954780858]}, "mutation_prompt": null}
{"id": "8bf1a0bc-173f-4502-b3b5-02417d0c15b5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.2))  # Change line\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.2 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Change line\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.6 + 0.25 * np.cos(self.used_evaluations * np.pi / (self.budget / 2))  # Change line\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = trial + (np.mean(population) - trial) * 0.12  # Change line\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance dynamic periodicity and mutation strategies in Adaptive Differential Evolution for improved exploration and exploitation.", "configspace": "", "generation": 23, "fitness": 0.9432743618964725, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.943 with standard deviation 0.002. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "687e44d2-752e-43cc-89fe-fd2b53ae7152", "metadata": {"aucs": [0.9429717692618336, 0.9452562538881087, 0.9415950625394754], "final_y": [0.1666310861059177, 0.16560408401368665, 0.1669875490181043]}, "mutation_prompt": null}
{"id": "0c2a5c9c-c128-4ce1-abf3-c026acd7a3ef", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget * np.random.uniform(1.4, 1.6)))  # Change line\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.15 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.6 + 0.3 * np.cos(self.used_evaluations * np.pi / (self.budget / 2))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = trial + (np.mean(population) - trial) * 0.1\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance parallel exploration by introducing stochastic scaling to the periodic factor computation in Differential Evolution.", "configspace": "", "generation": 24, "fitness": 0.9442243329705368, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.944 with standard deviation 0.007. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "687e44d2-752e-43cc-89fe-fd2b53ae7152", "metadata": {"aucs": [0.9454855670975459, 0.9515868279269465, 0.9356006038871181], "final_y": [0.1658868148584911, 0.16530237660527114, 0.16583312868464628]}, "mutation_prompt": null}
{"id": "35d54e9b-8521-437e-8958-ab940b935d45", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.15 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.35 * np.cos(self.used_evaluations * np.pi / (self.budget / 2))  # Change line\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = trial + (np.mean(population) - trial) * 0.1\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce a periodic modulation to the scaling factor to enhance adaptability in Adaptive Differential Evolution.", "configspace": "", "generation": 25, "fitness": 0.9311700396284003, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.931 with standard deviation 0.004. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "687e44d2-752e-43cc-89fe-fd2b53ae7152", "metadata": {"aucs": [0.9317299238548727, 0.9354689835667319, 0.9263112114635961], "final_y": [0.16641191027511804, 0.1666515413508176, 0.16651060866981993]}, "mutation_prompt": null}
{"id": "ac01c549-f9cd-4cbc-a06a-95c4d816aa21", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.15 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.6 + 0.3 * np.cos(self.used_evaluations * np.pi / (self.budget / 2))  # Change line\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = trial + (np.mean(population) - trial) * 0.1\n            trial = np.clip(trial + 0.1 * (self._select_best(population, fitness)[0] - trial), lb, ub)  # Enhance exploitation\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance exploitation by modifying the trial vector update to incorporate a weighted influence of the current best solution.", "configspace": "", "generation": 26, "fitness": 0.9622620643201579, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.962 with standard deviation 0.006. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "687e44d2-752e-43cc-89fe-fd2b53ae7152", "metadata": {"aucs": [0.9538480415759094, 0.9679702500711826, 0.9649679013133815], "final_y": [0.16658692872396874, 0.1657836038257664, 0.16585022235197255]}, "mutation_prompt": null}
{"id": "ac4151d4-00ff-4a3a-ae4b-ee24aa6fbde4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.2 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.4 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = trial + (np.mean(population) - trial) * 0.1\n            trial = np.clip(trial + 0.1 * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance exploration by incorporating adaptive crossover strategy and dynamic scaling factor adjustments to improve solution diversity.", "configspace": "", "generation": 27, "fitness": 0.9667235619019675, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.967 with standard deviation 0.003. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "ac01c549-f9cd-4cbc-a06a-95c4d816aa21", "metadata": {"aucs": [0.9676531614533561, 0.9698819838936628, 0.9626355403588837], "final_y": [0.16655132403369355, 0.16511157929985099, 0.16723088477815007]}, "mutation_prompt": null}
{"id": "c88ba302-b5a2-4d07-b944-8bf66ffcd8e4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.2 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.4 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.15 * np.sin(self.used_evaluations * np.pi / self.budget)  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.1\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce a dynamic scaling of elite influence to better preserve promising regions while maintaining exploration.", "configspace": "", "generation": 28, "fitness": 0.9675388678419902, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.001. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "ac4151d4-00ff-4a3a-ae4b-ee24aa6fbde4", "metadata": {"aucs": [0.9675228649080623, 0.9664626140271468, 0.9686311245907614], "final_y": [0.16747951251690918, 0.16609659502477636, 0.16594618740451061]}, "mutation_prompt": null}
{"id": "7c3b0bcd-53bc-49e2-aa9b-fe2f37aaaa78", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget)  # Dynamic elite influence (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity (changed line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance exploration by incorporating adaptive mutation strategies and periodicity-aware crossover in Differential Evolution (DE).", "configspace": "", "generation": 29, "fitness": 0.9708876255938579, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c88ba302-b5a2-4d07-b944-8bf66ffcd8e4", "metadata": {"aucs": [0.9728394157273345, 0.9743381797452679, 0.9654852813089712], "final_y": [0.16547478116425607, 0.16528032757697153, 0.16580787491193816]}, "mutation_prompt": null}
{"id": "8f6d3055-6117-4d75-87e1-c81bf6536daf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.3 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.6 + 0.35 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget)  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Improve adaptation of mutation strategies by dynamically adjusting scale factor and crossover probability in DE to enhance convergence.", "configspace": "", "generation": 30, "fitness": 0.9681504088770789, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7c3b0bcd-53bc-49e2-aa9b-fe2f37aaaa78", "metadata": {"aucs": [0.9723003231418746, 0.9683648090461089, 0.9637860944432534], "final_y": [0.16506719540505121, 0.16528059169761622, 0.1651140894170744]}, "mutation_prompt": null}
{"id": "86184894-a65f-47f5-9ff6-d3a1e94c5614", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c) + 0.1 * np.sin(self.used_evaluations * np.pi), lb, ub)  # Add a dynamic component\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget)  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce dynamic scaling in DE strategy to adaptively balance exploration and exploitation.", "configspace": "", "generation": 31, "fitness": 0.9708876255938579, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7c3b0bcd-53bc-49e2-aa9b-fe2f37aaaa78", "metadata": {"aucs": [0.9728394157273343, 0.9743381797452678, 0.9654852813089715], "final_y": [0.1654747811642563, 0.16528032757697153, 0.1658078749119386]}, "mutation_prompt": null}
{"id": "ae926e8a-e4bd-4814-8e09-6ab655403192", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.cos(self.used_evaluations * np.pi / (self.budget / 2))  # Dynamic elite influence (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity (changed line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance periodicity influence by dynamically adjusting elite influence and crossover strategies based on the budget usage.", "configspace": "", "generation": 32, "fitness": 0.9708337500159178, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.005. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "7c3b0bcd-53bc-49e2-aa9b-fe2f37aaaa78", "metadata": {"aucs": [0.9729873765028313, 0.9758375832930684, 0.9636762902518535], "final_y": [0.16562579565398416, 0.1651340058314169, 0.16880943473091503]}, "mutation_prompt": null}
{"id": "97209733-505b-414f-bd7e-c6036a678d49", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.35 + 0.15 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjusted scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.55 + 0.35 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjusted crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget)  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Adaptive Differential Evolution (DE) with refined adaptive mutation and crossover strategies for enhanced periodicity exploitation.", "configspace": "", "generation": 33, "fitness": 0.9695333454829703, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.970 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "7c3b0bcd-53bc-49e2-aa9b-fe2f37aaaa78", "metadata": {"aucs": [0.9699064371250808, 0.9712472253879597, 0.9674463739358702], "final_y": [0.16498017253290953, 0.16490471613936464, 0.16621546327711478]}, "mutation_prompt": null}
{"id": "848f3d97-f334-4f32-b30e-68d2f3b3a554", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        diversity_factor = np.std(population) / self.dim  # Added line to adjust starting point based on diversity\n        best_start_point = best_solution + diversity_factor * np.random.rand(self.dim)  # Adjusted starting point\n        result = minimize(func, best_start_point, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance solution refinement by dynamically adjusting the local search strategy's starting point based on the population's diversity.", "configspace": "", "generation": 34, "fitness": 0.9708876255938579, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7c3b0bcd-53bc-49e2-aa9b-fe2f37aaaa78", "metadata": {"aucs": [0.9728394157273345, 0.9743381797452679, 0.9654852813089712], "final_y": [0.16547478116425607, 0.16528032757697153, 0.16580787491193816]}, "mutation_prompt": null}
{"id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity (changed line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce dynamic scaling of elite influence in DE to better guide exploration and exploitation.", "configspace": "", "generation": 35, "fitness": 0.9715952658160573, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.003. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7c3b0bcd-53bc-49e2-aa9b-fe2f37aaaa78", "metadata": {"aucs": [0.9722155028751148, 0.9744233609966974, 0.9681469335763594], "final_y": [0.16580850058980812, 0.16541575123588415, 0.16535664112893567]}, "mutation_prompt": null}
{"id": "0de8704e-87c6-45d2-b4c9-6dac1fa9a037", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            improvement_factor = np.tanh((np.min(fitness) - fitness[i]) / np.std(fitness) + 1)  # New line\n            self.crossover_prob = 0.5 + 0.45 * improvement_factor * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjusted line\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity (changed line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce adaptive crossover probability adjustment based on fitness improvements to enhance local search efficiency.", "configspace": "", "generation": 36, "fitness": 0.9677960115215002, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.003. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9638280683057787, 0.9701482339765503, 0.9694117322821717], "final_y": [0.16549118349795988, 0.16606687400041165, 0.16544555861685295]}, "mutation_prompt": null}
{"id": "53f170a3-fa4c-48d7-87b7-d7a114d66f68", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.2 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.6 + 0.35 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance crossover and mutation adaptation by adjusting periodicity and elite influence to improve convergence.", "configspace": "", "generation": 37, "fitness": 0.9687820725856273, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.002. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9679792584752166, 0.9716236206256571, 0.9667433386560085], "final_y": [0.16815827898724622, 0.16503440418257498, 0.1650303527325686]}, "mutation_prompt": null}
{"id": "9d2268f1-e4f8-42af-ad39-33a312b22b8c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedDEWithAdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _ensemble_local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        ensemble_strategies = ['L-BFGS-B', 'TNC']\n        results = []\n        for strategy in ensemble_strategies:\n            result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                              method=strategy, options={'maxfun': (self.budget - self.used_evaluations) // len(ensemble_strategies)})\n            self.used_evaluations += result.nfev\n            if result.success:\n                results.append(result)\n        if results:\n            best_result = min(results, key=lambda r: r.fun)\n            return best_result.x\n        return best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._ensemble_local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "EnhancedDEWithAdaptiveLocalSearch", "description": "Integrate adaptive periodicity factors with ensemble local search techniques to enhance exploitation near promising regions.", "configspace": "", "generation": 38, "fitness": 0.9715952658160573, "feedback": "The algorithm EnhancedDEWithAdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.003. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9722155028751148, 0.9744233609966974, 0.9681469335763594], "final_y": [0.16580850058980812, 0.16541575123588415, 0.16535664112893567]}, "mutation_prompt": null}
{"id": "60f5493f-3f7e-4741-aeed-782ca49a3d6a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.35 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 2.0))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence (unchanged line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity (unchanged line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance DE by optimizing scaling of elite influence and mutation strategy for better periodicity and convergence.", "configspace": "", "generation": 39, "fitness": 0.9662580680230469, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9697543763209725, 0.9688313914263813, 0.9601884363217869], "final_y": [0.16646408114617217, 0.16549317678303177, 0.16578642226145535]}, "mutation_prompt": null}
{"id": "2b52e786-cc12-42eb-9e12-b88b58968bb2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.4 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity (changed line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Improved elite influence strategy by dynamically adjusting the attraction to the global best solution to enhance convergence.", "configspace": "", "generation": 40, "fitness": 0.9685510068690482, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.001. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9667549997893076, 0.9695456252050042, 0.9693523956128328], "final_y": [0.16754154274580324, 0.16733893176566472, 0.16528418625156338]}, "mutation_prompt": null}
{"id": "874ec7f8-3197-49b4-94ae-1489642a275b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.3))  # Adjust periodicity factor\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 2.1))  # Introduce chaotic patterns\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance exploration by dynamically adjusting the periodicity factor and introduce chaotic patterns in crossover.", "configspace": "", "generation": 41, "fitness": 0.968648240289007, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.006. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9764549623407703, 0.9623473165949047, 0.9671424419313464], "final_y": [0.16609648460021498, 0.16677395932393257, 0.16618710651966917]}, "mutation_prompt": null}
{"id": "e43a61ca-0c98-4632-be2f-e6438c18c233", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            diversity_factor = np.std(population) / np.mean(population)  # Calculate diversity factor (added line)\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor * diversity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance periodic influence by dynamically adjusting the scale factor based on population diversity.", "configspace": "", "generation": 42, "fitness": 0.964362379625591, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.003. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9617572618276986, 0.9687987302511992, 0.9625311467978754], "final_y": [0.1666653971667693, 0.16517443352627637, 0.16766854266378306]}, "mutation_prompt": null}
{"id": "3c26cac9-9985-4cde-9e21-e742c78acdab", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.tan(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity (changed line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance mutation strategy with selective adaptation of scale factor to promote diversity and exploration.", "configspace": "", "generation": 43, "fitness": 0.9648873252634972, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.005. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9578481298783429, 0.9706692476660496, 0.9661445982460993], "final_y": [0.16565129123141653, 0.16490391295514395, 0.16627029732635623]}, "mutation_prompt": null}
{"id": "19be8706-b8ec-46b9-a795-23e2b16f912b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.15 + 0.2 * np.cos(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Adjusted line\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Fine-tune the elite influence factor to dynamically adjust based on the cosine wave for enhanced performance.", "configspace": "", "generation": 44, "fitness": 0.9609788199968371, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.961 with standard deviation 0.013. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.974080005929175, 0.9655310575764061, 0.9433253964849302], "final_y": [0.16756066953980653, 0.16750166243380304, 0.17779656239185693]}, "mutation_prompt": null}
{"id": "4cac1b4e-7c8b-42c2-9ffa-93dc77006494", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            # Slight perturbation added here for enhanced exploration\n            trial += 0.01 * np.random.randn(*trial.shape)  # New line: added perturbation\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity (changed line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce a slight perturbation in the trial vector creation to enhance exploration in the DE step.", "configspace": "", "generation": 45, "fitness": 0.9565716238693117, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.957 with standard deviation 0.005. And the mean value of best solutions found was 0.169 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9633854719817618, 0.952127597231739, 0.9542018023944345], "final_y": [0.16781393093062769, 0.16897088318021103, 0.17014455078641]}, "mutation_prompt": null}
{"id": "0ac293b3-35c6-4ece-ba25-b828e627c1a5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n\n    def _chaotic_initialization(self, bounds, pop_size, chaotic_map):\n        lb, ub = bounds.lb, bounds.ub\n        chaotic_seq = chaotic_map(pop_size, self.dim)\n        population = lb + (ub - lb) * chaotic_seq\n        return population\n\n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)  # Switch to chaotic initialization\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance diversity by incorporating a chaotic map influenced initialization for better exploration.", "configspace": "", "generation": 46, "fitness": 0.9715952658160573, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.003. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9722155028751148, 0.9744233609966974, 0.9681469335763594], "final_y": [0.16580850058980812, 0.16541575123588415, 0.16535664112893567]}, "mutation_prompt": null}
{"id": "b3b0f4b1-1e56-43ea-8451-7515c88f9ddd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        fitness_variance = np.var(fitness)  # Calculate fitness variance (added line)\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / (dynamic_periodicity_factor + fitness_variance))  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance exploration by adjusting mutation strategy based on fitness variance and refining the periodicity factor.", "configspace": "", "generation": 47, "fitness": 0.9687358813146673, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9714280907660233, 0.9682772758060227, 0.9665022773719558], "final_y": [0.1656195746159802, 0.16507149956822742, 0.16566001820793963]}, "mutation_prompt": null}
{"id": "31ab0fbb-6289-447f-8821-fe22b67f1476", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / (self.budget * 0.9)) * dynamic_periodicity_factor  # Dynamic elite influence (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity (changed line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Minor adjustment to the periodicity modulation to enhance convergence precision.", "configspace": "", "generation": 48, "fitness": 0.9697799233199828, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.970 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9698877951879505, 0.9741834252431748, 0.9652685495288231], "final_y": [0.16643515322415792, 0.16542842526121881, 0.16527206909784353]}, "mutation_prompt": null}
{"id": "7c71bc26-30bc-49d6-9f6e-cea5cd43dce6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)\n            freq_modulation = 0.15 + 0.1 * np.cos(self.used_evaluations * np.pi / (self.budget / 3.0))  \n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial) * freq_modulation, lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance periodicity influence by adjusting the elite selection mechanism to incorporate frequency modulation.", "configspace": "", "generation": 49, "fitness": 0.9606076121298432, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.961 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9527879564261944, 0.9644050748210712, 0.964629805142264], "final_y": [0.1649145862617708, 0.16491795185194513, 0.164915057490151]}, "mutation_prompt": null}
{"id": "1df99ce0-72f4-4878-a93a-4166f04bc21e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim) + int(5 * np.sin(self.used_evaluations * np.pi / self.budget)), 50 * self.dim)  # Adjust population size with periodic component\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce periodic adjustments to adaptive population size to enhance search space exploration and exploitation balance.", "configspace": "", "generation": 50, "fitness": 0.9715952658160573, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.003. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9722155028751148, 0.9744233609966974, 0.9681469335763594], "final_y": [0.16580850058980812, 0.16541575123588415, 0.16535664112893567]}, "mutation_prompt": null}
{"id": "0e9f3688-9572-4f65-a5fa-4827cca88338", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.cos(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity (changed line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance dynamic scaling by incorporating a cosine function for better exploration and convergence.", "configspace": "", "generation": 51, "fitness": 0.96015225568318, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.960 with standard deviation 0.002. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9587035484894681, 0.9582822584192342, 0.9634709601408377], "final_y": [0.16897368402570145, 0.1668248918305978, 0.16645079564779264]}, "mutation_prompt": null}
{"id": "654ce95b-74aa-4d62-a2ad-80e651e48952", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        fitness_variance = np.var(fitness)  # Added line\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor) * (1 + fitness_variance)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity (changed line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce dynamic mutation based on fitness variance to enhance exploration-exploitation balance.", "configspace": "", "generation": 52, "fitness": 0.9694206383127654, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.006. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9707477417885864, 0.9766262843051489, 0.9608878888445609], "final_y": [0.1663441031100752, 0.16500226715491995, 0.1668493452496137]}, "mutation_prompt": null}
{"id": "294d7c27-b7d6-4fee-95dc-803ff644bcd6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  \n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.4 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjusted crossover modulation (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.15 + 0.25 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Adjusted elite influence (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance adaptive DE by refining elite influence and crossover probability modulation to improve convergence.", "configspace": "", "generation": 53, "fitness": 0.9686507206279723, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.003. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9726884907592379, 0.9659937232855208, 0.9672699478391584], "final_y": [0.16567648047082362, 0.16659478886167411, 0.16725704148364529]}, "mutation_prompt": null}
{"id": "04406c2f-537c-42f2-ade4-a701b1ed2b04", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.35 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (slightly changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity (changed line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Slightly adjusted scale factor dynamics in DE to enhance convergence speed and accuracy.", "configspace": "", "generation": 54, "fitness": 0.9672078784107582, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.967 with standard deviation 0.008. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.972528684315423, 0.9732648051973031, 0.9558301457195484], "final_y": [0.16547860349622356, 0.1654744611128517, 0.16832629916001707]}, "mutation_prompt": null}
{"id": "4c24ebb4-de72-4848-9cd6-252516e41eaa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, ub = bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8)) + 0.05 * np.sin(self.used_evaluations * np.pi / self.budget)  # Additional sine-based dynamic scale (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity (changed line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce an additional sine-based dynamic scale to the crossover probability to enhance adaptability and convergence.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('too many values to unpack (expected 2)').", "error": "ValueError('too many values to unpack (expected 2)')", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {}, "mutation_prompt": null}
{"id": "15fd0f98-ad12-4ba2-8f6a-bb44fab08850", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.cos(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.sin(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Refine dynamic scale factor and crossover probability using cosine influence to enhance exploration and exploitation balance.", "configspace": "", "generation": 56, "fitness": 0.9643961924889052, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.008. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.954459298690006, 0.9657514920136464, 0.9729777867630631], "final_y": [0.16868243182769505, 0.16530331956856237, 0.16576224101196368]}, "mutation_prompt": null}
{"id": "6909e62a-4ea2-4108-baf5-98cceecae7f5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.2 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity (changed line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance self-adaptation by increasing elite influence modulation for improved balance between exploration and exploitation.", "configspace": "", "generation": 57, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'ub' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'ub' referenced before assignment\")", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {}, "mutation_prompt": null}
{"id": "ad5fef79-611d-4fdc-86bc-ef6f2c9756a3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.3 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor (changed line)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy (changed line)\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence (changed line)\n            randomness_factor = np.random.uniform(-0.05, 0.05, size=self.dim)  # Introduce randomness (changed line)\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget) + randomness_factor  # Encourage periodicity (changed line)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance solution diversity by introducing controlled randomness in the trial vector generation process.", "configspace": "", "generation": 58, "fitness": 0.9561924530166798, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.956 with standard deviation 0.012. And the mean value of best solutions found was 0.169 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9408397154766721, 0.9710365053385592, 0.9567011382348078], "final_y": [0.17320699369040238, 0.16526928252841488, 0.16791163716662083]}, "mutation_prompt": null}
{"id": "a8eeda96-7e70-4eb9-957a-536522ce427b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce adaptive periodic scaling to enhance exploration and exploitation balance in Differential Evolution.", "configspace": "", "generation": 59, "fitness": 0.9720412148060514, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "53cbb81b-5d8a-4d0b-893e-0c4668cdc9e5", "metadata": {"aucs": [0.9756777866504398, 0.9732068683261221, 0.9672389894415923], "final_y": [0.16554087821189978, 0.16502061536479762, 0.16497719082418627]}, "mutation_prompt": null}
{"id": "0006e946-2136-45ec-badf-ceb415c201af", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n            self.scale_f = self.scale_f * 0.98 if current_fitness >= best_fitness else self.scale_f  # Dynamic step size adjustment based on progress\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce dynamic step size modulation based on progress to improve Adaptive DE performance.", "configspace": "", "generation": 60, "fitness": 0.9720412148060514, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a8eeda96-7e70-4eb9-957a-536522ce427b", "metadata": {"aucs": [0.9756777866504398, 0.9732068683261221, 0.9672389894415923], "final_y": [0.16554087821189978, 0.16502061536479762, 0.16497719082418627]}, "mutation_prompt": null}
{"id": "541c5603-0622-477d-a2a1-adb86282b404", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.2 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.1 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            alpha = 0.5 + 0.5 * np.cos(self.used_evaluations * np.pi / self.budget)  # New adaptive weighting factor\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial) * alpha, lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce adaptive weighting in the trial vector influence to enhance convergence in Differential Evolution.", "configspace": "", "generation": 61, "fitness": 0.9698117548818436, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.970 with standard deviation 0.003. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "a8eeda96-7e70-4eb9-957a-536522ce427b", "metadata": {"aucs": [0.9709012011223415, 0.9732317702652682, 0.9653022932579208], "final_y": [0.16680345743986946, 0.1649230490881607, 0.16530114647448657]}, "mutation_prompt": null}
{"id": "d4d32a29-b00d-4447-8187-a2717bafed42", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.8))  # Adjust crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce dynamic weighting in elite influence and periodic factor adjustments in AdaptiveDE for enhanced solution quality.  ", "configspace": "", "generation": 62, "fitness": 0.9740898289626544, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a8eeda96-7e70-4eb9-957a-536522ce427b", "metadata": {"aucs": [0.9774183504445574, 0.9751478341753711, 0.9697033022680347], "final_y": [0.16527632071737441, 0.16512756365674486, 0.16590222914089614]}, "mutation_prompt": null}
{"id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance crossover strategy by introducing a more aggressive cosine adjustment to increase exploration capabilities.", "configspace": "", "generation": 63, "fitness": 0.9744375605966725, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d4d32a29-b00d-4447-8187-a2717bafed42", "metadata": {"aucs": [0.9797432143898327, 0.9723962872525214, 0.9711731801476635], "final_y": [0.16513487490893652, 0.16562838974258098, 0.16507286349295547]}, "mutation_prompt": null}
{"id": "f7796ca4-5e04-4061-b51e-6347430aca88", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.cos(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce a more dynamic elite influence factor based on cosine adjustment to encourage better convergence.", "configspace": "", "generation": 64, "fitness": 0.9247884220456243, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.925 with standard deviation 0.024. And the mean value of best solutions found was 0.181 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.8909066044672533, 0.9458605717899782, 0.9375980898796417], "final_y": [0.1916171430909872, 0.17322065463384628, 0.1793037806082891]}, "mutation_prompt": null}
{"id": "32d1b031-0593-41e4-8b3e-ca8a2acc4f67", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            best_fitness = np.min(fitness)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5)) * (1 + best_fitness)  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Improve exploration by dynamically adjusting the crossover probability based on best fitness evolution, encouraging diverse solutions.", "configspace": "", "generation": 65, "fitness": 0.96418831800146, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.008. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9707656346289752, 0.9684974024928406, 0.9533019168825643], "final_y": [0.16721120108730136, 0.16633270836707126, 0.16882821838233997]}, "mutation_prompt": null}
{"id": "df1cc5ec-8719-4b14-8cf9-08312adc16cb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.sin(self.used_evaluations * np.pi / (self.budget / 2))  # Adjust crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Refine adaptive crossover probability to enhance exploration with a dynamic sine-based adjustment.", "configspace": "", "generation": 66, "fitness": 0.9652145170533469, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.948894403685002, 0.9695811109731614, 0.9771680365018777], "final_y": [0.16750642325310516, 0.1656819475714537, 0.1650652756551786]}, "mutation_prompt": null}
{"id": "b92a26a0-3cd7-47f1-8e9b-155a00a03c94", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial) * 1.05, lb, ub)  # Increase influence\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Slightly increase the influence of the best solution to enhance convergence speed without compromising exploration.", "configspace": "", "generation": 67, "fitness": 0.9732157165940064, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.973 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9789528384925953, 0.9695850691741803, 0.9711092421152435], "final_y": [0.16544020488175226, 0.16625748662598472, 0.16543390016352622]}, "mutation_prompt": null}
{"id": "04d21e39-fdbe-42f2-9219-55a21c58c298", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce adaptive crossover probability influenced by a sinusoidal function to improve solution diversity.", "configspace": "", "generation": 68, "fitness": 0.9716139094618117, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9732348013784978, 0.9749318502175579, 0.9666750767893794], "final_y": [0.16575246945332645, 0.16544574781231813, 0.16722036102424953]}, "mutation_prompt": null}
{"id": "0a5b0c62-dfcf-4bb5-9657-ab7fb1d93e03", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.15 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Improve the exploitative capabilities by refining the elite influence factor to better leverage the best solutions found during optimization.", "configspace": "", "generation": 69, "fitness": 0.9735730223156938, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.975053745377096, 0.9769667490846927, 0.9686985724852925], "final_y": [0.16684950869154558, 0.16534875902505275, 0.16722012951249]}, "mutation_prompt": null}
{"id": "0db686f0-f502-4fd1-838d-fc2c3a6cac7c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.45))  # Adjusted from 1.5 to 1.45\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Slightly adjust the dynamic periodicity factor to fine-tune exploration and exploitation balance for better convergence.", "configspace": "", "generation": 70, "fitness": 0.967886132906972, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.003. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9710328218412622, 0.9645845944977062, 0.9680409823819474], "final_y": [0.16712403213756477, 0.16600169748389437, 0.16641913715333534]}, "mutation_prompt": null}
{"id": "e167ccca-c056-4993-aa87-b7b795bef4b4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c) * dynamic_periodicity_factor, lb, ub)  # Dynamic mutation strategy\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce a dynamic mutation strategy by incorporating a periodic adjustment to enhance solution diversity.", "configspace": "", "generation": 71, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'ub' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'ub' referenced before assignment\")", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {}, "mutation_prompt": null}
{"id": "2871a467-5230-424a-a8b6-8f37fba765c5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 2))  # Dynamic crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce a dynamic crossover probability adjustment to enhance exploration and convergence.", "configspace": "", "generation": 72, "fitness": 0.9736552219446528, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9773155003230282, 0.9747834079904916, 0.9688667575204384], "final_y": [0.16599952175004684, 0.1651693967294393, 0.16597346802258817]}, "mutation_prompt": null}
{"id": "0c36f848-1339-4357-a17e-f66bc88bb8db", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * (0.15 + 0.05 * np.cos(self.used_evaluations * np.pi / (self.budget / 2)))  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Incorporate enhanced periodicity influence by dynamically adjusting the elite influence factor based on evaluation progress to refine solution quality.", "configspace": "", "generation": 73, "fitness": 0.9671436762094815, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.967 with standard deviation 0.006. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9611809669733923, 0.9747959136287438, 0.9654541480263084], "final_y": [0.16976721426637253, 0.16648419075259568, 0.1684305829032542]}, "mutation_prompt": null}
{"id": "72c5e801-96b7-4008-a93b-96cef1743e4c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.55 + 0.35 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.3))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Adjust periodic influence and crossover to enhance convergence speed and solution quality.", "configspace": "", "generation": 74, "fitness": 0.9659597570052707, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.003. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9694104546859054, 0.9650240275826929, 0.9634447887472137], "final_y": [0.16675830975139938, 0.1661709160243584, 0.16730306037369613]}, "mutation_prompt": null}
{"id": "cbdca002-cbf6-43d9-a598-8e906db35a3d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce a dynamic per-individual mutation rate to enhance diversity and adaptability in the search process.", "configspace": "", "generation": 75, "fitness": 0.9744375605966725, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9797432143898327, 0.9723962872525214, 0.9711731801476635], "final_y": [0.16513487490893652, 0.16562838974258098, 0.16507286349295547]}, "mutation_prompt": null}
{"id": "ea22cfb9-ed06-44d2-9c56-4894524aca32", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.7))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Improve search efficiency by introducing an adaptive threshold for the crossover probability.", "configspace": "", "generation": 76, "fitness": 0.9739872974684575, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9786089183096506, 0.9722906247596036, 0.9710623493361179], "final_y": [0.16536183767297197, 0.16561003237236327, 0.16526578902120503]}, "mutation_prompt": null}
{"id": "d40b6bf6-45e2-4714-93a2-70435c3f0413", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.25 * np.cos(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce a dynamic scaling factor influenced by a cosine function to enhance exploration and fine-tuning.", "configspace": "", "generation": 77, "fitness": 0.9706612629525257, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.003. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.966649120082048, 0.9750127461822227, 0.9703219225933067], "final_y": [0.16664578144147002, 0.16517787367289183, 0.16629570434412144]}, "mutation_prompt": null}
{"id": "21c1d7f3-b800-4708-b9d9-f396b0656057", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.6 + 0.35 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # Fine-tuned crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.2 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Enhanced elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Refine crossover probability and strengthen periodicity influence for enhanced exploration and convergence.", "configspace": "", "generation": 78, "fitness": 0.9707458032106349, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.003. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9740308052580127, 0.967562906445468, 0.9706436979284239], "final_y": [0.16779687715553104, 0.16832848676259793, 0.1667726571298218]}, "mutation_prompt": null}
{"id": "05595d3e-dbe0-4664-af56-91f2dda4f4f2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * ((b - c) + 0.1 * np.random.randn(self.dim)), lb, ub)  # Added random perturbation\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Incorporate a random perturbation in the mutation step to enhance exploration around locally promising solutions.", "configspace": "", "generation": 79, "fitness": 0.9641401823057177, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.002. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9610517742185529, 0.9659515671500202, 0.9654172055485802], "final_y": [0.16842569123475937, 0.16721866326098678, 0.16854682005330368]}, "mutation_prompt": null}
{"id": "e412b598-bfb7-437e-8f93-80cf7d9e1685", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * (np.cos(self.used_evaluations * np.pi / (self.budget / 1.5)) ** 2)  # Quadratic increment\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce a quadratic increment in the crossover probability to enhance exploration effectiveness.", "configspace": "", "generation": 80, "fitness": 0.9736680813373914, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9790691806103715, 0.9729324688819936, 0.9690025945198091], "final_y": [0.16537060975588413, 0.16555228852309023, 0.16619034937367183]}, "mutation_prompt": null}
{"id": "2ac8fe8d-da92-4eb9-9305-0f22ed44987b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance exploration by adjusting the mutation strategy with a dynamic scale factor influenced by a cosine wave.", "configspace": "", "generation": 81, "fitness": 0.965705751567327, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.006. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9724030643339546, 0.9667260770446707, 0.9579881133233557], "final_y": [0.16713379868653377, 0.16666096683460285, 0.1662871326401223]}, "mutation_prompt": null}
{"id": "33f788c2-c868-4d03-9cdf-598c54076802", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 2))  # Adjusted crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce a dynamic crossover probability to adaptively enhance exploration and exploitation balance.", "configspace": "", "generation": 82, "fitness": 0.9736552219446528, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9773155003230282, 0.9747834079904916, 0.9688667575204384], "final_y": [0.16599952175004684, 0.1651693967294393, 0.16597346802258817]}, "mutation_prompt": null}
{"id": "275d3233-4f5d-4296-aae7-ded5ef13da98", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial += 0.01 * np.random.randn(*trial.shape)  # Random perturbation for escaping local minima\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance the differential evolution by allowing a small, random perturbation in the trial vector to potentially escape local minima.", "configspace": "", "generation": 83, "fitness": 0.9604356190207709, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.960 with standard deviation 0.002. And the mean value of best solutions found was 0.169 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9571999958312847, 0.9624134581427038, 0.9616934030883241], "final_y": [0.1703634197145486, 0.16766149153737586, 0.1674886134377095]}, "mutation_prompt": null}
{"id": "ed98dec7-7537-45c1-b026-6321dc3729d1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.cos(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor with cosine\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance exploration by dynamically adjusting the scale factor with a cosine adjustment for better convergence.", "configspace": "", "generation": 84, "fitness": 0.9650893644983007, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.003. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9604738699476929, 0.9683450990157768, 0.9664491245314325], "final_y": [0.1686990923645587, 0.16582938603838904, 0.16614484928590945]}, "mutation_prompt": null}
{"id": "3c2e9e93-7f22-4344-8449-81b338d9fe6f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        diversity_factor = 0.5 + 0.5 * np.std(population) / (ub - lb).mean()  # New line: dynamic weighting factor\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor) * diversity_factor  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance mutation strategy by introducing a dynamic weighting factor based on population diversity to improve exploration.", "configspace": "", "generation": 85, "fitness": 0.9717706623908539, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.002. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9712600249160052, 0.9743404542257557, 0.9697115080308006], "final_y": [0.16557589418849827, 0.16521686449061146, 0.16624685221276103]}, "mutation_prompt": null}
{"id": "9c97485f-6005-466c-8553-ec7b9d4f5429", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor * np.sin(self.used_evaluations * np.pi / (self.budget / 2))  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduced a dynamic sine adjustment to the elite influence factor for enhanced solution refinement.", "configspace": "", "generation": 86, "fitness": 0.9737075772396452, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9752808087381433, 0.9758719144572682, 0.969970008523524], "final_y": [0.1656882023966595, 0.16509421320171247, 0.16554344204712368]}, "mutation_prompt": null}
{"id": "02b5ca49-bb8b-450b-bfc1-b861f3424c99", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial) + 0.05 * np.random.randn(self.dim), lb, ub)  # Stochastic influence\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce a stochastic influence factor to enhance exploration, maintaining balance with existing aggressive strategies.", "configspace": "", "generation": 87, "fitness": 0.959123175797023, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.006. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9506841322000787, 0.9633816015903479, 0.9633037936006422], "final_y": [0.17017855456847386, 0.1671903176316064, 0.16708920808365935]}, "mutation_prompt": null}
{"id": "bc65158e-5c82-412c-9249-1f6442358856", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.random.rand() * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce stochastic cosine adjustment to crossover probability for enhanced exploration.", "configspace": "", "generation": 88, "fitness": 0.9685201032187755, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.004. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9688644947082947, 0.9628774012502217, 0.9738184136978099], "final_y": [0.1673072294835668, 0.1671903172657918, 0.16632365420066064]}, "mutation_prompt": null}
{"id": "d48d86dd-4274-4401-bfac-c63f51218749", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.15 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance periodic influence by adjusting the elite influence weighting to better explore promising regions.  ", "configspace": "", "generation": 89, "fitness": 0.9735730223156938, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.975053745377096, 0.9769667490846927, 0.9686985724852925], "final_y": [0.16684950869154558, 0.16534875902505275, 0.16722012951249]}, "mutation_prompt": null}
{"id": "2204e9a7-9a63-4b9d-865f-360e93306b61", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))  # More aggressive crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor  # Dynamic elite influence\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)  # Encourage periodicity\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance crossover strategy by introducing a more aggressive cosine adjustment to increase exploration capabilities.", "configspace": "", "generation": 64, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9797432143898327, 0.9723962872525214, 0.9711731801476635], "final_y": [0.16513487490893652, 0.16562838974258098, 0.16507286349295547]}, "mutation_prompt": null}
{"id": "c3a3ab0d-0e07-4d88-a6da-cd1ebe451f75", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.5 * (1 + np.tanh(self.used_evaluations / (self.budget / 2)))  # Adjust scale factor\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.4 * np.tanh(self.used_evaluations / (self.budget / 2))  # Adjust crossover strategy\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / self.budget) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget) \n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Improve exploration by dynamically adjusting the scale and crossover rates using a hyperbolic tangent function for better diversity control.", "configspace": "", "generation": 91, "fitness": 0.9662093097912425, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9693783610176716, 0.961092492849136, 0.9681570755069202], "final_y": [0.16556247798887735, 0.16692958237148225, 0.16591684224603587]}, "mutation_prompt": null}
{"id": "c0529225-9391-4819-ab1b-eaf44b2c906c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Refine dynamic elite influence to increase convergence speed by altering its periodic adjustment.", "configspace": "", "generation": 92, "fitness": 0.9762330190677576, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.003. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "aa6c6334-6959-44cb-9d7e-07127d5b8fd9", "metadata": {"aucs": [0.9719286375526207, 0.979978506041548, 0.9767919136091039], "final_y": [0.1656053735222629, 0.1650824384175963, 0.16583578953325695]}, "mutation_prompt": null}
{"id": "8a7ebd25-0aa8-4b3f-a579-6d5ac7d6986e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.25 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance mutation strategy with adaptive scaling to boost convergence by fine-tuning influence factors.", "configspace": "", "generation": 93, "fitness": 0.9713374684824753, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.004. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c0529225-9391-4819-ab1b-eaf44b2c906c", "metadata": {"aucs": [0.9663060028638183, 0.9735677585193634, 0.9741386440642441], "final_y": [0.16555300818981444, 0.16553306329710848, 0.1661033492330145]}, "mutation_prompt": null}
{"id": "66e4e586-e1c7-4fe2-b1ab-1614b7232e6d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance fine-tuning of the best solution by increasing the influence of periodicity in the trial solution adjustment.", "configspace": "", "generation": 94, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'ub' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'ub' referenced before assignment\")", "parent_id": "c0529225-9391-4819-ab1b-eaf44b2c906c", "metadata": {}, "mutation_prompt": null}
{"id": "b45a1fdd-ddfb-4dc9-b355-7361231d8dfc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Refine dynamic elite influence to increase convergence speed by altering its periodic adjustment.", "configspace": "", "generation": 93, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "c0529225-9391-4819-ab1b-eaf44b2c906c", "metadata": {"aucs": [0.9719286375526207, 0.979978506041548, 0.9767919136091039], "final_y": [0.1656053735222629, 0.1650824384175963, 0.16583578953325695]}, "mutation_prompt": null}
{"id": "b2b1f6a7-7522-47e4-b31b-d86878007168", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * np.cos(self.used_evaluations * np.pi / self.budget) * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Enhance the elite influence by introducing cosine modulation to further refine periodic adjustments.", "configspace": "", "generation": 96, "fitness": 0.9762541146381238, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c0529225-9391-4819-ab1b-eaf44b2c906c", "metadata": {"aucs": [0.973602621643473, 0.9786057402299533, 0.9765539820409451], "final_y": [0.1649889284958388, 0.1653198403563121, 0.16556712717645194]}, "mutation_prompt": null}
{"id": "753887a7-b3d6-4f28-8e45-2b748a2cf003", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / (self.budget / 1.5))\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * np.cos(self.used_evaluations * np.pi / self.budget + np.pi/4) * (self._select_best(population, fitness)[0] - trial), lb, ub)  # Phase shift of pi/4 added\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce phase shift modulation in the elite influence term to better harness constructive interference.", "configspace": "", "generation": 97, "fitness": 0.9669859172876901, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.967 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b2b1f6a7-7522-47e4-b31b-d86878007168", "metadata": {"aucs": [0.9644941038647021, 0.9709169656856155, 0.9655466823127528], "final_y": [0.16495953196600233, 0.16499640118471748, 0.1652685983429255]}, "mutation_prompt": null}
{"id": "ab93dab6-ce53-4fa7-b305-1fec95c42866", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.cos(self.used_evaluations * np.pi / dynamic_periodicity_factor)  # Change made here\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * np.cos(self.used_evaluations * np.pi / self.budget) * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce dynamic crossover probability modulation to enhance exploration and exploitation balance.", "configspace": "", "generation": 98, "fitness": 0.9721475603027692, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.003. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b2b1f6a7-7522-47e4-b31b-d86878007168", "metadata": {"aucs": [0.9696626130670064, 0.9759287399373734, 0.9708513279039277], "final_y": [0.16555480135009237, 0.1654009123691399, 0.16587193882827678]}, "mutation_prompt": null}
{"id": "6fb8f4ec-1e82-4daf-907a-7bcd685c7ca4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithPeriodicity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(10, 5 * dim)\n        self.scale_f = 0.5\n        self.crossover_prob = 0.7\n        self.used_evaluations = 0\n\n    def _adaptive_population_size(self):\n        return min(self.initial_population_size + self.used_evaluations // (5 * self.dim), 50 * self.dim)\n    \n    def _initialize_population(self, bounds, pop_size):\n        lb, ub = bounds.lb, bounds.ub\n        population = lb + (ub - lb) * np.random.rand(pop_size, self.dim)\n        return population\n\n    def _evaluate_population(self, func, population):\n        fitness = np.zeros(len(population))\n        for i in range(len(population)):\n            if self.used_evaluations < self.budget:\n                fitness[i] = func(population[i])\n                self.used_evaluations += 1\n        return fitness\n\n    def _select_best(self, population, fitness):\n        min_idx = np.argmin(fitness)\n        return population[min_idx], fitness[min_idx]\n\n    def _differential_evolution_step(self, func, population, fitness, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        new_population = np.copy(population)\n        dynamic_periodicity_factor = 1 + np.sin(self.used_evaluations * np.pi / (self.budget / 1.5))\n        for i in range(len(population)):\n            if self.used_evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            self.scale_f = 0.4 + 0.20 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor)\n            mutant = np.clip(a + self.scale_f * (b - c), lb, ub)\n            self.crossover_prob = 0.5 + 0.45 * np.sin(self.used_evaluations * np.pi / (self.budget / 2))  # Change here\n            cross_points = np.random.rand(self.dim) < self.crossover_prob\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            elite_influence = 0.1 + 0.3 * np.sin(self.used_evaluations * np.pi / dynamic_periodicity_factor) * dynamic_periodicity_factor\n            trial = trial + (np.mean(population) - trial) * 0.15 * np.cos(self.used_evaluations * np.pi / self.budget)\n            trial = np.clip(trial + elite_influence * np.cos(self.used_evaluations * np.pi / self.budget) * (self._select_best(population, fitness)[0] - trial), lb, ub)\n            if self.used_evaluations < self.budget:\n                trial_fitness = func(trial)\n                self.used_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n        elite_idx = np.argmin(fitness)\n        new_population[0] = population[elite_idx]\n        return new_population, fitness\n\n    def _local_fine_tuning(self, best_solution, func, bounds):\n        if self.used_evaluations >= self.budget:\n            return best_solution\n        result = minimize(func, best_solution, bounds=list(zip(bounds.lb, bounds.ub)), \n                          method='L-BFGS-B', options={'maxfun': self.budget - self.used_evaluations})\n        self.used_evaluations += result.nfev\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self._adaptive_population_size()\n        population = self._initialize_population(bounds, population_size)\n        fitness = self._evaluate_population(func, population)\n        best_solution, best_fitness = self._select_best(population, fitness)\n\n        while self.used_evaluations < self.budget:\n            population_size = self._adaptive_population_size()\n            population, fitness = self._differential_evolution_step(func, population, fitness, bounds)\n            current_best, current_fitness = self._select_best(population, fitness)\n            if current_fitness < best_fitness:\n                best_solution, best_fitness = current_best, current_fitness\n\n        best_solution = self._local_fine_tuning(best_solution, func, bounds)\n\n        return best_solution", "name": "AdaptiveDEWithPeriodicity", "description": "Introduce a sinusoidal modulation for crossover probability to enhance balance between exploration and exploitation.", "configspace": "", "generation": 99, "fitness": 0.9707667963704406, "feedback": "The algorithm AdaptiveDEWithPeriodicity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.002. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b2b1f6a7-7522-47e4-b31b-d86878007168", "metadata": {"aucs": [0.9693391698416604, 0.974261771859583, 0.9686994474100785], "final_y": [0.16625041224685633, 0.16546993535828092, 0.16639069297430442]}, "mutation_prompt": null}
