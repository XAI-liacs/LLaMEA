{"id": "f946158f-1125-4e58-ba64-19f04f3392f1", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 10)\n        self.w = 0.5  # inertia weight\n        self.c1 = 1.5  # cognitive (particle) weight\n        self.c2 = 1.5  # social (swarm) weight\n        self.temperature_initial = 1000\n        self.temperature_final = 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        personal_best_positions = particles.copy()\n        personal_best_scores = np.array([func(p) for p in particles])\n        global_best_position = personal_best_positions[personal_best_scores.argmin()]\n        \n        evaluations = self.num_particles\n        global_best_score = personal_best_scores.min()\n        \n        while evaluations < self.budget:\n            temperature = self.temperature_initial * ((self.temperature_final / self.temperature_initial) ** (evaluations / self.budget))\n            \n            for i, particle in enumerate(particles):\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * np.random.rand(self.dim) * (personal_best_positions[i] - particle) +\n                                 self.c2 * np.random.rand(self.dim) * (global_best_position - particle))\n                particles[i] = np.clip(particles[i] + velocities[i], lb, ub)\n                \n                current_score = func(particles[i])\n                evaluations += 1\n                \n                if current_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particles[i].copy()\n                    personal_best_scores[i] = current_score\n                    \n                if current_score < global_best_score:\n                    global_best_position = particles[i].copy()\n                    global_best_score = current_score\n                else:\n                    acceptance_prob = np.exp((global_best_score - current_score) / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        global_best_position = particles[i].copy()\n                        global_best_score = current_score\n                    \n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer", "description": "This novel algorithm combines principles of Particle Swarm Optimization (PSO) with Simulated Annealing (SA) to efficiently explore and exploit the search space, balancing local and global search strategies based on a dynamically decreasing temperature parameter.", "configspace": "", "generation": 0, "fitness": 0.6308401255774877, "feedback": "The algorithm PSO_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.631 with standard deviation 0.054. And the mean value of best solutions found was 0.320 (0. is the best) with standard deviation 0.031.", "error": "", "parent_id": null, "metadata": {"aucs": [0.6184268255122638, 0.5713816891191457, 0.7027118621010535], "final_y": [0.32661721458071935, 0.3544738716811261, 0.2797513760939141]}, "mutation_prompt": null}
{"id": "406325f6-c858-4526-bb21-8bd16f6dc83b", "solution": "import numpy as np\n\nclass DE_Levy_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.F = 0.8  # differential weight\n        self.CR = 0.9  # crossover probability\n        self.alpha = 0.5  # control parameter for Levy flight\n\n    def levy_flight(self, dim, beta=1.5):\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma_u, size=dim)\n        v = np.random.normal(0, 1, size=dim)\n        step = u / np.abs(v) ** (1 / beta)\n        return self.alpha * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n        \n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n        \n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices[0]], population[indices[1]], population[indices[2]]\n                \n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_score = func(trial)\n                evaluations += 1\n                \n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < global_best_score:\n                        global_best_position = trial.copy()\n                        global_best_score = trial_score\n                \n                # Perform Levy Flight\n                if evaluations < self.budget and np.random.rand() < 0.1:\n                    levy_step = self.levy_flight(self.dim)\n                    levied_position = np.clip(global_best_position + levy_step, lb, ub)\n                    levied_score = func(levied_position)\n                    evaluations += 1\n                    if levied_score < global_best_score:\n                        global_best_position = levied_position.copy()\n                        global_best_score = levied_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "DE_Levy_Optimizer", "description": "This new algorithm integrates Differential Evolution (DE) with a dynamic Levy Flight strategy to balance exploration and exploitation, providing robustness in diverse search landscapes through adaptive mutation and random walks.", "configspace": "", "generation": 1, "fitness": 0.8449766416861495, "feedback": "The algorithm DE_Levy_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.006. And the mean value of best solutions found was 0.186 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "f946158f-1125-4e58-ba64-19f04f3392f1", "metadata": {"aucs": [0.8493507547019656, 0.848512158450341, 0.8370670119061419], "final_y": [0.20156847681341594, 0.18433183567785283, 0.17065017621151202]}, "mutation_prompt": null}
{"id": "483f71dd-81a7-4446-a013-4167dc99c2cd", "solution": "import numpy as np\n\nclass DE_Levy_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.F = 0.8  # differential weight\n        self.CR = 0.9  # crossover probability\n        self.alpha = 0.5  # control parameter for Levy flight\n\n    def levy_flight(self, dim, beta=1.5):\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma_u, size=dim)\n        v = np.random.normal(0, 1, size=dim)\n        step = u / np.abs(v) ** (1 / beta)\n        return self.alpha * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n        \n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n        \n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices[0]], population[indices[1]], population[indices[2]]\n                \n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR * (1 - np.std(scores) / np.mean(scores))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_score = func(trial)\n                evaluations += 1\n                \n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < global_best_score:\n                        global_best_position = trial.copy()\n                        global_best_score = trial_score\n                \n                # Perform Levy Flight\n                if evaluations < self.budget and np.random.rand() < 0.1:\n                    levy_step = self.levy_flight(self.dim)\n                    levied_position = np.clip(global_best_position + levy_step, lb, ub)\n                    levied_score = func(levied_position)\n                    evaluations += 1\n                    if levied_score < global_best_score:\n                        global_best_position = levied_position.copy()\n                        global_best_score = levied_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "DE_Levy_Optimizer", "description": "This algorithm refines DE_Levy_Optimizer by enhancing the crossover operation through dynamic crossover probability adjustment based on population diversity to boost exploration capabilities.", "configspace": "", "generation": 2, "fitness": 0.8734241710860404, "feedback": "The algorithm DE_Levy_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.042. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "406325f6-c858-4526-bb21-8bd16f6dc83b", "metadata": {"aucs": [0.8393547798719211, 0.848512158450341, 0.932405574935859], "final_y": [0.1680263273376721, 0.18433183567785283, 0.1658112434061284]}, "mutation_prompt": null}
{"id": "126eee29-0a79-4d38-9633-bfd8c929e61d", "solution": "import numpy as np\n\nclass Quantum_Inspired_Genetic_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.alpha = 0.5  # quantum rotation angle\n        self.beta = 0.1   # mutation probability\n\n    def quantum_rotation(self, individual, best, worst):\n        rotation = np.random.uniform(-self.alpha, self.alpha, size=self.dim)\n        new_position = individual + rotation * (best - worst)\n        return new_position\n\n    def mutate(self, individual, lb, ub):\n        mutation = np.random.rand(self.dim) < self.beta\n        noise = np.random.uniform(lb, ub, size=self.dim)\n        return np.where(mutation, noise, individual)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            best_idx = scores.argmin()\n            worst_idx = scores.argmax()\n            best_individual = population[best_idx]\n            worst_individual = population[worst_idx]\n\n            for i in range(self.population_size):\n                new_position = self.quantum_rotation(population[i], best_individual, worst_individual)\n                new_position = np.clip(new_position, lb, ub)\n                new_position = self.mutate(new_position, lb, ub)\n\n                new_score = func(new_position)\n                evaluations += 1\n\n                if new_score < scores[i]:\n                    population[i] = new_position\n                    scores[i] = new_score\n                    if new_score < global_best_score:\n                        global_best_position = new_position.copy()\n                        global_best_score = new_score\n                \n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "Quantum_Inspired_Genetic_Optimizer", "description": "The Quantum-Inspired Genetic Optimizer incorporates quantum superposition principles to explore multiple solutions simultaneously, enhancing diversity and convergence in solving black box optimization problems.", "configspace": "", "generation": 3, "fitness": 0.7523962601239246, "feedback": "The algorithm Quantum_Inspired_Genetic_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.752 with standard deviation 0.028. And the mean value of best solutions found was 0.211 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "483f71dd-81a7-4446-a013-4167dc99c2cd", "metadata": {"aucs": [0.7207508936739944, 0.7898073117351214, 0.7466305749626577], "final_y": [0.21342659479050685, 0.1886762545239079, 0.23063476693271512]}, "mutation_prompt": null}
{"id": "4101d62d-0269-4cdd-9fa0-209b9ff83cf6", "solution": "import numpy as np\n\nclass DE_Levy_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = min(50, budget // 10)\n        self.population_size = self.initial_population_size\n        self.F = 0.8  # differential weight\n        self.CR = 0.9  # crossover probability\n        self.alpha = 0.5  # control parameter for Levy flight\n\n    def levy_flight(self, dim, beta=1.5):\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma_u, size=dim)\n        v = np.random.normal(0, 1, size=dim)\n        step = u / np.abs(v) ** (1 / beta)\n        return self.alpha * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n        \n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n        \n        while evaluations < self.budget:\n            if evaluations % 100 == 0 and self.population_size > 5:  # Reduce population size adaptively\n                self.population_size -= 1\n                population = population[:self.population_size]\n                scores = scores[:self.population_size]\n\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices[0]], population[indices[1]], population[indices[2]]\n                \n                adaptive_F = self.F * (1 - evaluations / self.budget)  # Adaptive mutation scaling\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR * (1 - np.std(scores) / np.mean(scores))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_score = func(trial)\n                evaluations += 1\n                \n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < global_best_score:\n                        global_best_position = trial.copy()\n                        global_best_score = trial_score\n                \n                # Perform Levy Flight\n                if evaluations < self.budget and np.random.rand() < 0.1:\n                    levy_step = self.levy_flight(self.dim)\n                    levied_position = np.clip(global_best_position + levy_step, lb, ub)\n                    levied_score = func(levied_position)\n                    evaluations += 1\n                    if levied_score < global_best_score:\n                        global_best_position = levied_position.copy()\n                        global_best_score = levied_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "DE_Levy_Optimizer", "description": "This algorithm integrates adaptive mutation scaling and dynamic population resizing in DE_Levy_Optimizer to enhance exploitation and exploration balance.", "configspace": "", "generation": 4, "fitness": 0.8503218316338632, "feedback": "The algorithm DE_Levy_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.850 with standard deviation 0.075. And the mean value of best solutions found was 0.169 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "483f71dd-81a7-4446-a013-4167dc99c2cd", "metadata": {"aucs": [0.7531688276964053, 0.8620992146550477, 0.9356974525501369], "final_y": [0.1768175044357434, 0.1652821919281875, 0.16522683328453347]}, "mutation_prompt": null}
{"id": "dcd0d2ad-6889-493a-be0e-f45127e1a82c", "solution": "import numpy as np\n\nclass DE_Levy_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.F = 0.8  # differential weight\n        self.CR = 0.9  # crossover probability\n        self.alpha = 0.5  # control parameter for Levy flight\n\n    def levy_flight(self, dim, beta=1.5):\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma_u, size=dim)\n        v = np.random.normal(0, 1, size=dim)\n        step = u / np.abs(v) ** (1 / beta)\n        return self.alpha * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n        \n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n        \n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices[0]], population[indices[1]], population[indices[2]]\n                \n                # Modified mutation strategy\n                F_dynamic = self.F * (1 + 0.1 * np.std(scores) / np.mean(scores))\n                mutant = np.clip(a + F_dynamic * (b - c), lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.CR * (1 - np.std(scores) / np.mean(scores))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_score = func(trial)\n                evaluations += 1\n                \n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < global_best_score:\n                        global_best_position = trial.copy()\n                        global_best_score = trial_score\n                \n                # Perform Levy Flight\n                if evaluations < self.budget and np.random.rand() < 0.1:\n                    levy_step = self.levy_flight(self.dim)\n                    levied_position = np.clip(global_best_position + levy_step, lb, ub)\n                    levied_score = func(levied_position)\n                    evaluations += 1\n                    if levied_score < global_best_score:\n                        global_best_position = levied_position.copy()\n                        global_best_score = levied_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "DE_Levy_Optimizer", "description": "This algorithm refines DE_Levy_Optimizer by adjusting the mutation strategy to adaptively scale the differential weight based on population diversity for enhanced exploration.", "configspace": "", "generation": 5, "fitness": 0.8797961725038318, "feedback": "The algorithm DE_Levy_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.035. And the mean value of best solutions found was 0.178 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "483f71dd-81a7-4446-a013-4167dc99c2cd", "metadata": {"aucs": [0.8617197972825561, 0.848512158450341, 0.9291565617785984], "final_y": [0.18354608672555672, 0.18433183567785283, 0.165829194791426]}, "mutation_prompt": null}
{"id": "e4b34838-9626-42e4-84ca-6f8e724e0b1d", "solution": "import numpy as np\n\nclass DE_Levy_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.F = 0.8  # differential weight\n        self.CR = 0.9  # crossover probability\n        self.alpha = 0.5  # control parameter for Levy flight\n\n    def levy_flight(self, dim, beta=1.5):\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma_u, size=dim)\n        v = np.random.normal(0, 1, size=dim)\n        step = u / np.abs(v) ** (1 / beta)\n        return self.alpha * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n        \n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n        \n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices[0]], population[indices[1]], population[indices[2]]\n                \n                # Modified mutation strategy\n                F_dynamic = self.F * (1 + 0.1 * np.std(scores) / np.mean(scores))\n                mutant = np.clip(a + F_dynamic * (b - c), lb, ub)\n\n                # Adaptive crossover probability\n                CR_dynamic = self.CR * (1.1 - 0.1 * np.std(scores) / np.mean(scores))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_score = func(trial)\n                evaluations += 1\n                \n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < global_best_score:\n                        global_best_position = trial.copy()\n                        global_best_score = trial_score\n                \n                # Perform Levy Flight\n                if evaluations < self.budget and np.random.rand() < 0.1:\n                    levy_step = self.levy_flight(self.dim)\n                    levied_position = np.clip(global_best_position + levy_step, lb, ub)\n                    levied_score = func(levied_position)\n                    evaluations += 1\n                    if levied_score < global_best_score:\n                        global_best_position = levied_position.copy()\n                        global_best_score = levied_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "DE_Levy_Optimizer", "description": "This algorithm further refines DE_Levy_Optimizer by employing an adaptive crossover probability that dynamically adjusts based on population diversity for improved exploitation.", "configspace": "", "generation": 6, "fitness": 0.8426731108732503, "feedback": "The algorithm DE_Levy_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.005. And the mean value of best solutions found was 0.180 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "dcd0d2ad-6889-493a-be0e-f45127e1a82c", "metadata": {"aucs": [0.842440162263268, 0.848512158450341, 0.8370670119061419], "final_y": [0.1836036380947651, 0.18433183567785283, 0.17065017621151202]}, "mutation_prompt": null}
{"id": "45c613a0-e088-4a57-8c65-20eb2437f321", "solution": "import numpy as np\n\nclass DE_Levy_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.F = 0.8  # differential weight\n        self.CR = 0.9  # crossover probability\n        self.alpha = 0.5  # control parameter for Levy flight\n\n    def levy_flight(self, dim, beta=1.5):\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma_u, size=dim)\n        v = np.random.normal(0, 1, size=dim)\n        step = u / np.abs(v) ** (1 / beta)\n        return self.alpha * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n        \n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n        initial_best_score = global_best_score\n        \n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices[0]], population[indices[1]], population[indices[2]]\n                \n                # Modified mutation strategy\n                F_dynamic = self.F * (1 + 0.1 * np.std(scores) / np.mean(scores))\n                mutant = np.clip(a + F_dynamic * (b - c), lb, ub)\n\n                relative_improvement = (initial_best_score - global_best_score) / abs(initial_best_score)\n                dynamic_CR = self.CR * (1 + 0.1 * relative_improvement)\n                \n                cross_points = np.random.rand(self.dim) < dynamic_CR * (1 - np.std(scores) / np.mean(scores))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_score = func(trial)\n                evaluations += 1\n                \n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < global_best_score:\n                        global_best_position = trial.copy()\n                        global_best_score = trial_score\n                \n                # Perform Levy Flight\n                if evaluations < self.budget and np.random.rand() < 0.1:\n                    levy_step = self.levy_flight(self.dim)\n                    levied_position = np.clip(global_best_position + levy_step, lb, ub)\n                    levied_score = func(levied_position)\n                    evaluations += 1\n                    if levied_score < global_best_score:\n                        global_best_position = levied_position.copy()\n                        global_best_score = levied_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "DE_Levy_Optimizer", "description": "Enhancing DE_Levy_Optimizer by adjusting crossover probability dynamically based on the relative improvement of the global best score.", "configspace": "", "generation": 7, "fitness": 0.8745634265674104, "feedback": "The algorithm DE_Levy_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.875 with standard deviation 0.039. And the mean value of best solutions found was 0.174 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "dcd0d2ad-6889-493a-be0e-f45127e1a82c", "metadata": {"aucs": [0.8460215594732919, 0.848512158450341, 0.9291565617785984], "final_y": [0.17134902292258058, 0.18433183567785283, 0.165829194791426]}, "mutation_prompt": null}
{"id": "6a8cf772-6c8c-4bc2-a649-60e5475f6196", "solution": "import numpy as np\n\nclass DE_Levy_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.F = 0.8  # differential weight\n        self.CR = 0.9  # crossover probability\n        self.alpha = 0.5  # control parameter for Levy flight\n\n    def levy_flight(self, dim, beta=1.5):\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma_u, size=dim)\n        v = np.random.normal(0, 1, size=dim)\n        step = u / np.abs(v) ** (1 / beta)\n        return self.alpha * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n        \n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n        \n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices[0]], population[indices[1]], population[indices[2]]\n                \n                # Modified mutation strategy\n                F_dynamic = self.F * (1 + 0.1 * np.std(scores) / np.mean(scores))\n                mutant = np.clip(a + F_dynamic * (b - c), lb, ub)\n\n                # Dynamic crossover strategy modification\n                CR_dynamic = self.CR * (0.5 + np.random.rand() * 0.5)\n\n                cross_points = np.random.rand(self.dim) < CR_dynamic * (1 - np.std(scores) / np.mean(scores))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_score = func(trial)\n                evaluations += 1\n                \n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < global_best_score:\n                        global_best_position = trial.copy()\n                        global_best_score = trial_score\n                \n                # Perform Levy Flight\n                if evaluations < self.budget and np.random.rand() < 0.1:\n                    levy_step = self.levy_flight(self.dim)\n                    levied_position = np.clip(global_best_position + levy_step, lb, ub)\n                    levied_score = func(levied_position)\n                    evaluations += 1\n                    if levied_score < global_best_score:\n                        global_best_position = levied_position.copy()\n                        global_best_score = levied_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "DE_Levy_Optimizer", "description": "This algorithm refines DE_Levy_Optimizer by introducing a dynamic crossover probability to balance exploration and exploitation more effectively.", "configspace": "", "generation": 8, "fitness": 0.8134168194199006, "feedback": "The algorithm DE_Levy_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.061. And the mean value of best solutions found was 0.193 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "dcd0d2ad-6889-493a-be0e-f45127e1a82c", "metadata": {"aucs": [0.7283299173132106, 0.8469296867808338, 0.8649908541656577], "final_y": [0.22091499044463803, 0.1892071133967197, 0.16952839882452786]}, "mutation_prompt": null}
{"id": "6d399a0f-1504-4fa7-985c-d1c7a794251c", "solution": "import numpy as np\n\nclass DE_Levy_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.F = 0.8  # differential weight\n        self.CR = 0.9  # crossover probability\n        self.alpha = 0.5  # control parameter for Levy flight\n\n    def levy_flight(self, dim, beta=1.5):\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma_u, size=dim)\n        v = np.random.normal(0, 1, size=dim)\n        step = u / np.abs(v) ** (1 / beta)\n        return self.alpha * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n        \n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n        \n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices[0]], population[indices[1]], population[indices[2]]\n                \n                # Modified mutation strategy\n                F_dynamic = self.F * (1 + 0.1 * np.std(scores) / np.mean(scores))\n                mutant = np.clip(a + F_dynamic * (b - c), lb, ub)\n\n                # Dynamically adjust CR based on population diversity\n                CR_dynamic = self.CR * (1 + 0.1 * np.std(scores) / np.mean(scores))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_score = func(trial)\n                evaluations += 1\n                \n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < global_best_score:\n                        global_best_position = trial.copy()\n                        global_best_score = trial_score\n                \n                # Perform Levy Flight\n                if evaluations < self.budget and np.random.rand() < 0.1:\n                    levy_step = self.levy_flight(self.dim)\n                    levied_position = np.clip(global_best_position + levy_step, lb, ub)\n                    levied_score = func(levied_position)\n                    evaluations += 1\n                    if levied_score < global_best_score:\n                        global_best_position = levied_position.copy()\n                        global_best_score = levied_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "DE_Levy_Optimizer", "description": "Refines DE_Levy_Optimizer by dynamically adjusting crossover probability based on population diversity to enhance solution diversity.", "configspace": "", "generation": 9, "fitness": 0.766882641590717, "feedback": "The algorithm DE_Levy_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.767 with standard deviation 0.062. And the mean value of best solutions found was 0.187 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "dcd0d2ad-6889-493a-be0e-f45127e1a82c", "metadata": {"aucs": [0.7532534294833233, 0.848512158450341, 0.6988823368384868], "final_y": [0.17325840983526009, 0.18433183567785283, 0.20406391965533155]}, "mutation_prompt": null}
{"id": "4c4261ff-a1ab-42a5-af5f-680d693631b7", "solution": "import numpy as np\n\nclass DE_Levy_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.F = 0.8  # differential weight\n        self.CR = 0.9  # crossover probability\n        self.alpha = 0.5  # control parameter for Levy flight\n\n    def levy_flight(self, dim, beta=1.5):\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma_u, size=dim)\n        v = np.random.normal(0, 1, size=dim)\n        step = u / np.abs(v) ** (1 / beta)\n        return self.alpha * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n        \n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n        \n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices[0]], population[indices[1]], population[indices[2]]\n                \n                # Modified mutation strategy\n                F_dynamic = self.F * (1 + 0.1 * np.std(scores) / np.mean(scores))\n                mutant = np.clip(a + F_dynamic * (b - c), lb, ub)\n\n                # Dynamic crossover probability\n                CR_dynamic = self.CR * (1 - np.std(scores) / np.mean(scores))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_score = func(trial)\n                evaluations += 1\n                \n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < global_best_score:\n                        global_best_position = trial.copy()\n                        global_best_score = trial_score\n                \n                # Perform Levy Flight\n                if evaluations < self.budget and np.random.rand() < 0.1:\n                    levy_step = self.levy_flight(self.dim)\n                    levied_position = np.clip(global_best_position + levy_step, lb, ub)\n                    levied_score = func(levied_position)\n                    evaluations += 1\n                    if levied_score < global_best_score:\n                        global_best_position = levied_position.copy()\n                        global_best_score = levied_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "DE_Levy_Optimizer", "description": "This algorithm refines DE_Levy_Optimizer by dynamically adjusting crossover probability based on convergence rate to enhance exploitation. ", "configspace": "", "generation": 10, "fitness": 0.8797961725038318, "feedback": "The algorithm DE_Levy_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.035. And the mean value of best solutions found was 0.178 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "dcd0d2ad-6889-493a-be0e-f45127e1a82c", "metadata": {"aucs": [0.8617197972825561, 0.848512158450341, 0.9291565617785984], "final_y": [0.18354608672555672, 0.18433183567785283, 0.165829194791426]}, "mutation_prompt": null}
{"id": "c6a52454-da5d-41aa-a105-b90882b89b37", "solution": "import numpy as np\n\nclass DE_Levy_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.F = 0.8  # differential weight\n        self.CR = 0.9  # crossover probability\n        self.alpha = 0.5  # control parameter for Levy flight\n\n    def levy_flight(self, dim, beta=1.5):\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma_u, size=dim)\n        v = np.random.normal(0, 1, size=dim)\n        step = u / np.abs(v) ** (1 / beta)\n        return self.alpha * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n        \n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n        \n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices[0]], population[indices[1]], population[indices[2]]\n                \n                # Modified mutation strategy\n                F_dynamic = self.F * (1 + 0.1 * np.std(scores) / np.mean(scores))\n                mutant = np.clip(a + F_dynamic * (b - c), lb, ub)\n\n                self.CR = self.CR * (1 - 0.2 * np.std(scores) / np.mean(scores))  # Dynamic CR adjustment\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_score = func(trial)\n                evaluations += 1\n                \n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < global_best_score:\n                        global_best_position = trial.copy()\n                        global_best_score = trial_score\n                \n                # Perform Levy Flight\n                if evaluations < self.budget and np.random.rand() < 0.1:\n                    levy_step = self.levy_flight(self.dim)\n                    levied_position = np.clip(global_best_position + levy_step, lb, ub)\n                    levied_score = func(levied_position)\n                    evaluations += 1\n                    if levied_score < global_best_score:\n                        global_best_position = levied_position.copy()\n                        global_best_score = levied_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "DE_Levy_Optimizer", "description": "This algorithm further refines DE_Levy_Optimizer by dynamically adjusting the crossover probability based on population diversity to enhance both exploration and exploitation.", "configspace": "", "generation": 11, "fitness": 0.8748843805785111, "feedback": "The algorithm DE_Levy_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.875 with standard deviation 0.058. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "dcd0d2ad-6889-493a-be0e-f45127e1a82c", "metadata": {"aucs": [0.9472440406646291, 0.8732752573931057, 0.8041338436777986], "final_y": [0.16493635287131636, 0.16786668838275798, 0.18471442879855604]}, "mutation_prompt": null}
{"id": "84fd462b-2b3d-4d48-9de4-ca1d618c2f9f", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n                \n                velocities[i] += cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "The Adaptive Quantum Particle Swarm Optimizer (AQPSO) leverages quantum-inspired dynamics and adaptive personal-best influence to enhance exploration and convergence in optimization tasks.", "configspace": "", "generation": 12, "fitness": 0.96014769982021, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.960 with standard deviation 0.010. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "dcd0d2ad-6889-493a-be0e-f45127e1a82c", "metadata": {"aucs": [0.9665044964677658, 0.9676920079930191, 0.9462465949998451], "final_y": [0.1656372448229807, 0.1650529726373403, 0.168930310237268]}, "mutation_prompt": null}
{"id": "9e15b3e3-edd5-4fb4-9a61-7e541af2b468", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n                \n                velocities[i] += cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.9 * (self.budget - evaluations) / self.budget)  # Changing line\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "The Enhanced Quantum Particle Swarm Optimizer (EQPSO) introduces a dynamic adaptive factor for improved exploration and convergence.", "configspace": "", "generation": 13, "fitness": 0.9639525362617722, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "84fd462b-2b3d-4d48-9de4-ca1d618c2f9f", "metadata": {"aucs": [0.97320477772566, 0.9709402800790286, 0.9477125509806281], "final_y": [0.16576276021387748, 0.16504002895825132, 0.17072338096357842]}, "mutation_prompt": null}
{"id": "4599bfe2-f540-4921-9036-60ebc37839ff", "solution": "import numpy as np\n\nclass ADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.F_min, self.F_max = 0.5, 1.0\n        self.CR_min, self.CR_max = 0.1, 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                F = self.F_min + np.random.rand() * (self.F_max - self.F_min)\n                CR = self.CR_min + (self.CR_max - self.CR_min) * (self.budget - evaluations) / self.budget\n\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, lb, ub)\n\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, population[i])\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        best_idx = scores.argmin()\n        return population[best_idx], scores[best_idx]", "name": "ADE", "description": "The Adaptive Differential Evolution (ADE) algorithm integrates self-adaptive scaling and crossover factors to enhance exploration and exploitation across dynamic search landscapes.", "configspace": "", "generation": 14, "fitness": 0.7408569153234396, "feedback": "The algorithm ADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.741 with standard deviation 0.045. And the mean value of best solutions found was 0.203 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "9e15b3e3-edd5-4fb4-9a61-7e541af2b468", "metadata": {"aucs": [0.7841863727203211, 0.6782357565903683, 0.7601486166596293], "final_y": [0.2108271543596425, 0.20268350330243, 0.194240888167181]}, "mutation_prompt": null}
{"id": "32ceba3c-e056-4274-b329-d3d33b948654", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = 0.9 - (0.8 * evaluations / self.budget) # Changing line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.9 * (self.budget - evaluations) / self.budget)\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Modify the inertia weight adaptation in AQPSO to improve balance between exploration and exploitation.", "configspace": "", "generation": 15, "fitness": 0.9640741444922551, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "9e15b3e3-edd5-4fb4-9a61-7e541af2b468", "metadata": {"aucs": [0.973204779643261, 0.9713051023453235, 0.947712551488181], "final_y": [0.16576275954207342, 0.16500057310728644, 0.17072338074158822]}, "mutation_prompt": null}
{"id": "e0999fc7-1307-43b2-b987-1b70674b6543", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = 0.9 - (0.8 * evaluations / self.budget) # Changing line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget)) # Changed line\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance the adaptive factor calculation in AQPSO to better adjust exploration and exploitation balance over time.", "configspace": "", "generation": 16, "fitness": 0.9652456631364924, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "32ceba3c-e056-4274-b329-d3d33b948654", "metadata": {"aucs": [0.9747750302111974, 0.9719067328780112, 0.9490552263202688], "final_y": [0.16540327191275694, 0.16493528527446322, 0.17011518384049573]}, "mutation_prompt": null}
{"id": "3906a150-708b-418e-a4ee-b0abeaaed9db", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = 0.9 - (0.8 * evaluations / self.budget) # Changing line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget)) # Changed line\n\n                self.c2 = 2.5 - (1.0 * evaluations / self.budget) # New line\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a dynamic update for the social coefficient to enhance global exploration and convergence.", "configspace": "", "generation": 17, "fitness": 0.9652430817060828, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "e0999fc7-1307-43b2-b987-1b70674b6543", "metadata": {"aucs": [0.9747749922546141, 0.9718990271308134, 0.9490552257328205], "final_y": [0.1654032449734082, 0.16493792439659294, 0.1701151841399411]}, "mutation_prompt": null}
{"id": "f23237cd-7aec-4c25-81c9-a6fcbdce145a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = 0.9 - (0.8 * evaluations / self.budget) # Changing line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget)) # Changed line\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (evaluations / self.budget) # Changed line\n            self.c2 = 0.5 + 1.5 * (evaluations / self.budget) # Changed line\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce dynamic social and cognitive coefficients in AQPSO to improve the balance between exploration and exploitation phases.", "configspace": "", "generation": 18, "fitness": 0.965318039567745, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "e0999fc7-1307-43b2-b987-1b70674b6543", "metadata": {"aucs": [0.9750154467642116, 0.971883442791093, 0.9490552291479303], "final_y": [0.1653050214536601, 0.16494384445376153, 0.17011518239911816]}, "mutation_prompt": null}
{"id": "034191ad-4b99-479f-9694-da7dba7b34b5", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = 0.9 - (0.7 * np.sin(np.pi * evaluations / (2 * self.budget)))  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(2 * np.pi * evaluations / self.budget))  # Changed line\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (evaluations / self.budget)\n            self.c2 = 0.5 + 1.5 * (evaluations / self.budget)\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Incorporate non-linear inertia weight and quantum perturbation scaling to enhance convergence dynamics.", "configspace": "", "generation": 19, "fitness": 0.9607827842334945, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.961 with standard deviation 0.013. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "f23237cd-7aec-4c25-81c9-a6fcbdce145a", "metadata": {"aucs": [0.9707239297304417, 0.969246251412689, 0.9423781715573523], "final_y": [0.16551319869258319, 0.16508341067475008, 0.1699575577846444]}, "mutation_prompt": null}
{"id": "06e49126-9ff9-4666-adbd-886a9799635d", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        velocity_scale = self.alpha  # Changed line\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + velocity_scale * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = 0.9 - (0.8 * evaluations / self.budget) # Changing line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget)) # Changed line\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (evaluations / self.budget) # Changed line\n            self.c2 = 0.5 + 1.5 * (evaluations / self.budget) # Changed line\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a dynamic velocity scaling factor in quantum updates to enhance convergence efficiency in AQPSO.", "configspace": "", "generation": 20, "fitness": 0.965318039567745, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "f23237cd-7aec-4c25-81c9-a6fcbdce145a", "metadata": {"aucs": [0.9750154467642116, 0.971883442791093, 0.9490552291479303], "final_y": [0.1653050214536601, 0.16494384445376153, 0.17011518239911816]}, "mutation_prompt": null}
{"id": "bd5c5640-521c-4ad7-898b-a4de2da00b5d", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        random_perturbation = np.random.uniform(-0.1, 0.1, size=self.dim)  # Changed line\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim) + random_perturbation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = 0.9 - (0.8 * evaluations / self.budget) # Changing line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget)) # Changed line\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (evaluations / self.budget) # Changed line\n            self.c2 = 0.5 + 1.5 * (evaluations / self.budget) # Changed line\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a random perturbation factor to enhance the diversity in the quantum position update of AQPSO for improved exploration.", "configspace": "", "generation": 21, "fitness": 0.9638205271480668, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.011. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "f23237cd-7aec-4c25-81c9-a6fcbdce145a", "metadata": {"aucs": [0.9644108913294657, 0.9506477704399858, 0.9764029196747489], "final_y": [0.16590195826588072, 0.16862332838009464, 0.16522242999426595]}, "mutation_prompt": null}
{"id": "c9daec0b-05b3-4231-b585-621de5d9af3d", "solution": "import numpy as np\n\nclass PredatorPreyDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.f = 0.5  # mutation factor\n        self.cr = 0.9  # crossover probability\n\n    def predator_prey_dynamics(self, population, prey_best):\n        predator = np.random.uniform(np.min(population, axis=0), np.max(population, axis=0), self.dim)\n        for i in range(self.population_size):\n            prey = population[i]\n            if np.random.rand() < 0.5:\n                prey = prey + self.f * (prey_best - predator)\n            else:\n                prey = prey - self.f * (predator - prey_best)\n            population[i] = np.clip(prey, lb, ub)\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n        \n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                idxs = np.random.choice(self.population_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n                mutant = x1 + self.f * (x2 - x3)\n                mutant = np.clip(mutant, lb, ub)\n                \n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < global_best_score:\n                        global_best_position = trial.copy()\n                        global_best_score = trial_score\n\n                if evaluations >= self.budget:\n                    break\n\n            population = self.predator_prey_dynamics(population, global_best_position)\n\n        return global_best_position, global_best_score", "name": "PredatorPreyDE", "description": "Introduce a bio-inspired predator-prey dynamic in a modified DE algorithm to enhance exploration and convergence.", "configspace": "", "generation": 22, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'lb' is not defined\").", "error": "NameError(\"name 'lb' is not defined\")", "parent_id": "f23237cd-7aec-4c25-81c9-a6fcbdce145a", "metadata": {}, "mutation_prompt": null}
{"id": "3650e473-d534-44cf-a797-fcd3733da1d7", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = 0.9 - 0.8 * (evaluations / self.budget)**2 # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (evaluations / self.budget)\n            self.c2 = 0.5 + 1.5 * (evaluations / self.budget)\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a nonlinear inertia weight decay in AQPSO to enhance dynamic balancing between exploration and exploitation phases.", "configspace": "", "generation": 23, "fitness": 0.9652372920743192, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "f23237cd-7aec-4c25-81c9-a6fcbdce145a", "metadata": {"aucs": [0.9747732925332454, 0.9718833590235262, 0.9490552246661861], "final_y": [0.16540161050875846, 0.16494385075942897, 0.17011518468364817]}, "mutation_prompt": null}
{"id": "61b4fe6f-522a-4eb1-ba21-d1b71788877a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (evaluations / self.budget)\n            self.c2 = 0.5 + 1.5 * (evaluations / self.budget)\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Refine the inertia weight strategy dynamically in AQPSO to better adapt exploration-exploitation balance.", "configspace": "", "generation": 24, "fitness": 0.9653799186918951, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "f23237cd-7aec-4c25-81c9-a6fcbdce145a", "metadata": {"aucs": [0.9751999470654916, 0.9718845850795755, 0.9490552239306183], "final_y": [0.16530885110553273, 0.16494377166431518, 0.17011518505859657]}, "mutation_prompt": null}
{"id": "f128d0ad-750f-4d9a-85c9-452e87b22e6c", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (evaluations / self.budget)\n            self.c2 = 0.5 + 1.5 * (evaluations / self.budget)\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Increase the diversity of exploration by introducing a random search step after updating positions.", "configspace": "", "generation": 25, "fitness": 0.9653799186918951, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "61b4fe6f-522a-4eb1-ba21-d1b71788877a", "metadata": {"aucs": [0.9751999470654916, 0.9718845850795755, 0.9490552239306183], "final_y": [0.16530885110553273, 0.16494377166431518, 0.17011518505859657]}, "mutation_prompt": null}
{"id": "ec918855-76d7-478b-9920-f25f414c1285", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                # Changed line\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget) * (1 + 0.1 * np.cos(2 * np.pi * evaluations / self.budget)) \n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (evaluations / self.budget)\n            self.c2 = 0.5 + 1.5 * (evaluations / self.budget)\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Refine the inertia weight strategy further by adding a dynamic modulation factor to better balance exploration and exploitation.", "configspace": "", "generation": 26, "fitness": 0.9653799137207811, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "61b4fe6f-522a-4eb1-ba21-d1b71788877a", "metadata": {"aucs": [0.9751999420414345, 0.9718845751902906, 0.9490552239306183], "final_y": [0.16530885352384506, 0.16494377239155245, 0.17011518505859657]}, "mutation_prompt": null}
{"id": "cba2ef4f-0c61-487d-8ccf-263c61ebf7a2", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget) # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.3 * np.sin(2 * np.pi * evaluations / self.budget))  # Changed line\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (evaluations / self.budget)\n            self.c2 = 0.5 + 1.5 * (evaluations / self.budget)\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a sinusoidal modulation in the adaptive factor of AQPSO for enhanced balance control.", "configspace": "", "generation": 27, "fitness": 0.962914660912641, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.963 with standard deviation 0.010. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "61b4fe6f-522a-4eb1-ba21-d1b71788877a", "metadata": {"aucs": [0.9710696439859384, 0.9692942334429414, 0.948380105309043], "final_y": [0.16553606460344328, 0.16509947322074692, 0.16956366344274176]}, "mutation_prompt": null}
{"id": "79e4369c-cc6d-4dd3-b506-9000d0c54e58", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (evaluations / self.budget)\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))  # Changed line\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a cosine function to dynamically adjust social coefficient, enhancing balance between exploration and exploitation.", "configspace": "", "generation": 28, "fitness": 0.965379933508928, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "61b4fe6f-522a-4eb1-ba21-d1b71788877a", "metadata": {"aucs": [0.9751999580015557, 0.9718846185946104, 0.9490552239306183], "final_y": [0.16530884585239514, 0.1649437810191965, 0.17011518505859657]}, "mutation_prompt": null}
{"id": "282b58c1-f447-4046-9872-38754bbdbb57", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 * np.exp(-evaluations / (0.5 * self.budget))  # Changed line\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Refine AQPSO by introducing an exponential decay in the cognitive coefficient to enhance convergence speed.", "configspace": "", "generation": 29, "fitness": 0.9653799318235241, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "79e4369c-cc6d-4dd3-b506-9000d0c54e58", "metadata": {"aucs": [0.9751999580019821, 0.9718846130767773, 0.949055224391813], "final_y": [0.1653088458522891, 0.1649437794280303, 0.17011518482350718]}, "mutation_prompt": null}
{"id": "6fd73cfb-a2ea-4a74-b7cf-4b78166adf7a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))  # Changed line\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Adjust the cognitive coefficient using a cosine function for adaptive exploration-exploitation balance.", "configspace": "", "generation": 30, "fitness": 0.965379934530269, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "79e4369c-cc6d-4dd3-b506-9000d0c54e58", "metadata": {"aucs": [0.9751999580013471, 0.9718846216588417, 0.9490552239306183], "final_y": [0.16530884585244754, 0.16494378191310588, 0.17011518505859657]}, "mutation_prompt": null}
{"id": "d70cb1e1-5530-4d80-b508-c1f3229f7c46", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance exploration by introducing a dynamic scaling factor into the inertia weight calculation.", "configspace": "", "generation": 31, "fitness": 0.9653799362584453, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "6fd73cfb-a2ea-4a74-b7cf-4b78166adf7a", "metadata": {"aucs": [0.9751999607397543, 0.971884622840335, 0.9490552251952463], "final_y": [0.16530884453454364, 0.16494378180944003, 0.17011518441396523]}, "mutation_prompt": null}
{"id": "abdfbf65-453d-425d-a4aa-819dcf7175c3", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c1 *= (0.5 + 0.5 * np.cos(2 * np.pi * evaluations / self.budget))  # Changed line\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a dynamic personal weight factor to balance exploration and exploitation dynamically.", "configspace": "", "generation": 32, "fitness": 0.965379936230875, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "d70cb1e1-5530-4d80-b508-c1f3229f7c46", "metadata": {"aucs": [0.9751999607399339, 0.9718846219492207, 0.9490552260034705], "final_y": [0.165308844534499, 0.1649437815487924, 0.17011518400198067]}, "mutation_prompt": null}
{"id": "cb2a8e9f-2831-4bc5-a2fa-ade29f807080", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / self.budget)  # Changed line\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a dynamic self-adaptive cognitive coefficient for enhanced balance between exploration and exploitation.", "configspace": "", "generation": 33, "fitness": 0.9653799180443002, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "d70cb1e1-5530-4d80-b508-c1f3229f7c46", "metadata": {"aucs": [0.975199960741246, 0.9718845683511702, 0.9490552250404847], "final_y": [0.16530884453417183, 0.16494376695314528, 0.17011518449285312]}, "mutation_prompt": null}
{"id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()  # Changed line\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a randomness factor to the social component to enhance diversity.", "configspace": "", "generation": 34, "fitness": 0.9712908544584535, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.009. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "d70cb1e1-5530-4d80-b508-c1f3229f7c46", "metadata": {"aucs": [0.9781273711510547, 0.977644917774182, 0.9581002744501238], "final_y": [0.16487555079980742, 0.16499059738060384, 0.16716405774849208]}, "mutation_prompt": null}
{"id": "1fa63ac6-ed08-4d0e-a4c1-fdcb0997f698", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * (1 / (1 + np.exp(-10 * (evaluations / self.budget - 0.5))))  # Changed line\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Utilize a sigmoid function to dynamically adjust social influence based on evaluations, balancing exploration and exploitation.", "configspace": "", "generation": 35, "fitness": 0.965728616184849, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.013. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9746189186237062, 0.9747828082391684, 0.9477841216916725], "final_y": [0.16545396066377793, 0.1648562557982901, 0.17061921318457574]}, "mutation_prompt": null}
{"id": "ccaae37b-2abd-4077-9e99-45815446395f", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.sin(2 * np.pi * np.random.rand())  # Changed line\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a nonlinear sinusoidal element to the social component for enhanced diversity.", "configspace": "", "generation": 36, "fitness": 0.9697646492193973, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.970 with standard deviation 0.011. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9774093185948062, 0.9774510653106969, 0.954433563752689], "final_y": [0.16496467817979177, 0.1650127731129597, 0.16793914820786038]}, "mutation_prompt": null}
{"id": "d0654612-d62e-444c-a0bd-c0e73e5c1ca3", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = 0.9 - 0.4 * (evaluations / self.budget) ** 2  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Integrate a non-linear time-varying inertia weight to balance exploration and exploitation dynamically.", "configspace": "", "generation": 37, "fitness": 0.9710540478383632, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.009. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9780007626870576, 0.9770360768820415, 0.9581253039459908], "final_y": [0.16490439950614844, 0.165048337115835, 0.16687521821194296]}, "mutation_prompt": null}
{"id": "ea4cee57-d2e0-4a77-9461-5252e3787d15", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            global_best_position += np.random.uniform(-0.1, 0.1, size=self.dim)  # Changed line\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Increasing randomness in global best update to enhance exploration.", "configspace": "", "generation": 38, "fitness": 0.95813638200758, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.958 with standard deviation 0.021. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9789677646609347, 0.9657989008904238, 0.9296424804713818], "final_y": [0.16518066287800182, 0.1663428304600032, 0.1732310989719692]}, "mutation_prompt": null}
{"id": "1db662ca-0811-46ba-af16-08da9905cb66", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                # Modified line\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)) * np.random.rand(0.8, 1.2)\n\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce randomness to inertia weight for improved exploration.", "configspace": "", "generation": 39, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'float' object cannot be interpreted as an integer\").", "error": "TypeError(\"'float' object cannot be interpreted as an integer\")", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {}, "mutation_prompt": null}
{"id": "521911f9-afca-4b44-8192-1cfcefec1143", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()  # Changed line\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget)) * np.random.uniform(0.9, 1.1)  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance diversity by introducing random scaling to inertia weight.", "configspace": "", "generation": 40, "fitness": 0.9538601568957256, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.954 with standard deviation 0.034. And the mean value of best solutions found was 0.170 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9058658173270557, 0.9705111101794482, 0.9852035431806726], "final_y": [0.1802793886782459, 0.16501020057533633, 0.1648674224079787]}, "mutation_prompt": null}
{"id": "fe0a2619-fbc0-402a-ac72-2ba893cbbc54", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()  # Changed line\n\n                inertia_weight = 0.9 * (0.5 + 0.5 * np.cos(np.pi * evaluations / self.budget))  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Adjust inertia weight to enhance exploration and exploitation balance.", "configspace": "", "generation": 41, "fitness": 0.9710540671259981, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.009. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9780007593423748, 0.9770360762779238, 0.9581253657576957], "final_y": [0.16490439937061618, 0.16504833711031852, 0.166875208137237]}, "mutation_prompt": null}
{"id": "d16c72fb-0711-4c3c-9af6-527b3163e6d9", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()  # Changed line\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                velocity_scale = 0.5 + 0.5 * np.sin(np.pi * evaluations / self.budget)  # Changed line\n                particle_position = population[i] + velocity_scale * velocities[i]  # Changed line\n                \n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce adaptive velocity scaling for enhanced convergence control.", "configspace": "", "generation": 42, "fitness": 0.4690868545357676, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.469 with standard deviation 0.028. And the mean value of best solutions found was 0.428 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.4289708504745888, 0.4887423048434897, 0.4895474082892243], "final_y": [0.4579171857346731, 0.41379549904142665, 0.41182864028001587]}, "mutation_prompt": null}
{"id": "663b52f4-5539-42e0-a1a2-4c8d1c78f939", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + (self.alpha * np.random.uniform(-1, 1, size=self.dim)) * (0.9 + 0.1 * np.cos(np.pi * self.alpha))  # Changed line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance convergence by adjusting the quantum position update's randomness factor dynamically.", "configspace": "", "generation": 43, "fitness": 0.9673609105222258, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.967 with standard deviation 0.008. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9763182471532775, 0.9690495674290843, 0.9567149169843155], "final_y": [0.16516109584991345, 0.1669719870530092, 0.1674047757713536]}, "mutation_prompt": null}
{"id": "6891fb8f-6be6-478c-8cce-706c5b628aa2", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand() * np.sin(np.pi * evaluations / (2 * self.budget))  # Changed line\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Adjust the randomness factor based on evaluations for improved balance between exploration and exploitation.", "configspace": "", "generation": 44, "fitness": 0.9678272297460584, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.013. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9779582752494412, 0.9765558192153174, 0.9489675947734163], "final_y": [0.16490394070272762, 0.1650757329522503, 0.16680992129625638]}, "mutation_prompt": null}
{"id": "9a6fd1ba-820e-4ced-bd05-25ef73752657", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim) * (0.5 + 0.5 * np.cos(np.pi * self.budget / self.budget))  # Changed line\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()  # Changed line\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Integrate a dynamic beta factor in quantum position update to enhance exploration-exploitation balance.", "configspace": "", "generation": 45, "fitness": 0.5123177578670123, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.512 with standard deviation 0.038. And the mean value of best solutions found was 0.384 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.46513413934133563, 0.5575864357054883, 0.5142326985542129], "final_y": [0.41930782451513704, 0.3481301530702232, 0.38485948013389204]}, "mutation_prompt": null}
{"id": "db0cdcaf-6046-4745-87e9-f206d91b668b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()  # Changed line\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget) * np.random.rand())  # Changed line\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Integrate a randomized component into the adaptive factor to enhance exploration and convergence.", "configspace": "", "generation": 46, "fitness": 0.964503233870665, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.014. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9715025746210686, 0.9770833046734718, 0.9449238223174546], "final_y": [0.16522722048830651, 0.16493349430866489, 0.16891298612585193]}, "mutation_prompt": null}
{"id": "e5f2570d-fc63-433b-9e89-950f90c341a2", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()  \n\n                # Dynamic adaptation line\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a dynamic adaptive factor into the social component to enhance exploration.", "configspace": "", "generation": 47, "fitness": 0.9712908544584535, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.009. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9781273711510547, 0.977644917774182, 0.9581002744501238], "final_y": [0.16487555079980742, 0.16499059738060384, 0.16716405774849208]}, "mutation_prompt": null}
{"id": "17d6cddd-a4c6-4ea7-b266-2df9146f59bb", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()  # Changed line\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                # Adaptive factor update adjustment\n                self.alpha = max(0.1, 0.5 + 0.4 * np.sin(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance adaptive factor update strategy for improved convergence.", "configspace": "", "generation": 48, "fitness": 0.9704738840071371, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.970 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9791583920122358, 0.9710971292346153, 0.9611661307745601], "final_y": [0.16486474567988063, 0.16506153745401908, 0.16494481465495914]}, "mutation_prompt": null}
{"id": "f50aac62-e098-48b4-9b2c-58ee949cefba", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = max(1.5, 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)))  # Changed line\n            self.c2 = min(2.5, 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)))  # Changed line\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance particle diversity by adjusting social and cognitive coefficients adaptively.", "configspace": "", "generation": 49, "fitness": 0.9712908544584535, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.009. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9781273711510547, 0.977644917774182, 0.9581002744501238], "final_y": [0.16487555079980742, 0.16499059738060384, 0.16716405774849208]}, "mutation_prompt": null}
{"id": "c13056ab-5ccc-4216-ab6b-ffb9c64a426f", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.alpha = 0.5\n\n    def levy_flight(self, dimension, beta=1.5):\n        sigma_u = np.power((np.gamma(1 + beta) * np.sin(np.pi * beta / 2)) /\n                           (np.gamma((1 + beta) / 2) * beta * np.power(2, (beta - 1) / 2)), 1 / beta)\n        u = np.random.normal(0, sigma_u, size=dimension)\n        v = np.random.normal(0, 1, size=dimension)\n        step = u / np.power(np.abs(v), 1 / beta)\n        return step\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.6:  # Changed line\n                    particle_position = population[i] + velocities[i] + self.levy_flight(self.dim)  # Changed line\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance exploration and exploitation by integrating a Lévy flight mechanism into particle updates.", "configspace": "", "generation": 50, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {}, "mutation_prompt": null}
{"id": "8bd6fc08-9e09-4824-b3b0-4a758b2d47a4", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()  # Changed line\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce dynamic cognitive and social coefficients to better balance exploration and exploitation.", "configspace": "", "generation": 51, "fitness": 0.9712908544584535, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.009. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9781273711510547, 0.977644917774182, 0.9581002744501238], "final_y": [0.16487555079980742, 0.16499059738060384, 0.16716405774849208]}, "mutation_prompt": null}
{"id": "15a8d3bc-c7b5-4372-af47-f5bdf192674f", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i]) * np.random.rand()  # Changed line\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance diversity by adding random perturbations to the cognitive component.", "configspace": "", "generation": 52, "fitness": 0.9538783553784164, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.954 with standard deviation 0.034. And the mean value of best solutions found was 0.170 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9058720802587064, 0.9705060304818152, 0.9852569553947272], "final_y": [0.18027768970252067, 0.16501077361372285, 0.1648611904663041]}, "mutation_prompt": null}
{"id": "1541beff-ef6c-49f2-b223-b1a70eae82c5", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * (0.5 + 0.5 * np.sin(np.pi * evaluations / self.budget))  # Changed line\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Utilize a sine wave to modulate the randomness factor in the social component for enhanced exploration.", "configspace": "", "generation": 53, "fitness": 0.9653836514107327, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.012. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.97520000249946, 0.9718957262263554, 0.9490552255063828], "final_y": [0.1653088260760509, 0.1649212150400461, 0.17011518425559857]}, "mutation_prompt": null}
{"id": "a6be0a06-3ea3-4fa3-9719-279e377e4bc9", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand() * np.sin(np.pi * evaluations / self.budget)  # Changed line\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Increase exploration by introducing a dynamic adjustment of the social randomness factor.", "configspace": "", "generation": 54, "fitness": 0.967870581399401, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.013. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9779501440343493, 0.9767321697648212, 0.948929430399032], "final_y": [0.16490674870340571, 0.1650716562549588, 0.16686531414015904]}, "mutation_prompt": null}
{"id": "87a1027b-3c0e-40d0-a301-e2d452d9ccaf", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                phase_factor = np.sin((np.pi * evaluations) / (2 * self.budget))  # Changed line\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * phase_factor * r1 * (personal_best_positions[i] - population[i])  # Changed line\n                social_component = self.c2 * phase_factor * r2 * (global_best_position - population[i]) * np.random.rand()  # Changed line\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 2.0 - 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce adaptive randomness in cognitive and social components based on evaluation phases to enhance exploration and exploitation balance.", "configspace": "", "generation": 55, "fitness": 0.9678078633106573, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.013. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9779591134584954, 0.9765696037617083, 0.948894872711768], "final_y": [0.16490311762919418, 0.1650657732703722, 0.16686932955861367]}, "mutation_prompt": null}
{"id": "48b37a2e-64fe-48ef-8274-e4f364c9d60a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adjust the social coefficient (c2) based on the improvement of the global best score\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance convergence by dynamically adjusting the social coefficient based on the best global score.", "configspace": "", "generation": 56, "fitness": 0.9712908563267426, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.009. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e60064fc-fc39-4dc3-b1ea-f5fa4cb90ebb", "metadata": {"aucs": [0.9781273710731385, 0.977644923456146, 0.9581002744509435], "final_y": [0.16487555079599303, 0.16499059705937036, 0.1671640577485508]}, "mutation_prompt": null}
{"id": "bf9147a9-efe9-4323-b0ad-2f515a874329", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                quantum_probability = 0.5 + 0.5 * evaluations / self.budget  # Adjusted probability for quantum update\n                if np.random.rand() < quantum_probability:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n                else:\n                    particle_position = population[i] + velocities[i]\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a sporadic exploitation phase by increasing the frequency of quantum updates as iterations progress.", "configspace": "", "generation": 57, "fitness": 0.9457749512972101, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.946 with standard deviation 0.019. And the mean value of best solutions found was 0.169 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "48b37a2e-64fe-48ef-8274-e4f364c9d60a", "metadata": {"aucs": [0.9424095531409099, 0.9701352411862607, 0.9247800595644599], "final_y": [0.16952518725170063, 0.16497099830194994, 0.17296814595616405]}, "mutation_prompt": null}
{"id": "2c0f94b7-a9ea-4c51-957e-1a1a89a48abc", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -np.abs(ub - lb), np.abs(ub - lb))  # Changed velocity clipping range\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adjust the social coefficient (c2) based on the improvement of the global best score\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Refine AQPSO by introducing adaptive dynamic range for velocity clipping for improved exploration-exploitation balance.", "configspace": "", "generation": 58, "fitness": 0.9651860195629149, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.013. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "48b37a2e-64fe-48ef-8274-e4f364c9d60a", "metadata": {"aucs": [0.9808674861640951, 0.9662436347468941, 0.9484469377777557], "final_y": [0.16486523207399317, 0.16584557825097368, 0.1701615982202449]}, "mutation_prompt": null}
{"id": "85a65af4-bdcc-4100-9fa3-60a11ba84411", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adjust the cognitive coefficient (c1) dynamically based on evaluations\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n\n            # Adjust the social coefficient (c2) based on the improvement of the global best score\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a dynamic adjustment to the cognitive coefficient (c1) based on the current evaluations to balance exploration and exploitation.", "configspace": "", "generation": 59, "fitness": 0.9725283884143442, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.973 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "48b37a2e-64fe-48ef-8274-e4f364c9d60a", "metadata": {"aucs": [0.9781273648667967, 0.9776449176240676, 0.9618128827521679], "final_y": [0.1648755508346741, 0.164990596812981, 0.1664603006042653]}, "mutation_prompt": null}
{"id": "56a720f9-aab1-46c4-a365-e29b2093957a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        diversity_factor = np.mean(np.std(personal_best - global_best, axis=0))\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + (self.alpha * diversity_factor) * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adjust the cognitive coefficient (c1) dynamically based on evaluations\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n\n            # Adjust the social coefficient (c2) based on the improvement of the global best score\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduced a dynamic diversity enhancement mechanism by adjusting the quantum position update based on the population's diversity to enhance exploration in sparse regions.", "configspace": "", "generation": 60, "fitness": 0.9222657065710641, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.922 with standard deviation 0.027. And the mean value of best solutions found was 0.172 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "85a65af4-bdcc-4100-9fa3-60a11ba84411", "metadata": {"aucs": [0.9332842458525363, 0.8854800382605785, 0.9480328356000772], "final_y": [0.16590910903148215, 0.18394450304727006, 0.16500337795819187]}, "mutation_prompt": null}
{"id": "23b75c22-56d4-4b59-a886-764151d3121b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    adaptive_alpha = self.alpha * (0.5 + 0.5 * np.random.rand()) # changed line\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adjust the cognitive coefficient (c1) dynamically based on evaluations\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n\n            # Adjust the social coefficient (c2) based on the improvement of the global best score\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introducing an adaptive quantum position update factor to enhance dynamic exploration and exploitation balance.", "configspace": "", "generation": 61, "fitness": 0.9614889673633019, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.961 with standard deviation 0.009. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "85a65af4-bdcc-4100-9fa3-60a11ba84411", "metadata": {"aucs": [0.9521656407735859, 0.9591902954284918, 0.9731109658878279], "final_y": [0.16632802081062426, 0.16663128457172782, 0.16560062747208226]}, "mutation_prompt": null}
{"id": "1dcbe8e7-f8b5-4578-9b2b-42d24f629983", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory = []  # Memory to store best solutions\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n        self.memory.append((global_best_position.copy(), global_best_score))\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n                        self.memory.append((global_best_position.copy(), global_best_score))\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adjust the cognitive coefficient (c1) dynamically based on evaluations\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n\n            # Adjust the social coefficient (c2) based on the improvement of the global best score\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n\n            # Exploit memory by occasionally resetting global best\n            if evaluations > self.budget // 2 and np.random.rand() < 0.1:\n                memory_best = min(self.memory, key=lambda x: x[1])\n                if memory_best[1] < global_best_score:\n                    global_best_position = memory_best[0].copy()\n                    global_best_score = memory_best[1]\n\n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a memory-based strategy to retain and exploit historically best-performing solutions for enhanced convergence.", "configspace": "", "generation": 62, "fitness": 0.9725011402132622, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.973 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "85a65af4-bdcc-4100-9fa3-60a11ba84411", "metadata": {"aucs": [0.9780613107971261, 0.9775312365041303, 0.9619108733385301], "final_y": [0.1649000966490981, 0.1650365631335008, 0.16639804650176337]}, "mutation_prompt": null}
{"id": "cd25e282-b8f0-46cc-a7e7-263fddb0ce4b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                quantum_prob = 0.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))  # Dynamic quantum update probability\n                if np.random.rand() < quantum_prob:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n                else:\n                    particle_position = population[i] + velocities[i]\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adjust the cognitive coefficient (c1) dynamically based on evaluations\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n\n            # Adjust the social coefficient (c2) based on the improvement of the global best score\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a dynamic adaptation of the quantum position update probability based on the current iteration to enhance the balance between exploration and exploitation.", "configspace": "", "generation": 63, "fitness": 0.9596752015056701, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.960 with standard deviation 0.009. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "85a65af4-bdcc-4100-9fa3-60a11ba84411", "metadata": {"aucs": [0.9691376475216993, 0.9617505507816928, 0.9481374062136179], "final_y": [0.1651740525645855, 0.16526899664525663, 0.16723207074610658]}, "mutation_prompt": null}
{"id": "27587517-6e48-44f2-8254-aabbc0e70877", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        rand_weight = np.random.rand(self.dim)  # Added line\n        position = (1 - beta) * personal_best + beta * global_best * rand_weight\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adjust the cognitive coefficient (c1) dynamically based on evaluations\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n\n            # Adjust the social coefficient (c2) based on the improvement of the global best score\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance exploration by incorporating a randomized weight in the quantum position update formula.", "configspace": "", "generation": 64, "fitness": 0.6651122667297398, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.665 with standard deviation 0.054. And the mean value of best solutions found was 0.294 (0. is the best) with standard deviation 0.031.", "error": "", "parent_id": "85a65af4-bdcc-4100-9fa3-60a11ba84411", "metadata": {"aucs": [0.6238781961168692, 0.7407715484597523, 0.6306870556125976], "final_y": [0.31759678189007245, 0.2509528349181175, 0.31356898882648276]}, "mutation_prompt": null}
{"id": "2e67110e-c4da-4b4b-bec1-0bc3e1532e9b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = (0.9 + 0.1 * np.sin(np.pi * evaluations / self.budget)) * (0.8 + 0.2 * np.cos(np.pi * evaluations / self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adjust the cognitive coefficient (c1) dynamically based on evaluations\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n\n            # Adjust the social coefficient (c2) based on the improvement of the global best score\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introducing a dynamic adjustment to the inertia weight for increased adaptability based on current evaluations.", "configspace": "", "generation": 65, "fitness": 0.9710538460347453, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.009. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "85a65af4-bdcc-4100-9fa3-60a11ba84411", "metadata": {"aucs": [0.9780007724422384, 0.9770360653318763, 0.9581247003301211], "final_y": [0.1649043998620261, 0.16504833701014876, 0.16687531666490985]}, "mutation_prompt": null}
{"id": "2ac55e18-936b-4e9e-b7ee-6d3032ad182e", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                inertia_weight = 0.9 * (1 - evaluations / self.budget) + 0.4  # updated inertia weight formula\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adjust the cognitive coefficient (c1) dynamically based on evaluations\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n\n            # Adjust the social coefficient (c2) based on the improvement of the global best score\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance swarm intelligence by dynamically adjusting particle velocities using a new inertia weight formula and improving position updates.", "configspace": "", "generation": 66, "fitness": 0.9710149377639095, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.009. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "85a65af4-bdcc-4100-9fa3-60a11ba84411", "metadata": {"aucs": [0.9780008409284597, 0.9770361789477524, 0.9580077934155162], "final_y": [0.16490439719249117, 0.16504833806020003, 0.16691639864101282]}, "mutation_prompt": null}
{"id": "c5d2ee40-c6c8-4e0d-a9ec-716822af6a77", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                # Updated inertia weight formula\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Update the inertia weight formula to enhance convergence speed while maintaining exploration and exploitation balance.", "configspace": "", "generation": 67, "fitness": 0.9725285731186467, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.973 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "85a65af4-bdcc-4100-9fa3-60a11ba84411", "metadata": {"aucs": [0.9781273355286104, 0.9776449525342812, 0.9618134312930484], "final_y": [0.16487555123786646, 0.1649905980452694, 0.16646021522469823]}, "mutation_prompt": null}
{"id": "8b2a342d-afeb-4608-a76a-166216e88f6d", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                # Updated inertia weight formula\n                inertia_weight = 0.7 + 0.3 * np.cos(np.pi * evaluations / (2 * self.budget))  # Changed sine to cosine\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Slightly enhance the exploration capability by adjusting the velocity update based on cosine instead of sine function.", "configspace": "", "generation": 68, "fitness": 0.9709983482529138, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.009. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "c5d2ee40-c6c8-4e0d-a9ec-716822af6a77", "metadata": {"aucs": [0.9780008182989364, 0.97703610034289, 0.9579581261169148], "final_y": [0.16490440030829068, 0.16504833732792257, 0.1669745896834448]}, "mutation_prompt": null}
{"id": "a04e0744-097f-4525-8f78-ffc9bfdbe52e", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                # Updated inertia weight formula\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget))\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / self.budget)\n            self.c2 = 0.5 + 1.5 * np.cos(np.pi * evaluations / (2 * self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Fine-tune cognitive and social coefficients adaptation to balance exploration and exploitation more effectively.", "configspace": "", "generation": 69, "fitness": 0.9723720208609398, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "c5d2ee40-c6c8-4e0d-a9ec-716822af6a77", "metadata": {"aucs": [0.9781274997926086, 0.9771751333591749, 0.9618134294310358], "final_y": [0.1648755567945881, 0.16505001818853393, 0.16646021472552786]}, "mutation_prompt": null}
{"id": "d34448b7-82b0-46a8-a5b8-0995fc14ecef", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                # Updated inertia weight formula\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.01 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce inertia weight decay based on time-varying functions to enhance adaptability during optimization. ", "configspace": "", "generation": 70, "fitness": 0.9725286799838809, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.973 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "c5d2ee40-c6c8-4e0d-a9ec-716822af6a77", "metadata": {"aucs": [0.9781273075290136, 0.977644976236709, 0.9618137561859206], "final_y": [0.16487555048470925, 0.16499059889633427, 0.1664601646733609]}, "mutation_prompt": null}
{"id": "84baaba9-d144-4e14-b6f6-bd11ba063ae0", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        # Change: Use a dynamic adaptive factor for the quantum position update\n        adaptive_alpha = 0.5 + 0.5 * np.sin(np.pi * self.budget / 2) * np.exp(-0.01 * self.budget)\n        return position + adaptive_alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                # Updated inertia weight formula\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.01 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance the quantum position update mechanism by introducing a dynamic adaptive factor based on time-varying functions.", "configspace": "", "generation": 71, "fitness": 0.9649008199653609, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.009. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "d34448b7-82b0-46a8-a5b8-0995fc14ecef", "metadata": {"aucs": [0.9750989342341826, 0.9664806619971309, 0.953122863664769], "final_y": [0.16493620072700355, 0.16636373690298278, 0.1665764060470426]}, "mutation_prompt": null}
{"id": "e9e6ebb4-d2c4-4590-9638-51b7e975a549", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i]) * np.random.rand()\n\n                # Updated inertia weight formula\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.01 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                # Dynamic reduction of the adaptive factor\n                self.alpha = max(0.1, 0.5 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a dynamic reduction of the adaptive factor to enhance exploration-exploitation balance.", "configspace": "", "generation": 72, "fitness": 0.9581274000265464, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.958 with standard deviation 0.011. And the mean value of best solutions found was 0.170 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "d34448b7-82b0-46a8-a5b8-0995fc14ecef", "metadata": {"aucs": [0.9712604026506753, 0.9591701469986453, 0.9439516504303186], "final_y": [0.1669745310573303, 0.17112154859553041, 0.17277852614405853]}, "mutation_prompt": null}
{"id": "18dade6d-cf30-4032-9249-20d56f39d102", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return population[neighbors_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.01 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce adaptive memory mechanism and dynamic neighborhood topology to enhance exploration and exploitation balance.", "configspace": "", "generation": 73, "fitness": 0.9740232504513929, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d34448b7-82b0-46a8-a5b8-0995fc14ecef", "metadata": {"aucs": [0.9735989403805617, 0.9742700014818817, 0.9742008094917356], "final_y": [0.16489941417736487, 0.1648871900700425, 0.16498787035868523]}, "mutation_prompt": null}
{"id": "931574c0-2bfd-4e39-929d-863fa1c45684", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return population[neighbors_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.01 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n\n            inertia_weight = 0.9 * (1 - evaluations / self.budget) + 0.4 * (global_best_score / (1 + global_best_score))  # This is the modified line\n            \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Improve convergence by dynamically adjusting inertia weight based on the best particle's improvements.", "configspace": "", "generation": 74, "fitness": 0.9740232504513929, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "18dade6d-cf30-4032-9249-20d56f39d102", "metadata": {"aucs": [0.9735989403805617, 0.9742700014818817, 0.9742008094917356], "final_y": [0.16489941417736487, 0.1648871900700425, 0.16498787035868523]}, "mutation_prompt": null}
{"id": "d5e18112-c07b-4f49-884b-6172bdfde4f0", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.35  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        factor = self.alpha * np.random.uniform(-1, 1, size=self.dim)\n        return np.clip(position + factor, -1, 1)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return population[neighbors_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.01 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance convergence by adjusting the adaptive memory factor and improving the quantum position update.", "configspace": "", "generation": 75, "fitness": 0.4789352372462548, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.479 with standard deviation 0.038. And the mean value of best solutions found was 0.420 (0. is the best) with standard deviation 0.030.", "error": "", "parent_id": "18dade6d-cf30-4032-9249-20d56f39d102", "metadata": {"aucs": [0.4271535091871955, 0.5166455307169813, 0.4930066718345877], "final_y": [0.4602115170857748, 0.38906842988320345, 0.41021886550440834]}, "mutation_prompt": null}
{"id": "fe5d09c1-5495-4bad-bee2-f2ca8c967348", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return population[neighbors_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.01 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget)**2)  # Changed line\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a non-linear decay factor to enhance adaptive memory and improve convergence speed.", "configspace": "", "generation": 76, "fitness": 0.973857348231037, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "18dade6d-cf30-4032-9249-20d56f39d102", "metadata": {"aucs": [0.9733430780532052, 0.9741599102104899, 0.9740690564294161], "final_y": [0.16487683251530316, 0.16488138079073678, 0.164878295937966]}, "mutation_prompt": null}
{"id": "61304330-a903-4049-aec3-63bca6b70623", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return population[neighbors_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.8 + 0.2 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.01 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -0.5, 0.5)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.2, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Refine adaptive factors and enhance velocity dynamics to improve convergence speed and solution quality.", "configspace": "", "generation": 77, "fitness": 0.973891947869685, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "18dade6d-cf30-4032-9249-20d56f39d102", "metadata": {"aucs": [0.9736324988373555, 0.9749292134418585, 0.973114131329841], "final_y": [0.16489497455833602, 0.16486624460905108, 0.16507723355983417]}, "mutation_prompt": null}
{"id": "f6d78ec5-69e5-4db2-9a9a-477c29bab925", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        dynamic_r = int(r * (1 + 0.5 * np.sin(2 * np.pi * idx / self.population_size)))\n        neighbors_idx = (idx + np.random.randint(-dynamic_r, dynamic_r + 1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return population[neighbors_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.01 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i] * np.cos(np.pi * i / self.population_size), -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n\n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance AQPSO by introducing a non-linear dynamic topology range variation and adaptive velocity scaling for better convergence.", "configspace": "", "generation": 78, "fitness": 0.9671424445055519, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.967 with standard deviation 0.003. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "18dade6d-cf30-4032-9249-20d56f39d102", "metadata": {"aucs": [0.9650063042835898, 0.9646670404135054, 0.9717539888195605], "final_y": [0.16698031129020385, 0.16711460708011283, 0.16570526032762878]}, "mutation_prompt": null}
{"id": "8b6c7b6c-795e-4873-845c-fa723379987e", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        # Modified line for improved exploration\n        position = (1 - beta) * personal_best + beta * global_best + self.alpha * np.random.uniform(-0.5, 0.5, size=self.dim)\n        return position\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return population[neighbors_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.01 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance the convergence by adjusting the quantum position update formula to encourage better exploration.", "configspace": "", "generation": 79, "fitness": 0.9706309600793454, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.001. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "18dade6d-cf30-4032-9249-20d56f39d102", "metadata": {"aucs": [0.9710760860508963, 0.9721317158499719, 0.9686850783371679], "final_y": [0.16576851428424433, 0.16559277538099237, 0.1667514049589116]}, "mutation_prompt": null}
{"id": "ae2b7dd5-8f62-41ef-b092-01e3c3c59fcb", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.01 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance dynamic topology and quantum update for improved exploration and exploitation balance.", "configspace": "", "generation": 80, "fitness": 0.9744808654048686, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "18dade6d-cf30-4032-9249-20d56f39d102", "metadata": {"aucs": [0.9735866040645009, 0.9753973305598416, 0.9744586615902632], "final_y": [0.16491903965914123, 0.16485696029085917, 0.16495658363828947]}, "mutation_prompt": null}
{"id": "d124d572-59a6-42aa-ba2e-c4edf2d9014b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / self.budget) ** 2  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a non-linear dynamic adjustment to the inertia weight for enhanced adaptability.", "configspace": "", "generation": 81, "fitness": 0.9744808560897522, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ae2b7dd5-8f62-41ef-b092-01e3c3c59fcb", "metadata": {"aucs": [0.9735865646821865, 0.9753973413831922, 0.9744586622038778], "final_y": [0.16491904412179437, 0.16485696018418972, 0.1649565834361456]}, "mutation_prompt": null}
{"id": "05b0f17c-0db7-4bd3-80e5-9b25ff203ca3", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(4, i, population)  # Changed line\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.01 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocity_bound = 0.5 * (ub - lb)  # Changed line\n                velocities[i] = np.clip(velocities[i], -velocity_bound, velocity_bound)  # Changed line\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce adaptive velocity bounds and dynamic neighbor topology to AQPSO for enhanced convergence.", "configspace": "", "generation": 82, "fitness": 0.9645643765226996, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.015. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "ae2b7dd5-8f62-41ef-b092-01e3c3c59fcb", "metadata": {"aucs": [0.9431889398773924, 0.9782425114059458, 0.9722616782847607], "final_y": [0.16835264411347728, 0.1650754172080131, 0.1654080801256136]}, "mutation_prompt": null}
{"id": "6d057a1d-09d9-458e-b80b-ff10a9e2b6cf", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.01 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                if evaluations % 10 == 0:\n                    velocities[i] += 0.05 * np.random.uniform(-1, 1, size=self.dim)  # Exploration boost\n\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.5:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a periodic exploration boost to the velocity component for enhanced exploration without disrupting current balance.", "configspace": "", "generation": 83, "fitness": 0.9084507169897789, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.908 with standard deviation 0.045. And the mean value of best solutions found was 0.181 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "ae2b7dd5-8f62-41ef-b092-01e3c3c59fcb", "metadata": {"aucs": [0.9709844078002798, 0.8899707699995741, 0.8643969731694827], "final_y": [0.1662586334034617, 0.18573201386274185, 0.1916722364227107]}, "mutation_prompt": null}
{"id": "e7df617a-6b1c-4031-ae5b-da2b45584a22", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.6 + 0.4 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.6:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce adaptive inertia and quantum update frequency to enhance exploration-exploitation balance.", "configspace": "", "generation": 84, "fitness": 0.9775534754444989, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "ae2b7dd5-8f62-41ef-b092-01e3c3c59fcb", "metadata": {"aucs": [0.9783671040652882, 0.9834446146268407, 0.9708487076413677], "final_y": [0.16486179383120847, 0.1649341621765269, 0.16619284158683545]}, "mutation_prompt": null}
{"id": "d01f847c-40c2-459f-b477-1609dd1488f3", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.cos(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.6:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Incorporate adaptive social coefficient and inertia weight for dynamic exploration-exploitation balancing.", "configspace": "", "generation": 85, "fitness": 0.9775716737842627, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e7df617a-6b1c-4031-ae5b-da2b45584a22", "metadata": {"aucs": [0.9783671422588188, 0.9834934021296536, 0.9708544769643155], "final_y": [0.16486180378897397, 0.16491882395962343, 0.1661919619895844]}, "mutation_prompt": null}
{"id": "803890aa-7462-4f73-8dae-332edd74efbb", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.cos(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.6:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n                self.memory_factor = 0.3 + 0.2 * np.sin(np.pi * evaluations / (2 * self.budget))  # Modified line\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce time-varying memory factors to refine exploration-exploitation trade-off dynamically.", "configspace": "", "generation": 86, "fitness": 0.9775716737842627, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "d01f847c-40c2-459f-b477-1609dd1488f3", "metadata": {"aucs": [0.9783671422588188, 0.9834934021296536, 0.9708544769643155], "final_y": [0.16486180378897397, 0.16491882395962343, 0.1661919619895844]}, "mutation_prompt": null}
{"id": "a2c2fca7-54e9-4239-ab6c-e3dd4c8bae49", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.normal(0, 0.5, size=self.dim)  # Changed from uniform to normal distribution\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = np.random.choice(range(self.population_size), size=2, replace=False)  # Changed selection of neighbors\n        return 0.5 * population[neighbors_idx[0]] + 0.5 * population[neighbors_idx[1]]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.cos(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.6:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce Gaussian perturbation and adaptive topology for enhanced exploration and exploitation.", "configspace": "", "generation": 87, "fitness": 0.9326571993315813, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.933 with standard deviation 0.027. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "d01f847c-40c2-459f-b477-1609dd1488f3", "metadata": {"aucs": [0.9479633111121684, 0.8942631142667301, 0.9557451726158457], "final_y": [0.17045517936212073, 0.1875192623173091, 0.16893456683797214]}, "mutation_prompt": null}
{"id": "27885200-5737-4d6e-8cf8-9e04832fd500", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.cos(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocity_clamp = np.clip(velocities[i], -0.5, 0.5)  # Adaptive velocity clamping\n                if np.random.rand() < 0.6:\n                    particle_position = population[i] + velocity_clamp  # Changed from velocities[i] to velocity_clamp\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                if np.random.rand() < 0.05:  # Introduce perturbation\n                    particle_position += np.random.uniform(-0.1, 0.1, size=self.dim)\n\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce adaptive velocity clamping and perturbation mechanism to enhance exploration and exploitation balance.", "configspace": "", "generation": 88, "fitness": 0.9638384850613115, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.005. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "d01f847c-40c2-459f-b477-1609dd1488f3", "metadata": {"aucs": [0.9703466389198431, 0.9622844809307577, 0.9588843353333338], "final_y": [0.16534490583910078, 0.16731778004368347, 0.16838668372727583]}, "mutation_prompt": null}
{"id": "5d59ba09-c169-4dd7-8b39-94e36ac7b9bf", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.9 + 0.1 * np.cos(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)  # Changed 0.7 to 0.9\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.55:  # Changed 0.6 to 0.55\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.6 + 0.4 * np.cos(np.pi * evaluations / self.budget))  # Changed 0.5 to 0.6\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance exploration-exploitation balance by refining velocity update and memory strategies.", "configspace": "", "generation": 89, "fitness": 0.9562162946797207, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.956 with standard deviation 0.021. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "d01f847c-40c2-459f-b477-1609dd1488f3", "metadata": {"aucs": [0.958215088039432, 0.9804511171031007, 0.9299826788966294], "final_y": [0.16663727757531344, 0.164858825613041, 0.17323194094640082]}, "mutation_prompt": null}
{"id": "3eb403ec-cf96-4127-8bcf-f0630b6a3da5", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.9 - 0.5 * np.cos(np.pi * evaluations / (2 * self.budget))  # Change for improved balance\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.6:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce dynamic coefficients and adapt inertia weight for improved convergence and exploration-exploitation balance.", "configspace": "", "generation": 90, "fitness": 0.9775584266844137, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "d01f847c-40c2-459f-b477-1609dd1488f3", "metadata": {"aucs": [0.9783670448481486, 0.9834630043534667, 0.970845230851626], "final_y": [0.16486177899514032, 0.16492572396994232, 0.16619115938217022]}, "mutation_prompt": null}
{"id": "68f13691-97bd-4c7b-9abe-bce6e4b6ec58", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.cos(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.7:  # Change 1\n                    particle_position = population[i] + velocities[i] * self.memory_factor  # Change 2\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n                self.memory_factor = 0.2 + 0.5 * np.sin(np.pi * evaluations / self.budget)  # Change 3\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance AQPSO by introducing a dynamic memory factor and refined particle position updates to improve convergence.", "configspace": "", "generation": 91, "fitness": 0.9590360253454993, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.010. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "d01f847c-40c2-459f-b477-1609dd1488f3", "metadata": {"aucs": [0.9655888108774701, 0.9663629973838406, 0.9451562677751871], "final_y": [0.16565174967686314, 0.1672919868046986, 0.16956463029037105]}, "mutation_prompt": null}
{"id": "73d095a0-4b90-41f0-b92f-b8b67ef8da4d", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.cos(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.6:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score))\n        \n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance AQPSO by adjusting cognitive and social coefficients adaptively for improved convergence.", "configspace": "", "generation": 92, "fitness": 0.9775716737842627, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "d01f847c-40c2-459f-b477-1609dd1488f3", "metadata": {"aucs": [0.9783671422588188, 0.9834934021296536, 0.9708544769643155], "final_y": [0.16486180378897397, 0.16491882395962343, 0.1661919619895844]}, "mutation_prompt": null}
{"id": "dc08395d-731f-4e49-af9a-fdb738ff805b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.cos(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.6:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score)) * (1 - np.mean(scores) / (1 + np.mean(scores)))\n\n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance AQPSO by introducing dynamic social coefficient adjustment based on relative particle performance.", "configspace": "", "generation": 93, "fitness": 0.9775717046194531, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "d01f847c-40c2-459f-b477-1609dd1488f3", "metadata": {"aucs": [0.9783671426506877, 0.9834934021292646, 0.9708545690784065], "final_y": [0.16486180386293636, 0.16491882395971869, 0.1661919433267176]}, "mutation_prompt": null}
{"id": "97ab4239-fb28-4960-9226-49dbc516c163", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.6:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score)) * (1 - np.mean(scores) / (1 + np.mean(scores)))\n\n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a sinusoidal adjustment to the inertia weight for improved balance between exploration and exploitation.", "configspace": "", "generation": 94, "fitness": 0.9775729569636328, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "dc08395d-731f-4e49-af9a-fdb738ff805b", "metadata": {"aucs": [0.9783671342458196, 0.9834977405596155, 0.9708539960854633], "final_y": [0.16486180171712628, 0.164918201924367, 0.16619198725838313]}, "mutation_prompt": null}
{"id": "4ea67db3-5e7d-4aa6-86f6-b709f188af3a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.6:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / (self.budget / 2)))  # Changed line\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score)) * (1 - np.mean(scores) / (1 + np.mean(scores)))\n\n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance the adaptive factor modulation based on evaluations to improve exploration-exploitation balance.", "configspace": "", "generation": 95, "fitness": 0.9757360225511204, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "97ab4239-fb28-4960-9226-49dbc516c163", "metadata": {"aucs": [0.9777142821380768, 0.9813715851346851, 0.968122200380599], "final_y": [0.1649086540357826, 0.16505396380757864, 0.1658213469721569]}, "mutation_prompt": null}
{"id": "22b46b21-6bc6-461b-a4f3-2bd1e831f3d7", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.6:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score)) * np.std(personal_best_scores) / (1 + np.std(personal_best_scores))  # Changed line\n\n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce a dynamic adjustment to the social coefficient for improved adaptability in diverse optimization landscapes.", "configspace": "", "generation": 96, "fitness": 0.977572983319392, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "97ab4239-fb28-4960-9226-49dbc516c163", "metadata": {"aucs": [0.9783671346085555, 0.9834977405247441, 0.9708540748248764], "final_y": [0.16486180178520127, 0.16491820201302088, 0.16619197143162057]}, "mutation_prompt": null}
{"id": "eb9c953f-5828-4984-93be-0b60cb7ace38", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -np.abs(ub-lb)/2, np.abs(ub-lb)/2)  # Changed line\n\n                if np.random.rand() < 0.6:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score)) * np.std(personal_best_scores) / (1 + np.std(personal_best_scores))\n\n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Introduce adaptive velocity clamping for improved convergence and exploration balance.", "configspace": "", "generation": 97, "fitness": 0.9614272240499737, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.961 with standard deviation 0.016. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "22b46b21-6bc6-461b-a4f3-2bd1e831f3d7", "metadata": {"aucs": [0.9536601231717795, 0.9842833755624005, 0.9463381734157411], "final_y": [0.1692731637841639, 0.1648610133351237, 0.17082709085168213]}, "mutation_prompt": null}
{"id": "bf7a958c-39a9-4cf6-ab05-0b56bfaafed6", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def levy_flight(self, L):\n        u = np.random.normal(0, 1, self.dim) * (1 / np.power(np.random.gamma(1, 1), 1 / L))\n        return u * np.random.uniform(-1, 1, self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.6:\n                    particle_position = population[i] + velocities[i] + self.levy_flight(1.5)  # Changed line\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score)) * np.std(personal_best_scores) / (1 + np.std(personal_best_scores))  # Changed line\n\n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Integrate a Lévy flight mechanism to enhance exploration capabilities in AQPSO for diverse optimization landscapes.", "configspace": "", "generation": 98, "fitness": 0.9460442072675307, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.946 with standard deviation 0.017. And the mean value of best solutions found was 0.170 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "22b46b21-6bc6-461b-a4f3-2bd1e831f3d7", "metadata": {"aucs": [0.9373848775253741, 0.9697320775849055, 0.9310156666923124], "final_y": [0.1695602430975649, 0.16547352490194278, 0.17518136943537155]}, "mutation_prompt": null}
{"id": "a800294b-deda-4bf0-a21b-d57e5de5d367", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 10)\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.alpha = 0.5  # adaptive factor\n        self.memory_factor = 0.3  # for adaptive memory\n\n    def quantum_position_update(self, particle, personal_best, global_best):\n        beta = np.random.rand(self.dim)\n        position = (1 - beta) * personal_best + beta * global_best\n        return position + self.alpha * np.random.uniform(-1, 1, size=self.dim)\n\n    def dynamic_topology(self, r, idx, population):\n        neighbors_idx = (idx + np.random.randint(-r, r+1)) % self.population_size\n        neighbors_idx = max(0, min(neighbors_idx, self.population_size - 1))\n        return 0.5 * population[neighbors_idx] + 0.5 * population[(neighbors_idx + 1) % self.population_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = population.copy()\n        scores = np.array([func(ind) for ind in population])\n        personal_best_scores = scores.copy()\n\n        evaluations = self.population_size\n        best_idx = scores.argmin()\n        global_best_position = population[best_idx].copy()\n        global_best_score = scores[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                neighbor = self.dynamic_topology(3, i, population)\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - neighbor) * np.random.rand()\n\n                inertia_weight = 0.7 + 0.3 * np.sin(np.pi * evaluations / (2 * self.budget)) * np.exp(-0.02 * evaluations)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -1, 1)\n\n                if np.random.rand() < 0.6:\n                    particle_position = population[i] + velocities[i]\n                else:\n                    particle_position = self.quantum_position_update(population[i], personal_best_positions[i], global_best_position)\n\n                particle_position = np.clip(particle_position, lb, ub)\n                particle_score = func(particle_position)\n                evaluations += 1\n\n                self.alpha = max(0.1, 0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget))\n\n                if particle_score < personal_best_scores[i]:\n                    personal_best_positions[i] = particle_position\n                    personal_best_scores[i] = particle_score\n                    if particle_score < global_best_score:\n                        global_best_position = particle_position.copy()\n                        global_best_score = particle_score\n\n                if evaluations >= self.budget:\n                    break\n\n            self.c1 = 1.5 + 0.5 * np.sin(np.pi * evaluations / (2 * self.budget))\n            self.c2 = 0.5 + 1.5 * (1 - np.cos(np.pi * evaluations / self.budget)) * (1 - global_best_score / (1 + global_best_score)) * np.std(personal_best_scores) / (1 + np.std(personal_best_scores)) * (1 + np.std(population))  # Changed line\n\n        return global_best_position, global_best_score", "name": "AQPSO", "description": "Enhance adaptive adjustments by integrating solution spread awareness into velocity updates.", "configspace": "", "generation": 99, "fitness": 0.9775727726483344, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "22b46b21-6bc6-461b-a4f3-2bd1e831f3d7", "metadata": {"aucs": [0.9783671273253326, 0.9834977404894143, 0.9708534501302559], "final_y": [0.16486180035459996, 0.16491820207028207, 0.16619198647927524]}, "mutation_prompt": null}
