{"id": "0c13c61a-4f48-4b82-87b4-e4a962a1ae84", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.inertia_weight = 0.5\n        self.cognitive_constant = 1.5\n        self.social_constant = 1.5\n        self.temperature_initial = 1.0\n        self.temperature_final = 0.01\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.zeros((self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_velocity_position(self, bounds):\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n        cognitive_velocity = self.cognitive_constant * r1 * (self.personal_best_positions - self.positions)\n        social_velocity = self.social_constant * r2 * (self.global_best_position - self.positions)\n        self.velocities = (self.inertia_weight * self.velocities) + cognitive_velocity + social_velocity\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)\n\n    def simulated_annealing(self, position, score, bounds):\n        temperature = self.temperature_initial\n        while temperature > self.temperature_final:\n            new_position = position + np.random.normal(0, 0.1, self.dim)\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = self.func(new_position)\n            if new_score < score or np.random.rand() < np.exp((score - new_score) / temperature):\n                position, score = new_position, new_score\n            temperature *= 0.9\n        return position, score\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            self.update_velocity_position(bounds)\n\n            # Apply simulated annealing on global best position\n            self.global_best_position, self.global_best_score = self.simulated_annealing(self.global_best_position, self.global_best_score, bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "HybridPSO_SA", "description": "A hybrid metaheuristic combining Particle Swarm Optimization (PSO) and Simulated Annealing (SA) for efficient exploration and exploitation in black box optimization.", "configspace": "", "generation": 0, "fitness": 0.2461323882393327, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.246 with standard deviation 0.071. And the mean value of best solutions found was 0.089 (0. is the best) with standard deviation 0.059.", "error": "", "parent_id": null, "metadata": {"aucs": [0.34517480931942157, 0.18288104247148407, 0.21034131292709246], "final_y": [0.008206772293892476, 0.11239408035345083, 0.14787866933848046]}, "mutation_prompt": null}
{"id": "c22432f4-b4ef-474d-af48-d262c437bb8b", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.inertia_weight_initial = 0.9  # Changed from 0.5 to 0.9\n        self.inertia_weight_final = 0.4\n        self.cognitive_constant = 1.5\n        self.social_constant = 1.5\n        self.temperature_initial = 1.0\n        self.temperature_final = 0.01\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.zeros((self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_velocity_position(self, bounds):\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n        # Dynamically adjust inertia weight\n        inertia_weight = self.inertia_weight_final + (self.inertia_weight_initial - self.inertia_weight_final) * (self.budget - evaluations) / self.budget\n        cognitive_velocity = self.cognitive_constant * r1 * (self.personal_best_positions - self.positions)\n        social_velocity = self.social_constant * r2 * (self.global_best_position - self.positions)\n        self.velocities = (inertia_weight * self.velocities) + cognitive_velocity + social_velocity\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)\n\n    def simulated_annealing(self, position, score, bounds):\n        temperature = self.temperature_initial\n        while temperature > self.temperature_final:\n            new_position = position + np.random.normal(0, 0.1, self.dim)\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = self.func(new_position)\n            if new_score < score or np.random.rand() < np.exp((score - new_score) / temperature):\n                position, score = new_position, new_score\n            temperature *= 0.9\n        return position, score\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            self.update_velocity_position(bounds)\n\n            # Apply simulated annealing on global best position\n            self.global_best_position, self.global_best_score = self.simulated_annealing(self.global_best_position, self.global_best_score, bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "HybridPSO_SA", "description": "A hybrid metaheuristic combining Particle Swarm Optimization (PSO) and Simulated Annealing (SA) with a dynamic inertia weight for adaptive exploration and exploitation in black box optimization.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "0c13c61a-4f48-4b82-87b4-e4a962a1ae84", "metadata": {}, "mutation_prompt": null}
{"id": "2e9697b3-159c-4ed1-acdc-bccb4bf0a2b1", "solution": "import numpy as np\n\nclass EnhancedHybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.inertia_weight_initial = 0.9\n        self.inertia_weight_final = 0.4\n        self.cognitive_constant = 1.5\n        self.social_constant = 1.5\n        self.temperature_initial = 1.0\n        self.temperature_final = 0.01\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.zeros((self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_velocity_position(self, bounds, iter_count, max_iter):\n        inertia_weight = self.inertia_weight_initial - (\n            (self.inertia_weight_initial - self.inertia_weight_final) * (iter_count / max_iter))\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n        cognitive_velocity = self.cognitive_constant * r1 * (self.personal_best_positions - self.positions)\n        social_velocity = self.social_constant * r2 * (self.global_best_position - self.positions)\n        self.velocities = (inertia_weight * self.velocities) + cognitive_velocity + social_velocity\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)\n\n    def adaptive_simulated_annealing(self, position, score, bounds, iter_count, max_iter):\n        temperature = self.temperature_initial * (1 - iter_count / max_iter)\n        new_position = position + np.random.normal(0, 0.1, self.dim)\n        new_position = np.clip(new_position, bounds.lb, bounds.ub)\n        new_score = self.func(new_position)\n        if new_score < score or np.random.rand() < np.exp((score - new_score) / temperature):\n            return new_position, new_score\n        return position, score\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        iter_count = 0\n        max_iter = self.budget // self.num_particles\n\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            self.update_velocity_position(bounds, iter_count, max_iter)\n\n            # Apply adaptive simulated annealing on global best position\n            self.global_best_position, self.global_best_score = self.adaptive_simulated_annealing(\n                self.global_best_position, self.global_best_score, bounds, iter_count, max_iter)\n\n            iter_count += 1\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedHybridPSO_SA", "description": "Enhanced HybridPSO_SA incorporates dynamic inertia weight and adaptive simulated annealing temperature to improve exploration-exploitation balance and convergence rate in black box optimization.", "configspace": "", "generation": 2, "fitness": 0.2275743380136086, "feedback": "The algorithm EnhancedHybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.228 with standard deviation 0.016. And the mean value of best solutions found was 0.073 (0. is the best) with standard deviation 0.063.", "error": "", "parent_id": "0c13c61a-4f48-4b82-87b4-e4a962a1ae84", "metadata": {"aucs": [0.24565749174120755, 0.20721461783210304, 0.22985090446751522], "final_y": [0.034569113241341265, 0.022787150284153776, 0.16264577505772032]}, "mutation_prompt": null}
{"id": "f51a9cce-3a0f-444e-94f7-cdfceb9b9019", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.inertia_weight = 0.9  # changed\n        self.cognitive_constant = 1.5\n        self.social_constant = 1.5\n        self.temperature_initial = 1.0\n        self.temperature_final = 0.01\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.zeros((self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_velocity_position(self, bounds):\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n        cognitive_velocity = self.cognitive_constant * r1 * (self.personal_best_positions - self.positions)\n        social_velocity = self.social_constant * r2 * (self.global_best_position - self.positions)\n        # changed inertia_weight to adapt over evaluations\n        inertia_weight = 0.9 - 0.5 * (self.evaluations / self.budget)\n        self.velocities = (inertia_weight * self.velocities) + cognitive_velocity + social_velocity\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)\n\n    def simulated_annealing(self, position, score, bounds):\n        temperature = self.temperature_initial\n        while temperature > self.temperature_final:\n            new_position = position + np.random.normal(0, 0.1, self.dim)\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = self.func(new_position)\n            if new_score < score or np.random.rand() < np.exp((score - new_score) / temperature):\n                position, score = new_position, new_score\n            temperature *= 0.9\n        return position, score\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        self.evaluations = 0\n        while self.evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                self.evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            self.update_velocity_position(bounds)\n\n            # Apply simulated annealing on global best position\n            self.global_best_position, self.global_best_score = self.simulated_annealing(self.global_best_position, self.global_best_score, bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "HybridPSO_SA", "description": "Improved HybridPSO_SA by integrating adaptive inertia weight to balance exploration and exploitation dynamically.", "configspace": "", "generation": 3, "fitness": 0.20784748538932943, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.208 with standard deviation 0.062. And the mean value of best solutions found was 0.820 (0. is the best) with standard deviation 0.932.", "error": "", "parent_id": "0c13c61a-4f48-4b82-87b4-e4a962a1ae84", "metadata": {"aucs": [0.1308490167666312, 0.20974245431561866, 0.28295098508573846], "final_y": [2.1324671320206385, 0.2715296302825984, 0.05676415144765509]}, "mutation_prompt": null}
{"id": "b1a2d8eb-836b-4aec-b232-64b62971d2c8", "solution": "import numpy as np\n\nclass QuantumInspiredDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 40\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.quantum_factor = 0.05\n\n    def initialize(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.pop_size, self.dim))\n        self.scores = np.full(self.pop_size, float('inf'))\n        self.best_idx = 0\n        self.best_score = float('inf')\n\n    def quantum_update(self, candidate, bounds):\n        quantum_shift = self.quantum_factor * (2 * np.random.random(self.dim) - 1)\n        new_candidate = candidate + quantum_shift\n        return np.clip(new_candidate, bounds.lb, bounds.ub)\n\n    def mutate_and_crossover(self, target_idx, bounds):\n        indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = self.quantum_update(mutant, bounds)\n        trial = np.copy(self.population[target_idx])\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial[crossover_mask] = mutant[crossover_mask]\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                trial = self.mutate_and_crossover(i, bounds)\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.scores[i]:\n                    self.population[i] = trial\n                    self.scores[i] = trial_score\n                    if trial_score < self.best_score:\n                        self.best_score = trial_score\n                        self.best_idx = i\n\n        best_position = self.population[self.best_idx]\n        return best_position, self.best_score", "name": "QuantumInspiredDE", "description": "Quantum-inspired Differential Evolution (QIDE) combines Differential Evolution with quantum-inspired updates for enhanced global exploration in high-dimensional optimization problems.", "configspace": "", "generation": 4, "fitness": 0.22414332422101424, "feedback": "The algorithm QuantumInspiredDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.224 with standard deviation 0.012. And the mean value of best solutions found was 0.045 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "0c13c61a-4f48-4b82-87b4-e4a962a1ae84", "metadata": {"aucs": [0.2185347652240166, 0.21302657676091707, 0.24086863067810904], "final_y": [0.06424600805412797, 0.028233014470003467, 0.04167938503838694]}, "mutation_prompt": null}
{"id": "9b0039f8-7101-40e5-85c4-c548b2fdbe3e", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.inertia_weight = 0.9  # Increased initial inertia weight\n        self.cognitive_constant = 1.5\n        self.social_constant = 1.5\n        self.temperature_initial = 1.0\n        self.temperature_final = 0.01\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.zeros((self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_velocity_position(self, bounds):\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n        cognitive_velocity = self.cognitive_constant * r1 * (self.personal_best_positions - self.positions)\n        social_velocity = self.social_constant * r2 * (self.global_best_position - self.positions)\n        self.velocities = (self.inertia_weight * self.velocities) + cognitive_velocity + social_velocity\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)\n        self.inertia_weight *= 0.99  # Dynamic inertia weight adjustment\n\n    def simulated_annealing(self, position, score, bounds):\n        temperature = self.temperature_initial\n        while temperature > self.temperature_final:\n            new_position = position + np.random.normal(0, 0.1, self.dim)\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = self.func(new_position)\n            if new_score < score or np.random.rand() < np.exp((score - new_score) / temperature):\n                position, score = new_position, new_score\n            temperature *= 0.95  # Adaptive temperature decay\n        return position, score\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            self.update_velocity_position(bounds)\n\n            # Apply simulated annealing on global best position\n            self.global_best_position, self.global_best_score = self.simulated_annealing(self.global_best_position, self.global_best_score, bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "HybridPSO_SA", "description": "Enhanced HybridPSO_SA by dynamically adjusting inertia weight and utilizing adaptive temperature decay in Simulated Annealing.", "configspace": "", "generation": 5, "fitness": 0.2522983129418043, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.252 with standard deviation 0.036. And the mean value of best solutions found was 0.101 (0. is the best) with standard deviation 0.087.", "error": "", "parent_id": "0c13c61a-4f48-4b82-87b4-e4a962a1ae84", "metadata": {"aucs": [0.24200127074039268, 0.3006013059195698, 0.21429236216545045], "final_y": [0.05273607479536632, 0.026565710464315922, 0.22221639312498204]}, "mutation_prompt": null}
{"id": "f499ce75-1cbf-4755-8b47-533fcfd2875c", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.inertia_weight = 0.9\n        self.cognitive_constant = 1.5\n        self.social_constant = 1.5\n        self.temperature_initial = 1.0\n        self.temperature_final = 0.01\n        self.restart_threshold = 10  # New: Restart threshold\n        \n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.zeros((self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n        self.stagnation_counter = 0  # New: Stagnation counter\n\n    def update_velocity_position(self, bounds):\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n        cognitive_velocity = self.cognitive_constant * r1 * (self.personal_best_positions - self.positions)\n        social_velocity = self.social_constant * r2 * (self.global_best_position - self.positions)\n        self.velocities = (self.inertia_weight * self.velocities) + cognitive_velocity + social_velocity\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)\n        self.inertia_weight *= 0.99\n\n    def simulated_annealing(self, position, score, bounds):\n        temperature = self.temperature_initial\n        while temperature > self.temperature_final:\n            new_position = position + np.random.normal(0, 0.1, self.dim)\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = self.func(new_position)\n            if new_score < score or np.random.rand() < np.exp((score - new_score) / temperature):\n                position, score = new_position, new_score\n            temperature *= 0.9  # Modified: Faster adaptive temperature decay\n        return position, score\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0  # New: Reset counter on improvement\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            self.update_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:  # New: Check stagnation\n                self.initialize(bounds)  # New: Random restart\n\n            self.global_best_position, self.global_best_score = self.simulated_annealing(self.global_best_position, self.global_best_score, bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "HybridPSO_SA", "description": "Enhanced HybridPSO_SA by introducing diversity maintenance through random restarts and adaptive cooling schedule in Simulated Annealing.", "configspace": "", "generation": 6, "fitness": 0.2620395802126107, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.262 with standard deviation 0.044. And the mean value of best solutions found was 0.060 (0. is the best) with standard deviation 0.045.", "error": "", "parent_id": "9b0039f8-7101-40e5-85c4-c548b2fdbe3e", "metadata": {"aucs": [0.2309252869134214, 0.32433761650663984, 0.2308558372177708], "final_y": [0.1232421254057924, 0.02145097210213082, 0.0341991749155421]}, "mutation_prompt": null}
{"id": "526359fc-e24d-46eb-9bf1-728e7eb44ee0", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.inertia_weight = 0.9\n        self.cognitive_constant = 1.5\n        self.social_constant = 1.5\n        self.temperature_initial = 1.0\n        self.temperature_final = 0.01\n        self.restart_threshold = 10  # New: Restart threshold\n        \n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.zeros((self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n        self.stagnation_counter = 0  # New: Stagnation counter\n\n    def update_velocity_position(self, bounds):\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n        cognitive_velocity = self.cognitive_constant * r1 * (self.personal_best_positions - self.positions)\n        social_velocity = self.social_constant * r2 * (self.global_best_position - self.positions)\n        self.velocities = (self.inertia_weight * self.velocities) + cognitive_velocity + social_velocity\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)\n        self.inertia_weight *= 0.98  # Modified: Adjusted inertia weight decay\n\n    def simulated_annealing(self, position, score, bounds):\n        temperature = self.temperature_initial\n        while temperature > self.temperature_final:\n            new_position = position + np.random.normal(0, 0.1, self.dim)\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = self.func(new_position)\n            if new_score < score or np.random.rand() < np.exp((score - new_score) / temperature):\n                position, score = new_position, new_score\n            temperature *= 0.9  # Modified: Faster adaptive temperature decay\n        return position, score\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0  # New: Reset counter on improvement\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            self.update_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:  # New: Check stagnation\n                self.initialize(bounds)  # New: Random restart\n\n            self.global_best_position, self.global_best_score = self.simulated_annealing(self.global_best_position, self.global_best_score, bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "HybridPSO_SA", "description": "Enhanced HybridPSO_SA by adjusting inertia weight decay for improved exploration-exploitation balance.", "configspace": "", "generation": 7, "fitness": 0.22498137292794285, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.225 with standard deviation 0.049. And the mean value of best solutions found was 0.231 (0. is the best) with standard deviation 0.199.", "error": "", "parent_id": "f499ce75-1cbf-4755-8b47-533fcfd2875c", "metadata": {"aucs": [0.1711003278879637, 0.28933274484370863, 0.2145110460521562], "final_y": [0.498269736920434, 0.019217764035697645, 0.17601468170763096]}, "mutation_prompt": null}
{"id": "c5a6f928-bb30-498f-8445-5e755762b6e0", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.inertia_weight = 0.9\n        self.cognitive_constant = 1.5\n        self.social_constant = 1.5\n        self.temperature_initial = 1.0\n        self.temperature_final = 0.01\n        self.restart_threshold = 10\n        self.mutation_prob = 0.1  # New: Mutation probability\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.zeros((self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n        self.stagnation_counter = 0\n\n    def update_velocity_position(self, bounds):\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n        adaptive_inertia = 0.4 + np.random.rand() * 0.5  # New: Dynamic inertia\n        cognitive_velocity = self.cognitive_constant * r1 * (self.personal_best_positions - self.positions)\n        social_velocity = self.social_constant * r2 * (self.global_best_position - self.positions)\n        self.velocities = (adaptive_inertia * self.velocities) + cognitive_velocity + social_velocity\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)\n\n    def mutation(self, position, bounds):  # New: Mutation function\n        if np.random.rand() < self.mutation_prob:\n            mutation_vector = np.random.normal(0, 0.05, self.dim)\n            position += mutation_vector\n            position = np.clip(position, bounds.lb, bounds.ub)\n        return position\n    \n    def simulated_annealing(self, position, score, bounds):\n        temperature = self.temperature_initial\n        while temperature > self.temperature_final:\n            new_position = position + np.random.normal(0, 0.1, self.dim)\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = self.func(new_position)\n            if new_score < score or np.random.rand() < np.exp((score - new_score) / temperature):\n                position, score = new_position, new_score\n            temperature *= 0.92  # Modified: Adjusted temperature decay\n        return position, score\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                self.positions[i] = self.mutation(self.positions[i], bounds)  # New: Apply mutation\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            self.update_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n            self.global_best_position, self.global_best_score = self.simulated_annealing(self.global_best_position, self.global_best_score, bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "HybridPSO_SA", "description": "Enhanced HybridPSO_SA by integrating dynamic parameter adjustment and introducing a mutation operator for improved exploration.", "configspace": "", "generation": 8, "fitness": 0.2526308350182419, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.253 with standard deviation 0.028. And the mean value of best solutions found was 0.065 (0. is the best) with standard deviation 0.037.", "error": "", "parent_id": "f499ce75-1cbf-4755-8b47-533fcfd2875c", "metadata": {"aucs": [0.21352895723822696, 0.2778568643571706, 0.26650668345932815], "final_y": [0.11722669330259652, 0.03681990312788745, 0.04220442685043944]}, "mutation_prompt": null}
{"id": "d35ec026-7876-4723-a371-5d5d272e9b1b", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.inertia_weight = 0.9\n        self.cognitive_constant = 1.5\n        self.social_constant = 1.5\n        self.temperature_initial = 1.0\n        self.temperature_final = 0.01\n        self.restart_threshold = 10  # New: Restart threshold\n        \n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.zeros((self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n        self.stagnation_counter = 0  # New: Stagnation counter\n\n    def update_velocity_position(self, bounds):\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n        cognitive_velocity = self.cognitive_constant * r1 * (self.personal_best_positions - self.positions)\n        social_velocity = self.social_constant * r2 * (self.global_best_position - self.positions)\n        self.velocities = (self.inertia_weight * self.velocities) + cognitive_velocity + social_velocity\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)\n        self.inertia_weight *= 0.995  # Modified: Slower inertia weight decay for better exploration\n\n    def simulated_annealing(self, position, score, bounds):\n        temperature = self.temperature_initial\n        while temperature > self.temperature_final:\n            new_position = position + np.random.normal(0, 0.1, self.dim)\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = self.func(new_position)\n            if new_score < score or np.random.rand() < np.exp((score - new_score) / temperature):\n                position, score = new_position, new_score\n            temperature *= 0.9  # Modified: Faster adaptive temperature decay\n        return position, score\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0  # New: Reset counter on improvement\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            self.update_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:  # New: Check stagnation\n                self.initialize(bounds)  # New: Random restart\n\n            self.global_best_position, self.global_best_score = self.simulated_annealing(self.global_best_position, self.global_best_score, bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "HybridPSO_SA", "description": "Optimized HybridPSO_SA by reducing inertia weight decay rate to enhance exploration and prevent early convergence.", "configspace": "", "generation": 9, "fitness": 0.19255081522119621, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.193 with standard deviation 0.023. And the mean value of best solutions found was 0.432 (0. is the best) with standard deviation 0.294.", "error": "", "parent_id": "f499ce75-1cbf-4755-8b47-533fcfd2875c", "metadata": {"aucs": [0.17029907577355663, 0.18275932044172583, 0.2245940494483062], "final_y": [0.8467641988119591, 0.20064471424099736, 0.2488437117498713]}, "mutation_prompt": null}
{"id": "00d3f0ef-7802-4573-a569-922ba8929945", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.inertia_weight = 0.9\n        self.cognitive_constant = 1.5\n        self.social_constant = 1.5\n        self.temperature_initial = 1.0\n        self.temperature_final = 0.01\n        self.restart_threshold = 10  # New: Restart threshold\n        \n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.zeros((self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n        self.stagnation_counter = 0  # New: Stagnation counter\n\n    def update_velocity_position(self, bounds):\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n        cognitive_velocity = self.cognitive_constant * r1 * (self.personal_best_positions - self.positions)\n        social_velocity = self.social_constant * r2 * (self.global_best_position - self.positions)\n        self.velocities = (self.inertia_weight * self.velocities) + cognitive_velocity + social_velocity\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)\n        self.inertia_weight *= 0.99\n\n    def simulated_annealing(self, position, score, bounds):\n        temperature = self.temperature_initial\n        while temperature > self.temperature_final:\n            new_position = position + np.random.normal(0, 0.1, self.dim)\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = self.func(new_position)\n            if new_score < score or np.random.rand() < np.exp((score - new_score) / temperature):\n                position, score = new_position, new_score\n            temperature *= 0.9  # Modified: Faster adaptive temperature decay\n        return position, score\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0  # New: Reset counter on improvement\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            self.update_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:  # New: Check stagnation\n                self.initialize(bounds)  # New: Random restart\n\n            self.global_best_position, self.global_best_score = self.simulated_annealing(self.global_best_position, self.global_best_score, bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "HybridPSO_SA", "description": "Enhanced HybridPSO_SA by introducing diversity maintenance through random restarts and adaptive cooling schedule in Simulated Annealing.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "f499ce75-1cbf-4755-8b47-533fcfd2875c", "metadata": {"aucs": [0.2309252869134214, 0.32433761650663984, 0.2308558372177708], "final_y": [0.1232421254057924, 0.02145097210213082, 0.0341991749155421]}, "mutation_prompt": null}
{"id": "4a23a45f-8ba3-4c11-a8e0-3f1e125aa5e9", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.inertia_weight = 0.9\n        self.cognitive_constant = 1.5\n        self.social_constant = 1.5\n        self.temperature_initial = 1.0\n        self.temperature_final = 0.01\n        self.restart_threshold = 15  # Modified: Increased restart threshold\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.zeros((self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n        self.stagnation_counter = 0\n\n    def update_velocity_position(self, bounds):\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n        cognitive_velocity = self.cognitive_constant * r1 * (self.personal_best_positions - self.positions)\n        social_velocity = self.social_constant * r2 * (self.global_best_position - self.positions)\n        self.velocities = (self.inertia_weight * self.velocities) + cognitive_velocity + social_velocity\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)\n        self.inertia_weight *= 0.95  # Modified: More adaptive inertia weight decay\n\n    def simulated_annealing(self, position, score, bounds):\n        temperature = self.temperature_initial\n        while temperature > self.temperature_final:\n            new_position = position + np.random.normal(0, 0.1, self.dim)\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = self.func(new_position)\n            if new_score < score or np.random.rand() < np.exp((score - new_score) / temperature):\n                position, score = new_position, new_score\n            temperature *= 0.9\n        return position, score\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            self.update_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n            self.global_best_position, self.global_best_score = self.simulated_annealing(self.global_best_position, self.global_best_score, bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "HybridPSO_SA", "description": "Enhanced HybridPSO_SA with improved diversity by adaptive inertia weight and increased restart threshold.  ", "configspace": "", "generation": 11, "fitness": 0.18971860602483048, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.190 with standard deviation 0.035. And the mean value of best solutions found was 0.312 (0. is the best) with standard deviation 0.250.", "error": "", "parent_id": "f499ce75-1cbf-4755-8b47-533fcfd2875c", "metadata": {"aucs": [0.2215350994699552, 0.14059443863025, 0.20702627997428624], "final_y": [0.03869459651026952, 0.6426633555657939, 0.25526374021666054]}, "mutation_prompt": null}
{"id": "d97853e4-ed41-46ed-a9e6-c81accc86a8a", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.inertia_weight = 0.9\n        self.cognitive_constant = 1.5\n        self.social_constant = 1.5\n        self.temperature_initial = 1.0\n        self.temperature_final = 0.01\n        self.restart_threshold = 10  # New: Restart threshold\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.zeros((self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n        self.stagnation_counter = 0  # New: Stagnation counter\n\n    def update_velocity_position(self, bounds):\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n        cognitive_velocity = self.cognitive_constant * r1 * (self.personal_best_positions - self.positions)\n        social_velocity = self.social_constant * r2 * (self.global_best_position - self.positions)\n        self.velocities = (self.inertia_weight * self.velocities) + cognitive_velocity + social_velocity\n        self.positions += self.velocities\n        self.positions = np.clip(self.positions, bounds.lb, bounds.ub)\n        self.inertia_weight *= 0.99\n        self.cognitive_constant *= 1.01  # Change 1: Adaptive cognitive constant\n        self.social_constant *= 0.99  # Change 2: Adaptive social constant\n\n    def simulated_annealing(self, position, score, bounds):\n        temperature = self.temperature_initial\n        while temperature > self.temperature_final:\n            new_position = position + np.random.normal(0, 0.1, self.dim)\n            new_position = np.clip(new_position, bounds.lb, bounds.ub)\n            new_score = self.func(new_position)\n            if new_score < score or np.random.rand() < np.exp((score - new_score) / temperature):\n                position, score = new_position, new_score\n            temperature *= 0.9  # Modified: Faster adaptive temperature decay\n        return position, score\n\n    def mutation(self, position, bounds):  # Change 3: New mutation method\n        mutation_strength = 0.05\n        mutated_position = position + np.random.normal(0, mutation_strength, self.dim)\n        return np.clip(mutated_position, bounds.lb, bounds.ub)\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0  # New: Reset counter on improvement\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            self.update_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:  # New: Check stagnation\n                self.initialize(bounds)  # New: Random restart\n\n            self.global_best_position, self.global_best_score = self.simulated_annealing(self.global_best_position, self.global_best_score, bounds)\n            self.global_best_position = self.mutation(self.global_best_position, bounds)  # Change 4: Apply mutation\n\n        return self.global_best_position, self.global_best_score", "name": "HybridPSO_SA", "description": "Enhanced HybridPSO_SA by adding adaptive parameters adjustment and introducing a mutation strategy to improve exploration.", "configspace": "", "generation": 12, "fitness": 0.18285076951839885, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.183 with standard deviation 0.016. And the mean value of best solutions found was 0.498 (0. is the best) with standard deviation 0.282.", "error": "", "parent_id": "f499ce75-1cbf-4755-8b47-533fcfd2875c", "metadata": {"aucs": [0.16882268391023658, 0.205767648067525, 0.17396197657743495], "final_y": [0.8784129116048057, 0.20597786555533096, 0.40889428751280144]}, "mutation_prompt": null}
{"id": "be66876d-c440-4816-b800-ca9b27cb88e1", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.99\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Adaptive Quantum-Inspired Particle Swarm Optimization (AQPSO) with dynamic descent direction tuning to enhance global exploration and local exploitation.", "configspace": "", "generation": 13, "fitness": 0.32587849508282707, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.326 with standard deviation 0.062. And the mean value of best solutions found was 0.022 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "f499ce75-1cbf-4755-8b47-533fcfd2875c", "metadata": {"aucs": [0.28196834184199215, 0.413286922537223, 0.28238022086926606], "final_y": [0.0328185338607015, 6.2172171358608976e-06, 0.03290622715280975]}, "mutation_prompt": null}
{"id": "f5aa86b9-aa87-4d22-8f39-c96f83a0bcbc", "solution": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.velocity_max = None\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n        self.velocity_max = 0.1 * (bounds.ub - bounds.lb)\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            neighbors = np.random.choice(self.num_particles, 5, replace=False)  # Select random neighbors\n            neighbor_best_position = self.personal_best_positions[neighbors].mean(axis=0)\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (neighbor_best_position - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.velocities[i] = np.clip(self.velocities[i], -self.velocity_max, self.velocity_max)  # Constrain velocity\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.99\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAQPSO", "description": "Enhanced Adaptive Quantum-Inspired Particle Swarm Optimization (Enhanced-AQPSO) with dynamic neighborhood communication and adaptive velocity constraints to improve search efficiency and solution quality.", "configspace": "", "generation": 14, "fitness": 0.1970751068765563, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.197 with standard deviation 0.028. And the mean value of best solutions found was 0.255 (0. is the best) with standard deviation 0.148.", "error": "", "parent_id": "be66876d-c440-4816-b800-ca9b27cb88e1", "metadata": {"aucs": [0.2361092385910668, 0.18374733872208981, 0.1713687433165123], "final_y": [0.04738663709721959, 0.37602445605081486, 0.3427247268270457]}, "mutation_prompt": null}
{"id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with improved dynamic tuning for better convergence.", "configspace": "", "generation": 15, "fitness": 0.409237455782405, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.409 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be66876d-c440-4816-b800-ca9b27cb88e1", "metadata": {"aucs": [0.40927648449477283, 0.39264222010137106, 0.4257936627510711], "final_y": [6.408764424817702e-06, 9.024056326952834e-06, 4.824010498717481e-06]}, "mutation_prompt": null}
{"id": "f534e59c-b215-432b-a38c-8cb3f986a515", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Change from 0.98 to 0.97\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with an improved alpha dynamic tuning rate for faster convergence.", "configspace": "", "generation": 16, "fitness": 0.20969490564688112, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.019. And the mean value of best solutions found was 0.313 (0. is the best) with standard deviation 0.186.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18277399365320546, 0.2197595873690229, 0.22655113591841503], "final_y": [0.5748527562911436, 0.21045366097782184, 0.15482021322350845]}, "mutation_prompt": null}
{"id": "2e2be082-ce47-40be-ad3e-6fc180fd77f0", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.memory = []  # Add memory to track score improvements\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        self.memory.append(score_improvement)\n        if len(self.memory) > 5:  # Adaptive tuning based on a window of improvements\n            self.memory.pop(0)\n\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if np.mean(self.memory) < 0.1:  # Adaptive restart mechanism based on memory\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with memory-based dynamic alpha-beta tuning and adaptive restart mechanism for superior convergence.", "configspace": "", "generation": 17, "fitness": 0.19151884857853, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.192 with standard deviation 0.012. And the mean value of best solutions found was 0.502 (0. is the best) with standard deviation 0.115.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18030663309532358, 0.2084935472170809, 0.18575636542318552], "final_y": [0.5773619717963007, 0.3400797076824846, 0.5885729531873908]}, "mutation_prompt": null}
{"id": "763e16d1-d6e7-4fb3-984c-e0a54225cd74", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n    \n    def neighborhood_search(self, bounds):\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        candidate_position = self.global_best_position + perturbation\n        candidate_position = np.clip(candidate_position, bounds.lb, bounds.ub)\n        candidate_score = self.func(candidate_position)\n        if candidate_score < self.global_best_score:\n            self.global_best_position = candidate_position\n            self.global_best_score = candidate_score\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if evaluations % 10 == 0:  # Trigger neighborhood search every 10 evaluations\n                self.neighborhood_search(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introduced a neighborhood search strategy and adaptive restart for improved exploration.", "configspace": "", "generation": 18, "fitness": 0.15833088976324972, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.158 with standard deviation 0.019. And the mean value of best solutions found was 1.130 (0. is the best) with standard deviation 0.422.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.1546342162913361, 0.18326893225208052, 0.13708952074633252], "final_y": [1.0950802915589797, 0.6311067235625465, 1.662537584928678]}, "mutation_prompt": null}
{"id": "5ba02355-77fc-413c-bb97-7380ee25116c", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.inertia_weight = 0.9  # Initial inertia weight\n        self.inertia_weight_decay = 0.99  # Inertia weight decay factor\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.inertia_weight * self.velocities[i] + \\\n                                 self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n            self.inertia_weight *= self.inertia_weight_decay  # Decay inertia weight over time\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Implemented an adaptive inertia weight and enhanced convergence criteria in AQPSO for improved solution quality and robustness.", "configspace": "", "generation": 19, "fitness": 0.17712647706933615, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.177 with standard deviation 0.019. And the mean value of best solutions found was 0.229 (0. is the best) with standard deviation 0.118.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.15948479610957145, 0.1678066553100972, 0.2040879797883398], "final_y": [0.27501363003556806, 0.34448022978181847, 0.06711119946407411]}, "mutation_prompt": null}
{"id": "71cb52cf-9b78-4a12-82f1-7aa01269b697", "solution": "import numpy as np\n\nclass QIGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 40\n        self.mutation_rate = 0.1\n        self.crossover_rate = 0.7\n\n    def initialize(self, bounds):\n        self.population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, float('inf'))\n        self.best_solution = None\n        self.best_score = float('inf')\n\n    def quantum_crossover(self, parent1, parent2):\n        phi = np.random.uniform(0, 1, self.dim)\n        offspring = phi * parent1 + (1 - phi) * parent2\n        return np.clip(offspring, bounds.lb, bounds.ub)\n\n    def mutate(self, individual, bounds):\n        mutation_vector = np.random.normal(0, 1, self.dim)\n        mutate_condition = np.random.rand(self.dim) < self.mutation_rate\n        individual[mutate_condition] += mutation_vector[mutate_condition]\n        return np.clip(individual, bounds.lb, bounds.ub)\n\n    def select_parents(self):\n        selected_indices = np.random.choice(self.population_size, 2, replace=False)\n        return self.population[selected_indices[0]], self.population[selected_indices[1]]\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            new_population = []\n            for _ in range(self.population_size // 2):\n                parent1, parent2 = self.select_parents()\n\n                if np.random.rand() < self.crossover_rate:\n                    offspring1 = self.quantum_crossover(parent1, parent2)\n                    offspring2 = self.quantum_crossover(parent2, parent1)\n                else:\n                    offspring1, offspring2 = parent1, parent2\n\n                offspring1 = self.mutate(offspring1, bounds)\n                offspring2 = self.mutate(offspring2, bounds)\n\n                new_population.extend([offspring1, offspring2])\n\n            self.population = np.array(new_population)\n\n            for i in range(self.population_size):\n                score = self.func(self.population[i])\n                evaluations += 1\n                if score < self.fitness[i]:\n                    self.fitness[i] = score\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_solution = self.population[i]\n\n        return self.best_solution, self.best_score", "name": "QIGA", "description": "Quantum-Inspired Genetic Algorithm (QIGA) combines quantum-inspired initialization and crossover with traditional genetic mutation for enhanced exploration and exploitation.", "configspace": "", "generation": 20, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'bounds' is not defined\").", "error": "NameError(\"name 'bounds' is not defined\")", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {}, "mutation_prompt": null}
{"id": "9ffcebec-f7b4-434c-a77a-f241526528d4", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.inertia_weight = 0.9  # Added inertia weight\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.inertia_weight * (self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i]))\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n        \n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n            self.inertia_weight *= 0.99  # Adaptive inertia weight adjustment\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n            self.inertia_weight *= 1.01\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with adaptive inertia weight and quantum velocity scaling for improved convergence.", "configspace": "", "generation": 21, "fitness": 0.2906900680283563, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.291 with standard deviation 0.018. And the mean value of best solutions found was 0.038 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.26623189682885984, 0.299282376622357, 0.306555930633852], "final_y": [0.07963271703771532, 0.012890165292359698, 0.022681518799123238]}, "mutation_prompt": null}
{"id": "4b852807-cb53-487a-93f5-0d054c2cd6f6", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n        self.lorenz_x = 0.1  # Initial condition for Lorenz Attractor\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def chaotic_search(self, bounds):\n        sigma = 10\n        rho = 28\n        beta = 8 / 3\n        dt = 0.01\n        y = 0.1\n        z = 0.1\n        for i in range(self.num_particles):\n            dx = sigma * (y - self.lorenz_x) * dt\n            dy = (self.lorenz_x * (rho - z) - y) * dt\n            dz = (self.lorenz_x * y - beta * z) * dt\n            self.lorenz_x += dx\n            y += dy\n            z += dz\n            self.positions[i] += np.array([dx, dy, dz])[:self.dim]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n                self.chaotic_search(bounds)  # Added chaotic search when restarting\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with chaotic search strategy for improved exploration and convergence.", "configspace": "", "generation": 22, "fitness": 0.20969490564688112, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.019. And the mean value of best solutions found was 0.313 (0. is the best) with standard deviation 0.186.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18277399365320546, 0.2197595873690229, 0.22655113591841503], "final_y": [0.5748527562911436, 0.21045366097782184, 0.15482021322350845]}, "mutation_prompt": null}
{"id": "db664b24-003d-4c43-a271-6b492da25b25", "solution": "import numpy as np\n\nclass QCDPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.crowding_distance = np.zeros(self.num_particles)\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def compute_crowding_distance(self, scores):\n        sorted_indices = np.argsort(scores)\n        self.crowding_distance.fill(0)\n        if self.num_particles > 2:\n            self.crowding_distance[sorted_indices[0]] = float('inf')\n            self.crowding_distance[sorted_indices[-1]] = float('inf')\n            for i in range(1, self.num_particles - 1):\n                self.crowding_distance[sorted_indices[i]] = (scores[sorted_indices[i+1]] - scores[sorted_indices[i-1]]) / \\\n                                                            (max(scores) - min(scores) + 1e-10)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            scores = []\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n                scores.append(current_score)\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            self.compute_crowding_distance(scores)\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "QCDPSO", "description": "Quantum-Enhanced Crowding Distance PSO (QCDPSO) integrates quantum behavior with crowding distance to maintain diversity and improve exploration-exploitation balance.", "configspace": "", "generation": 23, "fitness": 0.20969490564688112, "feedback": "The algorithm QCDPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.019. And the mean value of best solutions found was 0.313 (0. is the best) with standard deviation 0.186.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18277399365320546, 0.2197595873690229, 0.22655113591841503], "final_y": [0.5748527562911436, 0.21045366097782184, 0.15482021322350845]}, "mutation_prompt": null}
{"id": "0c0a089d-bc95-4e99-b642-8ba30d0e8d77", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 12  # Change from 15 to 12\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Change from 0.98 to 0.97\n            self.beta *= 1.02  # Change from 1.01 to 1.02\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with an improved restart strategy and dynamic parameter adjustment.", "configspace": "", "generation": 24, "fitness": 0.20969490564688112, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.019. And the mean value of best solutions found was 0.313 (0. is the best) with standard deviation 0.186.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18277399365320546, 0.2197595873690229, 0.22655113591841503], "final_y": [0.5748527562911436, 0.21045366097782184, 0.15482021322350845]}, "mutation_prompt": null}
{"id": "5029c140-70d6-4e1c-bd23-9e1b4feb6f48", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.inertia_weight = 0.9  # Added inertia weight\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n        self.elite_positions = np.copy(self.positions[:3])  # Store top 3 elite positions\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.inertia_weight * self.velocities[i] + \\\n                                 self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n        self.inertia_weight *= 0.99  # Decrease inertia over time\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n            # Update elite positions\n            elite_scores_indices = np.argsort(self.personal_best_scores)[:3]\n            self.elite_positions = self.personal_best_positions[elite_scores_indices]\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with adaptive inertia and elite strategy for improved exploration-exploitation balance.", "configspace": "", "generation": 25, "fitness": 0.2178722136725694, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.218 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.035.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.21196753299771898, 0.22253812581060617, 0.21911098220938308], "final_y": [0.16771609740464663, 0.2064851066583802, 0.11972253722231226]}, "mutation_prompt": null}
{"id": "b86b3956-6925-4118-867b-f853e5dddbe7", "solution": "import numpy as np\n\nclass SQISO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.5  # Quantum influence parameter\n        self.beta = 0.3   # Neighborhood influence parameter\n        self.gamma = 0.2  # Exploration parameter\n        self.restart_threshold = 10\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            neighborhood_best = self.personal_best_positions[np.random.choice(self.num_particles)]\n            exploration_component = self.gamma * (neighborhood_best - np.random.uniform(bounds.lb, bounds.ub, self.dim))\n            \n            self.velocities[i] = self.alpha * np.log1p(np.abs(1 / phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i]) + exploration_component\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def adaptive_parameters(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.95\n            self.beta *= 1.05\n        else:\n            self.alpha *= 1.05\n            self.beta *= 0.95\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.adaptive_parameters(score_improvement)\n            self.update_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "SQISO", "description": "Stochastic Quantum-Inspired Swarm Optimization (SQISO) with adaptive neighborhood influence and multi-directional exploration.", "configspace": "", "generation": 26, "fitness": 0.22568327135197844, "feedback": "The algorithm SQISO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.226 with standard deviation 0.042. And the mean value of best solutions found was 0.110 (0. is the best) with standard deviation 0.074.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.2800152093377467, 0.17918195539491655, 0.21785264932327209], "final_y": [0.02965303787521272, 0.2092485132268584, 0.09223771525026712]}, "mutation_prompt": null}
{"id": "0cea6fae-3db4-4174-8d3d-5aa1924a2459", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/(phi + 0.1))) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introduced a minor parameter adjustment to enhance quantum velocity updates.", "configspace": "", "generation": 27, "fitness": 0.13488754205717066, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.135 with standard deviation 0.028. And the mean value of best solutions found was 2.148 (0. is the best) with standard deviation 1.073.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.17485864304004717, 0.11423889637241647, 0.11556508675904831], "final_y": [0.6311939528765957, 2.924866941297938, 2.887547483286399]}, "mutation_prompt": null}
{"id": "c2632b81-2eb1-4985-bec1-bbceb276d93a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Adjusted from 0.98 to 0.97\n            self.beta *= 1.02   # Adjusted from 1.01 to 1.02\n        else:\n            self.alpha *= 1.02  # Adjusted from 1.01 to 1.02\n            self.beta *= 0.98   # Adjusted from 0.99 to 0.98\n\n    def adapt_swarm_size(self, bounds):  # New function to adapt swarm size\n        if self.stagnation_counter >= self.restart_threshold:\n            self.num_particles = max(10, self.num_particles - 5)  # Reduce particles on stagnation\n        else:\n            self.num_particles = min(50, self.num_particles + 5)  # Increase particles if improving\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n            self.adapt_swarm_size(bounds)  # Apply adaptive swarm size\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with adaptive swarm size and improved alpha-beta dynamic tuning to balance exploration and exploitation effectively.", "configspace": "", "generation": 28, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 30 is out of bounds for axis 0 with size 30').", "error": "IndexError('index 30 is out of bounds for axis 0 with size 30')", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {}, "mutation_prompt": null}
{"id": "c43ceb2d-b89a-4114-886b-a3f3fa787fc0", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 20 + dim // 5  # Adaptive particle count\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n        # Quantum-inspired crossover\n        for i in range(0, self.num_particles, 2):\n            if i+1 < self.num_particles:\n                parent1, parent2 = self.positions[i], self.positions[i+1]\n                crossover_point = np.random.randint(1, self.dim)\n                self.positions[i][:crossover_point], self.positions[i+1][:crossover_point] = parent2[:crossover_point], parent1[:crossover_point]\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introduced adaptive particle count and quantum-inspired crossover for enhanced convergence.", "configspace": "", "generation": 29, "fitness": 0.3026771549104244, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.303 with standard deviation 0.182. And the mean value of best solutions found was 0.539 (0. is the best) with standard deviation 0.391.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.560542999681177, 0.18049206910556181, 0.16699639594453441], "final_y": [3.8109612415904345e-08, 0.7009524377729509, 0.9161279397730274]}, "mutation_prompt": null}
{"id": "bd26f132-b65f-498b-9302-370e94ada9dc", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.diversity_threshold = 1e-5  # New parameter for diversity\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def check_diversity(self):\n        diversity = np.std(self.positions, axis=0)\n        return np.any(diversity > self.diversity_threshold)\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold or not self.check_diversity():\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introducing adaptive diversity enhancement and improved restart strategy to refine AQPSO's exploration capabilities.", "configspace": "", "generation": 30, "fitness": 0.3266472687795466, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.327 with standard deviation 0.158. And the mean value of best solutions found was 1.347 (0. is the best) with standard deviation 1.905.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.4267682349511843, 0.44943451468638107, 0.10373905670107442], "final_y": [3.777646553546717e-06, 2.6276980375533105e-06, 4.041198206312934]}, "mutation_prompt": null}
{"id": "3307171e-89e7-492e-92cb-b5dac5e83f16", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 40  # Changed from 30 to 40\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0.1, 1, self.dim)  # Changed lower limit from 0 to 0.1\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved AQPSO with adaptive particle count and quantum velocity adjustment for enhanced convergence.", "configspace": "", "generation": 31, "fitness": 0.2660636597566242, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.266 with standard deviation 0.057. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.087.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.3470448096295434, 0.22061691120401572, 0.23052925843631344], "final_y": [0.00010668901854940364, 0.21247815533353265, 0.12379605297777659]}, "mutation_prompt": null}
{"id": "a554fc23-2738-4b7e-9b51-207b768a0ec4", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 32  # Modified from 30 to 32\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i] * 0.9  # Added a velocity dampening factor\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved convergence by adjusting particle population and velocity update mechanism.", "configspace": "", "generation": 32, "fitness": 0.12429222330345761, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.124 with standard deviation 0.016. And the mean value of best solutions found was 2.513 (0. is the best) with standard deviation 0.810.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.1146999510775476, 0.147460373851806, 0.11071634498101923], "final_y": [2.8845182687516004, 1.3896816124093831, 3.266249407972782]}, "mutation_prompt": null}
{"id": "74383edf-5afe-421a-8685-be4f5c1b0ba1", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.restart_factor = 0.1  # New parameter for adaptive restart\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def adaptive_restart(self, bounds):\n        if self.stagnation_counter >= self.restart_threshold:\n            self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim)) * self.restart_factor + self.global_best_position * (1 - self.restart_factor)\n            self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n            self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            self.adaptive_restart(bounds)  # Use new adaptive restart method\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introduced adaptive particle swarm restart strategy for enhanced global exploration.", "configspace": "", "generation": 33, "fitness": 0.15164319596883125, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.152 with standard deviation 0.057. And the mean value of best solutions found was 2.795 (0. is the best) with standard deviation 2.868.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.1540354891804785, 0.22007207121819816, 0.08082202750781708], "final_y": [1.299242522056752, 0.27837129119962656, 6.8086996709111]}, "mutation_prompt": null}
{"id": "80b32c5e-e9eb-4560-afa9-ce969a9cf9c2", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.inertia_weight = 0.9  # New: inertia weight for velocity update\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            neighborhood_best = self.calculate_neighborhood_best(i)  # New: neighborhood influence\n\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] + \n                                  self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \n                                  self.beta * (neighborhood_best - self.positions[i]))\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def calculate_neighborhood_best(self, index):  # New: helper function for neighborhood influence\n        neighborhood_indices = [(index - 1) % self.num_particles, index, (index + 1) % self.num_particles]\n        neighborhood_scores = [self.personal_best_scores[j] for j in neighborhood_indices]\n        best_idx = neighborhood_indices[np.argmin(neighborhood_scores)]\n        return self.personal_best_positions[best_idx]\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n            self.inertia_weight *= 0.99  # New: adapt inertia weight\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n            self.inertia_weight *= 1.01  # New: adapt inertia weight\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introduced particle neighborhood influence and adaptive inertia weight for enhanced exploration and convergence.", "configspace": "", "generation": 34, "fitness": 0.17749867546200793, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.177 with standard deviation 0.024. And the mean value of best solutions found was 0.262 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.21022883309108875, 0.16672332917692245, 0.15554386411801258], "final_y": [0.23712747230412962, 0.3185676923574872, 0.22954031984801737]}, "mutation_prompt": null}
{"id": "6119136d-b041-4ccd-9f0c-e1eb0b5c65e0", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.975  # Adjusted from 0.98 for more aggressive adaptation\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced adaptive dynamic tuning in AQPSO with improved convergence control.", "configspace": "", "generation": 35, "fitness": 0.379262322327181, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.379 with standard deviation 0.091. And the mean value of best solutions found was 0.024 (0. is the best) with standard deviation 0.034.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.4610538762194205, 0.25193124322224025, 0.42480184753988226], "final_y": [2.098134420120511e-06, 0.07291041725332663, 5.23706221160433e-06]}, "mutation_prompt": null}
{"id": "c046d85b-d87c-47a7-af56-608945dd2369", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.restart_counter = 0  # Track restarts\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def dynamic_restart(self, bounds):\n        if self.restart_counter < 3:  # Limit number of restarts\n            self.initialize(bounds)\n            self.restart_counter += 1\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.dynamic_restart(bounds)  # Use dynamic restart\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introducing a dynamic restart mechanism to escape local optima and improve convergence in AQPSO.", "configspace": "", "generation": 36, "fitness": 0.20969490564688112, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.019. And the mean value of best solutions found was 0.313 (0. is the best) with standard deviation 0.186.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18277399365320546, 0.2197595873690229, 0.22655113591841503], "final_y": [0.5748527562911436, 0.21045366097782184, 0.15482021322350845]}, "mutation_prompt": null}
{"id": "da2a6945-aa78-4719-a527-09f99785037a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Changed from 0.98 to 0.97\n            self.beta *= 1.02   # Changed from 1.01 to 1.02\n        else:\n            self.alpha *= 1.02  # Changed from 1.01 to 1.02\n            self.beta *= 0.98   # Changed from 0.99 to 0.98\n\n    def opposition_based_learning(self, bounds):\n        opposition_positions = bounds.lb + bounds.ub - self.positions\n        for i in range(self.num_particles):\n            if self.func(opposition_positions[i]) < self.personal_best_scores[i]:\n                self.positions[i] = opposition_positions[i]\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.opposition_based_learning(bounds)  # Add opposition-based learning\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Incorporate enhanced adaptive control and opposition-based learning to boost convergence in AQPSO.", "configspace": "", "generation": 37, "fitness": 0.2100486143793132, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.016. And the mean value of best solutions found was 0.296 (0. is the best) with standard deviation 0.156.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18704478934165103, 0.2214728586602983, 0.2216281951359902], "final_y": [0.5160322542772202, 0.19492022417859184, 0.17790557205065563]}, "mutation_prompt": null}
{"id": "632c8a34-9512-4ea5-9581-692cc5b0e60f", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.inertia_weight = 0.9  # Inertia weight for velocity update\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.inertia_weight * self.velocities[i] + \\\n                                 self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n            self.inertia_weight *= 0.99  # Slightly reduce inertia weight for faster convergence\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n            self.inertia_weight *= 1.01  # Increase inertia weight to explore more\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introducing a dynamic inertia weight in AQPSO to enhance convergence stability and performance.", "configspace": "", "generation": 38, "fitness": 0.20246102282519954, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.202 with standard deviation 0.024. And the mean value of best solutions found was 0.370 (0. is the best) with standard deviation 0.304.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.21505950537690632, 0.22332550287447261, 0.1689980602242197], "final_y": [0.26549239931761753, 0.061364239758941924, 0.7839229715580845]}, "mutation_prompt": null}
{"id": "8346e344-9094-46cc-ba92-6151ca00846e", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n            \n            # Small local random exploration\n            if np.random.rand() < 0.3:  # Adjust probability threshold\n                perturbation = np.random.normal(scale=0.01, size=self.dim)  # Reduced scale\n                self.positions[i] += perturbation\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved Adaptive Quantum Particle Swarm Optimization with enhanced local exploitation.", "configspace": "", "generation": 39, "fitness": 0.15316732172925518, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.153 with standard deviation 0.036. And the mean value of best solutions found was 1.518 (0. is the best) with standard deviation 0.996.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.20004529536677074, 0.14675090487042353, 0.11270576495057127], "final_y": [0.40641302596671974, 1.3241713038193526, 2.8219558574388746]}, "mutation_prompt": null}
{"id": "c3e84a05-04f4-4825-9e16-eaccc96f6524", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * np.random.rand() * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Minor adjustment to velocity update to enhance convergence quality.", "configspace": "", "generation": 40, "fitness": 0.2257275333638501, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.226 with standard deviation 0.014. And the mean value of best solutions found was 0.039 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.2194331934653223, 0.21212124311325642, 0.24562816351297156], "final_y": [0.06167423740371607, 0.048488690637931216, 0.008237010117576517]}, "mutation_prompt": null}
{"id": "6bd65d92-b3a7-47b5-b108-3c4c80957db3", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.inertia_weight = 0.9  # Added inertia weight for velocity update\n        self.elite_fraction = 0.1  # Fraction of elite particles\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        elite_count = int(self.elite_fraction * self.num_particles)  # Determine number of elite particles\n        elite_indices = np.argsort(self.personal_best_scores)[:elite_count]  # Identify elite particles\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            elite_position = self.personal_best_positions[elite_indices[np.random.randint(elite_count)]]  # Random elite\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] + \n                                  self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) +\n                                  self.beta * (self.global_best_position - self.positions[i]) +\n                                  self.beta * (elite_position - self.positions[i]))  # Elite communication\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n            self.inertia_weight *= 0.99  # Adaptive inertia tuning\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n            self.inertia_weight *= 1.01\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Refined AQPSO with adaptive inertia and elite particle communication for enhanced exploration and exploitation balance.", "configspace": "", "generation": 41, "fitness": 0.18272012423335646, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.183 with standard deviation 0.008. And the mean value of best solutions found was 0.510 (0. is the best) with standard deviation 0.049.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.17155833388914155, 0.18866363435449518, 0.18793840445643262], "final_y": [0.569845329144985, 0.5097093631628222, 0.449783943779334]}, "mutation_prompt": null}
{"id": "459d6ee4-0e52-4f9b-98ed-129774934cd9", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.gamma = 0.9  # Inertia weight for velocity\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.gamma * self.velocities[i] + \\\n                                 self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n            self.gamma *= 0.99\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n            self.gamma *= 1.01\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved AQPSO by refining dynamic parameters and introducing inertia for better convergence.", "configspace": "", "generation": 42, "fitness": 0.171335907403945, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.171 with standard deviation 0.013. And the mean value of best solutions found was 0.618 (0. is the best) with standard deviation 0.031.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.1700895996001619, 0.1879396853812767, 0.1559784372303964], "final_y": [0.627055420570285, 0.5761817023226424, 0.6494401227286832]}, "mutation_prompt": null}
{"id": "26bc5970-66d2-45ca-aeaa-edc94fb79654", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 12  # Adjusted from 15 to 12\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Modified AQPSO with adjusted restart threshold for improved exploration and exploitation balance.", "configspace": "", "generation": 43, "fitness": 0.14671791130874245, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.147 with standard deviation 0.033. And the mean value of best solutions found was 1.830 (0. is the best) with standard deviation 1.137.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18937472239464925, 0.14199179946150065, 0.10878721207007747], "final_y": [0.5816463159810144, 1.5776897174400941, 3.3311003014747147]}, "mutation_prompt": null}
{"id": "9d3b0dd1-cb62-47e7-8fab-a05c0edf1622", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i] * (0.95 if i % 2 == 0 else 1)  # Adjusted velocity term\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Slightly adjust quantum velocity computation for enhanced convergence stability.", "configspace": "", "generation": 44, "fitness": 0.21110332613158408, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.211 with standard deviation 0.017. And the mean value of best solutions found was 0.289 (0. is the best) with standard deviation 0.158.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18728636406516408, 0.21747763385731222, 0.22854598047227592], "final_y": [0.5072422895207074, 0.21967227142278126, 0.13935081890817005]}, "mutation_prompt": null}
{"id": "5ff61a25-aba2-4559-af32-584c8a33fd9c", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            # Introducing a small mutation factor to enhance exploration\n            mutation_factor = np.random.uniform(-0.01, 0.01, self.dim)\n            self.positions[i] += self.velocities[i] + mutation_factor\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introduce a small adaptive mutation factor to enhance exploration and prevent premature convergence.", "configspace": "", "generation": 45, "fitness": 0.2642806167802825, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.264 with standard deviation 0.051. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.156.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.19687623503359286, 0.2745732758725711, 0.3213923394346835], "final_y": [0.347158729360435, 0.029238167700316134, 0.0054513822461866954]}, "mutation_prompt": null}
{"id": "f143ea1e-f5fa-4b2d-a3dd-c85adf559373", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.elite_memory = []  # Added elite memory for best positions\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n        self.elite_memory.clear()  # Initialize elite memory\n\n    def update_quantum_velocity_position(self, bounds):\n        inertia_weight = 0.9 - (0.5 * self.budget - 0.5) * (0.9 - 0.4) / self.budget  # Adaptive inertia weight\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = inertia_weight * self.velocities[i] + \\\n                                 self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                    self.elite_memory.append((self.positions[i], current_score))  # Store into elite memory\n                    if len(self.elite_memory) > 5:  # Maintain top 5 elites\n                        self.elite_memory.sort(key=lambda x: x[1])\n                        self.elite_memory = self.elite_memory[:5]\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introduced elite memory storage and adaptive inertia weights to enhance exploration and exploitation balance.", "configspace": "", "generation": 46, "fitness": 0.2140159047590783, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.214 with standard deviation 0.039. And the mean value of best solutions found was 0.228 (0. is the best) with standard deviation 0.194.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18984130044941983, 0.18293034818759457, 0.26927606564022055], "final_y": [0.17548911564709016, 0.487249539434553, 0.019989215280459]}, "mutation_prompt": null}
{"id": "6b9eead5-8d04-47b4-929f-a9fc29ce990e", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Change from 0.98 to 0.97\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Modified AQPSO with enhanced adaptive tuning parameters for improved convergence.", "configspace": "", "generation": 47, "fitness": 0.2272222234657989, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.227 with standard deviation 0.063. And the mean value of best solutions found was 0.405 (0. is the best) with standard deviation 0.343.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.16773576458561834, 0.19875525350520618, 0.3151756523065722], "final_y": [0.8470088608132743, 0.3557049696247359, 0.011303160911224566]}, "mutation_prompt": null}
{"id": "5578df34-d483-4e0d-a00e-8eddf7033106", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= (self.restart_threshold - 1):  # Changed from >= self.restart_threshold\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with improved dynamic tuning and slightly adjusted restart mechanism for improved convergence.", "configspace": "", "generation": 48, "fitness": 0.20969490564688112, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.019. And the mean value of best solutions found was 0.313 (0. is the best) with standard deviation 0.186.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18277399365320546, 0.2197595873690229, 0.22655113591841503], "final_y": [0.5748527562911436, 0.21045366097782184, 0.15482021322350845]}, "mutation_prompt": null}
{"id": "1c730d92-ff71-4f4e-af17-b3d0d848cf9e", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.7  # Quantum probability control parameter improved\n        self.beta = 0.45  # Adaptive tuning parameter improved\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def levy_flight(self, size):\n        beta = 1.5  # Levy distribution parameter\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1 / beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v)**(1 / beta)\n        return step\n\n    def update_quantum_velocity_position(self, bounds):\n        levy_step = self.levy_flight(self.dim)  # Introducing Levy flight\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i]) + levy_step\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.975  # Slightly adjusted for more adaptability\n            self.beta *= 1.02\n        else:\n            self.alpha *= 1.02\n            self.beta *= 0.98\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Hybrid Quantum Levy Walk PSO with adaptive parameters for enhanced exploration and convergence.", "configspace": "", "generation": 49, "fitness": 0.2046009859516782, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.205 with standard deviation 0.029. And the mean value of best solutions found was 0.229 (0. is the best) with standard deviation 0.086.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.2233632845608602, 0.22679027906453897, 0.16364939422963543], "final_y": [0.13122064738965403, 0.21640187509798248, 0.33980844943511557]}, "mutation_prompt": null}
{"id": "d3ce937f-01c6-4000-8d27-31761eddbcf0", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n        self.local_best_positions = np.copy(self.personal_best_positions)  # Added line\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            local_influence = np.random.uniform(0.5, 1.5)  # Added line\n            self.velocities[i] = self.alpha * np.log(np.abs(1 / phi)) * (self.local_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i]) * local_influence  # Modified line\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n        self.beta = np.clip(self.beta, 0.2, 0.6)  # Added line\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.local_best_positions[i] = self.personal_best_positions[i]  # Modified line\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introducing adaptive particle swarms with local learning and diverse restart strategies to enhance convergence.", "configspace": "", "generation": 50, "fitness": 0.3497906730667378, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.350 with standard deviation 0.093. And the mean value of best solutions found was 0.065 (0. is the best) with standard deviation 0.092.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.3830322321194499, 0.4435316606157368, 0.2228081264650268], "final_y": [0.0007795715287063866, 7.010222091417623e-06, 0.19541776022197582]}, "mutation_prompt": null}
{"id": "01480cd4-ded4-485a-bc8a-496e4a7398bb", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.inertia_weight = 0.9  # Initial inertia weight\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.inertia_weight * self.velocities[i] + \\\n                                 self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n            self.inertia_weight *= 0.99  # Decrease inertia weight\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n            self.inertia_weight *= 1.01  # Increase inertia weight\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Modified AQPSO with adaptive inertia weight for enhanced exploration-exploitation balance.", "configspace": "", "generation": 51, "fitness": 0.18643640695584252, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.186 with standard deviation 0.020. And the mean value of best solutions found was 0.450 (0. is the best) with standard deviation 0.378.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.1803936648147999, 0.1651507426790907, 0.21376481337363695], "final_y": [0.31343414390575114, 0.9658661485847373, 0.0710829943308954]}, "mutation_prompt": null}
{"id": "b0defe66-8848-4ac0-a78e-36ec00d9c63a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            if np.random.rand() < 0.1:  # Added Gaussian perturbation for exploration\n                self.velocities[i] += np.random.normal(0, 0.1, self.dim)\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with hybrid exploration-exploitation using Gaussian perturbation for better convergence.", "configspace": "", "generation": 52, "fitness": 0.32519085472841114, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.325 with standard deviation 0.093. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.175.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.4106404020719061, 0.1954601718059381, 0.3694719903073892], "final_y": [0.0002052723435869415, 0.3723871723981047, 0.0009314594633869599]}, "mutation_prompt": null}
{"id": "5c0a333b-100e-47e4-afc8-185466b12cb6", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.restart_prob = 0.05  # Probability to restart a particle's position\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n            if np.random.rand() < self.restart_prob:  # Random restart based on probability\n                self.positions[i] = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introduced an adaptive restart mechanism to enhance convergence and prevent premature stagnation in AQPSO.", "configspace": "", "generation": 53, "fitness": 0.29274023773091457, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.293 with standard deviation 0.075. And the mean value of best solutions found was 0.040 (0. is the best) with standard deviation 0.056.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.38678187328239055, 0.28710561555719116, 0.20433322435316204], "final_y": [0.00017442689318257642, 0.001871942039972886, 0.11913002917243781]}, "mutation_prompt": null}
{"id": "0d206ddd-e8f9-4e80-96f8-cec85379fd6b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.02  # Slightly increased from 1.01 to 1.02\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved convergence by adjusting the adaptive tuning parameter update rate slightly.", "configspace": "", "generation": 54, "fitness": 0.32313510670320555, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.323 with standard deviation 0.073. And the mean value of best solutions found was 0.072 (0. is the best) with standard deviation 0.100.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.2258396482519981, 0.400331158902696, 0.3432345129549226], "final_y": [0.21232840864623495, 1.1304310772280906e-05, 0.0022945857736826323]}, "mutation_prompt": null}
{"id": "2b145969-70e4-4adc-bbb8-516b930e162c", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Change from 0.98 to 0.97\n            self.beta *= 1.02   # Change from 1.01 to 1.02\n        else:\n            self.alpha *= 1.02  # Change from 1.01 to 1.02\n            self.beta *= 0.98   # Change from 0.99 to 0.98\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced dynamic tuning in AQPSO for improved convergence through adaptive particle diversification.", "configspace": "", "generation": 55, "fitness": 0.2100486143793132, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.016. And the mean value of best solutions found was 0.296 (0. is the best) with standard deviation 0.156.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18704478934165103, 0.2214728586602983, 0.2216281951359902], "final_y": [0.5160322542772202, 0.19492022417859184, 0.17790557205065563]}, "mutation_prompt": null}
{"id": "64c07e4b-f110-4b63-8e1d-7b1596883369", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 10  # Change from 15 to 10\n        self.stagnation_counter = 0\n        self.velocity_scaling = 0.5  # New parameter for velocity scaling\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.velocities[i] *= self.velocity_scaling  # Apply velocity scaling\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Modified AQPSO with enhanced restart mechanism and adaptive velocity scaling for improved convergence.", "configspace": "", "generation": 56, "fitness": 0.19469311809496206, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.195 with standard deviation 0.008. And the mean value of best solutions found was 0.229 (0. is the best) with standard deviation 0.075.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.1854856072574924, 0.2052457491339824, 0.1933479978934114], "final_y": [0.3332310431960965, 0.1581557291617501, 0.19411647515376212]}, "mutation_prompt": null}
{"id": "ddbe0ee4-6367-43c8-bc5b-69a56d404979", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            random_factor = np.random.uniform(0.9, 1.1)  # New element\n            self.velocities[i] = random_factor * (self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i]))\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n                self.stagnation_counter = 0  # Reset counter\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved AQPSO with adaptive restart mechanism and enhanced particle update strategy.", "configspace": "", "generation": 57, "fitness": 0.2126583893004973, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.213 with standard deviation 0.022. And the mean value of best solutions found was 0.300 (0. is the best) with standard deviation 0.190.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18375550051740552, 0.2184071639402385, 0.23581250344384785], "final_y": [0.561965016079458, 0.21866200972291513, 0.11940354919356336]}, "mutation_prompt": null}
{"id": "0f04ab44-4f60-4e74-bc53-e888d466f045", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.975  # Adjusted from 0.98 to 0.975\n            self.beta *= 1.015   # Adjusted from 1.01 to 1.015\n        else:\n            self.alpha *= 1.005\n            self.beta *= 0.995\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved AQPSO with reduced alpha decay and increased beta growth for better exploration.", "configspace": "", "generation": 58, "fitness": 0.15421287039299805, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.154 with standard deviation 0.040. And the mean value of best solutions found was 1.622 (0. is the best) with standard deviation 1.006.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.21066392014255841, 0.13205249441751832, 0.11992219661891745], "final_y": [0.24728489636608436, 1.995094627667244, 2.624693911316957]}, "mutation_prompt": null}
{"id": "e4724752-ffa5-4cc5-83ed-508579a76c59", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.015  # Adjusted from 1.01 to 1.015\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Slightly adjust the beta parameter increment for adaptive tuning to improve convergence.", "configspace": "", "generation": 59, "fitness": 0.20969490564688112, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.019. And the mean value of best solutions found was 0.313 (0. is the best) with standard deviation 0.186.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18277399365320546, 0.2197595873690229, 0.22655113591841503], "final_y": [0.5748527562911436, 0.21045366097782184, 0.15482021322350845]}, "mutation_prompt": null}
{"id": "6b051312-b17c-4c05-aff4-f95d09bd7fed", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0.1, 0.9, self.dim)  # Narrowed range for phi\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.02  # Slightly increased beta multiplication factor\n        else:\n            self.alpha *= 1.02  # Slightly increased alpha multiplication factor\n            self.beta *= 0.99\n\n    def particle_diversification(self, bounds):\n        for i in range(self.num_particles):\n            if np.random.rand() < 0.1:\n                self.positions[i] = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.particle_diversification(bounds)  # Introduce particle diversification\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved AQPSO with enhanced adaptive tuning and particle diversification for better exploratory behavior.", "configspace": "", "generation": 60, "fitness": 0.20969490564688112, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.019. And the mean value of best solutions found was 0.313 (0. is the best) with standard deviation 0.186.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18277399365320546, 0.2197595873690229, 0.22655113591841503], "final_y": [0.5748527562911436, 0.21045366097782184, 0.15482021322350845]}, "mutation_prompt": null}
{"id": "9aa0c2c7-0a43-4377-a146-1abf26b3e2d3", "solution": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.local_search_probability = 0.1\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def local_search(self, position, bounds):\n        lr = 0.05 * (bounds.ub - bounds.lb)\n        perturbation = np.random.uniform(-lr, lr, self.dim)\n        new_position = position + perturbation\n        new_position = np.clip(new_position, bounds.lb, bounds.ub)\n        return new_position\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                if np.random.rand() < self.local_search_probability:\n                    candidate_position = self.local_search(self.positions[i], bounds)\n                    candidate_score = self.func(candidate_position)\n                    evaluations += 1\n                    if candidate_score < self.personal_best_scores[i]:\n                        self.personal_best_positions[i] = candidate_position\n                        self.personal_best_scores[i] = candidate_score\n                        self.positions[i] = candidate_position\n                else:\n                    current_score = self.func(self.positions[i])\n                    evaluations += 1\n\n                    if current_score < self.personal_best_scores[i]:\n                        self.personal_best_positions[i] = self.positions[i]\n                        self.personal_best_scores[i] = current_score\n                        self.stagnation_counter = 0\n                    else:\n                        self.stagnation_counter += 1\n\n                if self.personal_best_scores[i] < self.global_best_score:\n                    self.global_best_position = self.personal_best_positions[i]\n                    self.global_best_score = self.personal_best_scores[i]\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAQPSO", "description": "Adaptive AQPSO with periodic restart and hybrid local search for improved exploration and exploitation balance.", "configspace": "", "generation": 61, "fitness": 0.33129395768062414, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.331 with standard deviation 0.067. And the mean value of best solutions found was 0.037 (0. is the best) with standard deviation 0.052.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.4024503737532866, 0.24080934892665684, 0.350622150361929], "final_y": [1.0430203526520829e-05, 0.11059843382299993, 0.00011250700442701976]}, "mutation_prompt": null}
{"id": "f40fdcf7-d4e4-4fe1-b538-059c441484ea", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Change from 0.98 to 0.97\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Slightly adjust the dynamic tuning factors to improve convergence speed in AQPSO.", "configspace": "", "generation": 62, "fitness": 0.16521175323621876, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.165 with standard deviation 0.043. And the mean value of best solutions found was 1.358 (0. is the best) with standard deviation 0.878.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.22352077215233057, 0.12317398250403855, 0.14894050505228718], "final_y": [0.2865738982260278, 2.4382089532366793, 1.3487511385235347]}, "mutation_prompt": null}
{"id": "1e64420a-3eff-4310-95c8-b95eed2ae83d", "solution": "import numpy as np\n\nclass EnhancedAQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.levy_factor = 1.5  # Levy flight scaling factor\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def levy_flight(self):\n        u = np.random.normal(0, 1, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / np.power(np.abs(v), 1/self.levy_factor)\n        return step\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            quantum_velocity = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i])\n            adaptive_velocity = self.beta * (self.global_best_position - self.positions[i])\n            levy_step = self.levy_flight()\n            self.velocities[i] = quantum_velocity + adaptive_velocity + levy_step\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def adaptive_respawn(self, bounds):\n        out_of_bounds = (self.positions < bounds.lb) | (self.positions > bounds.ub)\n        respawn_required = np.any(out_of_bounds, axis=1)\n        for i in range(self.num_particles):\n            if respawn_required[i]:\n                self.positions[i] = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n            self.adaptive_respawn(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAQPSO", "description": "Introducing a hybrid approach by integrating a Levy flight strategy for enhanced exploration and adaptive particle respawning to improve convergence robustness and diversity.", "configspace": "", "generation": 63, "fitness": 0.19161379779285148, "feedback": "The algorithm EnhancedAQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.192 with standard deviation 0.039. And the mean value of best solutions found was 0.245 (0. is the best) with standard deviation 0.081.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.1799367573092513, 0.24465121599924577, 0.15025342007005738], "final_y": [0.2798792185412887, 0.13241380251374135, 0.32151585514939146]}, "mutation_prompt": null}
{"id": "5a28f47e-6c66-4855-aed7-1ae83ddd8d95", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.55  # Modified alpha for dual learning rate mechanism\n        self.beta = 0.45   # Modified beta for improved convergence\n        self.restart_threshold = 10  # Reduced to encourage adaptive restart\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.02  # Increased adaptation rate\n        else:\n            self.alpha *= 1.02  # Increased adaptation rate\n            self.beta *= 0.98\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Modified AQPSO with dual learning rates and adaptive restart for enhanced convergence and exploration.", "configspace": "", "generation": 64, "fitness": 0.19087195396960585, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.191 with standard deviation 0.061. And the mean value of best solutions found was 1.371 (0. is the best) with standard deviation 1.674.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.25072585526047375, 0.21413748023902246, 0.10775252640932131], "final_y": [0.10529345975542283, 0.270354677758301, 3.7363001862815657]}, "mutation_prompt": null}
{"id": "2899c048-d829-4b05-8846-c0c6fe7df178", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.reset_probability = 0.1  # New reset probability for particles\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.reset_probability:  # Reset some particles\n                self.positions[i] = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introduce a particle reset mechanism to escape local optima and enhance convergence speed in AQPSO.", "configspace": "", "generation": 65, "fitness": 0.31228040798636997, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.312 with standard deviation 0.038. And the mean value of best solutions found was 0.008 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.2851859360011013, 0.3660194331960325, 0.28563585476197617], "final_y": [0.020382195100121195, 0.0006851011391449265, 0.0029819243095728074]}, "mutation_prompt": null}
{"id": "d832e12c-6a3c-42e9-8f3b-f64b2e9755da", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            opposition_vector = bounds.lb + bounds.ub - self.positions[i]  # Opposition-based learning\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - opposition_vector)\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "An enhanced AQPSO variant with a hybrid velocity update integrating opposition-based learning for improved exploration.", "configspace": "", "generation": 66, "fitness": 0.19044753587508265, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.190 with standard deviation 0.042. And the mean value of best solutions found was 0.742 (0. is the best) with standard deviation 0.418.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.24978245004664146, 0.16212637859251366, 0.1594337789860928], "final_y": [0.1577553794229248, 0.957968686354646, 1.110669603945005]}, "mutation_prompt": null}
{"id": "62f900a0-5eb6-471f-b84b-1adef6b8b76f", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.975  # Change from 0.98 to 0.975\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with slightly optimized dynamic tuning for improved convergence speed.", "configspace": "", "generation": 67, "fitness": 0.379262322327181, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.379 with standard deviation 0.091. And the mean value of best solutions found was 0.024 (0. is the best) with standard deviation 0.034.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.4610538762194205, 0.25193124322224025, 0.42480184753988226], "final_y": [2.098134420120511e-06, 0.07291041725332663, 5.23706221160433e-06]}, "mutation_prompt": null}
{"id": "f203b8b8-ddc6-40fa-9bb8-932c9efc3ef7", "solution": "import numpy as np\n\nclass EQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  \n        self.beta = 0.4   \n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.chaos_factor = 0.01  # New factor for chaotic perturbation\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        neighborhood_size = max(1, self.num_particles // 5)  # New neighborhood influence\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            local_best_pos = np.mean(self.personal_best_positions[np.random.choice(self.num_particles, neighborhood_size)], axis=0)\n            self.velocities[i] = (self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) +\n                                  self.beta * (local_best_pos - self.positions[i]))\n            self.positions[i] += self.velocities[i] + self.chaos_factor * np.random.uniform(-1, 1, self.dim)  # Chaotic perturbation\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.99  \n            self.beta *= 1.02   # Slightly more aggressive adaptation\n        else:\n            self.alpha *= 1.02\n            self.beta *= 0.98\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "EQPSO", "description": "Enhanced Quantum PSO (EQPSO) with adaptive neighborhood influence and chaos-based perturbation for better exploration.", "configspace": "", "generation": 68, "fitness": 0.15653738582940546, "feedback": "The algorithm EQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.157 with standard deviation 0.004. And the mean value of best solutions found was 0.502 (0. is the best) with standard deviation 0.058.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.16225892302486378, 0.15331845487445528, 0.15403477958889733], "final_y": [0.48800269916059996, 0.5794748550370863, 0.43950543093603817]}, "mutation_prompt": null}
{"id": "1fe77850-e9b7-43da-bc25-5dbf7c7038bd", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Adjusted from 0.98 to 0.97\n            self.beta *= 1.02   # Adjusted from 1.01 to 1.02\n        else:\n            self.alpha *= 1.02  # Adjusted from 1.01 to 1.02\n            self.beta *= 0.98   # Adjusted from 0.99 to 0.98\n\n    def adjust_restart_threshold(self):\n        if self.stagnation_counter >= self.restart_threshold:\n            self.restart_threshold = max(10, self.restart_threshold - 1)\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.adjust_restart_threshold()\n            self.update_quantum_velocity_position(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved Quantum Particle Swarm Optimization (I-QPSO) with enhanced dynamic tuning and adaptive restart threshold.", "configspace": "", "generation": 69, "fitness": 0.250978315896426, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.251 with standard deviation 0.140. And the mean value of best solutions found was 1.965 (0. is the best) with standard deviation 2.641.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.2327349400823866, 0.43144736403751816, 0.08875264356937318], "final_y": [0.19742719485937166, 2.8927833721666326e-05, 5.69766886042823]}, "mutation_prompt": null}
{"id": "42599b64-cf5c-437d-b952-e77210ea6ed0", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.inertia_weight = 0.9  # New: Inertia weight for velocity update\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) +\n                                  self.beta * (self.global_best_position - self.positions[i]))\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Further increase adaptation\n            self.beta *= 1.02\n            self.inertia_weight *= 0.99  # New: Adjust inertia weight\n        else:\n            self.alpha *= 1.02\n            self.beta *= 0.98\n            self.inertia_weight *= 1.01\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introducing dual-stage dynamic tuning and inertia weight adaptation to enhance convergence and exploration.", "configspace": "", "generation": 70, "fitness": 0.19948004532548777, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.199 with standard deviation 0.013. And the mean value of best solutions found was 0.417 (0. is the best) with standard deviation 0.167.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.19881925240104525, 0.18385021325713602, 0.215770670318282], "final_y": [0.41442050964896116, 0.6221252648720794, 0.21406582983480912]}, "mutation_prompt": null}
{"id": "4ccfc591-a871-4c78-a3d2-d07edfb19eb6", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Adjusted from 0.98 to 0.97\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Modified AQPSO with adjusted adaptive tuning parameters to enhance convergence.", "configspace": "", "generation": 71, "fitness": 0.14671791130874245, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.147 with standard deviation 0.033. And the mean value of best solutions found was 1.830 (0. is the best) with standard deviation 1.137.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18937472239464925, 0.14199179946150065, 0.10878721207007747], "final_y": [0.5816463159810144, 1.5776897174400941, 3.3311003014747147]}, "mutation_prompt": null}
{"id": "c828b2bc-6a6f-4c0f-a6ff-64f4fcbad8ad", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 10  # Change from 15 to 10\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n                self.stagnation_counter = 0  # Reset stagnation counter\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Refined AQPSO with adaptive particle diversification and stagnation reset for enhanced exploration.", "configspace": "", "generation": 72, "fitness": 0.20969490564688112, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.019. And the mean value of best solutions found was 0.313 (0. is the best) with standard deviation 0.186.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18277399365320546, 0.2197595873690229, 0.22655113591841503], "final_y": [0.5748527562911436, 0.21045366097782184, 0.15482021322350845]}, "mutation_prompt": null}
{"id": "9fcfee12-5c69-4b91-90cd-db03c177a864", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            chaos_factor = 0.5 * (np.random.uniform(0, 1, self.dim) - 0.5)  # New line\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i]) + chaos_factor  # Modified line\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Change from 0.98 to 0.97\n            self.beta *= 1.02  # Change from 1.01 to 1.02\n        else:\n            self.alpha *= 1.02  # Change from 1.01 to 1.02\n            self.beta *= 0.98  # Change from 0.99 to 0.98\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with self-adaptive particle updating and chaos-based reinitialization for improved exploration and convergence.", "configspace": "", "generation": 73, "fitness": 0.25558380667874875, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.256 with standard deviation 0.003. And the mean value of best solutions found was 0.076 (0. is the best) with standard deviation 0.043.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.25071709968901834, 0.25769587738679467, 0.2583384429604333], "final_y": [0.13388224993641631, 0.06445619961625705, 0.03078265795947976]}, "mutation_prompt": null}
{"id": "cc2f5b28-79b6-4b3c-a165-4149a04155fe", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 10  # Reduced to allow more frequent restarts\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Enhanced parameter dynamics\n            self.beta *= 1.02\n        else:\n            self.alpha *= 1.02\n            self.beta *= 0.98\n\n    def adaptive_restart(self, bounds):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        if diversity < 0.05:  # Trigger restart if diversity is low\n            self.initialize(bounds)\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.adaptive_restart(bounds)  # Use adaptive restart\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with adaptive restart mechanism and improved parameter tuning for better exploration.", "configspace": "", "generation": 74, "fitness": 0.3485430094668825, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.349 with standard deviation 0.045. And the mean value of best solutions found was 0.007 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.36761483941200424, 0.39101712250088483, 0.2869970664877586], "final_y": [9.528479900823249e-05, 5.3311952603224e-05, 0.021494513986388707]}, "mutation_prompt": null}
{"id": "0333c4e6-5bbc-4264-88f8-8645d847338d", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(30, budget // 100)  # Adjust particle count based on budget\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Change from 0.98 to 0.97\n            self.beta *= 1.02   # Change from 1.01 to 1.02\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved AQPSO with adaptive particle count and revised dynamic tuning for enhanced exploration.", "configspace": "", "generation": 75, "fitness": 0.09003992851938052, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.090 with standard deviation 0.067. And the mean value of best solutions found was 10.806 (0. is the best) with standard deviation 7.459.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18384114264120122, 0.03624354086497794, 0.05003510205196238], "final_y": [0.6440281310342806, 18.33625534977535, 13.437903668731757]}, "mutation_prompt": null}
{"id": "12bb08d9-8f8a-4436-b12d-b69e4e2a13d3", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Incorporate a restart mechanism with random reseeding to improve diversity and global exploration.", "configspace": "", "generation": 76, "fitness": 0.12195428763493892, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.122 with standard deviation 0.036. And the mean value of best solutions found was 3.240 (0. is the best) with standard deviation 1.840.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.17199883142669636, 0.09145377520340459, 0.10241025627471578], "final_y": [0.7162532794986203, 5.048308407424962, 3.9560699660633127]}, "mutation_prompt": null}
{"id": "c6aa82ec-264d-4e73-aeb2-47afc9f16a48", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.02  # Modified change from 1.01 to 1.02\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with refined dynamic tuning for better adaptability.", "configspace": "", "generation": 77, "fitness": 0.20969490564688112, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.019. And the mean value of best solutions found was 0.313 (0. is the best) with standard deviation 0.186.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18277399365320546, 0.2197595873690229, 0.22655113591841503], "final_y": [0.5748527562911436, 0.21045366097782184, 0.15482021322350845]}, "mutation_prompt": null}
{"id": "101ad251-bae4-47f6-a908-16229ef0a11a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = 0.5 * (self.positions[i] + self.personal_best_positions[i])  # Updated strategy\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhance AQPSO by adjusting the personal best position update strategy for improved convergence.", "configspace": "", "generation": 78, "fitness": 0.2280362048040043, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.228 with standard deviation 0.006. And the mean value of best solutions found was 0.042 (0. is the best) with standard deviation 0.027.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.23016382750319542, 0.23345167255834365, 0.2204931143504738], "final_y": [0.017069649552959112, 0.07924436005535238, 0.029761979521561593]}, "mutation_prompt": null}
{"id": "82844e6a-13f0-413d-81fa-9d8117ff1bc3", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            levy_step = self.levy_flight()\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i]) + levy_step\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def levy_flight(self, lambda_=1.5):  # New function for Levy flights\n        sigma = (np.math.gamma(1 + lambda_) * np.sin(np.pi * lambda_ / 2) / \\\n                 (np.math.gamma((1 + lambda_) / 2) * lambda_ * 2**((lambda_ - 1) / 2)))**(1 / lambda_)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / np.abs(v)**(1 / lambda_)\n        return step\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved AQPSO with diversity enhancement through Levy flights for better exploration of the search space.", "configspace": "", "generation": 79, "fitness": 0.19873123715730312, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.199 with standard deviation 0.020. And the mean value of best solutions found was 0.329 (0. is the best) with standard deviation 0.115.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.22086471075088732, 0.20387516033268105, 0.171453840388341], "final_y": [0.24184318419942927, 0.25236811729101283, 0.49153842604555054]}, "mutation_prompt": null}
{"id": "04358d17-f8fc-4450-8527-164a72b7f59c", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 10  # Changed from 15 to 10\n        self.stagnation_counter = 0\n        self.inertia_weight = 0.9  # Introduced inertia weight\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            quantum_velocity = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                               self.beta * (self.global_best_position - self.positions[i])\n            self.velocities[i] = self.inertia_weight * self.velocities[i] + quantum_velocity  # Added inertia weight\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n            self.inertia_weight *= 0.99  # Decrease inertia weight on improvement\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n            self.inertia_weight *= 1.01  # Increase inertia weight on stagnation\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Hybrid AQPSO with adaptive inertia and velocity reduction for enhanced convergence.", "configspace": "", "generation": 80, "fitness": 0.23872015258521131, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.239 with standard deviation 0.044. And the mean value of best solutions found was 0.251 (0. is the best) with standard deviation 0.149.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.3015918775206291, 0.20630710458158252, 0.20826147565342235], "final_y": [0.039414990746493746, 0.3628884990919148, 0.34945713945770973]}, "mutation_prompt": null}
{"id": "af7090af-5ee6-46ee-8e9b-25bfa4194138", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.41   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved AQPSO by adjusting the adaptive tuning parameter for enhanced dynamic tuning.", "configspace": "", "generation": 81, "fitness": 0.14478512057703727, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.145 with standard deviation 0.042. And the mean value of best solutions found was 2.392 (0. is the best) with standard deviation 2.114.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.09178274361564454, 0.1476483083685276, 0.19492430974693964], "final_y": [5.305495049037667, 1.51321149543804, 0.35724921956143474]}, "mutation_prompt": null}
{"id": "c3dabb59-af2f-488b-a4f4-012d5059d108", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.num_swarms = 3  # Multiswarm enhancement\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(\n            bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n        self.swarm_best_positions = np.copy(self.personal_best_positions)\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            swarm_idx = i % self.num_swarms\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * \\\n                (self.personal_best_positions[i] - self.positions[i]) + \\\n                self.beta * (self.swarm_best_positions[swarm_idx] - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n                swarm_idx = i % self.num_swarms\n                if current_score < np.min(self.personal_best_scores[swarm_idx::self.num_swarms]):\n                    self.swarm_best_positions[swarm_idx] = self.positions[i]\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved AQPSO with adaptive multi-swarm strategy for enhanced global search and convergence.", "configspace": "", "generation": 82, "fitness": 0.16172989096483825, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.162 with standard deviation 0.020. And the mean value of best solutions found was 0.991 (0. is the best) with standard deviation 0.345.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.19022334620989856, 0.14936617841660604, 0.14560014826801015], "final_y": [0.5655953724173471, 1.410806351880026, 0.995505830704293]}, "mutation_prompt": null}
{"id": "ad7b409b-2446-4498-b839-919eb756a32b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.inertia = 0.9  # Initial inertia weight\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.inertia * self.velocities[i] + \\\n                                 self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            mutation = np.random.uniform(-0.1, 0.1, self.dim)\n            self.positions[i] += self.velocities[i] + mutation\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n        \n        # Adaptive inertia weight adjustment\n        self.inertia = max(0.4, self.inertia * 0.99) if score_improvement else min(0.9, self.inertia * 1.01)\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with adaptive inertia and mutation for improved exploration and exploitation balance.", "configspace": "", "generation": 83, "fitness": 0.2048673129771709, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.205 with standard deviation 0.012. And the mean value of best solutions found was 0.208 (0. is the best) with standard deviation 0.106.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.2123717906222774, 0.21387189952911667, 0.18835824878011864], "final_y": [0.11852676905964712, 0.1493791714371406, 0.3566335050444911]}, "mutation_prompt": null}
{"id": "6a110928-c166-4f58-bf14-59545164dcd4", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.7  # Increased quantum probability control parameter\n        self.beta = 0.3   # Decreased adaptive tuning parameter\n        self.restart_threshold = 10\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Change from 0.98 to 0.97\n            self.beta *= 1.02   # Change from 1.01 to 1.02\n        else:\n            self.alpha *= 1.02  # Change from 1.01 to 1.02\n            self.beta *= 0.98   # Change from 0.99 to 0.98\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))  # Reinitialize positions only\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with selective reinitialization and dynamic parameter scaling for improved convergence and exploitation.", "configspace": "", "generation": 84, "fitness": 0.17221190307560175, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.172 with standard deviation 0.047. And the mean value of best solutions found was 1.213 (0. is the best) with standard deviation 1.078.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.17156758746008016, 0.23028396981891674, 0.11478415194780833], "final_y": [0.7743529373071666, 0.16723876385763137, 2.696286340667079]}, "mutation_prompt": null}
{"id": "64815a8c-f796-40e9-b38c-1ff18952638f", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.65  # Quantum probability control parameter\n        self.beta = 0.45   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Changed from 0.98 to 0.97\n            self.beta *= 1.02   # Changed from 1.01 to 1.02\n        else:\n            self.alpha *= 1.02  # Changed from 1.01 to 1.02\n            self.beta *= 0.98   # Changed from 0.99 to 0.98\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced dynamic tuning and quantum velocity update for accelerated convergence and robustness.", "configspace": "", "generation": 85, "fitness": 0.21043812101958856, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.030. And the mean value of best solutions found was 0.392 (0. is the best) with standard deviation 0.345.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.1674409006027342, 0.2295660127744764, 0.23430744968155504], "final_y": [0.8799993149219482, 0.16745135842695397, 0.12890221703289528]}, "mutation_prompt": null}
{"id": "5b48aa5e-4ce2-4127-8273-8c0b8f1a844a", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.gamma = 0.9  # Momentum factor\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n        self.previous_velocities = np.copy(self.velocities)\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            momentum = self.gamma * self.previous_velocities[i]\n            self.velocities[i] = momentum + self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n            self.previous_velocities[i] = self.velocities[i]\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n            if self.num_particles < 50:  # Increase particle count if stagnated\n                self.num_particles += 1\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Refined AQPSO with enhanced velocity update using momentum and adaptive particle count for improved convergence.", "configspace": "", "generation": 86, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 30 is out of bounds for axis 0 with size 30').", "error": "IndexError('index 30 is out of bounds for axis 0 with size 30')", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {}, "mutation_prompt": null}
{"id": "19dc9367-6ecf-4479-add5-0eebba2d690b", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Changed from 0.98 to 0.97\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Fine-tuned adaptive control parameters in AQPSO for improved convergence speed and solution accuracy.", "configspace": "", "generation": 87, "fitness": 0.1259285412872899, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.126 with standard deviation 0.004. And the mean value of best solutions found was 2.361 (0. is the best) with standard deviation 0.244.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.1287879428818387, 0.12074271331669661, 0.12825496766333433], "final_y": [2.1854833561285485, 2.7067735709286818, 2.1919108496669057]}, "mutation_prompt": null}
{"id": "d7beb2f3-f4b7-4251-9264-ebd89a412d19", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.5  # Quantum probability control parameter\n        self.beta = 0.5   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with refined dynamic tuning parameters for improved convergence. ", "configspace": "", "generation": 88, "fitness": 0.22034042834296064, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.220 with standard deviation 0.025. And the mean value of best solutions found was 0.284 (0. is the best) with standard deviation 0.204.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18549085150394318, 0.2430207231075685, 0.23250971041737023], "final_y": [0.5720093198819948, 0.1270956170256814, 0.1526899901741322]}, "mutation_prompt": null}
{"id": "021262bf-a5fd-4b67-acfc-f7e2ab2cdb8f", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Change from 0.98 to 0.97\n            self.beta *= 1.02   # Change from 1.01 to 1.02\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved AQPSO with modified dynamic tuning and restart criteria for better exploration.", "configspace": "", "generation": 89, "fitness": 0.20969490564688112, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.019. And the mean value of best solutions found was 0.313 (0. is the best) with standard deviation 0.186.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18277399365320546, 0.2197595873690229, 0.22655113591841503], "final_y": [0.5748527562911436, 0.21045366097782184, 0.15482021322350845]}, "mutation_prompt": null}
{"id": "dbd23ee7-ad10-4cb4-8cd9-f540e643b339", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 20  # Increased from 15 to 20\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.05  # Modified from 1.01 to 1.05\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Adaptive AQPSO with improved exploration through increased restart threshold and a more aggressive beta tuning.", "configspace": "", "generation": 90, "fitness": 0.20969490564688112, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.210 with standard deviation 0.019. And the mean value of best solutions found was 0.313 (0. is the best) with standard deviation 0.186.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18277399365320546, 0.2197595873690229, 0.22655113591841503], "final_y": [0.5748527562911436, 0.21045366097782184, 0.15482021322350845]}, "mutation_prompt": null}
{"id": "0657e231-6f31-4bbf-877e-33b271c57d2c", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  \n        self.beta = 0.4   \n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i] + np.random.normal(0, 0.1, self.dim) # Added stochastic component\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  \n            self.beta *= 1.02  # Slightly increased adaptation rate\n        else:\n            self.alpha *= 1.02  # Slightly increased adaptation rate\n            self.beta *= 0.98\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.global_best_position = np.random.uniform(bounds.lb, bounds.ub, self.dim) # Stochastic restart\n                self.stagnation_counter = 0\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Enhanced AQPSO with improved dynamic tuning and stochastic parameter restarting for better convergence and exploration.", "configspace": "", "generation": 91, "fitness": 0.2791335378255602, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.279 with standard deviation 0.031. And the mean value of best solutions found was 0.063 (0. is the best) with standard deviation 0.041.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.25128292411168984, 0.322113385322813, 0.2640043040421779], "final_y": [0.10371413670070762, 0.006435225030375254, 0.07860980355273671]}, "mutation_prompt": null}
{"id": "75f6d0dd-fba7-4741-b7f7-b0db0e4b90d4", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 10  # Changed from 15 to 10\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(1 + np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.97  # Change from 0.98 to 0.97\n            self.beta *= 1.02  # Change from 1.01 to 1.02\n        else:\n            self.alpha *= 1.02  # Change from 1.01 to 1.02\n            self.beta *= 0.98  # Change from 0.99 to 0.98\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved AQPSO with enhanced quantum exploration and adaptive restart mechanism for robustness.", "configspace": "", "generation": 92, "fitness": 0.2558360341492078, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.256 with standard deviation 0.073. And the mean value of best solutions found was 0.234 (0. is the best) with standard deviation 0.175.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.20933954524849352, 0.19932837126156677, 0.35884018593756317], "final_y": [0.2788377392326708, 0.42312132199728586, 0.00036245135913078627]}, "mutation_prompt": null}
{"id": "b74f56dc-7e26-4ec8-b382-1e83c9d96b10", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  \n        self.beta = 0.4   \n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.inertia_weight = 0.9  # Added inertia weight\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.inertia_weight * self.velocities[i] + \\\n                                 self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  \n            self.beta *= 1.01\n            self.inertia_weight *= 0.99  # Decrease inertia weight\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n            self.inertia_weight *= 1.01  # Increase inertia weight\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved AQPSO with adaptive inertia weights and enhanced diversity preservation.", "configspace": "", "generation": 93, "fitness": 0.20246102282519954, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.202 with standard deviation 0.024. And the mean value of best solutions found was 0.370 (0. is the best) with standard deviation 0.304.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.21505950537690632, 0.22332550287447261, 0.1689980602242197], "final_y": [0.26549239931761753, 0.061364239758941924, 0.7839229715580845]}, "mutation_prompt": null}
{"id": "1d25a4f0-ccc9-4f57-b15f-a526c6267819", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 0.5 * self.beta * (self.global_best_position - self.positions[i])  # Changed coefficient from self.beta to 0.5 * self.beta\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Refined AQPSO with adjusted quantum position update for enhanced exploration.", "configspace": "", "generation": 94, "fitness": 0.14516672158110175, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.145 with standard deviation 0.034. And the mean value of best solutions found was 1.735 (0. is the best) with standard deviation 1.090.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18847601608520803, 0.1407500206720793, 0.10627412798601787], "final_y": [0.5827004776302565, 1.425453732545561, 3.197033675787299]}, "mutation_prompt": null}
{"id": "bae49fc9-48cc-4133-a813-3489f153bf09", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.7  # Quantum probability control parameter\n        self.beta = 0.5   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved AQPSO with better dynamic tuning strategy through adjusted control parameters.", "configspace": "", "generation": 95, "fitness": 0.22034042834296064, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.220 with standard deviation 0.025. And the mean value of best solutions found was 0.284 (0. is the best) with standard deviation 0.204.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18549085150394318, 0.2430207231075685, 0.23250971041737023], "final_y": [0.5720093198819948, 0.1270956170256814, 0.1526899901741322]}, "mutation_prompt": null}
{"id": "67ac4252-0aa1-42a1-b9a4-db4d71522f64", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6\n        self.beta = 0.4\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n        self.momentum = 0.9  # Added momentum term\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.previous_velocities = np.copy(self.velocities)  # Track previous velocities\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            new_velocity = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                           self.beta * (self.global_best_position - self.positions[i])\n            self.velocities[i] = self.momentum * self.previous_velocities[i] + new_velocity  # Apply momentum\n            self.previous_velocities[i] = self.velocities[i]  # Update previous velocities\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved AQPSO with momentum-inspired velocity update for enhanced convergence stability.", "configspace": "", "generation": 96, "fitness": 0.20804721405289225, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.208 with standard deviation 0.016. And the mean value of best solutions found was 0.281 (0. is the best) with standard deviation 0.108.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.2266121519789306, 0.2100684597001249, 0.18746103047962126], "final_y": [0.18132656001420303, 0.23088514837161403, 0.4301253438481275]}, "mutation_prompt": null}
{"id": "1cf8d524-5dee-46af-aac1-8272520ca5c9", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            inertia_weight = 0.9  # New inertia weight adjustment\n            self.velocities[i] = inertia_weight * self.velocities[i] + self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Introduce a slight inertia weight adjustment to enhance exploration capabilities.", "configspace": "", "generation": 97, "fitness": 0.17920335840755167, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.179 with standard deviation 0.021. And the mean value of best solutions found was 0.419 (0. is the best) with standard deviation 0.232.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.20948833963758706, 0.16224204352452742, 0.16587969206054054], "final_y": [0.34185806292911064, 0.18175040540924664, 0.7335000013985029]}, "mutation_prompt": null}
{"id": "2218fc64-fa56-4fda-bd3d-85ebb196f54e", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.985  # Changed from 0.98 to 0.985\n            self.beta *= 1.015\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Improved dynamic tuning parameters to enhance convergence rates in AQPSO.", "configspace": "", "generation": 98, "fitness": 0.14671791130874245, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.147 with standard deviation 0.033. And the mean value of best solutions found was 1.830 (0. is the best) with standard deviation 1.137.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18937472239464925, 0.14199179946150065, 0.10878721207007747], "final_y": [0.5816463159810144, 1.5776897174400941, 3.3311003014747147]}, "mutation_prompt": null}
{"id": "570553d5-7ed0-44a3-bf1c-0f246b3f8650", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 30\n        self.alpha = 0.6  # Quantum probability control parameter\n        self.beta = 0.4   # Adaptive tuning parameter\n        self.restart_threshold = 15\n        self.stagnation_counter = 0\n\n    def initialize(self, bounds):\n        self.positions = np.random.uniform(bounds.lb, bounds.ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n        self.global_best_position = np.copy(self.personal_best_positions[0])\n        self.global_best_score = float('inf')\n\n    def update_quantum_velocity_position(self, bounds):\n        for i in range(self.num_particles):\n            phi = np.random.uniform(0, 1, self.dim)\n            self.velocities[i] = self.alpha * np.log(np.abs(1/phi)) * (self.personal_best_positions[i] - self.positions[i]) + \\\n                                 self.beta * (self.global_best_position - self.positions[i])\n            self.positions[i] += self.velocities[i]\n            self.positions[i] = np.clip(self.positions[i], bounds.lb, bounds.ub)\n            # Opposition-based learning\n            if np.random.rand() < 0.3:\n                opposite_position = bounds.lb + bounds.ub - self.positions[i]\n                opposite_position = np.clip(opposite_position, bounds.lb, bounds.ub)\n                if self.func(opposite_position) < self.personal_best_scores[i]:\n                    self.positions[i] = opposite_position\n\n    def dynamic_tuning(self, score_improvement):\n        if score_improvement:\n            self.alpha *= 0.98  # Change from 0.99 to 0.98\n            self.beta *= 1.01\n        else:\n            self.alpha *= 1.01\n            self.beta *= 0.99\n\n    def __call__(self, func):\n        self.func = func\n        bounds = func.bounds\n        self.initialize(bounds)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.num_particles):\n                current_score = self.func(self.positions[i])\n                evaluations += 1\n\n                if current_score < self.personal_best_scores[i]:\n                    self.personal_best_positions[i] = self.positions[i]\n                    self.personal_best_scores[i] = current_score\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if current_score < self.global_best_score:\n                    self.global_best_position = self.positions[i]\n                    self.global_best_score = current_score\n\n            score_improvement = self.global_best_score < np.min(self.personal_best_scores)\n            self.dynamic_tuning(score_improvement)\n            self.update_quantum_velocity_position(bounds)\n\n            if self.stagnation_counter >= self.restart_threshold:\n                self.initialize(bounds)\n\n        return self.global_best_position, self.global_best_score", "name": "AQPSO", "description": "Adaptive Quantum PSO with enhanced diversity through opposition-based learning for improved convergence.", "configspace": "", "generation": 99, "fitness": 0.20538148757889507, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.205 with standard deviation 0.018. And the mean value of best solutions found was 0.313 (0. is the best) with standard deviation 0.186.", "error": "", "parent_id": "46a746e9-c2ed-4118-976b-84644c4c4d6f", "metadata": {"aucs": [0.18053232076325376, 0.21545231924382457, 0.22015982272960688], "final_y": [0.5748528599266898, 0.21045402388737386, 0.15482033312265087]}, "mutation_prompt": null}
