{"id": "c048b504-97bb-48c0-ac94-73f1a5b63e9d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm that combines Differential Evolution for global search and Covariance Matrix Adaptation Evolution Strategy for local refinement, with adaptive layer complexity and robustness-focused cost adjustments.", "configspace": "", "generation": 0, "fitness": 0.7981341535255518, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.014. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": null, "metadata": {"aucs": [0.7777309213638787, 0.8094998819938464, 0.80717165721893], "final_y": [0.15163348466168347, 0.14233310905584073, 0.14380558686885603]}, "mutation_prompt": null}
{"id": "c8df0c81-c3ef-4359-8a9f-b8c8ceef5397", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 12  # Adjusted to improve convergence\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved hybrid metaheuristic with strategic layer iteration and adaptive local refinement for enhanced photovoltaic structure optimization.", "configspace": "", "generation": 1, "fitness": 0.8015129507503561, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.013. And the mean value of best solutions found was 0.149 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "c048b504-97bb-48c0-ac94-73f1a5b63e9d", "metadata": {"aucs": [0.7889143202882197, 0.7964900895380488, 0.8191344424248002], "final_y": [0.15276457190049098, 0.15195016731899735, 0.14333941081021206]}, "mutation_prompt": null}
{"id": "b0bfac22-3d58-4683-af60-d1f6642a8284", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.9, CR=0.85):  # Adjusted F and CR\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer by tuning parameters for improved exploration and exploitation balance in Differential Evolution.", "configspace": "", "generation": 1, "fitness": 0.7951022394582087, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.795 with standard deviation 0.014. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "c048b504-97bb-48c0-ac94-73f1a5b63e9d", "metadata": {"aucs": [0.7818791521117369, 0.8146525989876197, 0.7887749672752693], "final_y": [0.14260087811294286, 0.13455395987237573, 0.1383656265323744]}, "mutation_prompt": null}
{"id": "d68354e9-602f-4db0-a518-e733cc65305d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            adaptive_F = F * (1 - self.evaluations / self.budget)  # Change 1\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)  # Change 2\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        options = {'maxiter': 100, 'ftol': 1e-5}  # Change 3\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options=options)\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic with adaptive mutation rate and improved local refinement strategy for robust optimization.", "configspace": "", "generation": 1, "fitness": 0.8122507932033655, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.010. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "c048b504-97bb-48c0-ac94-73f1a5b63e9d", "metadata": {"aucs": [0.8252323720487486, 0.8095305063087616, 0.8019895012525865], "final_y": [0.13570607668630708, 0.1490614004896933, 0.13839630821131033]}, "mutation_prompt": null}
{"id": "e583d3a7-367e-4efb-9c9b-206d46d2e564", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n\n        # Introduce a diversity measure in the decision process\n        diversity_threshold = np.std(global_solution) * 0.05\n        if local_cost < global_cost and np.std(local_solution) > diversity_threshold:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Optimized HybridMetaheuristicOptimizer by enhancing the selection process and integrating a diversity measure to maintain a robust search space exploration.", "configspace": "", "generation": 1, "fitness": 0.8080735158621732, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.008. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "c048b504-97bb-48c0-ac94-73f1a5b63e9d", "metadata": {"aucs": [0.8160988170751439, 0.8116766501746454, 0.7964450803367303], "final_y": [0.1402211039565121, 0.1450309511288661, 0.13986214604615022]}, "mutation_prompt": null}
{"id": "bf55821b-f29f-4b7e-936b-1b4679178565", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='trust-constr')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "An enhanced hybrid metaheuristic algorithm incorporating adaptive population size and improved local refinement to optimize high-dimensional problems efficiently.", "configspace": "", "generation": 1, "fitness": 0.8210739706269568, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.821 with standard deviation 0.018. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "c048b504-97bb-48c0-ac94-73f1a5b63e9d", "metadata": {"aucs": [0.8160089348132187, 0.8456102722856128, 0.8016027047820387], "final_y": [0.13757969112975543, 0.1243051766495391, 0.1449348999230704]}, "mutation_prompt": null}
{"id": "752b7ce7-6fea-43fc-bc0a-50124bf1365c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Improved mutation strategy\n                mutant = np.clip(a + F * (b - c) + 0.5 * (np.mean(population, axis=0) - population[i]), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='trust-constr')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic optimizer with improved mutation strategy for better global exploration.", "configspace": "", "generation": 2, "fitness": 0.812545042878719, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.009. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "bf55821b-f29f-4b7e-936b-1b4679178565", "metadata": {"aucs": [0.8110823915655064, 0.8027880442874331, 0.8237646927832174], "final_y": [0.14847728937049576, 0.1518714731744547, 0.1443017440476132]}, "mutation_prompt": null}
{"id": "83fd4b13-f996-477a-9008-19db4327e8b9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved local refinement by switching to L-BFGS-B method for better handling of noisy functions.", "configspace": "", "generation": 2, "fitness": 0.8128259626122333, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.010. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "bf55821b-f29f-4b7e-936b-1b4679178565", "metadata": {"aucs": [0.8103611456711237, 0.8263598349258952, 0.8017569072396808], "final_y": [0.13272941013024941, 0.13652185687231133, 0.14340965510832182]}, "mutation_prompt": null}
{"id": "49592da2-5bea-4a9b-9d4c-83416463d8dd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                if np.random.rand() < 0.5:  # Adaptive mutation strategy\n                    F_adapt = 0.7 + 0.3 * np.random.rand()  \n                else:\n                    F_adapt = F\n                mutant = np.clip(a + F_adapt * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='trust-constr')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid algorithm with adaptive mutation strategies for refined exploration-exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.8107650476265751, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.018. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "bf55821b-f29f-4b7e-936b-1b4679178565", "metadata": {"aucs": [0.8349541884034922, 0.8054298848987312, 0.7919110695775018], "final_y": [0.1288646091576281, 0.14484987277252936, 0.1395600364660856]}, "mutation_prompt": null}
{"id": "428e5012-ac14-48c5-be6b-15825ec9ad1e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def adaptive_population_size(self, pop_size, complexity_factor):\n        return max(4, int(pop_size * complexity_factor))  # Adaptive population size\n    \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        adaptive_factor = 1 + (np.log(self.dim) / 10)  # New complexity factor\n        pop_size = self.adaptive_population_size(pop_size, adaptive_factor) # Adjusted population\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='trust-constr')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "An enhanced metaheuristic algorithm employing adaptive layer complexity and refined local search integration to optimize high-dimensional black-box problems effectively.", "configspace": "", "generation": 2, "fitness": 0.7980950394605335, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.010. And the mean value of best solutions found was 0.149 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "bf55821b-f29f-4b7e-936b-1b4679178565", "metadata": {"aucs": [0.8092042492492153, 0.8006673824003807, 0.7844134867320047], "final_y": [0.13822180500714742, 0.1524829238280343, 0.15655648155159363]}, "mutation_prompt": null}
{"id": "dea2c825-d208-48cd-8762-c82a9162fda3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_adapt = F * (1 - self.evaluations / self.budget)  # Dynamic mutation adaptation\n                mutant = np.clip(a + F_adapt * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')  # Refined local search method\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "A refined hybrid metaheuristic algorithm introducing dynamic mutation adaptation and refined local search strategies to enhance solution quality and convergence speed.", "configspace": "", "generation": 2, "fitness": 0.8084628519487626, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.005. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "bf55821b-f29f-4b7e-936b-1b4679178565", "metadata": {"aucs": [0.8102520510165403, 0.8010242076522477, 0.8141122971774997], "final_y": [0.1292794157435012, 0.14717276738236396, 0.1472213232117362]}, "mutation_prompt": null}
{"id": "b37b1f48-6210-4dbc-8e0c-1a52e3e1e978", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if _ % 10 == 0:  # Dynamic adjustment of population size\n                pop_size = max(4, min(pop_size + 1, 100))\n\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        method = 'TNC' if self.evaluations < self.budget * 0.5 else 'L-BFGS-B'  # Adaptive local search method\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method=method)\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration and exploitation balance using adaptive population size in DE and hybrid local search techniques.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 25 is out of bounds for axis 0 with size 25').", "error": "IndexError('index 25 is out of bounds for axis 0 with size 25')", "parent_id": "83fd4b13-f996-477a-9008-19db4327e8b9", "metadata": {}, "mutation_prompt": null}
{"id": "54655dca-4b93-4bb1-8c2b-0a3281a8eb80", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced differential evolution step by introducing adaptive mutation factor for better exploration.", "configspace": "", "generation": 3, "fitness": 0.8325461120387194, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.833 with standard deviation 0.008. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "83fd4b13-f996-477a-9008-19db4327e8b9", "metadata": {"aucs": [0.8323653370921871, 0.8421632718180765, 0.8231097272058947], "final_y": [0.13468684230909567, 0.1356881254854615, 0.14294488534002814]}, "mutation_prompt": null}
{"id": "3d4db83a-bee7-46de-b913-8e926da1e99b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.95):  # Increased CR to 0.95\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration by increasing the crossover rate in differential evolution to improve search diversity.", "configspace": "", "generation": 3, "fitness": 0.8264649567229303, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.015. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "83fd4b13-f996-477a-9008-19db4327e8b9", "metadata": {"aucs": [0.8466138294668618, 0.8118446544892252, 0.8209363862127039], "final_y": [0.12842055785334883, 0.14399158281510138, 0.1388574413528797]}, "mutation_prompt": null}
{"id": "fb7d6448-b7f8-4873-9cd4-34d8ea4de4b9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def adaptive_differential_evolution(self, func, bounds, iters, pop_size=50, F=0.5, CR=0.5):\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = np.random.uniform(0.5, 1.0)  # Adaptive mutation factor\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        # Noise-resilient sampling strategy\n        perturbed_solutions = [x0 + np.random.normal(0, 1e-2, self.dim) for _ in range(5)]\n        perturbed_fitness = [func(sol) for sol in perturbed_solutions]\n        best_perturbed = perturbed_solutions[np.argmin(perturbed_fitness)]\n        result = minimize(func, best_perturbed, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.adaptive_differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive differential evolution to enhance exploration and integrate noise-resilient sampling in local refinement.", "configspace": "", "generation": 3, "fitness": 0.8051686339007732, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.805 with standard deviation 0.007. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "83fd4b13-f996-477a-9008-19db4327e8b9", "metadata": {"aucs": [0.8002613857415917, 0.7996277448737187, 0.8156167710870095], "final_y": [0.13541169531027342, 0.13942778978111914, 0.14823272972656798]}, "mutation_prompt": null}
{"id": "31241c61-0728-43b6-87f8-0ab393745c4b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size)  # Dynamic population size for better diversity\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration with a dynamic population size in differential evolution for improved solution diversity.", "configspace": "", "generation": 3, "fitness": 0.7985823901441148, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.799 with standard deviation 0.046. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "83fd4b13-f996-477a-9008-19db4327e8b9", "metadata": {"aucs": [0.7381688168419774, 0.8495298980318479, 0.808048455558519], "final_y": [0.1603270993644239, 0.13726213638126372, 0.14180046096775345]}, "mutation_prompt": null}
{"id": "273fdc2a-f3b8-4b3a-9106-ed5848f925b6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n            CR = 0.9 - (0.4 * self.evaluations / self.budget)  # Adaptive crossover rate\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Utilize adaptive crossover rate in differential evolution for enhanced exploration-exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.820262888177323, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.820 with standard deviation 0.006. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "54655dca-4b93-4bb1-8c2b-0a3281a8eb80", "metadata": {"aucs": [0.8270562416459603, 0.8118965115140968, 0.8218359113719118], "final_y": [0.1354847251388197, 0.14291737801239457, 0.13461577557911364]}, "mutation_prompt": null}
{"id": "be9a4576-c9a0-4614-8f52-909e2b13ac0f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))  # Adaptive crossover rate\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing differential evolution by adjusting crossover rate adaptively for improved exploration-exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.8434009885546957, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.021. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "54655dca-4b93-4bb1-8c2b-0a3281a8eb80", "metadata": {"aucs": [0.8561558812616334, 0.813798918277471, 0.8602481661249826], "final_y": [0.126517292216888, 0.14275315985304304, 0.127082862617104]}, "mutation_prompt": null}
{"id": "a0b10618-3c63-4026-a2f1-868c1ac23eb2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        # Adaptive crossover probability\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            CR = 0.5 + 0.4 * (1 - self.evaluations / self.budget)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n\n        # Layer-wise complexity scaling\n        layer_increment = max(1, self.dim // 4)\n        current_dim = min(self.dim, layer_increment)\n        global_solution, global_cost = None, float('inf')\n\n        while current_dim <= self.dim and self.evaluations < self.budget:\n            global_solution, global_cost = self.differential_evolution(\n                func, bounds, iters=iterations, pop_size=50\n            )\n            current_dim += layer_increment\n\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Hybrid exploration-exploitation using adaptive crossover and layer-wise complexity scaling for efficient optimization.", "configspace": "", "generation": 4, "fitness": 0.8329674352940938, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.833 with standard deviation 0.023. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "54655dca-4b93-4bb1-8c2b-0a3281a8eb80", "metadata": {"aucs": [0.8035721607474061, 0.8586819703779538, 0.8366481747569218], "final_y": [0.1350729083701281, 0.12850945940264924, 0.12794697613217076]}, "mutation_prompt": null}
{"id": "00b2cdd4-5de5-4038-b3c4-587ce54f99c9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n            CR = 0.5 + 0.4 * (1 - (self.evaluations / self.budget))  # Dynamic crossover probability\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved mutation strategy by incorporating dynamic crossover probability adjustment to enhance exploration-exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.837419588318513, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.012. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "54655dca-4b93-4bb1-8c2b-0a3281a8eb80", "metadata": {"aucs": [0.8545142309972302, 0.8264599324783042, 0.8312846014800046], "final_y": [0.12722180844581166, 0.1306433570570903, 0.13069717960759408]}, "mutation_prompt": null}
{"id": "c95ab682-aed1-47a8-a330-0c8cbfcb7eff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n\n            CR = 0.7 + 0.3 * np.sin(np.pi * self.evaluations / self.budget)  # Dynamic crossover rate\n\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduced dynamic crossover rate adaptation to enhance exploration and exploitation balance in the differential evolution step.", "configspace": "", "generation": 4, "fitness": 0.8331076487811684, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.833 with standard deviation 0.006. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "54655dca-4b93-4bb1-8c2b-0a3281a8eb80", "metadata": {"aucs": [0.8397242573288668, 0.824282963724881, 0.8353157252897576], "final_y": [0.12304681305484022, 0.12292762652019518, 0.13657298976199927]}, "mutation_prompt": null}
{"id": "0a95fbf4-cf29-49a0-b846-e86a60b53613", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))  # Adaptive crossover rate\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i] or np.std(population) < 1e-5:  # Diversity-based selection\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing differential evolution by incorporating diversity-based selection for maintaining higher population diversity.", "configspace": "", "generation": 5, "fitness": 0.8302977548490044, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.006. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "be9a4576-c9a0-4614-8f52-909e2b13ac0f", "metadata": {"aucs": [0.8359788190685546, 0.8322610942091865, 0.8226533512692717], "final_y": [0.13415486317841585, 0.1273858825992228, 0.1312871688821392]}, "mutation_prompt": null}
{"id": "b1d4806a-ffef-4fd3-bd06-c83797aad829", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        # Dynamically adjust pop_size based on progress\n        pop_size = max(4, int(pop_size * (1 - 0.5 * self.evaluations / self.budget)))\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))  # Adaptive crossover rate\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introducing a dynamic population size strategy to enhance the balance between exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.824656044652491, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.825 with standard deviation 0.024. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "be9a4576-c9a0-4614-8f52-909e2b13ac0f", "metadata": {"aucs": [0.7947605240702975, 0.8252359172301436, 0.8539716926570319], "final_y": [0.14697403935359032, 0.12902213713398958, 0.13271852438033727]}, "mutation_prompt": null}
{"id": "0e70ad9b-39e3-4162-b90b-5d56362d5be7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size)  # Reverted to original pop_size for exploration boost\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))  # Adaptive crossover rate\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='TNC')  # Changed method for refinement\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Slightly boosting exploration and local refinement by adjusting population size and refinement method.", "configspace": "", "generation": 5, "fitness": 0.8195227407823497, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.820 with standard deviation 0.008. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "be9a4576-c9a0-4614-8f52-909e2b13ac0f", "metadata": {"aucs": [0.8087642607453658, 0.8225419055700492, 0.8272620560316336], "final_y": [0.14039346706389044, 0.12986426714879906, 0.12909248255714956]}, "mutation_prompt": null}
{"id": "f5f0f892-52bd-4345-9cc6-75b3fd86b739", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * np.sin(np.pi * (self.evaluations / self.budget)))  # Dynamic mutation factor using sine function\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))  # Adaptive crossover rate\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing differential evolution by dynamically adjusting both crossover and mutation rates for better balance between exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.8175031957723377, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.007. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "be9a4576-c9a0-4614-8f52-909e2b13ac0f", "metadata": {"aucs": [0.8114587158449572, 0.8268082472621467, 0.8142426242099086], "final_y": [0.13538392006845756, 0.12763803509734006, 0.14254149570203312]}, "mutation_prompt": null}
{"id": "1d674843-6145-4abd-9d48-497b45a82ace", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improving differential evolution by dynamically resizing the population size and introducing elitism for enhanced performance.", "configspace": "", "generation": 5, "fitness": 0.8366121957772811, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.019. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "be9a4576-c9a0-4614-8f52-909e2b13ac0f", "metadata": {"aucs": [0.8099082870335662, 0.8518446335447369, 0.84808366675354], "final_y": [0.14249357120078643, 0.12836525878917748, 0.13190941565019154]}, "mutation_prompt": null}
{"id": "e65a89d8-42b0-4bd9-a481-6a59774b3aea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            diversity = np.std(population, axis=0).mean()\n            F = 0.5 + (0.5 * diversity)  # Adaptive F based on diversity\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive F and CR parameters based on population diversity to enhance exploration and exploitation in differential evolution.", "configspace": "", "generation": 6, "fitness": 0.7876293346856821, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.788 with standard deviation 0.025. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "1d674843-6145-4abd-9d48-497b45a82ace", "metadata": {"aucs": [0.7560456644119335, 0.791022408907229, 0.8158199307378833], "final_y": [0.16018484581985615, 0.1498419562689144, 0.14268465778006434]}, "mutation_prompt": null}
{"id": "2d309796-1af5-475e-9615-60a3fa02db47", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)\n            CR = np.var(fitness) / (np.max(fitness) - np.min(fitness) + 1e-9)  # Change: Adapt CR based on fitness variance\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced differential evolution by adapting crossover rates based on fitness variance for improved exploration and exploitation balance.", "configspace": "", "generation": 6, "fitness": 0.8105972825251859, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.023. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "1d674843-6145-4abd-9d48-497b45a82ace", "metadata": {"aucs": [0.7802863479462293, 0.8348654528748015, 0.8166400467545271], "final_y": [0.15307554766331544, 0.13114842862047282, 0.13070171541250641]}, "mutation_prompt": null}
{"id": "f497b7ab-25af-44d0-938c-cdcb2336fd7d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * (1 - fitness.min() / fitness.max()))  # Adaptive scaling based on fitness spread\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Incorporating adaptive mutation scaling and dynamic crossover probability adjustments in differential evolution to enhance exploration and exploitation balance.", "configspace": "", "generation": 6, "fitness": 0.8014050083677504, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.801 with standard deviation 0.008. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "1d674843-6145-4abd-9d48-497b45a82ace", "metadata": {"aucs": [0.7935809818621887, 0.7977138764125951, 0.8129201668284676], "final_y": [0.14724933159932052, 0.14701943488023272, 0.1395262796133322]}, "mutation_prompt": null}
{"id": "442a0487-e9e2-4fbe-a92e-0a8dad85e11c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())  # Line changed for adaptive F\n            CR = 0.7 + (0.3 * np.random.rand())  # Line changed for adaptive CR\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing differential evolution by introducing adaptive control parameters and refinement through local search for robust optimization.", "configspace": "", "generation": 6, "fitness": 0.8227278770045791, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.823 with standard deviation 0.010. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "1d674843-6145-4abd-9d48-497b45a82ace", "metadata": {"aucs": [0.8096256672348985, 0.8338941065181726, 0.8246638572606662], "final_y": [0.13338672105048077, 0.1392955080883569, 0.1289146581568651]}, "mutation_prompt": null}
{"id": "c78c39a5-5bc7-4401-bfd0-d85ef53626fb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = np.random.uniform(0.5, 1.0)  # Adaptive mutation factor\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance differential evolution by introducing adaptive mutation factors and elitism for improved convergence.", "configspace": "", "generation": 6, "fitness": 0.8157507547448223, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.816 with standard deviation 0.006. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "1d674843-6145-4abd-9d48-497b45a82ace", "metadata": {"aucs": [0.8246396039602598, 0.8132244944379107, 0.8093881658362967], "final_y": [0.13531674415009642, 0.1441295928613603, 0.14081212344482807]}, "mutation_prompt": null}
{"id": "a6005ff7-4730-47b5-bede-46c1d20dd2ca", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())  # Line changed for adaptive F\n            CR = 0.7 + (0.3 * np.random.rand())  # Line changed for adaptive CR\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size += 2  # Changed to increase by 2 instead of 1\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improving differential evolution by introducing adaptive population size and elitism for enhanced convergence.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "442a0487-e9e2-4fbe-a92e-0a8dad85e11c", "metadata": {}, "mutation_prompt": null}
{"id": "bbc5ea5a-23d4-4a08-b9b7-b694e97457eb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())  # Line changed for adaptive F\n            CR = 0.7 + (0.3 * np.random.rand())  # Line changed for adaptive CR\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            # New line: Perform local search on the best individual every few generations\n            if generation % 5 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Incorporating memetic enhancements into the differential evolution phase by introducing a local search every few iterations for faster convergence and better solution quality.", "configspace": "", "generation": 7, "fitness": 0.9036893301995027, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.029. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "442a0487-e9e2-4fbe-a92e-0a8dad85e11c", "metadata": {"aucs": [0.8630597554791912, 0.917571719993835, 0.9304365151254814], "final_y": [0.13039511519522407, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "768061c7-f361-4525-9aaf-11176de38a5a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        stagnation_counter = 0  # Counter for stagnation detection\n        best_fitness = np.min(fitness)\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())\n            CR = 0.7 + (0.3 * np.random.rand())\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n\n            current_best_fitness = np.min(fitness)\n            if current_best_fitness >= best_fitness:\n                stagnation_counter += 1\n            else:\n                best_fitness = current_best_fitness\n                stagnation_counter = 0\n\n            if stagnation_counter > 10 and pop_size < 100:  # Change: Adapt population size based on stagnation\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n                stagnation_counter = 0  # Reset on population increase\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introducing a feedback mechanism to adaptively tune population size based on convergence rate in differential evolution.", "configspace": "", "generation": 7, "fitness": 0.8909410626559794, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.047. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "442a0487-e9e2-4fbe-a92e-0a8dad85e11c", "metadata": {"aucs": [0.8248149528486217, 0.917571719993835, 0.9304365151254814], "final_y": [0.1265917293470573, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "a705e1bb-7894-4436-a5d9-9417a592820a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(8, pop_size // 2)  # Adjusted initial pop_size\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = np.random.uniform(0.5, 1.0)  # Adaptive mutation factor\n            CR = np.random.uniform(0.6, 0.9)  # Adaptive crossover rate\n            ranked_indices = np.argsort(fitness)  # Rank population\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                if i < pop_size // 2:\n                    a, b, c = population[ranked_indices[np.random.choice(pop_size // 2, 3, replace=False)]]\n                else:\n                    a, b, c = population[np.random.choice(population.shape[0], 3, replace=False)]\n                \n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            \n            if pop_size < 100:\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        best_idx = np.argmin(fitness)\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved differential evolution by incorporating adaptive population ranking and dynamic mutation strategies for enhanced robustness and efficiency.", "configspace": "", "generation": 7, "fitness": 0.9001514846662403, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.900 with standard deviation 0.036. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "442a0487-e9e2-4fbe-a92e-0a8dad85e11c", "metadata": {"aucs": [0.8503414138310285, 0.917571719993835, 0.9325413201738576], "final_y": [0.12427790472750411, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "41ec4391-9500-4584-8fa0-1b182374008b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())  # Line changed for adaptive F\n            CR = 0.7 + (0.3 * np.random.rand())  # Line changed for adaptive CR\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 12 # Adjust number of iterations\n        global_solution = np.random.rand(self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        for i in range(2, self.dim + 1, 2): # Gradual dimension increase\n            global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n            local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n            if local_cost < global_cost:\n                global_solution = local_solution\n        return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a layered optimization strategy with gradual complexity increase and robustness checks to enhance the adaptive differential evolution method.", "configspace": "", "generation": 7, "fitness": 0.8843197478530754, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.884 with standard deviation 0.057. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "442a0487-e9e2-4fbe-a92e-0a8dad85e11c", "metadata": {"aucs": [0.8034741952008849, 0.917571719993835, 0.9319133283645062], "final_y": [0.14786207489442504, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
