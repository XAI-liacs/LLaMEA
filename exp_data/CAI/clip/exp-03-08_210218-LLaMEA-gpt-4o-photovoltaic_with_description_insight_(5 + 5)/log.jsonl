{"id": "10e1678a-541a-4c38-be74-73130da5d59b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.7  # Differential weight\n        self.cr = 0.9  # Crossover probability\n\n    def differential_evolution(self, func, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        fitness = np.apply_along_axis(func, 1, pop)\n        budget_used = self.population_size\n\n        while budget_used < self.budget:\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + self.f * (x1 - x2), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n                budget_used += 1\n                if trial_fitness < fitness[i]:\n                    pop[i], fitness[i] = trial, trial_fitness\n\n                if budget_used >= self.budget:\n                    break\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]\n\n    def local_search(self, func, x, bounds):\n        def penalty_func(x):\n            penalty = 0\n            if np.any(x < bounds.lb) or np.any(x > bounds.ub):\n                penalty = np.sum((np.minimum(x - bounds.lb, 0) ** 2) + \n                                 (np.maximum(x - bounds.ub, 0) ** 2))\n            return func(x) + penalty\n\n        res = minimize(penalty_func, x, bounds=[(l, u) for l, u in zip(bounds.lb, bounds.ub)],\n                       method='L-BFGS-B', options={'maxfun': self.budget - self.budget_used})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        # First stage: Differential Evolution\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        self.budget_used = self.budget - self.local_search_budget\n        # Second stage: Local Search\n        best_solution, best_fitness = self.local_search(func, best_solution, bounds)\n        return best_solution", "name": "HybridDELocalSearch", "description": "A hybrid differential evolution and local search metaheuristic that adapts layer complexity dynamically while maintaining robust design against noise.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\nAttributeError: 'HybridDELocalSearch' object has no attribute 'local_search_budget'\n.", "error": "AttributeError(\"'HybridDELocalSearch' object has no attribute 'local_search_budget'\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\nAttributeError: 'HybridDELocalSearch' object has no attribute 'local_search_budget'\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "1e24fe01-91b6-48ab-8c02-c2a96ba21572", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n        \n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic combining Differential Evolution for global exploration with Nelder-Mead for local refinement, incorporating adaptive layer-wise complexity scaling and robustness integration to efficiently tackle high-dimensional noisy optimization.", "configspace": "", "generation": 0, "fitness": 0.8207331142167114, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.821 with standard deviation 0.005. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8142031292869409, 0.8224635234774964, 0.8255326898856965], "final_y": [0.14139800255773605, 0.13069222351285215, 0.1306985084970148]}, "mutation_prompt": null}
{"id": "2b42b091-a973-4518-831e-775eef0bcdac", "solution": "import numpy as np\n\nclass HybridLayerOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.crossover_rate = 0.8\n        self.mutation_rate = 0.1\n        self.current_budget = 0\n        self.layers_increment_step = 5\n\n    def __call__(self, func):\n        # Initialize population\n        pop = self.initialize_population(func.bounds.lb, func.bounds.ub)\n        best_solution = None\n        best_score = float('-inf')\n        \n        while self.current_budget < self.budget:\n            scores = self.evaluate_population(pop, func)\n\n            # Update the best solution found so far\n            for i, score in enumerate(scores):\n                if score > best_score:\n                    best_score = score\n                    best_solution = pop[i].copy()\n\n            # Selection\n            selected_parents = self.tournament_selection(pop, scores)\n\n            # Crossover\n            offspring = self.crossover(selected_parents, func.bounds.lb, func.bounds.ub)\n\n            # Mutation\n            self.mutate(offspring, func.bounds.lb, func.bounds.ub)\n\n            # Local Search and Incremental Layer Optimization\n            self.local_search_and_layer_increment(offspring, func)\n\n            # Next generation\n            pop = offspring\n\n        return best_solution\n\n    def initialize_population(self, lb, ub):\n        return [np.random.uniform(lb, ub, self.dim) for _ in range(self.population_size)]\n\n    def evaluate_population(self, pop, func):\n        scores = []\n        for p in pop:\n            if self.current_budget < self.budget:\n                score = func(p)\n                self.current_budget += 1\n                scores.append(score)\n            else:\n                break\n        return scores\n    \n    def tournament_selection(self, pop, scores):\n        selected = []\n        for _ in range(self.population_size):\n            i, j = np.random.choice(len(pop), 2, replace=False)\n            if scores[i] > scores[j]:\n                selected.append(pop[i])\n            else:\n                selected.append(pop[j])\n        return selected\n\n    def crossover(self, parents, lb, ub):\n        offspring = []\n        for i in range(0, len(parents), 2):\n            if np.random.rand() < self.crossover_rate:\n                cross_point = np.random.randint(1, self.dim - 1)\n                child1 = np.concatenate((parents[i][:cross_point], parents[i+1][cross_point:]))\n                child2 = np.concatenate((parents[i+1][:cross_point], parents[i][cross_point:]))\n                offspring.extend([child1, child2])\n            else:\n                offspring.extend([parents[i], parents[i+1]])\n        return np.clip(offspring, lb, ub)\n\n    def mutate(self, offspring, lb, ub):\n        for individual in offspring:\n            if np.random.rand() < self.mutation_rate:\n                mutation_indices = np.random.randint(0, self.dim, int(self.dim * 0.1))\n                individual[mutation_indices] = np.random.uniform(lb[mutation_indices], ub[mutation_indices])\n        return np.clip(offspring, lb, ub)\n    \n    def local_search_and_layer_increment(self, offspring, func):\n        for i, ind in enumerate(offspring):\n            if self.current_budget < self.budget:\n                perturbed = ind + np.random.normal(0, 0.1, self.dim)\n                perturbed_score = func(np.clip(perturbed, func.bounds.lb, func.bounds.ub))\n                self.current_budget += 1\n                if perturbed_score > func(ind):\n                    offspring[i] = perturbed\n                \n                # Increment layer complexity if budget allows\n                if self.current_budget + self.layers_increment_step <= self.budget:\n                    new_dim = min(self.dim + self.layers_increment_step, len(func.bounds.ub))\n                    if new_dim > len(ind):\n                        extra_layers = np.random.uniform(func.bounds.lb[len(ind):new_dim], func.bounds.ub[len(ind):new_dim])\n                        offspring[i] = np.concatenate((offspring[i], extra_layers))\n                        self.dim = new_dim", "name": "HybridLayerOptimizer", "description": "A hybrid genetic and local search algorithm that adaptively refines solutions through modular decomposition and layer incrementing for efficient optimization of complex multilayer structures.", "configspace": "", "generation": 0, "fitness": 0.7313918467434322, "feedback": "The algorithm HybridLayerOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.731 with standard deviation 0.027. And the mean value of best solutions found was 0.186 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": null, "metadata": {"aucs": [0.7049150692087611, 0.76890311738062, 0.7203573536409152], "final_y": [0.1969523871887523, 0.1702433386818767, 0.19046050992345487]}, "mutation_prompt": null}
{"id": "30929ab5-864b-48d0-8763-1bf98e08c399", "solution": "import numpy as np\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, pop_size, F=0.8, CR=0.9):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        population = bounds[0] + (bounds[1] - bounds[0]) * np.random.rand(pop_size, self.dim)\n        fitness = np.apply_along_axis(func, 1, population)\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        \n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n            \n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution, fitness[best_idx]\n\n    def local_refinement(self, solution, func):\n        step_size = 0.01 * (func.bounds.ub - func.bounds.lb)\n        for _ in range(10):  # Local refinement iterations\n            for i in range(self.dim):\n                if self.evaluations >= self.budget:\n                    break\n                perturb = np.zeros(self.dim)\n                perturb[i] = step_size[i]\n                candidate = solution + perturb\n                candidate_fitness = func(candidate)\n                self.evaluations += 1\n                if candidate_fitness < func(solution):\n                    solution = candidate\n                else:\n                    candidate = solution - perturb\n                    candidate_fitness = func(candidate)\n                    self.evaluations += 1\n                    if candidate_fitness < func(solution):\n                        solution = candidate\n        return solution\n\n    def __call__(self, func):\n        pop_size = 10 + self.dim * 2\n        best_solution, best_fitness = self.differential_evolution(func, pop_size)\n        best_solution = self.local_refinement(best_solution, func)\n        return best_solution", "name": "PhotonicOptimizer", "description": "A hybrid metaheuristic that combines Differential Evolution for global exploration with a local gradient-based refinement, leveraging a multistage approach to gradually increase dimensionality and ensure robustness against parameter perturbations.", "configspace": "", "generation": 0, "fitness": 0.831508493732701, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.832 with standard deviation 0.017. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8151777668251561, 0.854863455969139, 0.8244842584038077], "final_y": [0.13258029681166006, 0.12784849278532806, 0.13451538085380266]}, "mutation_prompt": null}
{"id": "cdc28e68-1fc8-49e5-b7e7-275688822e5f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_prob = 0.9\n        self.local_iter = 5\n        self.convergence_threshold = 1e-6\n\n    def _differential_evolution(self, func):\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in pop])\n        evaluations = self.population_size\n        best_idx = np.argmin(scores)\n        best = pop[best_idx, :]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.mutation_factor * (b - c), func.bounds.lb, func.bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.crossover_prob\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i, :])\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < scores[i]:\n                    scores[i] = trial_score\n                    pop[i, :] = trial\n\n                    if trial_score < scores[best_idx]:\n                        best_idx = i\n                        best = pop[i, :]\n\n                if evaluations >= self.budget:\n                    break\n\n            # Early stopping if convergence is reached\n            if np.std(scores) < self.convergence_threshold:\n                break\n\n        return best\n\n    def _local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(func.bounds.lb, func.bounds.ub)), options={'maxiter': self.local_iter})\n        return res.x\n\n    def __call__(self, func):\n        best_solution = self._differential_evolution(func)\n        refined_solution = self._local_optimization(func, best_solution)\n        return refined_solution", "name": "HybridOptimizer", "description": "A hybrid algorithm that combines Differential Evolution (DE) for global exploration and a local optimization method with layer-wise perturbations to iteratively refine solutions while dynamically allocating resources based on convergence rates.", "configspace": "", "generation": 0, "fitness": 0.8340793165807155, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.834 with standard deviation 0.015. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8131160507484498, 0.8444243155790998, 0.8446975834145971], "final_y": [0.1382166316309421, 0.12314647557659553, 0.126331834195632]}, "mutation_prompt": null}
{"id": "aa3efd10-b707-4027-9523-823a61acc0d2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.f = 0.7  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.local_search_budget = int(0.2 * budget)  # Allocate 20% of the budget for local search\n\n    def differential_evolution(self, func, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        fitness = np.apply_along_axis(func, 1, pop)\n        budget_used = self.population_size\n\n        while budget_used < (self.budget - self.local_search_budget):\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + self.f * (x1 - x2), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n                budget_used += 1\n                if trial_fitness < fitness[i]:\n                    pop[i], fitness[i] = trial, trial_fitness\n\n                if budget_used >= (self.budget - self.local_search_budget):\n                    break\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]\n\n    def local_search(self, func, x, bounds):\n        def penalty_func(x):\n            penalty = 0\n            if np.any(x < bounds.lb) or np.any(x > bounds.ub):\n                penalty = np.sum((np.minimum(x - bounds.lb, 0) ** 2) + \n                                 (np.maximum(x - bounds.ub, 0) ** 2))\n            return func(x) + penalty\n\n        res = minimize(penalty_func, x, bounds=[(l, u) for l, u in zip(bounds.lb, bounds.ub)],\n                       method='L-BFGS-B', options={'maxfun': self.local_search_budget})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        # First stage: Differential Evolution\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        self.budget_used = self.budget - self.local_search_budget\n        # Second stage: Local Search\n        best_solution, best_fitness = self.local_search(func, best_solution, bounds)\n        return best_solution", "name": "HybridDELocalSearch", "description": "Improved hybrid differential evolution and local search metaheuristic with fixed local search budget allocation to maintain robust design against noise.", "configspace": "", "generation": 1, "fitness": 0.8025072650531625, "feedback": "The algorithm HybridDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.001. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "10e1678a-541a-4c38-be74-73130da5d59b", "metadata": {"aucs": [0.8039174603620616, 0.8018123904428055, 0.8017919443546204], "final_y": [0.12100820397676737, 0.14470977533751983, 0.11342871765110285]}, "mutation_prompt": null}
{"id": "247dc0d1-dd0f-4c5a-986c-d936efd8a88b", "solution": "import numpy as np\n\nclass HybridLayerOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.crossover_rate = 0.8\n        self.mutation_rate = 0.1\n        self.current_budget = 0\n        self.layers_increment_step = 5\n\n    def __call__(self, func):\n        # Initialize population\n        pop = self.initialize_population(func.bounds.lb, func.bounds.ub)\n        best_solution = None\n        best_score = float('-inf')\n        \n        while self.current_budget < self.budget:\n            scores = self.evaluate_population(pop, func)\n\n            # Update the best solution found so far\n            for i, score in enumerate(scores):\n                if score > best_score:\n                    best_score = score\n                    best_solution = pop[i].copy()\n\n            # Selection\n            selected_parents = self.tournament_selection(pop, scores)\n\n            # Crossover\n            offspring = self.crossover(selected_parents, func.bounds.lb, func.bounds.ub)\n\n            # Mutation\n            self.mutate(offspring, func.bounds.lb, func.bounds.ub)\n\n            # Local Search and Incremental Layer Optimization\n            self.local_search_and_layer_increment(offspring, func)\n\n            # Elitism: Retain the best solution\n            worst_offspring_index = np.argmin([func(ind) for ind in offspring])\n            offspring[worst_offspring_index] = best_solution.copy()\n\n            # Next generation\n            pop = offspring\n\n        return best_solution\n\n    def initialize_population(self, lb, ub):\n        return [np.random.uniform(lb, ub, self.dim) for _ in range(self.population_size)]\n\n    def evaluate_population(self, pop, func):\n        scores = []\n        for p in pop:\n            if self.current_budget < self.budget:\n                score = func(p)\n                self.current_budget += 1\n                scores.append(score)\n            else:\n                break\n        return scores\n    \n    def tournament_selection(self, pop, scores):\n        selected = []\n        for _ in range(self.population_size):\n            i, j = np.random.choice(len(pop), 2, replace=False)\n            if scores[i] > scores[j]:\n                selected.append(pop[i])\n            else:\n                selected.append(pop[j])\n        return selected\n\n    def crossover(self, parents, lb, ub):\n        offspring = []\n        for i in range(0, len(parents), 2):\n            if np.random.rand() < self.crossover_rate:\n                cross_point = np.random.randint(1, self.dim - 1)\n                child1 = np.concatenate((parents[i][:cross_point], parents[i+1][cross_point:]))\n                child2 = np.concatenate((parents[i+1][:cross_point], parents[i][cross_point:]))\n                offspring.extend([child1, child2])\n            else:\n                offspring.extend([parents[i], parents[i+1]])\n        return np.clip(offspring, lb, ub)\n\n    def mutate(self, offspring, lb, ub):\n        for individual in offspring:\n            if np.random.rand() < self.mutation_rate:\n                mutation_indices = np.random.randint(0, self.dim, int(self.dim * 0.1))\n                individual[mutation_indices] = np.random.uniform(lb[mutation_indices], ub[mutation_indices])\n        return np.clip(offspring, lb, ub)\n    \n    def local_search_and_layer_increment(self, offspring, func):\n        for i, ind in enumerate(offspring):\n            if self.current_budget < self.budget:\n                perturbed = ind + np.random.normal(0, 0.1, self.dim)\n                perturbed_score = func(np.clip(perturbed, func.bounds.lb, func.bounds.ub))\n                self.current_budget += 1\n                if perturbed_score > func(ind):\n                    offspring[i] = perturbed\n                \n                # Increment layer complexity if budget allows\n                if self.current_budget + self.layers_increment_step <= self.budget:\n                    new_dim = min(self.dim + self.layers_increment_step, len(func.bounds.ub))\n                    if new_dim > len(ind):\n                        extra_layers = np.random.uniform(func.bounds.lb[len(ind):new_dim], func.bounds.ub[len(ind):new_dim])\n                        offspring[i] = np.concatenate((offspring[i], extra_layers))\n                        self.dim = new_dim", "name": "HybridLayerOptimizer", "description": "Introduced elitism to retain the best solution between generations to improve convergence in the HybridLayerOptimizer.", "configspace": "", "generation": 1, "fitness": 0.7309146495733702, "feedback": "The algorithm HybridLayerOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.731 with standard deviation 0.028. And the mean value of best solutions found was 0.186 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "2b42b091-a973-4518-831e-775eef0bcdac", "metadata": {"aucs": [0.7046703141697086, 0.76890311738062, 0.7191705171697818], "final_y": [0.1969486853111726, 0.1702433386818767, 0.19103396660959382]}, "mutation_prompt": null}
{"id": "bd4a671e-9c78-4784-9312-3196802dd1ab", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            F_dynamic = F * (1 - iteration / max_iter)  # Dynamically adjusting F\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + F_dynamic * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n        \n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced the mutation strategy in Differential Evolution by adjusting the mutation factor dynamically based on the iteration count to improve exploration-exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.8278736631968805, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.008. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "1e24fe01-91b6-48ab-8c02-c2a96ba21572", "metadata": {"aucs": [0.824763705982453, 0.8386461328582404, 0.8202111507499481], "final_y": [0.12542112186288823, 0.138035244332823, 0.13596113271644938]}, "mutation_prompt": null}
{"id": "178e44fe-148d-4704-8588-706931982004", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n        \n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 30  # Changed from 20 to 30\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic combining Differential Evolution for global exploration with Nelder-Mead for local refinement, now with an increased population size for enhanced diversity and exploration efficacy in high-dimensional noisy optimization.", "configspace": "", "generation": 1, "fitness": 0.8373435151702595, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.024. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "1e24fe01-91b6-48ab-8c02-c2a96ba21572", "metadata": {"aucs": [0.8059276198795662, 0.8409827109024322, 0.8651202147287802], "final_y": [0.1326023234073247, 0.12843754230339355, 0.12720580055048625]}, "mutation_prompt": null}
{"id": "50257f3b-be13-4fca-a41f-c36a5229b36f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            CR = 0.5 + 0.5 * (iteration / max_iter)  # Adaptive crossover probability\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n\n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 10 + self.dim  # Dynamic population size\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic enhancing Differential Evolution with adaptive crossover probability and dynamic population size, combined with Nelder-Mead for local refinement.", "configspace": "", "generation": 1, "fitness": 0.8306967221017744, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.831 with standard deviation 0.016. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "1e24fe01-91b6-48ab-8c02-c2a96ba21572", "metadata": {"aucs": [0.8475228505367242, 0.8093115829877872, 0.8352557327808117], "final_y": [0.12284317259382538, 0.1367231249366232, 0.13376277744658804]}, "mutation_prompt": null}
{"id": "98740da3-c40f-4107-b272-db01a4c2354d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                F_dynamic = F * (1 - iteration / max_iter)  # Line 1 change\n                mutant = np.clip(x0 + F_dynamic * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < (CR - iteration / (2 * max_iter))  # Line 2 change\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n        \n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 30  # Changed from 20 to 30\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Improved the mutation strategy by incorporating a time-varying scaling factor and adjusted the crossover rate for better exploration-exploitation balance in high-dimensional noisy optimization.", "configspace": "", "generation": 2, "fitness": 0.8357367497769376, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.836 with standard deviation 0.025. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "178e44fe-148d-4704-8588-706931982004", "metadata": {"aucs": [0.8134529070450778, 0.8709605763926924, 0.8227967658930425], "final_y": [0.12897498987516254, 0.11845054105980657, 0.12777875654904436]}, "mutation_prompt": null}
{"id": "156b4776-c385-4c9d-926d-1e6f3cf18ca2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + F * (x1 - x2), bounds.lb, bounds.ub)\n                # Adjust CR dynamically based on iteration\n                dynamic_CR = CR * (1 - iteration / max_iter)\n                crossover = np.random.rand(self.dim) < dynamic_CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n        \n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 30  # Changed from 20 to 30\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced the crossover probability in Differential Evolution by adjusting it dynamically based on the iteration, aiming for better balance between exploration and exploitation. ", "configspace": "", "generation": 2, "fitness": 0.8334775590747575, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.833 with standard deviation 0.007. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "178e44fe-148d-4704-8588-706931982004", "metadata": {"aucs": [0.8239661817778949, 0.8349975545479588, 0.8414689408984191], "final_y": [0.13881524268820322, 0.13022048387462004, 0.13078953542610605]}, "mutation_prompt": null}
{"id": "7dca17a5-3ac6-460f-9ed2-1654782858f4", "solution": "import numpy as np\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, pop_size, F=0.8, CR=0.9):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        population = bounds[0] + (bounds[1] - bounds[0]) * np.random.rand(pop_size, self.dim)\n        fitness = np.apply_along_axis(func, 1, population)\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        \n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                CR = 0.5 + 0.4 * (1 - self.evaluations / self.budget)  # Dynamic CR update\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n            \n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution, fitness[best_idx]\n\n    def local_refinement(self, solution, func):\n        step_size = 0.01 * (func.bounds.ub - func.bounds.lb)\n        for _ in range(10):  # Local refinement iterations\n            step_size *= 0.95  # Adaptive step-size reduction\n            for i in range(self.dim):\n                if self.evaluations >= self.budget:\n                    break\n                perturb = np.zeros(self.dim)\n                perturb[i] = step_size[i]\n                candidate = solution + perturb\n                candidate_fitness = func(candidate)\n                self.evaluations += 1\n                if candidate_fitness < func(solution):\n                    solution = candidate\n                else:\n                    candidate = solution - perturb\n                    candidate_fitness = func(candidate)\n                    self.evaluations += 1\n                    if candidate_fitness < func(solution):\n                        solution = candidate\n        return solution\n\n    def __call__(self, func):\n        pop_size = 10 + self.dim * 2\n        best_solution, best_fitness = self.differential_evolution(func, pop_size)\n        best_solution = self.local_refinement(best_solution, func)\n        return best_solution", "name": "PhotonicOptimizer", "description": "Improved PhotonicOptimizer by adjusting the crossover probability dynamically and introducing adaptive step size for enhanced local refinement.", "configspace": "", "generation": 2, "fitness": 0.8262983488738899, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.023. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "30929ab5-864b-48d0-8763-1bf98e08c399", "metadata": {"aucs": [0.7986115558594884, 0.854197049730369, 0.8260864410318125], "final_y": [0.1404055958966004, 0.12302517222175902, 0.13106474288028014]}, "mutation_prompt": null}
{"id": "47d69e14-1c3f-46ce-bd72-caee5b24ed61", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            CR = 0.5 + 0.5 * (iteration / max_iter)  # Adaptive crossover probability\n            dynamic_F = 0.5 + 0.3 * (np.sin(iteration / max_iter * np.pi))  # Dynamic mutation factor\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + dynamic_F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n\n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 10 + self.dim  # Dynamic population size\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduced a dynamic mutation factor in the Differential Evolution algorithm to enhance exploration and exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.8510846550096774, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.851 with standard deviation 0.015. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "50257f3b-be13-4fca-a41f-c36a5229b36f", "metadata": {"aucs": [0.8400499470969199, 0.8403320873657568, 0.8728719305663559], "final_y": [0.12665268610552072, 0.13602821295810097, 0.12416495839203212]}, "mutation_prompt": null}
{"id": "75861d33-d955-4066-ac48-1d1050c952f5", "solution": "import numpy as np\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, pop_size, F=0.8, CR=0.9):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        population = bounds[0] + (bounds[1] - bounds[0]) * np.random.rand(pop_size, self.dim)\n        fitness = np.apply_along_axis(func, 1, population)\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        \n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds[0], bounds[1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    if f < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n            \n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution, fitness[best_idx]\n\n    def local_refinement(self, solution, func):\n        step_size = 0.01 * (func.bounds.ub - func.bounds.lb) * (1 - func(solution))\n        for _ in range(10):  # Local refinement iterations\n            for i in range(self.dim):\n                if self.evaluations >= self.budget:\n                    break\n                perturb = np.zeros(self.dim)\n                perturb[i] = step_size[i]\n                candidate = solution + perturb\n                candidate_fitness = func(candidate)\n                self.evaluations += 1\n                if candidate_fitness < func(solution):\n                    solution = candidate\n                else:\n                    candidate = solution - perturb\n                    candidate_fitness = func(candidate)\n                    self.evaluations += 1\n                    if candidate_fitness < func(solution):\n                        solution = candidate\n        return solution\n\n    def __call__(self, func):\n        pop_size = 10 + self.dim * 2\n        best_solution, best_fitness = self.differential_evolution(func, pop_size)\n        best_solution = self.local_refinement(best_solution, func)\n        return best_solution", "name": "PhotonicOptimizer", "description": "Enhanced the local refinement method by dynamically adjusting the step size based on the current fitness value to improve exploitation in high-dimensional spaces.", "configspace": "", "generation": 2, "fitness": 0.8267698992151571, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.004. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "30929ab5-864b-48d0-8763-1bf98e08c399", "metadata": {"aucs": [0.8209510092417358, 0.8315990215980885, 0.8277596668056468], "final_y": [0.13693762004365528, 0.13253200336068316, 0.13410666449141373]}, "mutation_prompt": null}
{"id": "2d612665-a9e9-41ad-afc2-0304ca667b7e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_prob = 0.9\n        self.local_iter = 10  # Increased from 5 to 10\n        self.convergence_threshold = 1e-6\n\n    def _differential_evolution(self, func):\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in pop])\n        evaluations = self.population_size\n        best_idx = np.argmin(scores)\n        best = pop[best_idx, :]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.mutation_factor * (b - c), func.bounds.lb, func.bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.crossover_prob\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i, :])\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < scores[i]:\n                    scores[i] = trial_score\n                    pop[i, :] = trial\n\n                    if trial_score < scores[best_idx]:\n                        best_idx = i\n                        best = pop[i, :]\n\n                if evaluations >= self.budget:\n                    break\n\n            # Early stopping if convergence is reached\n            if np.std(scores) < self.convergence_threshold:\n                break\n\n            self.mutation_factor = 0.5 + 0.5 * np.std(scores) / (1 + np.std(scores))  # Dynamic mutation factor adjustment\n\n        return best\n\n    def _local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(func.bounds.lb, func.bounds.ub)), options={'maxiter': self.local_iter})\n        return res.x\n\n    def __call__(self, func):\n        best_solution = self._differential_evolution(func)\n        refined_solution = self._local_optimization(func, best_solution)\n        return refined_solution", "name": "HybridOptimizer", "description": "Improved local optimization by increasing max iterations and dynamically adjusting mutation factor for better convergence.", "configspace": "", "generation": 3, "fitness": 0.8716825119441275, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.015. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cdc28e68-1fc8-49e5-b7e7-275688822e5f", "metadata": {"aucs": [0.8527254171220209, 0.8719748395020877, 0.8903472792082741], "final_y": [0.11663564378263991, 0.11647658496727464, 0.11573598373760285]}, "mutation_prompt": null}
{"id": "7c0b6bc6-0814-4134-9c41-d78ae1c10bae", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                F_dynamic = F * (1 - iteration / max_iter)  # Dynamic F\n                mutant = np.clip(x0 + F_dynamic * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n        \n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 30  # Changed from 20 to 30\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "This update introduces a dynamic adaptation of the mutation factor F in Differential Evolution, allowing it to decrease over time to enhance exploitation in later stages of optimization.", "configspace": "", "generation": 3, "fitness": 0.8344932361179698, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.834 with standard deviation 0.031. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "178e44fe-148d-4704-8588-706931982004", "metadata": {"aucs": [0.7928230818430047, 0.8436534053924227, 0.8670032211184818], "final_y": [0.1365844787570365, 0.13458746717336534, 0.12421824612266952]}, "mutation_prompt": null}
{"id": "6f054de5-3f5e-4dbc-86d6-e934eb5d3672", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            CR = 0.5 + 0.5 * (iteration / max_iter)  # Adaptive crossover probability\n            dynamic_F = 0.5 + 0.3 * (np.sin(iteration / max_iter * np.pi))  # Dynamic mutation factor\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + dynamic_F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n\n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        remaining_budget = self.budget - self.evaluations\n        refinement_budget = min(remaining_budget, 50)  # Fixed budget or remaining evaluations\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': refinement_budget, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 10 + self.dim  # Dynamic population size\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduced adaptive scaling of the local refinement budget based on convergence rate to enhance exploitation within the evaluation budget constraints.", "configspace": "", "generation": 3, "fitness": 0.8461262158644732, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.846 with standard deviation 0.020. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "47d69e14-1c3f-46ce-bd72-caee5b24ed61", "metadata": {"aucs": [0.848823961724538, 0.8204058760553261, 0.8691488098135554], "final_y": [0.1328662236627608, 0.13521490035932626, 0.1202215170490133]}, "mutation_prompt": null}
{"id": "e9f34e78-c0c4-497b-96ef-730f6b226d4c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n        \n        for iteration in range(max_iter):\n            CR = 0.5 + 0.5 * (iteration / max_iter)  # Adaptive crossover probability\n            dynamic_F = 0.5 + 0.2 * (1 + np.sin(2 * np.pi * iteration / max_iter))  # Improved dynamic mutation factor\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                layer_wise_F = F * (1 + np.random.normal(0, 0.1, self.dim))  # Layer-wise mutation rate\n                mutant = np.clip(x0 + dynamic_F * layer_wise_F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n\n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 10 + self.dim  # Dynamic population size\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced dynamic mutation strategy by introducing sinusoidal modulation and adaptive layer-wise mutation rate to improve exploration and exploitation balance in high-dimensional noisy optimization.", "configspace": "", "generation": 3, "fitness": 0.8488420141159537, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.018. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "47d69e14-1c3f-46ce-bd72-caee5b24ed61", "metadata": {"aucs": [0.8246198552018151, 0.8662961478161573, 0.8556100393298891], "final_y": [0.12786637390556777, 0.11948552447004834, 0.1298458464245832]}, "mutation_prompt": null}
{"id": "8a98ff65-a5fe-448c-8f76-9dc2dc0b61af", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, initial_pop_size, max_iter, F=0.8, CR=0.9):\n        pop_size = initial_pop_size * (1 + np.log1p(self.dim))  # Line 1 change for adaptive population size\n        pop = np.random.rand(int(pop_size), self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += int(pop_size)\n\n        for iteration in range(max_iter):\n            for i in range(int(pop_size)):\n                indices = np.random.choice(int(pop_size), 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                F_dynamic = F * (1 - iteration / max_iter)\n                mutant = np.clip(x0 + F_dynamic * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < (CR - iteration / (2 * max_iter))\n                trial = np.where(crossover, mutant, pop[i])\n                \n                # Line 2 change for layer-wise perturbation\n                if i % 2 == 0: trial = trial + np.random.normal(0, 0.01, self.dim) \n\n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n        \n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 25  # Changed from 30 to 25\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduced adaptive population size and layer-wise perturbation to enhance exploration in high-dimensional noisy optimization.", "configspace": "", "generation": 3, "fitness": 0.8365327613842477, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.013. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "98740da3-c40f-4107-b272-db01a4c2354d", "metadata": {"aucs": [0.8183987682201006, 0.8500519307774381, 0.8411475851552044], "final_y": [0.13590678473395268, 0.13518790683923176, 0.12822950406471423]}, "mutation_prompt": null}
{"id": "8ef59bb5-cc28-4614-aa2a-80bc8ec19478", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n        \n        for iteration in range(max_iter):\n            dynamic_pop_size = population_size + int(0.1 * population_size * np.sin(2 * np.pi * iteration / max_iter))\n            CR = 0.5 + 0.5 * (iteration / max_iter)\n            dynamic_F = 0.5 + 0.2 * (1 + np.sin(2 * np.pi * iteration / max_iter))\n            for i in range(dynamic_pop_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                layer_wise_F = F * (1 + np.random.normal(0, 0.1, self.dim))\n                mutant = np.clip(x0 + dynamic_F * layer_wise_F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n\n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        if self.evaluations < self.budget:\n            result = minimize(func, solution, method='BFGS', options={'maxiter': self.budget - self.evaluations})\n            self.evaluations += result.nfev\n            if result.success:\n                return result.x\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 10 + self.dim\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduced a dynamic population size during Differential Evolution iterations and enhanced local refinement with a hybrid Nelder-Mead and BFGS approach for improved convergence.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "e9f34e78-c0c4-497b-96ef-730f6b226d4c", "metadata": {}, "mutation_prompt": null}
{"id": "3006a7fd-2351-40f6-9351-b574d1b0d6c3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.95): # Changed CR from 0.9 to 0.95\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n        \n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 30  # Changed from 20 to 30\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Modified the crossover probability in Differential Evolution to enhance exploration and exploitation balance.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "178e44fe-148d-4704-8588-706931982004", "metadata": {}, "mutation_prompt": null}
{"id": "42115164-3a45-4028-9068-3c19c1fd57d8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            F_dynamic = 0.5 + 0.3 * np.sin(np.pi * iteration / max_iter)  # Sinusoidal mutation factor variation\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + F_dynamic * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n        \n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': (self.budget - self.evaluations) // 1.5, 'adaptive': True})  # Adjusted budget allocation\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 30  # Changed from 20 to 30\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced population diversity by introducing a sinusoidal mutation factor variation and improved budget allocation for local refinement.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "178e44fe-148d-4704-8588-706931982004", "metadata": {}, "mutation_prompt": null}
{"id": "f06f6799-d315-4635-9c04-bf4b50e5fa96", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n        \n        for iteration in range(max_iter):\n            CR = 0.5 + 0.5 * (iteration / max_iter)  # Adaptive crossover probability\n            noise_factor = np.var(pop_fitness) / 100  # Consider intra-population variance as noise factor\n            dynamic_F = 0.5 + 0.3 * (1 + np.sin(2 * np.pi * iteration / max_iter)) - noise_factor  # Improved dynamic mutation factor\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                layer_wise_F = F * (1 + np.random.normal(0, 0.1, self.dim))  # Layer-wise mutation rate\n                mutant = np.clip(x0 + dynamic_F * layer_wise_F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n\n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 10 + self.dim  # Dynamic population size\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Applied adaptive dynamic F tuning with noise consideration and refined local search bound adjustments to enhance search performance in noisy environments.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "e9f34e78-c0c4-497b-96ef-730f6b226d4c", "metadata": {}, "mutation_prompt": null}
{"id": "e22dfd10-72f3-4cf6-9cc5-dc3941ec5d1a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_prob = 0.9\n        self.local_iter = 10  # Increased from 5 to 10\n        self.convergence_threshold = 1e-6\n\n    def _differential_evolution(self, func):\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in pop])\n        evaluations = self.population_size\n        best_idx = np.argmin(scores)\n        best = pop[best_idx, :]\n\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.budget / 100))  # Dynamic population size adjustment\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.mutation_factor * (b - c), func.bounds.lb, func.bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.crossover_prob\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i, :])\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < scores[i]:\n                    scores[i] = trial_score\n                    pop[i, :] = trial\n\n                    if trial_score < scores[best_idx]:\n                        best_idx = i\n                        best = pop[i, :]\n\n                if evaluations >= self.budget:\n                    break\n\n            # Early stopping if convergence is reached\n            if np.std(scores) < self.convergence_threshold:\n                break\n\n            self.mutation_factor = 0.5 + 0.5 * np.std(scores) / (1 + np.std(scores))  # Dynamic mutation factor adjustment\n\n        return best\n\n    def _local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(func.bounds.lb, func.bounds.ub)), options={'maxiter': self.local_iter})\n        return res.x\n\n    def __call__(self, func):\n        best_solution = self._differential_evolution(func)\n        refined_solution = self._local_optimization(func, best_solution)\n        return refined_solution", "name": "HybridOptimizer", "description": "Enhanced the strategy by dynamically adjusting the population size based on the remaining budget to better balance exploration and exploitation.", "configspace": "", "generation": 4, "fitness": 0.8208484013038099, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.821 with standard deviation 0.069. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.027.", "error": "", "parent_id": "2d612665-a9e9-41ad-afc2-0304ca667b7e", "metadata": {"aucs": [0.7280166015139338, 0.840455314204376, 0.8940732881931196], "final_y": [0.18512127432342773, 0.13883138119362715, 0.11980118054329347]}, "mutation_prompt": null}
{"id": "39b72a99-9383-444b-b388-a81000c1240a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            CR = 0.5 + 0.5 * (iteration / max_iter)  # Adaptive crossover probability\n            dynamic_F = 0.4 + 0.2 * (np.sin(iteration / max_iter * np.pi))  # Dynamic mutation factor adjustment\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + dynamic_F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n\n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        remaining_budget = self.budget - self.evaluations\n        refinement_budget = min(remaining_budget, 50)  # Fixed budget or remaining evaluations\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': refinement_budget, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = max(10, int(0.1 * (self.budget - self.evaluations)))  # Dynamic population size adjustment\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduced a dynamic population size based on evaluations left and adaptive mutation scaling for improved convergence.", "configspace": "", "generation": 5, "fitness": 0.8210724922545148, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.821 with standard deviation 0.024. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "6f054de5-3f5e-4dbc-86d6-e934eb5d3672", "metadata": {"aucs": [0.7904866286236497, 0.8490113422520635, 0.8237195058878315], "final_y": [0.15101891704907167, 0.11805164725055206, 0.13163338381853462]}, "mutation_prompt": null}
{"id": "dcfcc575-633f-476d-9558-9b73688bc94e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n        \n        for iteration in range(max_iter):\n            CR = 0.5 + 0.5 * (iteration / max_iter)  # Adaptive crossover probability\n            dynamic_F = 0.5 + 0.2 * (1 + np.sin(2 * np.pi * iteration / max_iter))  # Improved dynamic mutation factor\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                layer_wise_F = F * (1 + np.random.normal(0, 0.1, self.dim))  # Layer-wise mutation rate\n                mutant = np.clip(x0 + dynamic_F * layer_wise_F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget or np.std(pop_fitness) < 0.01:  # Allow early stopping\n                break\n\n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 15 + self.dim  # Adjusted population size for better balance\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduced adaptive population size based on convergence rate to enhance exploration and exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.8339176917577978, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.834 with standard deviation 0.009. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "e9f34e78-c0c4-497b-96ef-730f6b226d4c", "metadata": {"aucs": [0.8286596221453737, 0.8470314832714626, 0.8260619698565572], "final_y": [0.1208974154086051, 0.12497942803042361, 0.12759726967460183]}, "mutation_prompt": null}
{"id": "c7f8b098-6194-4f44-8bd1-ba29b78d6df4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            dynamic_F = 0.5 + 0.5 * np.sin(np.pi * iteration / max_iter)  # Dynamically adjust F\n            dynamic_CR = 0.5 + 0.5 * np.cos(np.pi * iteration / max_iter)  # Dynamically adjust CR\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + dynamic_F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < dynamic_CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n        \n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 30  # Changed from 20 to 30\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced exploration-exploitation balance by dynamically adjusting mutation and crossover rates during Differential Evolution.", "configspace": "", "generation": 5, "fitness": 0.8374713136815055, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.004. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "178e44fe-148d-4704-8588-706931982004", "metadata": {"aucs": [0.8420824191811496, 0.8317432692077438, 0.8385882526556234], "final_y": [0.12890051805077285, 0.1275660563580221, 0.13114740363873922]}, "mutation_prompt": null}
{"id": "747dca02-df1f-465b-9e3a-9fd3e6cbbc84", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            CR = 0.5 + 0.5 * (iteration / max_iter)  # Adaptive crossover probability\n            dynamic_F = 0.5 + 0.3 * (np.cos(iteration / max_iter * np.pi))  # Dynamic mutation factor with cosine modulation\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + dynamic_F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n\n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 10 + self.dim  # Dynamic population size\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced dynamic mutation by introducing cosine modulation for better exploration-exploitation trade-off.", "configspace": "", "generation": 5, "fitness": 0.8262519301623253, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.007. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "47d69e14-1c3f-46ce-bd72-caee5b24ed61", "metadata": {"aucs": [0.8173072632278162, 0.8286031757227784, 0.8328453515363813], "final_y": [0.13042863947002048, 0.12952810494368994, 0.1300708172145041]}, "mutation_prompt": null}
{"id": "2b0b965b-5cbd-4725-8e47-27e41c5e3ac6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n        \n        for iteration in range(max_iter):\n            CR = 0.5 + 0.5 * (iteration / max_iter)  # Adaptive crossover probability\n            dynamic_F = 0.5 + 0.2 * (1 + np.sin(2 * np.pi * iteration / max_iter))  # Improved dynamic mutation factor\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                layer_wise_F = F * (1 + np.random.normal(0, 0.1, self.dim))  # Layer-wise mutation rate\n                mutant = np.clip(x0 + dynamic_F * layer_wise_F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n\n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='BFGS', options={'maxiter': self.budget - self.evaluations})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 10 + self.dim  # Dynamic population size\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced local refinement by utilizing BFGS optimization method for improved convergence in high-dimensional noisy optimization.", "configspace": "", "generation": 5, "fitness": 0.8383469129442803, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.838 with standard deviation 0.007. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "e9f34e78-c0c4-497b-96ef-730f6b226d4c", "metadata": {"aucs": [0.8422408173825928, 0.828358115568772, 0.8444418058814764], "final_y": [0.13165193391673047, 0.13011164996340085, 0.12451201470268969]}, "mutation_prompt": null}
{"id": "2255bd7c-6d16-48ac-903d-b060996a049e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_prob = 0.9\n        self.local_iter = 10  # Increased from 5 to 10\n        self.convergence_threshold = 1e-7  # Adjusted for improved precision\n\n    def _differential_evolution(self, func):\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in pop])\n        evaluations = self.population_size\n        best_idx = np.argmin(scores)\n        best = pop[best_idx, :]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.mutation_factor * (b - c), func.bounds.lb, func.bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.crossover_prob\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i, :])\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < scores[i]:\n                    scores[i] = trial_score\n                    pop[i, :] = trial\n\n                    if trial_score < scores[best_idx]:\n                        best_idx = i\n                        best = pop[i, :]\n\n                if evaluations >= self.budget:\n                    break\n\n            # Early stopping if convergence is reached\n            if np.std(scores) < self.convergence_threshold:\n                break\n\n            self.mutation_factor = 0.5 + 0.5 * np.std(scores) / (1 + np.std(scores))  # Dynamic mutation factor adjustment\n\n        return best\n\n    def _local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(func.bounds.lb, func.bounds.ub)), options={'maxiter': self.local_iter})\n        return res.x\n\n    def __call__(self, func):\n        best_solution = self._differential_evolution(func)\n        refined_solution = self._local_optimization(func, best_solution)\n        return refined_solution", "name": "HybridOptimizer", "description": "Improved local optimization by modifying the convergence threshold to enhance precision and convergence speed.", "configspace": "", "generation": 6, "fitness": 0.8569215359043921, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.021. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "2d612665-a9e9-41ad-afc2-0304ca667b7e", "metadata": {"aucs": [0.8650310116239923, 0.8280219803511116, 0.8777116157380721], "final_y": [0.11559012604932839, 0.1322297967335181, 0.11806467266559206]}, "mutation_prompt": null}
{"id": "b5b9a065-9790-4ce5-aa27-4ba3d19f8acc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n        \n        for iteration in range(max_iter):\n            CR = 0.5 + 0.5 * (iteration / max_iter)  # Adaptive crossover probability\n            dynamic_F = 0.5 + 0.3 * (1 + np.sin(2 * np.pi * iteration / max_iter))  # Adjusted dynamic mutation factor\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                layer_wise_F = F * (1 + np.random.normal(0, 0.1, self.dim))  # Layer-wise mutation rate\n                mutant = np.clip(x0 + dynamic_F * layer_wise_F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n\n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='BFGS', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'disp': False})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 10 + self.dim  # Dynamic population size\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced adaptive mutation strategy with an oscillatory pattern and improved local refinement using the BFGS method for better convergence and exploitation in noisy optimization.", "configspace": "", "generation": 6, "fitness": 0.8389096061601906, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.839 with standard deviation 0.010. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "e9f34e78-c0c4-497b-96ef-730f6b226d4c", "metadata": {"aucs": [0.8335491028790147, 0.8297042708988227, 0.8534754447027343], "final_y": [0.13203698580862666, 0.1384780228094511, 0.12537150872849534]}, "mutation_prompt": null}
{"id": "832cca81-e3a3-401f-9329-dbbf312dfd5f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_prob = 0.9\n        self.local_iter = 10  # Increased from 5 to 10\n        self.convergence_threshold = 1e-6\n\n    def _differential_evolution(self, func):\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in pop])\n        evaluations = self.population_size\n        best_idx = np.argmin(scores)\n        best = pop[best_idx, :]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.mutation_factor * (b - c), func.bounds.lb, func.bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.crossover_prob\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i, :])\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < scores[i]:\n                    scores[i] = trial_score\n                    pop[i, :] = trial\n\n                    if trial_score < scores[best_idx]:\n                        best_idx = i\n                        best = pop[i, :]\n\n                if evaluations >= self.budget:\n                    break\n\n            # Early stopping if convergence is reached\n            if np.std(scores) < self.convergence_threshold:\n                break\n\n            self.mutation_factor = 0.6 + 0.4 * np.std(scores) / (1 + np.std(scores))  # Modified dynamic mutation factor adjustment\n\n        return best\n\n    def _local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(func.bounds.lb, func.bounds.ub)), options={'maxiter': self.local_iter})\n        return res.x\n\n    def __call__(self, func):\n        best_solution = self._differential_evolution(func)\n        refined_solution = self._local_optimization(func, best_solution)\n        return refined_solution", "name": "HybridOptimizer", "description": "Enhanced the dynamic mutation strategy by modifying the mutation factor adjustment formula to improve adaptability in noisy environments.", "configspace": "", "generation": 6, "fitness": 0.8550309843727636, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.010. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "2d612665-a9e9-41ad-afc2-0304ca667b7e", "metadata": {"aucs": [0.8486294444198872, 0.8474796204200503, 0.8689838882783532], "final_y": [0.1267416537178676, 0.1294111393775651, 0.11540875923556426]}, "mutation_prompt": null}
{"id": "d01ea101-0806-4309-89b2-3dd829ad1761", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            CR = 0.5 + 0.5 * (iteration / max_iter)  # Adaptive crossover probability\n            dynamic_F = 0.5 + 0.3 * (np.sin(iteration / max_iter * np.pi))  # Dynamic mutation factor\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + dynamic_F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n\n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        remaining_budget = self.budget - self.evaluations\n        refinement_budget = min(remaining_budget, 50)  # Fixed budget or remaining evaluations\n        noisy_solution = solution + np.random.normal(0, 0.01, self.dim)  # Added Gaussian noise\n        result = minimize(func, noisy_solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': refinement_budget, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = int(10 + self.dim * (self.budget - self.evaluations) / self.budget)  # Adjusted population size\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Improved balance between exploration and exploitation by dynamically adjusting the population size based on remaining budget and adding Gaussian noise during local refinement for robustness.", "configspace": "", "generation": 6, "fitness": 0.85392546038491, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.003. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "6f054de5-3f5e-4dbc-86d6-e934eb5d3672", "metadata": {"aucs": [0.8493844467674876, 0.8557156832108151, 0.8566762511764276], "final_y": [0.12494966340748548, 0.11961089471749031, 0.12756704055105106]}, "mutation_prompt": null}
{"id": "630009e0-8995-4ded-90de-ea9f565301db", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, population_size, max_iter, F=0.8, CR=0.9):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        pop_fitness = np.array([func(ind) for ind in pop])\n        self.evaluations += population_size\n\n        for iteration in range(max_iter):\n            CR = 0.5 + 0.5 * (iteration / max_iter)  # Adaptive crossover probability\n            dynamic_F = 0.5 + 0.3 * (1 + np.cos(iteration / max_iter * np.pi))  # Cosine modulation for mutation factor\n            for i in range(population_size):\n                indices = np.random.choice(population_size, 3, replace=False)\n                x0, x1, x2 = pop[indices]\n                mutant = np.clip(x0 + dynamic_F * (x1 - x2), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[i])\n                \n                if self.evaluations < self.budget:\n                    trial_fitness = func(trial)\n                    self.evaluations += 1\n                    if trial_fitness < pop_fitness[i]:\n                        pop[i] = trial\n                        pop_fitness[i] = trial_fitness\n\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations >= self.budget:\n                break\n\n        return pop[np.argmin(pop_fitness)]\n\n    def local_refinement(self, func, solution, bounds):\n        result = minimize(func, solution, method='Nelder-Mead', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)],\n                          options={'maxfev': self.budget - self.evaluations, 'adaptive': True})\n        self.evaluations += result.nfev\n        return result.x if result.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 10 + self.dim  # Dynamic population size\n        max_iter = 100\n        phase_budget = self.budget // 2\n        \n        best_solution = self.differential_evolution(func, bounds, population_size, max_iter)\n        if self.evaluations < self.budget:\n            best_solution = self.local_refinement(func, best_solution, bounds)\n        \n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced dynamic mutation strategy by introducing cosine modulation for mutation factor to improve search efficiency and exploitation balance in high-dimensional noisy optimization.", "configspace": "", "generation": 6, "fitness": 0.8296519183138399, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.015. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "47d69e14-1c3f-46ce-bd72-caee5b24ed61", "metadata": {"aucs": [0.8101301334219508, 0.8469234627458692, 0.8319021587737], "final_y": [0.13312682757343108, 0.13571125919081028, 0.12986636674416008]}, "mutation_prompt": null}
