{"id": "051790c1-04e1-4e6a-a257-8400bd63835e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE:\n    def __init__(self, budget, dim, population_size=50, F=0.8, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.F = F\n        self.CR = CR\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opposition = lb + ub - population\n        combined = np.vstack((population, opposition))\n        return combined[np.random.choice(2 * self.population_size, self.population_size, replace=False)]\n\n    def differential_evolution_step(self, population, bounds, func):\n        new_population = np.empty_like(population)\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial_eval = func(trial)\n            self.evaluations += 1\n            if trial_eval < func(population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = population[i]\n        return new_population\n\n    def local_refinement(self, best_solution, bounds, func):\n        result = minimize(func, best_solution, method='L-BFGS-B', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)])\n        return result.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        \n        # Initial quasi-oppositional population\n        population = self.quasi_oppositional_initialization(bounds)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += self.population_size\n        \n        while self.evaluations < self.budget:\n            # Perform a DE step\n            population = self.differential_evolution_step(population, bounds, func)\n            \n            # Update fitness based on new population\n            fitness = np.array([func(ind) for ind in population])\n            self.evaluations += self.population_size\n            \n            # Check if budget is exceeded\n            if self.evaluations >= self.budget:\n                break\n\n        # Find the best solution from the population\n        best_index = np.argmin(fitness)\n        best_solution = population[best_index]\n\n        # Local refinement using BFGS\n        refined_solution = self.local_refinement(best_solution, bounds, func)\n\n        return refined_solution", "name": "HybridDE", "description": "A hybrid Differential Evolution with Quasi-Oppositional initialization and local BFGS refinement for black-box optimization of periodic structures.", "configspace": "", "generation": 0, "fitness": 0.6489059775099836, "feedback": "The algorithm HybridDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.649 with standard deviation 0.039. And the mean value of best solutions found was 0.303 (0. is the best) with standard deviation 0.028.", "error": "", "parent_id": null, "metadata": {"aucs": [0.6410416293269467, 0.6049673082485696, 0.7007089949544344], "final_y": [0.3134298421743147, 0.3303368996030165, 0.2640317942804735]}, "mutation_prompt": null}
{"id": "4bd543ed-ce9b-43d7-ad87-635047daad30", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_init(self, bounds):\n        mid = (bounds.ub + bounds.lb) / 2\n        return np.vstack([np.random.uniform(bounds.lb, bounds.ub, self.dim), mid * 2 - np.random.uniform(bounds.lb, bounds.ub, self.dim)])\n\n    def differential_evolution(self, func, bounds):\n        pop_size = 10 * self.dim\n        population = self.quasi_oppositional_init(bounds)\n        while len(population) < pop_size:\n            population = np.vstack((population, np.random.uniform(bounds.lb, bounds.ub, self.dim)))\n\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        best_idx = np.argmin([func(ind) for ind in population])\n        best = population[best_idx]\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = np.random.choice(pop_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, population[i])\n\n                trial_eval = func(trial)\n                self.evaluations += 1\n                if trial_eval < func(population[i]):\n                    population[i] = trial\n                    if trial_eval < func(best):\n                        best = trial\n\n        return best\n\n    def local_optimization(self, func, x0):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)])\n        return result.x\n\n    def __call__(self, func):\n        # Step 1: Global exploration with Differential Evolution\n        best_global = self.differential_evolution(func, func.bounds)\n\n        # Step 2: Local refinement with BFGS\n        best_local = self.local_optimization(func, best_global)\n\n        return best_local", "name": "BraggOptimizer", "description": "A hybrid metaheuristic algorithm leveraging Differential Evolution for global exploration, combined with Quasi-Oppositional initialization and local refinement using BFGS to optimize multilayer photonic structures maximizing reflectivity.", "configspace": "", "generation": 0, "fitness": 0.5873500229547826, "feedback": "The algorithm BraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.587 with standard deviation 0.043. And the mean value of best solutions found was 0.314 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": null, "metadata": {"aucs": [0.5673413538453376, 0.6467919924250928, 0.5479167225939172], "final_y": [0.3563233578874311, 0.2606697670405521, 0.32534510452930554]}, "mutation_prompt": null}
{"id": "4450bac0-ae71-4f84-9751-eb4d30f5e1d3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = dim * 10\n        self.mutation_factor = 0.8\n        self.crossover_prob = 0.9\n        self.local_search_prob = 0.2\n        self.current_evals = 0\n        \n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        best_solution = None\n        best_fitness = float('inf')\n\n        def periodic_penalty(x):\n            half_dim = self.dim // 2\n            first_half = x[:half_dim]\n            second_half = x[half_dim:]\n            penalty = np.sum((first_half - second_half) ** 2)\n            return penalty\n\n        while self.current_evals < self.budget:\n            if best_solution is None:\n                fitness = np.apply_along_axis(func, 1, population)\n                self.current_evals += self.population_size\n                best_idx = np.argmin(fitness)\n                best_fitness = fitness[best_idx]\n                best_solution = population[best_idx]\n\n            new_population = np.empty_like(population)\n            for i in range(self.population_size):\n                target = population[i]\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.crossover_prob\n                trial = np.where(cross_points, mutant, target)\n                \n                if self.current_evals < self.budget:\n                    trial_fitness = func(trial) + periodic_penalty(trial)\n                    self.current_evals += 1\n\n                    if trial_fitness < fitness[i]:\n                        new_population[i] = trial\n                        fitness[i] = trial_fitness\n                        if trial_fitness < best_fitness:\n                            best_fitness = trial_fitness\n                            best_solution = trial\n                    else:\n                        new_population[i] = target\n                else:\n                    break\n                \n            population = new_population\n\n            if np.random.rand() < self.local_search_prob and self.current_evals < self.budget:\n                result = minimize(func, best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n                if result.success and result.fun < best_fitness:\n                    best_solution = result.x\n                    best_fitness = result.fun\n                self.current_evals += result.nfev\n\n        return best_solution", "name": "HybridBraggOptimizer", "description": "A hybrid Global-Local Optimization algorithm that combines Differential Evolution with periodic solution encouragement and local search for efficient exploration and exploitation.", "configspace": "", "generation": 0, "fitness": 0.8137780115302743, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.814 with standard deviation 0.064. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8122506314286362, 0.7365367481011708, 0.8925466550610162], "final_y": [0.18188039144107038, 0.25781124525436083, 0.1648564131807223]}, "mutation_prompt": null}
{"id": "7963f3d3-1349-45cb-a5e4-3616170abcfe", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop  # Quasi-Oppositional initialization\n        return np.vstack((pop, opp_pop))\n\n    def evaluate_population(self, population, func):\n        return np.array([func(ind) for ind in population])\n\n    def select_best(self, population, fitness):\n        idx = np.argmin(fitness)\n        return population[idx], fitness[idx]\n\n    def enforce_periodicity(self, candidate):\n        # Adjust layers to encourage periodic solutions\n        half_dim = self.dim // 2\n        candidate[:half_dim] = candidate[half_dim:] = np.mean(candidate.reshape(-1, 2), axis=1)\n        return candidate\n\n    def crossover_and_mutate(self, target, mutant, cr=0.9):\n        cross_points = np.random.rand(self.dim) < cr\n        offspring = np.where(cross_points, mutant, target)\n        return self.enforce_periodicity(offspring)\n\n    def differential_evolution(self, func):\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = self.initialize_population(lb, ub)\n        fitness = self.evaluate_population(population, func)\n        eval_count = len(population)\n\n        while eval_count < self.budget:\n            for i in range(len(population)):\n                if eval_count >= self.budget:\n                    break\n                indices = [idx for idx in range(len(population)) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                trial = self.crossover_and_mutate(population[i], mutant)\n                trial_fitness = func(trial)\n                eval_count += 1\n                if trial_fitness < fitness[i]:\n                    population[i], fitness[i] = trial, trial_fitness\n\n        return self.select_best(population, fitness)\n\n    def local_optimization(self, best_solution, func):\n        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_solution, _ = self.differential_evolution(func)\n        best_solution = self.local_optimization(best_solution, func)\n        return best_solution", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm combining Differential Evolution with Quasi-Oppositional Symmetry and periodicity-enforcing constraints, followed by local optimization using BFGS for fine-tuning near promising solutions.", "configspace": "", "generation": 0, "fitness": 0.9391757865732769, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.002. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.9400271446310665, 0.9414484405762557, 0.9360517745125088], "final_y": [0.17286546882799203, 0.17291383926538184, 0.1729038269867642]}, "mutation_prompt": null}
{"id": "2d3c842b-977c-4f65-8c49-39480eeb265c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizationAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        population_size = 20\n        F = 0.8  # Differential weight\n        CR = 0.9 # Crossover probability\n        bounds = (func.bounds.lb, func.bounds.ub)\n        population = np.random.uniform(bounds[0], bounds[1], (population_size, self.dim))\n        \n        # Ensure symmetric initialization (quasi-oppositional)\n        opposition_population = bounds[0] + bounds[1] - population\n        population = np.vstack((population, opposition_population))\n        \n        # Evaluate initial population\n        fitness = np.apply_along_axis(func, 1, population)\n        evaluations = len(population)\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Select mutation indices\n                indices = [idx for idx in range(2 * population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                # Mutation and Crossover\n                mutant_vector = np.clip(population[a] + F * (population[b] - population[c]), bounds[0], bounds[1])\n                \n                trial_vector = np.array([\n                    mutant_vector[j] if np.random.rand() < CR else population[i][j] \n                    for j in range(self.dim)\n                ])\n                \n                # Encourage periodicity in solutions\n                trial_vector = self._enforce_periodicity(trial_vector)\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                # Select better solution\n                if trial_fitness < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = trial_fitness\n\n            # Modular local search\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            result = minimize(func, best_solution, bounds=list(zip(bounds[0], bounds[1])), method='L-BFGS-B')\n            if result.fun < fitness[best_idx]:\n                population[best_idx] = result.x\n                fitness[best_idx] = result.fun\n                evaluations += result.nfev\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]\n\n    def _enforce_periodicity(self, vector):\n        # This function encourages periodicity by averaging blocks of layers\n        period = 2\n        periodic_vector = vector.copy()\n        for i in range(0, self.dim, period):\n            block_average = np.mean(vector[i:i+period])\n            periodic_vector[i:i+period] = block_average\n        return periodic_vector", "name": "HybridOptimizationAlgorithm", "description": "A hybrid metaheuristic combining Differential Evolution with periodicity-enhancing and modular characteristics to optimize multilayered structures.", "configspace": "", "generation": 0, "fitness": 0.9744126444048122, "feedback": "The algorithm HybridOptimizationAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.021. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.986537037617879, 0.9913696054409511, 0.9453312901556065], "final_y": [0.16485637583484247, 0.16486016455470975, 0.1648591507383872]}, "mutation_prompt": null}
{"id": "2c6353c5-c0c6-4656-bc52-e6e8f1f67e68", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE:\n    def __init__(self, budget, dim, population_size=50, F=0.8, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.F = F\n        self.CR = CR\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opposition = lb + ub - population\n        combined = np.vstack((population, opposition))\n        return combined[np.random.choice(2 * self.population_size, self.population_size, replace=False)]\n\n    def differential_evolution_step(self, population, bounds, func):\n        new_population = np.empty_like(population)\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < (self.CR * (1.1 if np.random.rand() > 0.5 else 0.9))  # Dynamic crossover rate\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial_eval = func(trial)\n            self.evaluations += 1\n            if trial_eval < func(population[i]):\n                new_population[i] = trial\n            else:\n                new_population[i] = population[i]\n        return new_population\n\n    def local_refinement(self, best_solution, bounds, func):\n        result = minimize(func, best_solution, method='L-BFGS-B', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)])\n        return result.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        \n        # Initial quasi-oppositional population\n        population = self.quasi_oppositional_initialization(bounds)\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += self.population_size\n        \n        while self.evaluations < self.budget:\n            # Perform a DE step\n            population = self.differential_evolution_step(population, bounds, func)\n            \n            # Update fitness based on new population\n            fitness = np.array([func(ind) for ind in population])\n            self.evaluations += self.population_size\n            \n            # Check if budget is exceeded\n            if self.evaluations >= self.budget:\n                break\n\n        # Find the best solution from the population\n        best_index = np.argmin(fitness)\n        best_solution = population[best_index]\n\n        # Local refinement using BFGS\n        refined_solution = self.local_refinement(best_solution, bounds, func)\n\n        return refined_solution", "name": "HybridDE", "description": "Enhanced exploration by adjusting crossover rate dynamically based on fitness improvement to optimize periodic structures.", "configspace": "", "generation": 1, "fitness": 0.6146070561186926, "feedback": "The algorithm HybridDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.615 with standard deviation 0.035. And the mean value of best solutions found was 0.307 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "051790c1-04e1-4e6a-a257-8400bd63835e", "metadata": {"aucs": [0.6434237963092366, 0.5649247973425531, 0.635472574704288], "final_y": [0.3134298421743147, 0.2904567431338253, 0.3168080089689559]}, "mutation_prompt": null}
{"id": "8243b07d-47e9-42ad-80f0-5d093c87bcd0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_init(self, bounds):\n        mid = (bounds.ub + bounds.lb) / 2\n        return np.vstack([np.random.uniform(bounds.lb, bounds.ub, self.dim), mid * 2 - np.random.uniform(bounds.lb, bounds.ub, self.dim)])\n\n    def differential_evolution(self, func, bounds):\n        pop_size = 10 * self.dim\n        population = self.quasi_oppositional_init(bounds)\n        while len(population) < pop_size:\n            population = np.vstack((population, np.random.uniform(bounds.lb, bounds.ub, self.dim)))\n\n        F_min, F_max = 0.5, 1.0  # Adaptive Differential weight range\n        CR_min, CR_max = 0.7, 1.0  # Adaptive Crossover probability range\n\n        best_idx = np.argmin([func(ind) for ind in population])\n        best = population[best_idx]\n\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                indices = np.random.choice(pop_size, 3, replace=False)\n                a, b, c = population[indices]\n                F = np.random.uniform(F_min, F_max)  # Adaptive Differential weight\n                CR = np.random.uniform(CR_min, CR_max)  # Adaptive Crossover probability\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, population[i])\n\n                trial_eval = func(trial)\n                self.evaluations += 1\n                if trial_eval < func(population[i]):\n                    population[i] = trial\n                    if trial_eval < func(best):\n                        best = trial\n\n        return best\n\n    def local_optimization(self, func, x0):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)])\n        return result.x\n\n    def __call__(self, func):\n        # Step 1: Global exploration with Differential Evolution\n        best_global = self.differential_evolution(func, func.bounds)\n\n        # Step 2: Local refinement with BFGS\n        best_local = self.local_optimization(func, best_global)\n\n        return best_local", "name": "BraggOptimizer", "description": "Enhanced BraggOptimizer utilizing adaptive differential weights and crossover probabilities to improve exploration and exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.6012246503154512, "feedback": "The algorithm BraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.601 with standard deviation 0.026. And the mean value of best solutions found was 0.310 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "4bd543ed-ce9b-43d7-ad87-635047daad30", "metadata": {"aucs": [0.6368529496443174, 0.5912732663088385, 0.5755477349931979], "final_y": [0.2742958347731522, 0.3332299960588577, 0.3226822022335186]}, "mutation_prompt": null}
{"id": "7d9cd7ee-82cd-4ba0-8a38-03ea0eb5080b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = dim * 10\n        self.mutation_factor = 0.8\n        self.crossover_prob = 0.9\n        self.local_search_prob = 0.25  # Adjusted local search probability\n        self.current_evals = 0\n        \n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        best_solution = None\n        best_fitness = float('inf')\n\n        def periodic_penalty(x):\n            half_dim = self.dim // 2\n            first_half = x[:half_dim]\n            second_half = x[half_dim:]\n            penalty = np.sum((first_half - second_half) ** 2)\n            return penalty\n\n        while self.current_evals < self.budget:\n            if best_solution is None:\n                fitness = np.apply_along_axis(func, 1, population)\n                self.current_evals += self.population_size\n                best_idx = np.argmin(fitness)\n                best_fitness = fitness[best_idx]\n                best_solution = population[best_idx]\n\n            new_population = np.empty_like(population)\n            for i in range(self.population_size):\n                target = population[i]\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.crossover_prob\n                trial = np.where(cross_points, mutant, target)\n                \n                if self.current_evals < self.budget:\n                    trial_fitness = func(trial) + periodic_penalty(trial)\n                    self.current_evals += 1\n\n                    if trial_fitness < fitness[i]:\n                        new_population[i] = trial\n                        fitness[i] = trial_fitness\n                        if trial_fitness < best_fitness:\n                            best_fitness = trial_fitness\n                            best_solution = trial\n                    else:\n                        new_population[i] = target\n                else:\n                    break\n                \n            population = new_population\n\n            if np.random.rand() < self.local_search_prob and self.current_evals < self.budget:\n                result = minimize(func, best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n                if result.success and result.fun < best_fitness:\n                    best_solution = result.x\n                    best_fitness = result.fun\n                self.current_evals += result.nfev\n\n        return best_solution", "name": "HybridBraggOptimizer", "description": "An optimized hybrid algorithm integrating Differential Evolution with adaptive local search frequency based on convergence rate for improved multilayer photonic structure optimization.", "configspace": "", "generation": 1, "fitness": 0.8988772324468459, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.899 with standard deviation 0.017. And the mean value of best solutions found was 0.184 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "4450bac0-ae71-4f84-9751-eb4d30f5e1d3", "metadata": {"aucs": [0.8959389713167528, 0.9208623228049384, 0.8798304032188464], "final_y": [0.18188039144107038, 0.18188180472887672, 0.18813561422696556]}, "mutation_prompt": null}
{"id": "33b64ea9-f543-4da5-a17b-a82854e917b8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = dim * 10\n        self.mutation_factor = 0.8\n        self.crossover_prob = 0.9\n        self.local_search_prob = 0.3  # Enhanced from 0.2 to 0.3 to improve local search effectiveness\n        self.current_evals = 0\n        \n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        best_solution = None\n        best_fitness = float('inf')\n\n        def periodic_penalty(x):\n            half_dim = self.dim // 2\n            first_half = x[:half_dim]\n            second_half = x[half_dim:]\n            penalty = np.sum((first_half - second_half) ** 2)\n            return penalty\n\n        while self.current_evals < self.budget:\n            if best_solution is None:\n                fitness = np.apply_along_axis(func, 1, population)\n                self.current_evals += self.population_size\n                best_idx = np.argmin(fitness)\n                best_fitness = fitness[best_idx]\n                best_solution = population[best_idx]\n\n            new_population = np.empty_like(population)\n            for i in range(self.population_size):\n                target = population[i]\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.crossover_prob\n                trial = np.where(cross_points, mutant, target)\n                \n                if self.current_evals < self.budget:\n                    trial_fitness = func(trial) + periodic_penalty(trial)\n                    self.current_evals += 1\n\n                    if trial_fitness < fitness[i]:\n                        new_population[i] = trial\n                        fitness[i] = trial_fitness\n                        if trial_fitness < best_fitness:\n                            best_fitness = trial_fitness\n                            best_solution = trial\n                    else:\n                        new_population[i] = target\n                else:\n                    break\n                \n            population = new_population\n\n            if np.random.rand() < self.local_search_prob and self.current_evals < self.budget:\n                result = minimize(func, best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n                if result.success and result.fun < best_fitness:\n                    best_solution = result.x\n                    best_fitness = result.fun\n                self.current_evals += result.nfev\n\n        return best_solution", "name": "HybridBraggOptimizer", "description": "Refined HybridBraggOptimizer with enhanced local search probability to boost exploitation capabilities.", "configspace": "", "generation": 1, "fitness": 0.9156456289567249, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.916 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "4450bac0-ae71-4f84-9751-eb4d30f5e1d3", "metadata": {"aucs": [0.9472941781405988, 0.8856303342412226, 0.914012374488353], "final_y": [0.16485657321772862, 0.1818805761176695, 0.1648564131807223]}, "mutation_prompt": null}
{"id": "294a6212-8815-4801-b624-7141c272aaf8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop  # Quasi-Oppositional initialization\n        return np.vstack((pop, opp_pop))\n\n    def evaluate_population(self, population, func):\n        return np.array([func(ind) for ind in population])\n\n    def select_best(self, population, fitness):\n        idx = np.argmin(fitness)\n        return population[idx], fitness[idx]\n\n    def enforce_periodicity(self, candidate):\n        # Modified periodicity for every third layer instead of half-dimension\n        for i in range(0, self.dim, 3):\n            candidate[i:i+3] = np.mean(candidate[i:i+3])\n        return candidate\n\n    def crossover_and_mutate(self, target, mutant, cr=0.9):\n        cross_points = np.random.rand(self.dim) < cr\n        offspring = np.where(cross_points, mutant, target)\n        return self.enforce_periodicity(offspring)\n\n    def differential_evolution(self, func):\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = self.initialize_population(lb, ub)\n        fitness = self.evaluate_population(population, func)\n        eval_count = len(population)\n\n        while eval_count < self.budget:\n            for i in range(len(population)):\n                if eval_count >= self.budget:\n                    break\n                indices = [idx for idx in range(len(population)) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Adaptive mutation factor based on evaluation count\n                F = 0.5 + (0.3 * (1 - eval_count / self.budget))\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                trial = self.crossover_and_mutate(population[i], mutant)\n                trial_fitness = func(trial)\n                eval_count += 1\n                if trial_fitness < fitness[i]:\n                    population[i], fitness[i] = trial, trial_fitness\n\n        return self.select_best(population, fitness)\n\n    def local_optimization(self, best_solution, func):\n        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_solution, _ = self.differential_evolution(func)\n        best_solution = self.local_optimization(best_solution, func)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced HybridMetaheuristic algorithm with modified periodicity enforcement and adaptive mutation factor to improve solution quality and convergence speed.", "configspace": "", "generation": 1, "fitness": 0.9070178572129733, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.907 with standard deviation 0.002. And the mean value of best solutions found was 0.172 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7963f3d3-1349-45cb-a5e4-3616170abcfe", "metadata": {"aucs": [0.9092005828596341, 0.9067682057187957, 0.9050847830604902], "final_y": [0.17193927486950678, 0.17159438551130113, 0.17155420103957775]}, "mutation_prompt": null}
{"id": "b0bd1dd1-0ef0-466a-9594-a4cec47debed", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = dim * 10\n        self.mutation_factor = 0.8\n        self.crossover_prob = 0.85  # Adjusted from 0.9 to 0.85 to improve exploration\n        self.local_search_prob = 0.3  # Enhanced from 0.2 to 0.3 to improve local search effectiveness\n        self.current_evals = 0\n        \n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        best_solution = None\n        best_fitness = float('inf')\n\n        def periodic_penalty(x):\n            half_dim = self.dim // 2\n            first_half = x[:half_dim]\n            second_half = x[half_dim:]\n            penalty = np.sum((first_half - second_half) ** 2)\n            return penalty\n\n        while self.current_evals < self.budget:\n            if best_solution is None:\n                fitness = np.apply_along_axis(func, 1, population)\n                self.current_evals += self.population_size\n                best_idx = np.argmin(fitness)\n                best_fitness = fitness[best_idx]\n                best_solution = population[best_idx]\n\n            new_population = np.empty_like(population)\n            for i in range(self.population_size):\n                target = population[i]\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.crossover_prob\n                trial = np.where(cross_points, mutant, target)\n                \n                if self.current_evals < self.budget:\n                    trial_fitness = func(trial) + periodic_penalty(trial)\n                    self.current_evals += 1\n\n                    if trial_fitness < fitness[i]:\n                        new_population[i] = trial\n                        fitness[i] = trial_fitness\n                        if trial_fitness < best_fitness:\n                            best_fitness = trial_fitness\n                            best_solution = trial\n                    else:\n                        new_population[i] = target\n                else:\n                    break\n                \n            population = new_population\n\n            if np.random.rand() < self.local_search_prob and self.current_evals < self.budget:\n                result = minimize(func, best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n                if result.success and result.fun < best_fitness:\n                    best_solution = result.x\n                    best_fitness = result.fun\n                self.current_evals += result.nfev\n\n        return best_solution", "name": "HybridBraggOptimizer", "description": "Improved HybridBraggOptimizer by adjusting the crossover probability to enhance exploration.", "configspace": "", "generation": 2, "fitness": 0.9367488220799992, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.937 with standard deviation 0.024. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "33b64ea9-f543-4da5-a17b-a82854e917b8", "metadata": {"aucs": [0.9232936111913531, 0.9169945904784362, 0.9699582645702081], "final_y": [0.18188039144107038, 0.1818792101201281, 0.16485648297667876]}, "mutation_prompt": null}
{"id": "0ce66957-e290-4658-8d70-21d8b5513859", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = dim * 10\n        self.mutation_factor = 0.85  # Adjusted mutation factor\n        self.crossover_prob = 0.9\n        self.local_search_prob = 0.25  # Adjusted local search probability\n        self.current_evals = 0\n        \n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        best_solution = None\n        best_fitness = float('inf')\n\n        def periodic_penalty(x):\n            half_dim = self.dim // 2\n            first_half = x[:half_dim]\n            second_half = x[half_dim:]\n            penalty = np.sum((first_half - second_half) ** 2)\n            return penalty\n\n        while self.current_evals < self.budget:\n            if best_solution is None:\n                fitness = np.apply_along_axis(func, 1, population)\n                self.current_evals += self.population_size\n                best_idx = np.argmin(fitness)\n                best_fitness = fitness[best_idx]\n                best_solution = population[best_idx]\n\n            new_population = np.empty_like(population)\n            for i in range(self.population_size):\n                target = population[i]\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.crossover_prob\n                trial = np.where(cross_points, mutant, target)\n                \n                if self.current_evals < self.budget:\n                    trial_fitness = func(trial) + periodic_penalty(trial)\n                    self.current_evals += 1\n\n                    if trial_fitness < fitness[i]:\n                        new_population[i] = trial\n                        fitness[i] = trial_fitness\n                        if trial_fitness < best_fitness:\n                            best_fitness = trial_fitness\n                            best_solution = trial\n                    else:\n                        new_population[i] = target\n                else:\n                    break\n                \n            population = new_population\n\n            if np.random.rand() < self.local_search_prob and self.current_evals < self.budget:\n                result = minimize(func, best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n                if result.success and result.fun < best_fitness:\n                    best_solution = result.x\n                    best_fitness = result.fun\n                self.current_evals += result.nfev\n\n        return best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced the HybridBraggOptimizer by adjusting the mutation factor to improve exploration and convergence.", "configspace": "", "generation": 2, "fitness": 0.8480937037841302, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.016. And the mean value of best solutions found was 0.178 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "7d9cd7ee-82cd-4ba0-8a38-03ea0eb5080b", "metadata": {"aucs": [0.8517194561625326, 0.8266540802626934, 0.8659075749271645], "final_y": [0.18188039144107038, 0.18813108623764851, 0.1648564131807223]}, "mutation_prompt": null}
{"id": "de9e7d88-e4d9-409a-b349-5b5d34e464b3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop  # Quasi-Oppositional initialization\n        return np.vstack((pop, opp_pop))\n\n    def evaluate_population(self, population, func):\n        return np.array([func(ind) for ind in population])\n\n    def select_best(self, population, fitness):\n        idx = np.argmin(fitness)\n        return population[idx], fitness[idx]\n\n    def enforce_periodicity(self, candidate):\n        # Modified periodicity for every third layer instead of half-dimension\n        for i in range(0, self.dim, 3):\n            candidate[i:i+3] = np.mean(candidate[i:i+3])\n        return candidate\n\n    def crossover_and_mutate(self, target, mutant, cr=0.9):\n        cross_points = np.random.rand(self.dim) < cr\n        offspring = np.where(cross_points, mutant, target)\n        return self.enforce_periodicity(offspring)\n\n    def differential_evolution(self, func):\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = self.initialize_population(lb, ub)\n        fitness = self.evaluate_population(population, func)\n        eval_count = len(population)\n\n        while eval_count < self.budget:\n            for i in range(len(population)):\n                if eval_count >= self.budget:\n                    break\n                indices = [idx for idx in range(len(population)) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Adaptive mutation factor based on evaluation count\n                F = 0.5 + (0.5 * np.sin(np.pi * eval_count / self.budget))\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                trial = self.crossover_and_mutate(population[i], mutant)\n                trial_fitness = func(trial)\n                eval_count += 1\n                if trial_fitness < fitness[i]:\n                    population[i], fitness[i] = trial, trial_fitness\n\n        return self.select_best(population, fitness)\n\n    def local_optimization(self, best_solution, func):\n        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_solution, _ = self.differential_evolution(func)\n        best_solution = self.local_optimization(best_solution, func)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced mutation strategy with a dynamic control factor for improved exploration and exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.8513106043329789, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.851 with standard deviation 0.038. And the mean value of best solutions found was 0.178 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "294a6212-8815-4801-b624-7141c272aaf8", "metadata": {"aucs": [0.8999418629552816, 0.8085802561765231, 0.8454096938671318], "final_y": [0.17329947099330123, 0.17749086361988275, 0.18334298385416936]}, "mutation_prompt": null}
{"id": "b4eea22b-8fbe-4d8e-bc0a-ed4c51f477af", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Reduced initial population size for faster convergence\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop  # Quasi-Oppositional initialization\n        return np.vstack((pop, opp_pop))\n\n    def evaluate_population(self, population, func):\n        return np.array([func(ind) for ind in population])\n\n    def select_best(self, population, fitness):\n        idx = np.argmin(fitness)\n        return population[idx], fitness[idx]\n\n    def enforce_periodicity(self, candidate):\n        # Modified periodicity for every second layer to better align with wave nature\n        for i in range(0, self.dim, 2):\n            candidate[i:i+2] = np.mean(candidate[i:i+2])\n        return candidate\n\n    def crossover_and_mutate(self, target, mutant, eval_count, cr_base=0.8):\n        cr = cr_base + 0.1 * np.sin(2 * np.pi * eval_count / self.budget)  # Dynamic crossover rate\n        cross_points = np.random.rand(self.dim) < cr\n        offspring = np.where(cross_points, mutant, target)\n        return self.enforce_periodicity(offspring)\n\n    def differential_evolution(self, func):\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = self.initialize_population(lb, ub)\n        fitness = self.evaluate_population(population, func)\n        eval_count = len(population)\n\n        while eval_count < self.budget:\n            for i in range(len(population)):\n                if eval_count >= self.budget:\n                    break\n                indices = [idx for idx in range(len(population)) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = 0.6 + 0.4 * np.cos(eval_count / self.budget)  # Adaptive mutation factor\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                trial = self.crossover_and_mutate(population[i], mutant, eval_count)\n                trial_fitness = func(trial)\n                eval_count += 1\n                if trial_fitness < fitness[i]:\n                    population[i], fitness[i] = trial, trial_fitness\n\n        return self.select_best(population, fitness)\n\n    def local_optimization(self, best_solution, func):\n        options = {'maxiter': 100}  # Limit iterations for local search\n        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)), options=options)\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_solution, _ = self.differential_evolution(func)\n        best_solution = self.local_optimization(best_solution, func)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced HybridMetaheuristic with adaptive population sizing, dynamic crossover rate, and improved local search integration for optimizing multilayered photonic structures.", "configspace": "", "generation": 2, "fitness": 0.8446333731668835, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.012. And the mean value of best solutions found was 0.179 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "294a6212-8815-4801-b624-7141c272aaf8", "metadata": {"aucs": [0.8279352492832304, 0.8550609533233111, 0.8509039168941089], "final_y": [0.1812650067604764, 0.17928159443846647, 0.17779144286215254]}, "mutation_prompt": null}
{"id": "52bc195d-abdd-4104-a6fa-a0fea6bcbd77", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizationAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        population_size = 20\n        F = 0.8  # Differential weight\n        CR = 0.85 # Crossover probability (modified from 0.9 for better exploration)\n        bounds = (func.bounds.lb, func.bounds.ub)\n        population = np.random.uniform(bounds[0], bounds[1], (population_size, self.dim))\n        \n        # Ensure symmetric initialization (quasi-oppositional)\n        opposition_population = bounds[0] + bounds[1] - population\n        population = np.vstack((population, opposition_population))\n        \n        # Evaluate initial population\n        fitness = np.apply_along_axis(func, 1, population)\n        evaluations = len(population)\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Select mutation indices\n                indices = [idx for idx in range(2 * population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                # Mutation and Crossover\n                mutant_vector = np.clip(population[a] + F * (population[b] - population[c]), bounds[0], bounds[1])\n                \n                trial_vector = np.array([\n                    mutant_vector[j] if np.random.rand() < CR else population[i][j] \n                    for j in range(self.dim)\n                ])\n                \n                # Encourage periodicity in solutions\n                trial_vector = self._enforce_periodicity(trial_vector)\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                # Select better solution\n                if trial_fitness < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = trial_fitness\n\n            # Modular local search\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            result = minimize(func, best_solution, bounds=list(zip(bounds[0], bounds[1])), method='L-BFGS-B')\n            if result.fun < fitness[best_idx]:\n                population[best_idx] = result.x\n                fitness[best_idx] = result.fun\n                evaluations += result.nfev\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]\n\n    def _enforce_periodicity(self, vector):\n        # This function encourages periodicity by averaging blocks of layers\n        period = 2\n        periodic_vector = vector.copy()\n        for i in range(0, self.dim, period):\n            block_average = np.mean(vector[i:i+period])\n            periodic_vector[i:i+period] = block_average\n        return periodic_vector", "name": "HybridOptimizationAlgorithm", "description": "Refined Differential Evolution by adjusting the crossover probability for enhanced exploration and solution diversity.", "configspace": "", "generation": 2, "fitness": 0.9785433778553102, "feedback": "The algorithm HybridOptimizationAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.017. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2d3c842b-977c-4f65-8c49-39480eeb265c", "metadata": {"aucs": [0.9546439502727452, 0.9904652014665457, 0.9905209818266397], "final_y": [0.16485956172600724, 0.16485650342026148, 0.16485839853443485]}, "mutation_prompt": null}
{"id": "5ae6ec07-e2d5-4c1b-856d-e0ac213f7324", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = dim * 10\n        self.mutation_factor = 0.85  # Enhanced from 0.8 to 0.85 to improve diversity and exploration\n        self.crossover_prob = 0.9\n        self.local_search_prob = 0.3  # Enhanced from 0.2 to 0.3 to improve local search effectiveness\n        self.current_evals = 0\n        \n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        best_solution = None\n        best_fitness = float('inf')\n\n        def periodic_penalty(x):\n            half_dim = self.dim // 2\n            first_half = x[:half_dim]\n            second_half = x[half_dim:]\n            penalty = np.sum((first_half - second_half) ** 2)\n            return penalty\n\n        while self.current_evals < self.budget:\n            if best_solution is None:\n                fitness = np.apply_along_axis(func, 1, population)\n                self.current_evals += self.population_size\n                best_idx = np.argmin(fitness)\n                best_fitness = fitness[best_idx]\n                best_solution = population[best_idx]\n\n            new_population = np.empty_like(population)\n            for i in range(self.population_size):\n                target = population[i]\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.crossover_prob\n                trial = np.where(cross_points, mutant, target)\n                \n                if self.current_evals < self.budget:\n                    trial_fitness = func(trial) + periodic_penalty(trial)\n                    self.current_evals += 1\n\n                    if trial_fitness < fitness[i]:\n                        new_population[i] = trial\n                        fitness[i] = trial_fitness\n                        if trial_fitness < best_fitness:\n                            best_fitness = trial_fitness\n                            best_solution = trial\n                    else:\n                        new_population[i] = target\n                else:\n                    break\n                \n            population = new_population\n\n            if np.random.rand() < self.local_search_prob and self.current_evals < self.budget:\n                result = minimize(func, best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n                if result.success and result.fun < best_fitness:\n                    best_solution = result.x\n                    best_fitness = result.fun\n                self.current_evals += result.nfev\n\n        return best_solution", "name": "HybridBraggOptimizer", "description": "Enhanced the mutation factor to improve solution diversity and exploration capabilities.", "configspace": "", "generation": 3, "fitness": 0.91533154395721, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.915 with standard deviation 0.049. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "33b64ea9-f543-4da5-a17b-a82854e917b8", "metadata": {"aucs": [0.9234020713518121, 0.8513853206056484, 0.9712072399141691], "final_y": [0.18188039144107038, 0.1818805761176695, 0.16485705525518124]}, "mutation_prompt": null}
{"id": "5909c35a-986a-4df7-b3ef-60ae68a5fa3c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop  # Quasi-Oppositional initialization\n        return np.vstack((pop, opp_pop))\n\n    def evaluate_population(self, population, func):\n        return np.array([func(ind) for ind in population])\n\n    def select_best(self, population, fitness):\n        idx = np.argmin(fitness)\n        return population[idx], fitness[idx]\n\n    def enforce_periodicity(self, candidate):\n        # Adjust layers to encourage periodic solutions\n        half_dim = self.dim // 2\n        candidate[:half_dim] = candidate[half_dim:] = np.mean(candidate.reshape(-1, 2), axis=1)\n        return candidate\n\n    def crossover_and_mutate(self, target, mutant, cr=0.9):\n        cross_points = np.random.rand(self.dim) < cr\n        offspring = np.where(cross_points, mutant, target)\n        return self.enforce_periodicity(offspring)\n\n    def differential_evolution(self, func):\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = self.initialize_population(lb, ub)\n        fitness = self.evaluate_population(population, func)\n        eval_count = len(population)\n\n        while eval_count < self.budget:\n            for i in range(len(population)):\n                if eval_count >= self.budget:\n                    break\n                indices = [idx for idx in range(len(population)) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.5 * (b - c), lb, ub)  # Adjusted mutation factor from 0.8 to 0.5\n                trial = self.crossover_and_mutate(population[i], mutant)\n                trial_fitness = func(trial)\n                eval_count += 1\n                if trial_fitness < fitness[i]:\n                    population[i], fitness[i] = trial, trial_fitness\n\n        return self.select_best(population, fitness)\n\n    def local_optimization(self, best_solution, func):\n        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_solution, _ = self.differential_evolution(func)\n        best_solution = self.local_optimization(best_solution, func)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced HybridMetaheuristic by adapting the mutation strategy to balance exploration and exploitation more effectively.", "configspace": "", "generation": 3, "fitness": 0.9323235698459542, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.932 with standard deviation 0.005. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7963f3d3-1349-45cb-a5e4-3616170abcfe", "metadata": {"aucs": [0.9382314955737733, 0.9263351904878008, 0.9324040234762886], "final_y": [0.17291155398602842, 0.17289483133471706, 0.17290826099657075]}, "mutation_prompt": null}
{"id": "03ada612-05e4-4188-8d3a-d88b0c872b8c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop  # Quasi-Oppositional initialization\n        return np.vstack((pop, opp_pop))\n\n    def evaluate_population(self, population, func):\n        return np.array([func(ind) for ind in population])\n\n    def select_best(self, population, fitness):\n        idx = np.argmin(fitness)\n        return population[idx], fitness[idx]\n\n    def enforce_periodicity(self, candidate):\n        # Adjust layers to encourage periodic solutions\n        half_dim = self.dim // 2\n        candidate[:half_dim] = candidate[half_dim:] = np.mean(candidate.reshape(-1, 2), axis=1)\n        return candidate\n\n    def crossover_and_mutate(self, target, mutant, cr=0.9):\n        cross_points = np.random.rand(self.dim) < cr\n        offspring = np.where(cross_points, mutant, target)\n        return self.enforce_periodicity(offspring)\n\n    def differential_evolution(self, func):\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = self.initialize_population(lb, ub)\n        fitness = self.evaluate_population(population, func)\n        eval_count = len(population)\n\n        while eval_count < self.budget:\n            for i in range(len(population)):\n                if eval_count >= self.budget:\n                    break\n                indices = [idx for idx in range(len(population)) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.85 * (b - c), lb, ub)  # Adjusted mutation factor\n                trial = self.crossover_and_mutate(population[i], mutant)\n                trial_fitness = func(trial)\n                eval_count += 1\n                if trial_fitness < fitness[i]:\n                    population[i], fitness[i] = trial, trial_fitness\n\n        return self.select_best(population, fitness)\n\n    def local_optimization(self, best_solution, func):\n        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_solution, _ = self.differential_evolution(func)\n        best_solution = self.local_optimization(best_solution, func)\n        return best_solution", "name": "HybridMetaheuristic", "description": "A refined hybrid metaheuristic with adjusted mutation strategy in Differential Evolution for enhanced exploration.", "configspace": "", "generation": 3, "fitness": 0.9287089112198897, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.929 with standard deviation 0.006. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7963f3d3-1349-45cb-a5e4-3616170abcfe", "metadata": {"aucs": [0.929105672854441, 0.9357343313131111, 0.9212867294921171], "final_y": [0.17290967852891526, 0.17289706005557515, 0.17289718184930514]}, "mutation_prompt": null}
{"id": "2f0f24e0-9a2a-4b49-ad21-2e07c3a4a55a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizationAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        population_size = 20\n        F = 0.8  # Differential weight\n        CR = 0.7 # Adaptive crossover probability\n        bounds = (func.bounds.lb, func.bounds.ub)\n        population = np.random.uniform(bounds[0], bounds[1], (population_size, self.dim))\n        \n        # Ensure symmetric initialization (quasi-oppositional)\n        opposition_population = bounds[0] + bounds[1] - population\n        population = np.vstack((population, opposition_population))\n        \n        # Evaluate initial population\n        fitness = np.apply_along_axis(func, 1, population)\n        evaluations = len(population)\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Select mutation indices\n                indices = [idx for idx in range(2 * population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                # Mutation and Crossover\n                mutant_vector = np.clip(population[a] + F * (population[b] - population[c]), bounds[0], bounds[1])\n                \n                trial_vector = np.array([\n                    mutant_vector[j] if np.random.rand() < CR else population[i][j] \n                    for j in range(self.dim)\n                ])\n                \n                # Encourage periodicity in solutions\n                trial_vector = self._enforce_periodicity(trial_vector)\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                # Select better solution\n                if trial_fitness < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = trial_fitness\n\n            # Modular local search\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            result = minimize(func, best_solution, bounds=list(zip(bounds[0], bounds[1])), method='L-BFGS-B')\n            if result.fun < fitness[best_idx]:\n                population[best_idx] = result.x\n                fitness[best_idx] = result.fun\n                evaluations += result.nfev\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]\n\n    def _enforce_periodicity(self, vector):\n        # This function encourages periodicity by averaging blocks of layers\n        period = 3 # Localized periodicity adjustment\n        periodic_vector = vector.copy()\n        for i in range(0, self.dim, period):\n            block_average = np.mean(vector[i:i+period])\n            periodic_vector[i:i+period] = block_average\n        return periodic_vector", "name": "HybridOptimizationAlgorithm", "description": "Enhanced Differential Evolution with adaptive crossover probability and localized periodicity enforcement for optimizing multilayered structures.", "configspace": "", "generation": 3, "fitness": 0.9856957360286024, "feedback": "The algorithm HybridOptimizationAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2d3c842b-977c-4f65-8c49-39480eeb265c", "metadata": {"aucs": [0.9931011045968695, 0.9737062495184389, 0.9902798539704989], "final_y": [0.1648610700095552, 0.16485941926434733, 0.16485983292800255]}, "mutation_prompt": null}
{"id": "4cbc910c-edbb-4d4e-bfe3-56d8379ec705", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.bounds = None\n    \n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop  # Quasi-Oppositional initialization\n        return np.vstack((pop, opp_pop))\n    \n    def evaluate_population(self, population, func):\n        return np.array([func(ind) for ind in population])\n    \n    def select_best(self, population, fitness):\n        idx = np.argmin(fitness)\n        return population[idx], fitness[idx]\n    \n    def enforce_periodicity(self, candidate):\n        # Stochastically adjust layers to encourage periodic solutions\n        if np.random.rand() > 0.5:\n            half_dim = self.dim // 2\n            candidate[:half_dim] = candidate[half_dim:] = np.mean(candidate.reshape(-1, 2), axis=1)\n        return candidate\n\n    def crossover_and_mutate(self, target, mutant, cr=0.9):\n        cross_points = np.random.rand(self.dim) < cr\n        offspring = np.where(cross_points, mutant, target)\n        return self.enforce_periodicity(offspring)\n    \n    def differential_evolution(self, func):\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = self.initialize_population(lb, ub)\n        fitness = self.evaluate_population(population, func)\n        eval_count = len(population)\n        \n        while eval_count < self.budget:\n            for i in range(len(population)):\n                if eval_count >= self.budget:\n                    break\n                indices = [idx for idx in range(len(population)) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                f = 0.5 + np.random.rand() * 0.5  # Adaptive mutation factor\n                mutant = np.clip(a + f * (b - c), lb, ub)\n                trial = self.crossover_and_mutate(population[i], mutant)\n                trial_fitness = func(trial)\n                eval_count += 1\n                if trial_fitness < fitness[i]:\n                    population[i], fitness[i] = trial, trial_fitness\n\n        return self.select_best(population, fitness)\n    \n    def local_optimization(self, best_solution, func):\n        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return result.x if result.success else best_solution\n    \n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_solution, _ = self.differential_evolution(func)\n        best_solution = self.local_optimization(best_solution, func)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic using stochastic periodicity enforcement and adaptive mutation strategies to improve black box optimization efficiency.", "configspace": "", "generation": 3, "fitness": 0.923888350698145, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.924 with standard deviation 0.012. And the mean value of best solutions found was 0.172 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "7963f3d3-1349-45cb-a5e4-3616170abcfe", "metadata": {"aucs": [0.9113410213402939, 0.9404483731776921, 0.9198756575764488], "final_y": [0.17140775812356246, 0.1728793238532066, 0.17288149227653904]}, "mutation_prompt": null}
{"id": "40440bf3-0423-4b12-8143-55e7c3c89314", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizationAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        population_size = 20\n        F = 0.8  # Differential weight\n        CR = 0.7 # Adaptive crossover probability\n        bounds = (func.bounds.lb, func.bounds.ub)\n        population = np.random.uniform(bounds[0], bounds[1], (population_size, self.dim))\n        \n        # Ensure symmetric initialization (quasi-oppositional)\n        opposition_population = bounds[0] + bounds[1] - population\n        population = np.vstack((population, opposition_population))\n        \n        # Evaluate initial population\n        fitness = np.apply_along_axis(func, 1, population)\n        evaluations = len(population)\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Select mutation indices\n                indices = [idx for idx in range(2 * population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                # Mutation and Crossover\n                mutant_vector = np.clip(population[a] + F * (population[b] - population[c]), bounds[0], bounds[1])\n                \n                # Encourage periodicity in solutions\n                trial_vector = np.array([\n                    mutant_vector[j] if np.random.rand() < CR else population[i][j] \n                    for j in range(self.dim)\n                ])\n\n                # Encourage periodicity in solutions (mutant vector adjustment)\n                trial_vector = self._enforce_periodicity(trial_vector)\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                # Select better solution\n                if trial_fitness < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = trial_fitness\n\n            # Modular local search\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            result = minimize(func, best_solution, bounds=list(zip(bounds[0], bounds[1])), method='L-BFGS-B')\n            if result.fun < fitness[best_idx]:\n                population[best_idx] = result.x\n                fitness[best_idx] = result.fun\n                evaluations += result.nfev\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]\n\n    def _enforce_periodicity(self, vector):\n        # This function encourages periodicity by averaging blocks of layers\n        period = 3 # Localized periodicity adjustment\n        periodic_vector = vector.copy()\n        for i in range(0, self.dim, period):\n            block_average = np.mean(vector[i:i+period])\n            periodic_vector[i:i+period] = block_average\n        return periodic_vector", "name": "HybridOptimizationAlgorithm", "description": "Enhanced Differential Evolution with adaptive crossover probability and localized periodicity enforcement, introducing a mutation strategy tweak for optimizing multilayered structures.", "configspace": "", "generation": 4, "fitness": 0.9571416810466143, "feedback": "The algorithm HybridOptimizationAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.957 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2f0f24e0-9a2a-4b49-ad21-2e07c3a4a55a", "metadata": {"aucs": [0.9917467144272252, 0.9345824691262282, 0.9450958595863898], "final_y": [0.1648596115032669, 0.18187968060544402, 0.1648582847239214]}, "mutation_prompt": null}
{"id": "84be5098-d289-4657-8019-b96a3d7bcb0f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizationAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        population_size = 20\n        F = 0.85  # Differential weight (increased from 0.8 for better exploration)\n        CR = 0.85 # Crossover probability (modified from 0.9 for better exploration)\n        bounds = (func.bounds.lb, func.bounds.ub)\n        population = np.random.uniform(bounds[0], bounds[1], (population_size, self.dim))\n        \n        # Ensure symmetric initialization (quasi-oppositional)\n        opposition_population = bounds[0] + bounds[1] - population\n        population = np.vstack((population, opposition_population))\n        \n        # Evaluate initial population\n        fitness = np.apply_along_axis(func, 1, population)\n        evaluations = len(population)\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Select mutation indices\n                indices = [idx for idx in range(2 * population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                # Mutation and Crossover\n                mutant_vector = np.clip(population[a] + F * (population[b] - population[c]), bounds[0], bounds[1])\n                \n                trial_vector = np.array([\n                    mutant_vector[j] if np.random.rand() < CR else population[i][j] \n                    for j in range(self.dim)\n                ])\n                \n                # Encourage periodicity in solutions\n                trial_vector = self._enforce_periodicity(trial_vector)\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                # Select better solution\n                if trial_fitness < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = trial_fitness\n\n            # Modular local search\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            result = minimize(func, best_solution, bounds=list(zip(bounds[0], bounds[1])), method='L-BFGS-B')\n            if result.fun < fitness[best_idx]:\n                population[best_idx] = result.x\n                fitness[best_idx] = result.fun\n                evaluations += result.nfev\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]\n\n    def _enforce_periodicity(self, vector):\n        # This function encourages periodicity by averaging blocks of layers\n        period = 2\n        periodic_vector = vector.copy()\n        for i in range(0, self.dim, period):\n            block_average = np.mean(vector[i:i+period])\n            periodic_vector[i:i+period] = block_average\n        return periodic_vector", "name": "HybridOptimizationAlgorithm", "description": "Enhanced exploration by a slight increase in the Differential weight to improve solution diversity and convergence.", "configspace": "", "generation": 4, "fitness": 0.9526252107781225, "feedback": "The algorithm HybridOptimizationAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.953 with standard deviation 0.038. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "52bc195d-abdd-4104-a6fa-a0fea6bcbd77", "metadata": {"aucs": [0.9925527882255107, 0.9016187703994558, 0.963704073709401], "final_y": [0.16485774779715257, 0.1648573642946891, 0.16486458384578273]}, "mutation_prompt": null}
{"id": "8c1fb759-bca4-4038-b708-5464c07f5162", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizationAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        population_size = 20\n        F = 0.9  # Differential weight (changed from 0.8 for better exploration)\n        CR = 0.85 # Crossover probability (modified from 0.9 for better exploration)\n        bounds = (func.bounds.lb, func.bounds.ub)\n        population = np.random.uniform(bounds[0], bounds[1], (population_size, self.dim))\n        \n        # Ensure symmetric initialization (quasi-oppositional)\n        opposition_population = bounds[0] + bounds[1] - population\n        population = np.vstack((population, opposition_population))\n        \n        # Evaluate initial population\n        fitness = np.apply_along_axis(func, 1, population)\n        evaluations = len(population)\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Select mutation indices\n                indices = [idx for idx in range(2 * population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                # Mutation and Crossover\n                mutant_vector = np.clip(population[a] + F * (population[b] - population[c]), bounds[0], bounds[1])\n                \n                trial_vector = np.array([\n                    mutant_vector[j] if np.random.rand() < CR else population[i][j] \n                    for j in range(self.dim)\n                ])\n                \n                # Encourage periodicity in solutions\n                trial_vector = self._enforce_periodicity(trial_vector)\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                # Select better solution\n                if trial_fitness < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = trial_fitness\n\n            # Modular local search\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            result = minimize(func, best_solution, bounds=list(zip(bounds[0], bounds[1])), method='L-BFGS-B')\n            if result.fun < fitness[best_idx]:\n                population[best_idx] = result.x\n                fitness[best_idx] = result.fun\n                evaluations += result.nfev\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]\n\n    def _enforce_periodicity(self, vector):\n        # This function encourages periodicity by averaging blocks of layers\n        period = 2\n        periodic_vector = vector.copy()\n        for i in range(0, self.dim, period):\n            block_average = np.mean(vector[i:i+period])\n            periodic_vector[i:i+period] = block_average\n        return periodic_vector", "name": "HybridOptimizationAlgorithm", "description": "Enhanced Refined Differential Evolution with adjusted differential weight for improved solution exploration and diversity.", "configspace": "", "generation": 4, "fitness": 0.9554396635774814, "feedback": "The algorithm HybridOptimizationAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.955 with standard deviation 0.026. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "52bc195d-abdd-4104-a6fa-a0fea6bcbd77", "metadata": {"aucs": [0.9377166758594129, 0.9918283919868385, 0.936773922886193], "final_y": [0.1818868392322217, 0.16486323960153004, 0.1818802601312386]}, "mutation_prompt": null}
{"id": "4b7e9f05-ca15-41a1-bae9-0a1ecf899298", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizationAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        population_size = 20\n        F = 0.8  # Differential weight\n        CR = 0.85 # Crossover probability (modified from 0.9 for better exploration)\n        bounds = (func.bounds.lb, func.bounds.ub)\n        population = np.random.uniform(bounds[0], bounds[1], (population_size, self.dim))\n        \n        # Ensure symmetric initialization (quasi-oppositional)\n        opposition_population = bounds[0] + bounds[1] - population\n        population = np.vstack((population, opposition_population))\n        \n        # Evaluate initial population\n        fitness = np.apply_along_axis(func, 1, population)\n        evaluations = len(population)\n\n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Select mutation indices\n                indices = [idx for idx in range(2 * population_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                # Enhance exploration: dynamically adjust F\n                F = 0.5 + np.random.rand() * 0.5  # New line\n\n                # Mutation and Crossover\n                mutant_vector = np.clip(population[a] + F * (population[b] - population[c]), bounds[0], bounds[1])\n                \n                trial_vector = np.array([\n                    mutant_vector[j] if np.random.rand() < CR else population[i][j] \n                    for j in range(self.dim)\n                ])\n                \n                # Encourage periodicity in solutions\n                trial_vector = self._enforce_periodicity(trial_vector)\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                # Select better solution\n                if trial_fitness < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = trial_fitness\n\n            # Modular local search\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            result = minimize(func, best_solution, bounds=list(zip(bounds[0], bounds[1])), method='L-BFGS-B')\n            if result.fun < fitness[best_idx]:\n                population[best_idx] = result.x\n                fitness[best_idx] = result.fun\n                evaluations += result.nfev\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]\n\n    def _enforce_periodicity(self, vector):\n        # This function encourages periodicity by averaging blocks of layers\n        period = 2\n        periodic_vector = vector.copy()\n        for i in range(0, self.dim, period):\n            block_average = np.mean(vector[i:i+period])\n            periodic_vector[i:i+period] = block_average\n        return periodic_vector", "name": "HybridOptimizationAlgorithm", "description": "Enhanced Differential Evolution by dynamically adjusting the differential weight for better exploration and exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.9445246098850811, "feedback": "The algorithm HybridOptimizationAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.945 with standard deviation 0.010. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "52bc195d-abdd-4104-a6fa-a0fea6bcbd77", "metadata": {"aucs": [0.9582205717677726, 0.9397502298959867, 0.9356030279914843], "final_y": [0.1648606842759801, 0.16485998529162926, 0.18187848045879262]}, "mutation_prompt": null}
{"id": "234ed245-77d3-4e0d-9c45-59d2501f6b07", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop  # Quasi-Oppositional initialization\n        return np.vstack((pop, opp_pop))\n\n    def evaluate_population(self, population, func):\n        return np.array([func(ind) for ind in population])\n\n    def select_best(self, population, fitness):\n        idx = np.argmin(fitness)\n        return population[idx], fitness[idx]\n\n    def enforce_periodicity(self, candidate):\n        # Adjust layers to encourage periodic solutions\n        half_dim = self.dim // 2\n        candidate[:half_dim] = candidate[half_dim:] = np.mean(candidate.reshape(-1, 2), axis=1)\n        return candidate\n\n    def crossover_and_mutate(self, target, mutant, gen_count, cr_start=0.9, cr_end=0.5):\n        cr = cr_start + (cr_end - cr_start) * (gen_count / (self.budget / self.population_size))\n        cross_points = np.random.rand(self.dim) < cr\n        offspring = np.where(cross_points, mutant, target)\n        return self.enforce_periodicity(offspring)\n\n    def differential_evolution(self, func):\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = self.initialize_population(lb, ub)\n        fitness = self.evaluate_population(population, func)\n        eval_count = len(population)\n\n        gen_count = 0  # Added line for generation count\n\n        while eval_count < self.budget:\n            for i in range(len(population)):\n                if eval_count >= self.budget:\n                    break\n                indices = [idx for idx in range(len(population)) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                trial = self.crossover_and_mutate(population[i], mutant, gen_count)\n                trial_fitness = func(trial)\n                eval_count += 1\n                if trial_fitness < fitness[i]:\n                    population[i], fitness[i] = trial, trial_fitness\n\n            gen_count += 1  # Added line to increment generation count\n\n        return self.select_best(population, fitness)\n\n    def local_optimization(self, best_solution, func):\n        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return result.x if result.success else best_solution\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_solution, _ = self.differential_evolution(func)\n        best_solution = self.local_optimization(best_solution, func)\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced the crossover probability dynamically based on the generation count to balance exploration and exploitation.", "configspace": "", "generation": 4, "fitness": 0.9341686903278982, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.934 with standard deviation 0.006. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7963f3d3-1349-45cb-a5e4-3616170abcfe", "metadata": {"aucs": [0.9402941239450225, 0.9265983342929458, 0.9356136127457264], "final_y": [0.17286448061165172, 0.17288705766053136, 0.17289619290972025]}, "mutation_prompt": null}
