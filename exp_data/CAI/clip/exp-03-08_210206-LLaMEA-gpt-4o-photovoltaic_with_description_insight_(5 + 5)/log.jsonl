{"id": "5d68566c-6547-4726-a450-417dc65abed8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.best_solution = None\n        self.best_fitness = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        self.budget -= self.population_size\n\n        while self.budget > 0:\n            for i in range(self.population_size):\n                a, b, c = population[np.random.choice(self.population_size, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < 0.9, mutant, population[i])\n                trial_fitness = func(trial)\n                self.budget -= 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                if fitness[i] < self.best_fitness:\n                    self.best_fitness = fitness[i]\n                    self.best_solution = population[i]\n\n                if self.budget <= 0:\n                    break\n\n    def nelder_mead_local_refinement(self, func, bounds):\n        if self.best_solution is not None:\n            result = minimize(func, self.best_solution, method='Nelder-Mead',\n                              bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)],\n                              options={'maxiter': self.budget})\n            if result.fun < self.best_fitness:\n                self.best_fitness = result.fun\n                self.best_solution = result.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        layers = min(10, self.dim)\n        \n        while layers <= self.dim:\n            sub_bounds = np.array_split(np.column_stack((bounds.lb, bounds.ub)), layers, axis=0)\n            def sub_func(x):\n                full_x = np.concatenate([x] * (self.dim // layers))[:self.dim]\n                return func(full_x)\n            \n            self.differential_evolution(sub_func, sub_bounds)\n            self.nelder_mead_local_refinement(sub_func, sub_bounds)\n            layers += 10\n\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm that combines Differential Evolution (DE) for global exploration and Nelder-Mead for local refinement, incorporating a modularity-preserving mechanism and adaptive layer scaling to efficiently solve high-dimensional black box optimization problems with noisy cost functions.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\n  File \"<string>\", line 13, in differential_evolution\nAttributeError: 'list' object has no attribute 'lb'\n.", "error": "AttributeError(\"'list' object has no attribute 'lb'\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\n  File \"<string>\", line 13, in differential_evolution\nAttributeError: 'list' object has no attribute 'lb'\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "3a25f92a-c14d-4f58-b870-d2a220f454dd", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n        self.population = np.random.rand(self.population_size, dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        # Using a simple gradient-based local search\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        # Initial evaluation\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            # DE for exploration\n            self.differential_evolution_step(func)\n            # Local search for refinement\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for global exploration with a local search phase that dynamically adapts layer roles and robustness, leveraging a gradual layer increase to handle complexity and computational costs effectively.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 142, in evaluatePhotonic\n    algorithm = globals()[algorithm_name](budget=budget, dim=dim)\n  File \"<string>\", line 12, in __init__\nNameError: name 'func' is not defined\n.", "error": "NameError(\"name 'func' is not defined\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 142, in evaluatePhotonic\n    algorithm = globals()[algorithm_name](budget=budget, dim=dim)\n  File \"<string>\", line 12, in __init__\nNameError: name 'func' is not defined\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "286df453-21c8-4f7d-8b8c-a2bd92b82bdf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def differential_evolution(self, func, bounds, population_size=20, generations=100):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n\n        for gen in range(generations):\n            for i in range(population_size):\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < 0.9\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n                    if f_trial < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n            if gen % 10 == 0 and len(generations) * population_size >= self.budget:\n                break\n\n        return best\n\n    def nelder_mead(self, func, x0):\n        result = minimize(func, x0, method='Nelder-Mead', options={'maxfev': self.budget // 2})\n        return result.x, result.fun\n\n    def __call__(self, func):\n        layer_increment = max(1, self.dim // 10)\n        current_dim = layer_increment\n        best_solution = None\n        best_fitness = float('inf')\n\n        while current_dim <= self.dim and self.budget > 0:\n            subset_func = lambda x: func(np.pad(x, (0, self.dim - len(x)), 'constant'))\n            bounds = np.array([[func.bounds.lb[i], func.bounds.ub[i]] for i in range(current_dim)])\n            de_solution = self.differential_evolution(subset_func, bounds, population_size=20, generations=100)\n            \n            local_solution, local_fitness = self.nelder_mead(subset_func, de_solution)\n            if local_fitness < best_fitness:\n                best_fitness = local_fitness\n                best_solution = np.pad(local_solution, (0, self.dim - len(local_solution)), 'constant')\n\n            current_dim += layer_increment\n        \n        return best_solution", "name": "HybridOptimizer", "description": "A hybrid metaheuristic algorithm that combines Differential Evolution for global exploration with a Nelder-Mead simplex method for local refinement, incorporating a layer-wise adaptive strategy to gradually increase problem complexity in high-dimensional noisy optimization tasks.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 48, in __call__\n  File \"<string>\", line 11, in differential_evolution\nAttributeError: 'numpy.ndarray' object has no attribute 'lb'\n.", "error": "AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 48, in __call__\n  File \"<string>\", line 11, in differential_evolution\nAttributeError: 'numpy.ndarray' object has no attribute 'lb'\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "d9b49482-414f-4257-899e-0fa4b5995e3c", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.f = 0.8  # DE scale factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_prob = 0.3  # Probability to perform local search\n        self.increment_step = 2  # Layer-wise complexity increment\n        self.current_budget = 0\n\n    def local_search(self, candidate, func, bounds):\n        \"\"\"Perform a simple local search around a candidate solution.\"\"\"\n        best_candidate = candidate.copy()\n        best_score = func(candidate)\n        for d in range(self.dim):\n            perturbation = np.random.uniform(-0.1, 0.1) * (bounds.ub[d] - bounds.lb[d])\n            new_candidate = candidate.copy()\n            new_candidate[d] += perturbation\n            new_candidate[d] = np.clip(new_candidate[d], bounds.lb[d], bounds.ub[d])\n            score = func(new_candidate)\n            if score > best_score:\n                best_score = score\n                best_candidate = new_candidate\n            self.current_budget += 1\n            if self.current_budget >= self.budget:\n                break\n        return best_candidate, best_score\n\n    def differential_evolution(self, population, func, bounds):\n        \"\"\"Perform a single iteration of Differential Evolution.\"\"\"\n        new_population = []\n        for i in range(len(population)):\n            if self.current_budget >= self.budget:\n                break\n            a, b, c = np.random.choice(len(population), 3, replace=False)\n            mutant = population[a] + self.f * (population[b] - population[c])\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            trial = np.copy(population[i])\n            jrand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.cr or j == jrand:\n                    trial[j] = mutant[j]\n            trial_score = func(trial)\n            self.current_budget += 1\n            if trial_score > func(population[i]):\n                new_population.append(trial)\n            else:\n                new_population.append(population[i])\n        return np.array(new_population)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget = self.population_size\n\n        while self.current_budget < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            if np.random.rand() < self.local_search_prob:\n                for i in range(len(population)):\n                    if self.current_budget >= self.budget:\n                        break\n                    population[i], scores[i] = self.local_search(population[i], func, bounds)\n            \n            # If allowed, increment layer complexity gradually\n            if self.dim + self.increment_step <= self.budget - self.current_budget:\n                self.dim += self.increment_step\n\n        best_idx = np.argmax(scores)\n        return population[best_idx], scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Hybrid Metaheuristic that combines Differential Evolution with Local Search and Layer-wise Incremental Complexity for optimizing multilayered photonic structures.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 61, in __call__\n  File \"<string>\", line 45, in differential_evolution\nIndexError: index 10 is out of bounds for axis 0 with size 10\n.", "error": "IndexError('index 10 is out of bounds for axis 0 with size 10')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 61, in __call__\n  File \"<string>\", line 45, in differential_evolution\nIndexError: index 10 is out of bounds for axis 0 with size 10\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "a104b688-1b0b-4ec3-8a75-0441d32c1fbd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.population_size, dim)\n        self.best_solution = None\n        self.best_score = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        for i in range(self.population_size):\n            self.population[i] = bounds.lb + self.population[i] * (bounds.ub - bounds.lb)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutation step\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, bounds.lb, bounds.ub)\n\n                # Crossover step\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection step\n                trial_score = func(trial)\n                evaluations += 1\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_solution = trial\n\n                if trial_score < func(self.population[i]):\n                    self.population[i] = trial\n\n                if evaluations >= self.budget:\n                    break\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, method='SLSQP', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n\n        # Step 1: Global Optimization with DE\n        self.differential_evolution(func, bounds)\n\n        # Step 2: Local Search for Refinement\n        if self.best_solution is not None:\n            self.best_solution = self.local_search(func, self.best_solution, bounds)\n\n        return self.best_solution", "name": "HybridOptimizer", "description": "A hybrid evolutionary algorithm combining Differential Evolution for global exploration and a local search strategy using a Sequential Quadratic Programming (SQP) method for refinement, incorporating a dynamic layer expansion technique to balance problem complexity and computational cost.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 48, in __call__\n  File \"<string>\", line 11, in differential_evolution\nAttributeError: 'numpy.ndarray' object has no attribute 'lb'\n.", "error": "AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 48, in __call__\n  File \"<string>\", line 11, in differential_evolution\nAttributeError: 'numpy.ndarray' object has no attribute 'lb'\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "c40ee12f-5f26-4942-887c-a688ec60ad0b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def differential_evolution(self, func, bounds, population_size=20, generations=100):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds[:, 0] + pop * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n\n        for gen in range(generations):\n            if self.budget <= 0:  # Dynamic budget check\n                break\n            for i in range(population_size):\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < 0.9\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n                    if f_trial < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                self.budget -= 1  # Decrement budget\n            if gen % 10 == 0 and gen * population_size >= self.budget:\n                break\n\n        return best\n\n    def nelder_mead(self, func, x0):\n        result = minimize(func, x0, method='Nelder-Mead', options={'maxfev': max(1, self.budget // 2)})\n        return result.x, result.fun\n\n    def __call__(self, func):\n        layer_increment = max(1, self.dim // 10)\n        current_dim = layer_increment\n        best_solution = None\n        best_fitness = float('inf')\n\n        while current_dim <= self.dim and self.budget > 0:\n            subset_func = lambda x: func(np.pad(x, (0, self.dim - len(x)), 'constant'))\n            bounds = np.array([[func.bounds.lb[i], func.bounds.ub[i]] for i in range(current_dim)])\n            de_solution = self.differential_evolution(subset_func, bounds, population_size=20, generations=100)\n            \n            local_solution, local_fitness = self.nelder_mead(subset_func, de_solution)\n            if local_fitness < best_fitness:\n                best_fitness = local_fitness\n                best_solution = np.pad(local_solution, (0, self.dim - len(local_solution)), 'constant')\n\n            current_dim += layer_increment\n        \n        return best_solution", "name": "HybridOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution for global exploration with a Nelder-Mead simplex method for local refinement, incorporating dynamic budgeting and robustness metrics in high-dimensional noisy optimization tasks.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (20,10) (2,) ').", "error": "ValueError('operands could not be broadcast together with shapes (20,10) (2,) ')", "parent_id": "286df453-21c8-4f7d-8b8c-a2bd92b82bdf", "metadata": {}, "mutation_prompt": null}
{"id": "329dd65b-ebad-4bb4-b7bb-64fdf91f6423", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n        self.population = None\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm that combines Differential Evolution (DE) for global exploration with a local search phase, dynamically adapting layer roles and robustness, and utilizing a gradual layer increase to handle complexity and computational costs effectively.", "configspace": "", "generation": 1, "fitness": 0.7829874135950948, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.031. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "3a25f92a-c14d-4f58-b870-d2a220f454dd", "metadata": {"aucs": [0.7455270479841309, 0.8223419543952348, 0.7810932384059188], "final_y": [0.168414459180121, 0.13945145495321587, 0.1526299038723753]}, "mutation_prompt": null}
{"id": "612d0cb0-589c-4fb1-8959-605fd30fd6b6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.population_size, dim)\n        self.best_solution = None\n        self.best_score = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        for i in range(self.population_size):\n            self.population[i] = bounds['lb'] + self.population[i] * (bounds['ub'] - bounds['lb'])\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutation step\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, bounds['lb'], bounds['ub'])\n\n                # Crossover step\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection step\n                trial_score = func(trial)\n                evaluations += 1\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_solution = trial\n\n                if trial_score < func(self.population[i]):\n                    self.population[i] = trial\n\n                if evaluations >= self.budget:\n                    break\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, method='SLSQP', bounds=list(zip(bounds['lb'], bounds['ub'])))\n        return result.x if result.success else x0\n\n    def __call__(self, func):\n        bounds = {'lb': func.bounds.lb, 'ub': func.bounds.ub}\n\n        # Step 1: Global Optimization with DE\n        self.differential_evolution(func, bounds)\n\n        # Step 2: Local Search for Refinement\n        if self.best_solution is not None:\n            self.best_solution = self.local_search(func, self.best_solution, bounds)\n\n        return self.best_solution", "name": "HybridOptimizer", "description": "A hybrid evolutionary algorithm combining Differential Evolution for global exploration and Sequential Quadratic Programming (SQP) for local refinement, with a corrected initialization of bounds for effective optimization of multilayered photonic structures.", "configspace": "", "generation": 1, "fitness": 0.7719083504124358, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.772 with standard deviation 0.015. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "a104b688-1b0b-4ec3-8a75-0441d32c1fbd", "metadata": {"aucs": [0.7625278414443453, 0.7934519114875975, 0.7597452983053643], "final_y": [0.16527015440278225, 0.1601017669387199, 0.17185375608456643]}, "mutation_prompt": null}
{"id": "e39b4f7e-cc40-4a5b-891c-42b85ed444c1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.population_size, dim)\n        self.best_solution = None\n        self.best_score = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        for i in range(self.population_size):\n            self.population[i] = bounds[0] + self.population[i] * (bounds[1] - bounds[0])  # Modified this line\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutation step\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, bounds[0], bounds[1])\n\n                # Crossover step\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection step\n                trial_score = func(trial)\n                evaluations += 1\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_solution = trial\n\n                if trial_score < func(self.population[i]):\n                    self.population[i] = trial\n\n                if evaluations >= self.budget:\n                    break\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, method='SLSQP', bounds=list(zip(bounds[0], bounds[1])))\n        return result.x if result.success else x0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n\n        # Step 1: Global Optimization with DE\n        self.differential_evolution(func, bounds)\n\n        # Step 2: Local Search for Refinement\n        if self.best_solution is not None:\n            self.best_solution = self.local_search(func, self.best_solution, bounds)\n\n        return self.best_solution", "name": "HybridOptimizer", "description": "A hybrid evolutionary algorithm utilizing Differential Evolution and Sequential Quadratic Programming (SQP) with dynamic layer expansion, now handling bounds using individual elements instead of attributes for better robustness.", "configspace": "", "generation": 1, "fitness": 0.7926921837667541, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.793 with standard deviation 0.007. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "a104b688-1b0b-4ec3-8a75-0441d32c1fbd", "metadata": {"aucs": [0.7822609733490271, 0.7983776570472522, 0.7974379209039832], "final_y": [0.15108102923921896, 0.1532217058935713, 0.15808392127048776]}, "mutation_prompt": null}
{"id": "00fcbd30-c23a-4366-98e7-32173833819a", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        # Using a simple gradient-based local search\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        # Initial evaluation\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            # DE for exploration\n            self.differential_evolution_step(func)\n            # Local search for refinement\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for global exploration with a local search phase and a corrected initialization to handle high-dimensional noisy black box optimization problems effectively.", "configspace": "", "generation": 1, "fitness": 0.7898819284426141, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.790 with standard deviation 0.005. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3a25f92a-c14d-4f58-b870-d2a220f454dd", "metadata": {"aucs": [0.7823137205120347, 0.7948745380687572, 0.7924575267470503], "final_y": [0.15927623690139803, 0.15944404567468917, 0.14207510990682803]}, "mutation_prompt": null}
{"id": "647fa39d-5592-4043-b137-4a2e891a50fd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.best_solution = None\n        self.best_fitness = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        bounds = np.array(bounds)  # Corrected line to convert bounds to a numpy array\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        self.budget -= self.population_size\n\n        while self.budget > 0:\n            for i in range(self.population_size):\n                a, b, c = population[np.random.choice(self.population_size, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), bounds[:, 0], bounds[:, 1])\n                trial = np.where(np.random.rand(self.dim) < 0.9, mutant, population[i])\n                trial_fitness = func(trial)\n                self.budget -= 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                if fitness[i] < self.best_fitness:\n                    self.best_fitness = fitness[i]\n                    self.best_solution = population[i]\n\n                if self.budget <= 0:\n                    break\n\n    def nelder_mead_local_refinement(self, func, bounds):\n        if self.best_solution is not None:\n            result = minimize(func, self.best_solution, method='Nelder-Mead',\n                              bounds=[(low, high) for low, high in zip(bounds[:, 0], bounds[:, 1])],\n                              options={'maxiter': self.budget})\n            if result.fun < self.best_fitness:\n                self.best_fitness = result.fun\n                self.best_solution = result.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        layers = min(10, self.dim)\n        \n        while layers <= self.dim:\n            sub_bounds = np.array_split(np.column_stack((bounds.lb, bounds.ub)), layers, axis=0)\n            def sub_func(x):\n                full_x = np.concatenate([x] * (self.dim // layers))[:self.dim]\n                return func(full_x)\n            \n            self.differential_evolution(sub_func, sub_bounds)\n            self.nelder_mead_local_refinement(sub_func, sub_bounds)\n            layers += 10\n\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm combining Differential Evolution for global exploration and Nelder-Mead for local refinement, with a corrected handling of bounds using numpy arrays to address the AttributeError.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 1 is out of bounds for axis 1 with size 1').", "error": "IndexError('index 1 is out of bounds for axis 1 with size 1')", "parent_id": "5d68566c-6547-4726-a450-417dc65abed8", "metadata": {}, "mutation_prompt": null}
{"id": "00eb89e2-4854-4dfa-8da9-769dd732b608", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.85  # Crossover probability updated for better balance\n        self.current_evaluations = 0\n        self.population = None\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Hybrid metaheuristic algorithm with modified crossover probability for enhanced exploration-exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.7848371960035729, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.785 with standard deviation 0.018. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "329dd65b-ebad-4bb4-b7bb-64fdf91f6423", "metadata": {"aucs": [0.7651841916224582, 0.8082579577310377, 0.7810694386572232], "final_y": [0.15694145052446395, 0.15453064803689986, 0.15072918840542493]}, "mutation_prompt": null}
{"id": "21bd0012-b6af-42c3-a147-e25f9c8f3212", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim) * (1.0 / (1.0 + self.fitness[i]))\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) and Adaptive Gradient Perturbation for improved local search, enhancing the global exploration and refinement balance in high-dimensional noisy black box optimization.", "configspace": "", "generation": 2, "fitness": 0.8003471600757445, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.003. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "00fcbd30-c23a-4366-98e7-32173833819a", "metadata": {"aucs": [0.8006687875091604, 0.7961964777523047, 0.8041762149657685], "final_y": [0.15414178000159007, 0.15434990677923066, 0.13472354108920515]}, "mutation_prompt": null}
{"id": "61b5af5f-788c-4e28-a4db-92ffda1887ef", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.8  # Adjusted crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        # Using a simple gradient-based local search\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        # Initial evaluation\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            # DE for exploration\n            self.differential_evolution_step(func)\n            # Local search for refinement\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Optimized crossover probability to enhance exploration and diversity in the DE phase.", "configspace": "", "generation": 2, "fitness": 0.7940347961210731, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.012. And the mean value of best solutions found was 0.155 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "00fcbd30-c23a-4366-98e7-32173833819a", "metadata": {"aucs": [0.79758723175152, 0.8064017604718341, 0.7781153961398652], "final_y": [0.15433240880228816, 0.1506181587946418, 0.16032984966452868]}, "mutation_prompt": null}
{"id": "5d2ad6a7-9058-48ee-8e0b-903ecc4b4a14", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.F = 0.85  # DE mutation factor adjusted for better exploration\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.population_size, dim)\n        self.best_solution = None\n        self.best_score = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        for i in range(self.population_size):\n            self.population[i] = bounds['lb'] + self.population[i] * (bounds['ub'] - bounds['lb'])\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutation step\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, bounds['lb'], bounds['ub'])\n\n                # Crossover step\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection step\n                trial_score = func(trial)\n                evaluations += 1\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_solution = trial\n\n                if trial_score < func(self.population[i]):\n                    self.population[i] = trial\n\n                if evaluations >= self.budget:\n                    break\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, method='SLSQP', bounds=list(zip(bounds['lb'], bounds['ub'])))\n        return result.x if result.success else x0\n\n    def __call__(self, func):\n        bounds = {'lb': func.bounds.lb, 'ub': func.bounds.ub}\n\n        # Step 1: Global Optimization with DE\n        self.differential_evolution(func, bounds)\n\n        # Step 2: Local Search for Refinement\n        if self.best_solution is not None:\n            self.best_solution = self.local_search(func, self.best_solution, bounds)\n\n        return self.best_solution", "name": "HybridOptimizer", "description": "Enhance the algorithm by slightly adjusting the DE mutation factor for better exploration balance.", "configspace": "", "generation": 2, "fitness": 0.7916370118701995, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.792 with standard deviation 0.016. And the mean value of best solutions found was 0.155 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "612d0cb0-589c-4fb1-8959-605fd30fd6b6", "metadata": {"aucs": [0.7695128590972821, 0.804653228787483, 0.8007449477258334], "final_y": [0.16097775109968815, 0.1528905884060644, 0.15192248989724277]}, "mutation_prompt": null}
{"id": "ac4b9a58-4cb8-46ef-a921-981183a58599", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.F = 0.5 + np.random.rand() * 0.4  # Dynamic mutation factor for better exploration\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.population_size, dim)\n        self.best_solution = None\n        self.best_score = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        for i in range(self.population_size):\n            self.population[i] = bounds['lb'] + self.population[i] * (bounds['ub'] - bounds['lb'])\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutation step\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, bounds['lb'], bounds['ub'])\n\n                # Crossover step\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection step with noise handling\n                trial_score = func(trial) + np.random.normal(0, 0.01)\n                evaluations += 1\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_solution = trial\n\n                if trial_score < func(self.population[i]):\n                    self.population[i] = trial\n\n                if evaluations >= self.budget:\n                    break\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(lambda x: func(x) + np.random.normal(0, 0.005), x0, method='SLSQP', bounds=list(zip(bounds['lb'], bounds['ub'])))\n        return result.x if result.success else x0\n\n    def __call__(self, func):\n        bounds = {'lb': func.bounds.lb, 'ub': func.bounds.ub}\n\n        # Step 1: Global Optimization with DE\n        self.differential_evolution(func, bounds)\n\n        # Step 2: Local Search for Refinement\n        if self.best_solution is not None:\n            self.best_solution = self.local_search(func, self.best_solution, bounds)\n\n        return self.best_solution", "name": "HybridOptimizer", "description": "The hybrid algorithm improves solution refinement by integrating a noise-handling mechanism in the local search phase and a dynamic mutation factor in DE.", "configspace": "", "generation": 3, "fitness": 0.7633198475211902, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.763 with standard deviation 0.017. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "5d2ad6a7-9058-48ee-8e0b-903ecc4b4a14", "metadata": {"aucs": [0.7401407308630596, 0.7821875432224056, 0.7676312684781055], "final_y": [0.17185816131107357, 0.1482132309836096, 0.1605111894289838]}, "mutation_prompt": null}
{"id": "45c7ab1f-6a43-4474-8843-8a578978a0d1", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.8  # Adjusted crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            self.F = np.random.uniform(0.4, 0.9)  # Adaptive mutation factor\n            self.CR = np.random.uniform(0.7, 0.9)  # Adaptive crossover rate\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration and convergence by incorporating adaptive mutation and crossover rates in the Differential Evolution phase.", "configspace": "", "generation": 3, "fitness": 0.8491544948632606, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.062. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.025.", "error": "", "parent_id": "61b5af5f-788c-4e28-a4db-92ffda1887ef", "metadata": {"aucs": [0.7622321721932014, 0.8808685578899188, 0.9043627545066614], "final_y": [0.16724543060826302, 0.11475344227320516, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "a8ce709e-87ca-4070-a74f-07ac194cdaff", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic leveraging Differential Evolution (DE) with Adaptive Gradient Perturbation, introducing an adaptive crossover probability and a new local search mechanism based on L-BFGS for enhanced convergence in noisy high-dimensional optimization.", "configspace": "", "generation": 3, "fitness": 0.860811754210403, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.041. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "21bd0012-b6af-42c3-a147-e25f9c8f3212", "metadata": {"aucs": [0.8059509466742598, 0.8715855561183212, 0.9048987598386281], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "c4f13578-285f-4402-b4ea-28cf6431c6b8", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.05, self.dim) * (1.0 / (1.0 + self.fitness[i]))\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Hybrid metaheuristic algorithm with an improved local search phase using adaptive Gaussian perturbation, enhancing exploration and exploitation balance.", "configspace": "", "generation": 3, "fitness": 0.8535468376044207, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.049. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "21bd0012-b6af-42c3-a147-e25f9c8f3212", "metadata": {"aucs": [0.7879101712448924, 0.8684123323565255, 0.9043180092118446], "final_y": [0.15708437473640635, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "1d25d39b-a129-4b8e-8181-6bdf1bce0835", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.population_size, dim)\n        self.best_solution = None\n        self.best_score = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        for i in range(self.population_size):\n            self.population[i] = bounds[0] + self.population[i] * (bounds[1] - bounds[0])  # Modified this line\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutation step\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n\n                # Introduce linear adaptation of the mutation factor\n                F_adapt = self.F * (1 - evaluations / self.budget)\n                mutant = self.population[a] + F_adapt * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, bounds[0], bounds[1])\n\n                # Crossover step\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection step\n                trial_score = func(trial)\n                evaluations += 1\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_solution = trial\n\n                if trial_score < func(self.population[i]):\n                    self.population[i] = trial\n\n                if evaluations >= self.budget:\n                    break\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, method='SLSQP', bounds=list(zip(bounds[0], bounds[1])))\n        return result.x if result.success else x0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n\n        # Step 1: Global Optimization with DE\n        self.differential_evolution(func, bounds)\n\n        # Step 2: Local Search for Refinement\n        if self.best_solution is not None:\n            self.best_solution = self.local_search(func, self.best_solution, bounds)\n\n        return self.best_solution", "name": "HybridOptimizer", "description": "Enhanced the DE mutation strategy by introducing a linear adaptation of the mutation factor for improved exploration and exploitation balance.", "configspace": "", "generation": 3, "fitness": 0.7714349685149783, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.771 with standard deviation 0.017. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "e39b4f7e-cc40-4a5b-891c-42b85ed444c1", "metadata": {"aucs": [0.7678379371250867, 0.7943141207430209, 0.7521528476768276], "final_y": [0.15436518292089962, 0.1524067893030503, 0.17001760958635304]}, "mutation_prompt": null}
{"id": "80c554e3-ca4c-485d-a298-0cd16c907695", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.8  # Adjusted crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            self.F = np.random.uniform(0.4, 0.9)  # Adaptive mutation factor\n            self.CR = np.random.uniform(0.7, 0.9)  # Adaptive crossover rate\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.005, self.dim)  # Improved perturbation variance\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration and convergence by incorporating adaptive mutation and crossover rates in the Differential Evolution phase, with improved perturbation in the local search step.", "configspace": "", "generation": 4, "fitness": 0.7821243514612841, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.782 with standard deviation 0.013. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "45c7ab1f-6a43-4474-8843-8a578978a0d1", "metadata": {"aucs": [0.7680414759311456, 0.7993084489680236, 0.7790231294846833], "final_y": [0.15009670597782232, 0.15762731491642934, 0.15049238635568374]}, "mutation_prompt": null}
{"id": "7c1cc158-5b84-40b3-a850-5113daae3e21", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5\n        self.CR = 0.8\n        self.current_evaluations = 0\n        self.trust_region_radius = 0.1\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            dynamic_F = self.F * np.random.uniform(0.5, 1.5)\n            mutant = np.clip(a + dynamic_F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        # Using a trust region-based local search\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, self.trust_region_radius, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n            else:\n                self.trust_region_radius *= 0.9\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved exploration-exploitation balance by dynamically adjusting mutation factor and integrating a more sophisticated local search using a trust region.", "configspace": "", "generation": 4, "fitness": 0.7971235591426998, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.008. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "61b5af5f-788c-4e28-a4db-92ffda1887ef", "metadata": {"aucs": [0.7955010083457192, 0.807671930164911, 0.7881977389174694], "final_y": [0.13765501155724924, 0.1432085912533715, 0.161198493992877]}, "mutation_prompt": null}
{"id": "bcc3ff00-f4d4-413d-bd16-7f677a7cf9f0", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.1 * np.tanh(fitness_variance)\n        \n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim) * (1.0 / (1.0 + self.fitness[i]))\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved the mutation strategy by adjusting the differential weight (F) dynamically based on fitness variance to enhance exploration in the DE phase.", "configspace": "", "generation": 4, "fitness": 0.7970503477382215, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.026. And the mean value of best solutions found was 0.150 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "21bd0012-b6af-42c3-a147-e25f9c8f3212", "metadata": {"aucs": [0.768792284005073, 0.8313469989521683, 0.7910117602574234], "final_y": [0.15210038297850548, 0.14167047092808494, 0.1563574252010268]}, "mutation_prompt": null}
{"id": "1e97f05e-1607-44c1-a693-9f9e640554cc", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = self.F * (1 + (np.min(self.fitness) - self.fitness[i]) / (np.ptp(self.fitness) + 1e-9))\n            mutant = np.clip(a + adaptive_F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            precision_factor = 0.05 * (1.0 / (1.0 + self.fitness[i]))\n            perturb = np.random.normal(0, precision_factor, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved exploration and convergence by integrating adaptive mutation scaling and fitness-based local search precision in DE.", "configspace": "", "generation": 4, "fitness": 0.7867657431173116, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.787 with standard deviation 0.012. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "c4f13578-285f-4402-b4ea-28cf6431c6b8", "metadata": {"aucs": [0.7882882555446176, 0.8006516489046838, 0.7713573249026333], "final_y": [0.15516302599788256, 0.15130605708198308, 0.15407134131600475]}, "mutation_prompt": null}
{"id": "cec1fe9c-2f8c-4bca-91e3-d93a6c105a2c", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.8  # Adjusted crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            self.F = np.random.uniform(0.4, 0.9)  # Adaptive mutation factor\n            self.CR = np.random.uniform(0.7, 0.9)  # Adaptive crossover rate\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.015, self.dim)  # Adjusted perturbation noise\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved adaptive mutation and crossover strategies by adjusting perturbation noise during local search for better convergence in noisy environments.", "configspace": "", "generation": 4, "fitness": 0.7764100020411422, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.776 with standard deviation 0.017. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "45c7ab1f-6a43-4474-8843-8a578978a0d1", "metadata": {"aucs": [0.7543110868752925, 0.7945975584680356, 0.7803213607800984], "final_y": [0.16282710478633278, 0.16009569312905536, 0.15065262723100625]}, "mutation_prompt": null}
{"id": "8cd42a79-5273-45d7-ae4b-a00c77dd22eb", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            adapt_factor = np.exp(-self.fitness[i])  # Adaptive scaling factor\n            perturb = np.random.normal(0, 0.05 * adapt_factor, self.dim) * (1.0 / (1.0 + self.fitness[i]))\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce an adaptive scaling factor to the Gaussian perturbation in local search for improved exploration efficiency.", "configspace": "", "generation": 5, "fitness": 0.8489121242327687, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.057. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.019.", "error": "", "parent_id": "c4f13578-285f-4402-b4ea-28cf6431c6b8", "metadata": {"aucs": [0.7709707050462619, 0.8714476584401996, 0.9043180092118446], "final_y": [0.15472202679617364, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "270c0985-a6d1-48ba-ab25-d67e2b113dc2", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Initial crossover probability\n        self.CR_decay = 0.99  # Decay factor for crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        self.CR *= self.CR_decay  # Decay the crossover probability\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.05, self.dim) * (1.0 / (1.0 + self.fitness[i]))\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration-exploitation balance by introducing a decay factor to the Differential Evolution's crossover probability, improving convergence in noisy environments.", "configspace": "", "generation": 5, "fitness": 0.8525254278064404, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.853 with standard deviation 0.050. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "c4f13578-285f-4402-b4ea-28cf6431c6b8", "metadata": {"aucs": [0.7859957829408777, 0.8667159134022812, 0.9048645870761621], "final_y": [0.14905529809509244, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic enhancing DE with Adaptive Gradient Perturbation by incorporating a fitness diversity mechanism and dynamic mutation scaling for improved exploration and convergence in noisy high-dimensional optimization.", "configspace": "", "generation": 5, "fitness": 0.8678666553190683, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.036. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "a8ce709e-87ca-4070-a74f-07ac194cdaff", "metadata": {"aucs": [0.8193405943521177, 0.8769942158822075, 0.9072651557228798], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "325813f1-30eb-4f5a-b67f-bf82d59aa9a1", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.05, self.dim) * (1.0 / (1.0 + np.log1p(self.fitness[i])))\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic optimizer with adaptive population size and improved Gaussian perturbation scaling to increase exploration and exploitation efficiency.", "configspace": "", "generation": 5, "fitness": 0.8502471308227948, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.850 with standard deviation 0.064. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.025.", "error": "", "parent_id": "c4f13578-285f-4402-b4ea-28cf6431c6b8", "metadata": {"aucs": [0.7609798831891648, 0.8791492391796771, 0.9106122700995424], "final_y": [0.16615619053992936, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "ee4a5609-e43a-408e-ad0b-a79d9046839e", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F_initial = 0.5  # Initial differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        F_dynamic = self.F_initial * (1 - self.current_evaluations / self.budget)  # Adaptive F\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing the HybridMetaheuristicOptimizer by introducing an adaptive differential weight with a decay factor to improve convergence in high-dimensional noisy optimization.", "configspace": "", "generation": 5, "fitness": 0.8532777139395448, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.853 with standard deviation 0.039. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "a8ce709e-87ca-4070-a74f-07ac194cdaff", "metadata": {"aucs": [0.8081820973443354, 0.8473330352624542, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "46a3705c-62bf-4c3e-a417-230aca89ac21", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n        self.initial_temperature = 1.0  # Initial temperature for simulated annealing\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        temperature = self.initial_temperature * (1 - self.current_evaluations / self.budget)  # Temperature decay\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random()) * temperature  # Dynamic mutation with temperature\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing HybridMetaheuristicOptimizer by introducing a temperature-based simulated annealing mechanism to modulate mutation scaling dynamically for improved convergence in noisy high-dimensional optimization.", "configspace": "", "generation": 6, "fitness": 0.866111450863134, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.036. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {"aucs": [0.8183004640583854, 0.8744738217983861, 0.9055600667326306], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "2b5fdcf6-77a1-416c-92f4-9245481d0634", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        # Adaptive dynamic population resizing based on evaluations\n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "An enhanced hybrid metaheuristic that incorporates adaptive dynamic scaling of population size and an enriched local search phase, optimizing convergence in noisy high-dimensional environments.", "configspace": "", "generation": 6, "fitness": 0.8640352619306834, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.042. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {"aucs": [0.8059509466742598, 0.8818368299059458, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "7af471b2-b20e-438c-9b06-4ece30d999e8", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            sigma = 0.05 * (1.0 / (1.0 + self.fitness[i]))  # Dynamic sigma adjustment\n            perturb = np.random.normal(0, sigma, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search by modifying mutation scaling and integrating a dynamic sigma in Gaussian perturbation, improving noise resilience.", "configspace": "", "generation": 6, "fitness": 0.8569369159701482, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.046. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "c4f13578-285f-4402-b4ea-28cf6431c6b8", "metadata": {"aucs": [0.7950450802584001, 0.8714476584401996, 0.9043180092118446], "final_y": [0.15650465425847504, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "501374d5-7594-46ae-b536-609ded4e6c9e", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Initial crossover probability\n        self.CR_decay = 0.99  # Decay factor for crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        # Adaptive mutation factor based on population diversity\n        pop_std = np.std(self.population, axis=0)\n        adapt_F = self.F + (np.mean(pop_std) / (np.max(pop_std) + 1e-8))\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + adapt_F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        self.CR *= self.CR_decay  # Decay the crossover probability\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.05, self.dim) * (1.0 / (1.0 + self.fitness[i]))\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive mutation factor scaling based on population diversity to enhance exploration-exploitation balance in noisy environments.", "configspace": "", "generation": 6, "fitness": 0.8449663304971242, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.063. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "270c0985-a6d1-48ba-ab25-d67e2b113dc2", "metadata": {"aucs": [0.7582999189349621, 0.8722810633445659, 0.9043180092118446], "final_y": [0.16320038087579902, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "34647fa9-4ec3-4052-aafc-afbaf48dda5d", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_diversity = np.std(self.fitness) / np.mean(self.fitness)  # Added line for fitness diversity\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * fitness_diversity * (b - c), func.bounds.lb, func.bounds.ub)  # Adjusted mutation\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration by adapting mutation factor based on fitness diversity to improve convergence in high-dimensional noisy optimization.", "configspace": "", "generation": 6, "fitness": 0.8629682631568244, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.041. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "a8ce709e-87ca-4070-a74f-07ac194cdaff", "metadata": {"aucs": [0.8075925643764211, 0.8769942158822075, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "5b7fedd1-ab2d-4c61-86a0-99afd26478b6", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\nfrom scipy.stats.qmc import Sobol\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        sobol_sampler = Sobol(d=self.dim, scramble=True)\n        self.population = sobol_sampler.random_base2(m=int(np.log2(self.population_size)))\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random(self.dim))  # Layer-wise adaptive mutation\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic enhancing DE with Adaptive Gradient Perturbation, utilizing layer-wise adaptive mutation scaling and incorporating a Sobol sequence initializer for improved exploration and convergence in noisy high-dimensional optimization.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 64 is out of bounds for axis 0 with size 64').", "error": "IndexError('index 64 is out of bounds for axis 0 with size 64')", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {}, "mutation_prompt": null}
{"id": "c31870d7-1cb6-4318-a500-b61891ccb771", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        # Adaptive population size adjustment\n        self.population_size = max(2, int(self.population_size * (1 - 0.01 * (self.current_evaluations / self.budget))))\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic enhancing DE with Adaptive Gradient Perturbation by incorporating a fitness diversity mechanism and dynamic mutation scaling, now with adaptive population size adjustment to improve exploration and convergence in noisy high-dimensional optimization.", "configspace": "", "generation": 7, "fitness": 0.8655767379702812, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.040. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {"aucs": [0.8111396266067811, 0.8812725780922179, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "a4ae8dda-897e-4095-8010-d07356bc8823", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n        self.initial_temperature = 1.0  # Initial temperature for simulated annealing\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        temperature = self.initial_temperature * (1 - self.current_evaluations / self.budget)  # Temperature decay\n        diversity_decay = 1 - self.current_evaluations / self.budget  # New line added\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random()) * temperature  # Dynamic mutation with temperature\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold * diversity_decay:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introducing a decay factor to the diversity threshold for better adaptation during optimization.", "configspace": "", "generation": 7, "fitness": 0.8616436653035334, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.862 with standard deviation 0.037. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "46a3705c-62bf-4c3e-a417-230aca89ac21", "metadata": {"aucs": [0.8141982927485253, 0.8664146939502304, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "b44387c1-a0ce-44fa-9556-744d358f0f68", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        # Adaptive dynamic population resizing based on evaluations\n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved local search integration using L-BFGS-B with enhanced gradient estimation for refined solutions in high-dimensional noisy optimization.", "configspace": "", "generation": 7, "fitness": 0.8577612409762754, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.858 with standard deviation 0.040. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "2b5fdcf6-77a1-416c-92f4-9245481d0634", "metadata": {"aucs": [0.8059509466742598, 0.8627372200236668, 0.9045955562308994], "final_y": [0.13953098700030486, 0.12078533840043593, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a feedback mechanism to dynamically adjust the differential weight `F` based on population fitness variance.", "configspace": "", "generation": 7, "fitness": 0.889093761091551, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.889 with standard deviation 0.018. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "a8ce709e-87ca-4070-a74f-07ac194cdaff", "metadata": {"aucs": [0.863297142617574, 0.8996661314452346, 0.9043180092118446], "final_y": [0.12104294428798801, 0.11430045219118112, 0.11210677698085747]}, "mutation_prompt": null}
