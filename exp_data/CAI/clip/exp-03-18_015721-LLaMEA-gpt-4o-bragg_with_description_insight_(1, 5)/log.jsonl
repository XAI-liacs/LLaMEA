{"id": "ae15303c-ba37-4990-a679-72be3f705c16", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        return mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.8   # Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.symmetric_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "A hybrid metaheuristic algorithm combining Symmetric Initialization, Differential Evolution (DE), and Local Search, tailored to leverage periodic structures for optimizing multilayer photonic designs under function evaluation constraints.", "configspace": "", "generation": 0, "fitness": 0.9590710508491115, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.9669070730609979, 0.9535065722556272, 0.9567995072307097], "final_y": [0.16485656135976123, 0.1648564103644211, 0.16485696631035363]}, "mutation_prompt": null}
{"id": "6010fcc5-dc72-4723-bb12-b543a64efe06", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.5 + np.random.rand() * 0.5  # Adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)  # Updated initialization\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced hybrid metaheuristic algorithm integrating Quasi-Oppositional Initialization and adaptive mutation strategy in Differential Evolution for improved optimization of multilayer photonic designs.", "configspace": "", "generation": 1, "fitness": 0.970543668940823, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ae15303c-ba37-4990-a679-72be3f705c16", "metadata": {"aucs": [0.974736231764357, 0.9555809514737095, 0.9813138235844026], "final_y": [0.16485658669995285, 0.16485642045621518, 0.16485668467242698]}, "mutation_prompt": null}
{"id": "a70a11a1-cce4-4653-87a3-a534fa248b55", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        return mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = np.clip(0.5 + np.random.rand() * 0.5, 0.1, 1.0)  # Adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.symmetric_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced Hybrid Metaheuristic Algorithm with Adaptive Differential Weight for Improved Optimization of Multilayer Photonic Designs.", "configspace": "", "generation": 1, "fitness": 0.9654435258857371, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ae15303c-ba37-4990-a679-72be3f705c16", "metadata": {"aucs": [0.9594358025990993, 0.9555809514737095, 0.9813138235844026], "final_y": [0.16485624935513787, 0.16485642045621518, 0.16485668467242698]}, "mutation_prompt": null}
{"id": "fd86b003-36dc-423d-840b-93894965a277", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        return mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.8   # Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            penalized_trial = self.penalize_periodicity(trial, func, bounds)\n            if penalized_trial < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def penalize_periodicity(self, solution, func, bounds):\n        # Apply a periodicity penalty to encourage periodic solutions\n        period = self.dim // 4  # Example period size\n        periodic_penalty = np.sum((solution[:-period] - solution[period:])**2)\n        return func(solution) + 0.1 * periodic_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.symmetric_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Integrate a penalty on the cost function to promote periodicity in multilayer structures, enhancing solution quality.", "configspace": "", "generation": 1, "fitness": 0.9686647830621015, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ae15303c-ba37-4990-a679-72be3f705c16", "metadata": {"aucs": [0.9690995741281927, 0.9555809514737095, 0.9813138235844026], "final_y": [0.16485624935513787, 0.16485642045621518, 0.16485668467242698]}, "mutation_prompt": null}
{"id": "c9c2b9ce-ccc1-441c-b60d-769ae92a3a8b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        return mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.7 + 0.3 * np.random.rand()  # Adaptive Crossover probability\n        F = 0.5 + 0.3 * np.random.rand()   # Adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.symmetric_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "An enhanced hybrid metaheuristic algorithm with adaptive DE parameters, integrating Symmetric Initialization and Local Search for optimizing multilayer photonic designs efficiently. ", "configspace": "", "generation": 1, "fitness": 0.9692764074940982, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ae15303c-ba37-4990-a679-72be3f705c16", "metadata": {"aucs": [0.9708823594762656, 0.9556330394216263, 0.9813138235844026], "final_y": [0.16485624935513787, 0.16485642045621518, 0.16485668467242698]}, "mutation_prompt": null}
{"id": "f152f5c2-021c-4857-b4c3-636ae8f24483", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        return mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F_base = 0.5   # Base Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            F = F_base + 0.3 * np.random.rand()  # Adaptive F\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.enforce_periodicity(trial, bounds)  # Enforce periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def enforce_periodicity(self, solution, bounds):\n        solution = np.round(solution / ((bounds.ub - bounds.lb) / 2)) * ((bounds.ub - bounds.lb) / 2)  # Align to periods\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.symmetric_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced hybrid algorithm utilizing adaptive differential weights and periodicity constraints to improve photonic design optimization.", "configspace": "", "generation": 1, "fitness": 0.9696156963330028, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.970 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ae15303c-ba37-4990-a679-72be3f705c16", "metadata": {"aucs": [0.9719523139408964, 0.9555809514737095, 0.9813138235844026], "final_y": [0.16485643043845932, 0.16485642045621518, 0.16485668467242698]}, "mutation_prompt": null}
{"id": "84e043af-58f7-45d0-a33c-7f123cac349d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.8  # Crossover probability adjusted\n        F = 0.5 + np.random.rand() * 0.5  # Adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Improved local exploitation by adjusting the crossover probability in Differential Evolution for enhanced convergence to optimal solutions.", "configspace": "", "generation": 2, "fitness": 0.9317065659701073, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.932 with standard deviation 0.056. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6010fcc5-dc72-4723-bb12-b543a64efe06", "metadata": {"aucs": [0.9747093217079731, 0.8524578761319956, 0.9679525000703532], "final_y": [0.16485658669995285, 0.164856444745746, 0.1648565713131348]}, "mutation_prompt": null}
{"id": "14580c53-89c7-4536-b996-98592d3a7d01", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Improved hybrid metaheuristic using Quasi-Oppositional Initialization, adaptive mutation in Differential Evolution, and periodic pattern encouragement for multilayer photonic optimization.", "configspace": "", "generation": 2, "fitness": 0.9339397729376473, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.934 with standard deviation 0.058. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6010fcc5-dc72-4723-bb12-b543a64efe06", "metadata": {"aucs": [0.9814093024035724, 0.8524575163390166, 0.9679525000703532], "final_y": [0.16485630127384576, 0.16486065522594617, 0.1648565713131348]}, "mutation_prompt": null}
{"id": "4c453f25-d280-458d-9b47-3ff683e15a4b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = np.random.normal(0.5, 0.1)  # Adjusted mutation factor for exploration\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)  # Updated initialization\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Optimized adaptive mutation strategy in Differential Evolution to further enhance exploration capabilities.", "configspace": "", "generation": 2, "fitness": 0.9114029583968937, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.911 with standard deviation 0.047. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6010fcc5-dc72-4723-bb12-b543a64efe06", "metadata": {"aucs": [0.9137988587813112, 0.8524575163390166, 0.9679525000703532], "final_y": [0.16485658669995285, 0.16486065522594617, 0.1648565713131348]}, "mutation_prompt": null}
{"id": "fe2a7f16-f857-4f3c-a40d-16407cd6d2a9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.5 + (1 - (self.evaluations / self.budget)) * 0.5  # Time-varying adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)  # Updated initialization\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced hybrid metaheuristic with improved mutation strategy using time-varying schemes in Differential Evolution for better optimization.", "configspace": "", "generation": 2, "fitness": 0.91869134851505, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.919 with standard deviation 0.049. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6010fcc5-dc72-4723-bb12-b543a64efe06", "metadata": {"aucs": [0.9356640291357801, 0.8524575163390166, 0.9679525000703532], "final_y": [0.16485658669995285, 0.16486065522594617, 0.1648565713131348]}, "mutation_prompt": null}
{"id": "19aeacf8-95f1-4115-a372-4577b730d31c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.5 + np.random.rand() * 0.5  # Adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            # Encourage periodicity by averaging layers\n            if np.random.rand() < 0.1: \n                mutant_vector[:self.dim//2] = mutant_vector[self.dim//2:]\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced hybrid metaheuristic algorithm integrating Quasi-Oppositional Initialization, adaptive mutation strategy, and periodicity constraints in Differential Evolution for improved optimization of multilayer photonic designs.", "configspace": "", "generation": 2, "fitness": 0.9232497916468505, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.923 with standard deviation 0.051. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6010fcc5-dc72-4723-bb12-b543a64efe06", "metadata": {"aucs": [0.9493393585311817, 0.8524575163390166, 0.9679525000703532], "final_y": [0.16485658669995285, 0.16486065522594617, 0.1648565713131348]}, "mutation_prompt": null}
{"id": "1635014d-62e8-4f75-a901-863c1f46b21a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9\n        F = 0.6 + np.random.rand() * 0.4\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 20  # More inspired periodicity encouragement\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        adaptive_threshold = self.budget // 4  # Delay local search to exploit initial exploration\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            if self.evaluations > adaptive_threshold:  # Start local search later\n                for i in range(population_size):\n                    if self.evaluations >= self.budget:\n                        break\n                    population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced hybrid algorithm with adaptive local search timing and wave interference-inspired periodicity encouragement for photonic optimization.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ZeroDivisionError('integer modulo by zero').", "error": "ZeroDivisionError('integer modulo by zero')", "parent_id": "14580c53-89c7-4536-b996-98592d3a7d01", "metadata": {}, "mutation_prompt": null}
{"id": "767f8577-b776-4f93-b889-91cf024a1015", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        # Line changed to introduce adaptive randomization to enhance diversity\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced Quasi-Oppositional Initialization by introducing diversity through adaptive randomization, improving exploration and convergence.", "configspace": "", "generation": 3, "fitness": 0.9949779195488486, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.995 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "14580c53-89c7-4536-b996-98592d3a7d01", "metadata": {"aucs": [0.9956574612710024, 0.9971912583359139, 0.9920850390396295], "final_y": [0.16485587876344165, 0.16485751747887967, 0.16485653291583635]}, "mutation_prompt": null}
{"id": "6fb5ba20-0911-4c60-b2dd-208bf2271865", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.5 + np.random.rand() * 0.5  # Enhance adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = np.mean(solution[i % period_length::period_length])  # Enforce stronger periodicity\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced hybrid metaheuristic using adaptive Quasi-Oppositional Initialization and periodic promotion strategies with self-adjusting parameterization for photonic optimization.", "configspace": "", "generation": 3, "fitness": 0.9943548878098233, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "14580c53-89c7-4536-b996-98592d3a7d01", "metadata": {"aucs": [0.9937883660539261, 0.9971912583359139, 0.9920850390396295], "final_y": [0.16485658669995285, 0.16485751747887967, 0.16485653291583635]}, "mutation_prompt": null}
{"id": "6ecfb60c-2088-43a0-9ff8-a26d232385dc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.8 + 0.2 * np.random.rand()  # Adaptive crossover probability\n        F = 0.6 + np.random.rand() * 0.4\n        new_population = np.copy(population)\n        diversity_threshold = 0.1  # Threshold for diversity maintenance\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)\n            # Encourage diversity by checking for similarity\n            if np.std(population, axis=0).mean() > diversity_threshold:\n                if func(trial) < func(population[i]):\n                    new_population[i] = trial\n                    self.evaluations += 1\n            else:\n                new_population[i] = population[np.random.choice(len(population))]\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced hybrid metaheuristic by integrating adaptive crossover and diversity maintenance while improving periodicity handling for multilayer photonic optimization.", "configspace": "", "generation": 3, "fitness": 0.9903555327210173, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.990 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "14580c53-89c7-4536-b996-98592d3a7d01", "metadata": {"aucs": [0.981790299857702, 0.9971912583359139, 0.9920850399694359], "final_y": [0.16485623046341125, 0.16485751747887967, 0.1648565273437439]}, "mutation_prompt": null}
{"id": "7f32ebc4-66d7-4425-bcf8-1cd29074a9dc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.5 + np.random.rand() * 0.5  # Adjusted adaptive Differential weight for better exploration\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced adaptive mutation strategy in Differential Evolution for improved exploration and convergence.", "configspace": "", "generation": 3, "fitness": 0.9902342712310827, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.990 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "14580c53-89c7-4536-b996-98592d3a7d01", "metadata": {"aucs": [0.9814264463146155, 0.9971912583359139, 0.9920851090427185], "final_y": [0.16485629675106173, 0.16485751747887967, 0.1648563900057134]}, "mutation_prompt": null}
{"id": "32896c4b-9efb-4a66-918d-567d328cb6e9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        # Line changed to introduce adaptive randomization to enhance diversity\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        fitness_sorted_indices = np.argsort([func(ind) for ind in new_population])[:len(population)//2]\n        return new_population[fitness_sorted_indices]  # Retain top half\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced search by incorporating fitness-based sorting to prioritize promising candidates during evolution.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 10 is out of bounds for axis 0 with size 10').", "error": "IndexError('index 10 is out of bounds for axis 0 with size 10')", "parent_id": "767f8577-b776-4f93-b889-91cf024a1015", "metadata": {}, "mutation_prompt": null}
{"id": "5c073c70-77cf-4f77-b493-e8ddb6ff59c1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.5 + 0.5 * np.random.rand()  # Adaptive Differential weight modified\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n            # Elitist replacement added\n            if func(new_population[i]) < func(population[i]):\n                population[i] = new_population[i]\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced Hybrid Optimization by incorporating adaptive mutation scaling and elitist replacement to boost convergence and diversity.", "configspace": "", "generation": 4, "fitness": 0.996087577282241, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "767f8577-b776-4f93-b889-91cf024a1015", "metadata": {"aucs": [0.9967013405193282, 0.9956053565115837, 0.9959560348158113], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "5d4f3fee-b51e-47ee-ae6b-49e95eb556df", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span, range_span, (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.85  # Slightly adjusted Crossover probability\n        F = 0.5 + np.random.rand() * 0.5  # Adjusted adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n                if self.evaluations % 5 == 0:  # Trigger local search based on evaluations\n                    new_population[i] = self.local_search(trial, func, bounds)\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 5  # Adjusted periodicity influencing parameter\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 25  # Increased population size for better exploration\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced HybridOptimization with adaptive local search triggering and modified periodicity application for improved convergence in complex landscapes.", "configspace": "", "generation": 4, "fitness": 0.9862273867234225, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "767f8577-b776-4f93-b889-91cf024a1015", "metadata": {"aucs": [0.9702901996616091, 0.9956053565115837, 0.9927866039970744], "final_y": [0.16485578334712347, 0.16485617387406792, 0.1648558735407496]}, "mutation_prompt": null}
{"id": "1adb6bfe-7b2e-4b71-bfc9-79038b841fd0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        # Line changed to introduce adaptive randomization to enhance diversity\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.8 + 0.1 * np.cos(self.evaluations)  # Dynamically adjusted Crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhancement by dynamically adjusting the crossover rate in Differential Evolution to maintain diversity and improve convergence.", "configspace": "", "generation": 4, "fitness": 0.9953754398899742, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.995 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "767f8577-b776-4f93-b889-91cf024a1015", "metadata": {"aucs": [0.9956574612710024, 0.9945128235831091, 0.9959560348158113], "final_y": [0.16485587876344165, 0.16485596921936407, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "e12d6a11-e7e5-4287-9c1a-2f530660b919", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            # Line changed to introduce adaptive mutation strategy\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Introducing adaptive mutation strategy to the differential evolution process, enhancing convergence.", "configspace": "", "generation": 4, "fitness": 0.9965351647067529, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "767f8577-b776-4f93-b889-91cf024a1015", "metadata": {"aucs": [0.9993570942613507, 0.9942923650430965, 0.9959560348158113], "final_y": [0.16485607222766785, 0.16485621006665407, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "d3237f6d-a449-46c8-b138-813d11272924", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            # Modified line with periodic mutation strategy\n            mutant_vector = np.clip(a + np.sin(np.random.rand() * (b - c)), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Introducing periodic mutation strategy in differential evolution for improved performance in layered optimization problems.", "configspace": "", "generation": 5, "fitness": 0.9966044682992091, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e12d6a11-e7e5-4287-9c1a-2f530660b919", "metadata": {"aucs": [0.9976222506076601, 0.996235119474156, 0.9959560348158113], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "936f58c7-3a1a-4216-9395-bd63196207d9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        # Line changed to introduce dynamic scaling factor\n        F = 0.5 + np.cos(np.pi * self.evaluations / self.budget) * 0.5\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Further refine the adaptive mutation strategy by using a dynamic scaling factor for improved exploration-exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.9900703812004279, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.990 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e12d6a11-e7e5-4287-9c1a-2f530660b919", "metadata": {"aucs": [0.9786540487858454, 0.9956053565115837, 0.9959517383038548], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "21a4fd9e-60ef-4427-a4b5-949e764bd716", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        inertia_weight = 0.9 - (0.7 * self.evaluations / self.budget)  # Adaptive inertia weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + inertia_weight * np.random.rand() * (b - c), bounds.lb, bounds.ub)  # Added inertia weight\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhance convergence by incorporating inertia weight adaptation in the Differential Evolution process.", "configspace": "", "generation": 5, "fitness": 0.9969528943022846, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e12d6a11-e7e5-4287-9c1a-2f530660b919", "metadata": {"aucs": [0.9992972915794592, 0.9956053565115837, 0.9959560348158113], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "e2839cb8-8be1-4530-af6f-4317ff125bda", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.5 + 0.5 * (self.evaluations / self.budget)  # Dynamic crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Introducing a dynamic crossover rate to adaptively enhance exploration and exploitation balance in the differential evolution process.", "configspace": "", "generation": 5, "fitness": 0.9970109345909797, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e12d6a11-e7e5-4287-9c1a-2f530660b919", "metadata": {"aucs": [0.9993570607141361, 0.9977019536746492, 0.9939737893841535], "final_y": [0.16485607222766785, 0.16485617387406792, 0.16485648711023282]}, "mutation_prompt": null}
{"id": "4d4e30c6-f586-4762-9f1a-9042b2f4297c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'learning_rate': 0.01 + 0.09 * np.random.rand()})\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Improved local refinement by introducing adaptive learning rates in the L-BFGS-B local search step.", "configspace": "", "generation": 5, "fitness": 0.9953297515418207, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.995 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e12d6a11-e7e5-4287-9c1a-2f530660b919", "metadata": {"aucs": [0.9993570942613507, 0.9906804220602564, 0.9959517383038548], "final_y": [0.16485607222766785, 0.16485604085997962, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "cfa9bbb1-3c95-422a-bbe0-45ca8de2394d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.5 + 0.5 * (self.evaluations / self.budget)  # Dynamic crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)  # Improved mutation strategy\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(0, self.dim, period_length):  # Apply periodicity in blocks\n            solution[i:i+period_length] = np.mean(solution[i:i+period_length])\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Refined solution enhancing diversity by adding a mutation strategy and improved periodic application for superior convergence.", "configspace": "", "generation": 6, "fitness": 0.9843530677980553, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.018. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e2839cb8-8be1-4530-af6f-4317ff125bda", "metadata": {"aucs": [0.9595424065165133, 0.9960828048044518, 0.9974339920732012], "final_y": [0.16485607222766785, 0.16485604085997962, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "96df610b-ba3f-4469-992d-9070fe9ea5cb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.5 + 0.5 * (self.evaluations / self.budget)  # Dynamic crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            if self.evaluations % 10 == 0:  # Apply periodicity less frequently\n                trial = self.apply_periodicity(trial, bounds)\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced hybrid optimization by adjusting periodicity application frequency for better convergence.", "configspace": "", "generation": 6, "fitness": 0.9956812655493116, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e2839cb8-8be1-4530-af6f-4317ff125bda", "metadata": {"aucs": [0.9993573047131756, 0.9946022737443242, 0.9930842181904346], "final_y": [0.16485607222766785, 0.16485617387406792, 0.16485617413642306]}, "mutation_prompt": null}
{"id": "57011070-2cd1-4f32-98c0-75598f21a93b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.5 + 0.5 * (self.evaluations / self.budget)  # Dynamic crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (10 + int(self.evaluations / self.budget * 10)))  # Adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Introducing adaptive periodicity adjustments to enhance fitting and convergence in differential evolution.", "configspace": "", "generation": 6, "fitness": 0.9977933606580501, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.998 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e2839cb8-8be1-4530-af6f-4317ff125bda", "metadata": {"aucs": [0.9993570581335811, 0.9962063624842182, 0.9978166613563509], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648559526931307]}, "mutation_prompt": null}
{"id": "758ff080-8705-403c-937a-200a376bff07", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.5 + 0.5 * (self.evaluations / self.budget)  # Dynamic crossover probability\n        diversity = np.std(population, axis=0).mean() / np.ptp([bounds.lb, bounds.ub])  # Calculate diversity\n        F = 0.4 + diversity * 0.6  # Adjusted adaptive Differential weight based on diversity\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Incorporates adaptive mutation scaling based on current population diversity to enhance convergence in the differential evolution process.", "configspace": "", "generation": 6, "fitness": 0.9918413575516155, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e2839cb8-8be1-4530-af6f-4317ff125bda", "metadata": {"aucs": [0.9827835805276932, 0.9944789737750308, 0.9982615183521224], "final_y": [0.16485607222766785, 0.1648558720567933, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "6a0de9eb-318d-4819-974d-4f6a2385f73e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.5 + 0.5 * (self.evaluations / self.budget)  # Dynamic crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c) + 0.5 * (a - b), bounds.lb, bounds.ub)  # Hybrid mutation strategy\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = self.dim // 10\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Integrate a hybrid mutation strategy for enhanced exploration in differential evolution.", "configspace": "", "generation": 6, "fitness": 0.9910045450220353, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e2839cb8-8be1-4530-af6f-4317ff125bda", "metadata": {"aucs": [0.9792839829261268, 0.9954681337878565, 0.9982615183521224], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "aa8a4ce9-ec5a-4010-8216-6276d5e3221f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.5 + 0.5 * (self.evaluations / self.budget) + np.random.uniform(-0.05, 0.05)  # Modified line: Added stochastic element for CR\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (10 + int(self.evaluations / self.budget * 10)))  # Adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Further tune crossover probability by introducing a small stochastic element for enhanced exploration.", "configspace": "", "generation": 7, "fitness": 0.9902967281939424, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.990 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "57011070-2cd1-4f32-98c0-75598f21a93b", "metadata": {"aucs": [0.9792677062894531, 0.9959198053707168, 0.9957026729216577], "final_y": [0.16485607222766785, 0.16485627061725572, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "04fb9a8c-3011-45e8-a306-4fff57d4d51b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.6 + 0.4 * np.sin(np.pi * self.evaluations / self.budget)  # Dynamic crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 10)))  # Adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced adaptive periodicity and dynamic crossover for better exploration in differential evolution.", "configspace": "", "generation": 7, "fitness": 0.9971052236115536, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "57011070-2cd1-4f32-98c0-75598f21a93b", "metadata": {"aucs": [0.9993570607141361, 0.996255937198867, 0.9957026729216577], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "b75885f5-fa93-4222-b7cf-0c904d103245", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.5 + 0.5 * (self.evaluations / self.budget)  # Dynamic crossover probability\n        # Modified line: Dynamically adjust F to improve exploration\n        F = 0.5 + (0.9 - 0.5) * (1 - self.evaluations / self.budget)  # Dynamic scaling factor\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (10 + int(self.evaluations / self.budget * 10)))  # Adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhance population diversity by dynamically adjusting the weighting factor in differential evolution.", "configspace": "", "generation": 7, "fitness": 0.9941280416575715, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "57011070-2cd1-4f32-98c0-75598f21a93b", "metadata": {"aucs": [0.9920791783067326, 0.9946022737443242, 0.9957026729216577], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "39e003ae-f03b-43cc-b4a7-886d8bdef520", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.5 + 0.5 * (self.evaluations / self.budget)  # Dynamic crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 12)))  # Enhanced adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced adaptive periodicity by dynamically adjusting period length based on evaluations to improve convergence.", "configspace": "", "generation": 7, "fitness": 0.9972034713320433, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "57011070-2cd1-4f32-98c0-75598f21a93b", "metadata": {"aucs": [0.9993570281455498, 0.9946022737443242, 0.9976511121062558], "final_y": [0.16485607222766785, 0.16485617387406792, 0.16485624824072564]}, "mutation_prompt": null}
{"id": "c4b1e37f-98a3-463b-b9c1-1d45ee26202f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.5 + 0.5 * (self.evaluations / self.budget)  # Dynamic crossover probability\n        F = 0.6 + np.random.rand() * 0.2  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            perturbation = (np.random.rand() - 0.5) * (bounds.ub - bounds.lb) * 0.1  # Added perturbation\n            mutant_vector = np.clip(a + F * (b - c) + perturbation, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (10 + int(self.evaluations / self.budget * 10)))  # Adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Integrating perturbation-based exploration to improve discovery of diverse solutions in differential evolution.", "configspace": "", "generation": 7, "fitness": 0.9965211979003071, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "57011070-2cd1-4f32-98c0-75598f21a93b", "metadata": {"aucs": [0.9992586470349393, 0.9946022737443242, 0.9957026729216577], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "c62fa1eb-12cf-4efd-a0da-eea243af7af5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.5 + 0.5 * (self.evaluations / self.budget)\n        F = 0.6 + np.random.rand() * 0.4\n        new_population = np.copy(population)\n        fitness_values = np.array([func(ind) for ind in population])\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False, p=fitness_values/fitness_values.sum())]  # Updated line\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 12)))\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Improved selection strategy by integrating fitness diversity to enhance convergence and prevent premature stagnation.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "39e003ae-f03b-43cc-b4a7-886d8bdef520", "metadata": {}, "mutation_prompt": null}
{"id": "9e0a3efc-6cd9-4507-bc17-9994a36fe721", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.5 + 0.5 * (self.evaluations / self.budget)  # Dynamic crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 12)))  # Enhanced adaptive periodicity\n        subgroup_length = max(1, period_length // 2)  # Adaptive subgrouping for enhanced modularity\n        for i in range(self.dim):\n            solution[i] = solution[i % subgroup_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced modularity by introducing adaptive subgrouping in periodicity to optimize constructive interference.", "configspace": "", "generation": 8, "fitness": 0.996554186064546, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "39e003ae-f03b-43cc-b4a7-886d8bdef520", "metadata": {"aucs": [0.9993570607141361, 0.9946028245578444, 0.9957026729216577], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "3f75f1f2-70e3-4484-9905-986fb7946fe4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.8  # Fixed higher crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 12)))  # Enhanced adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 20})  # Limit iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Modify crossover probability and enhance local search to improve convergence efficiency.", "configspace": "", "generation": 8, "fitness": 0.9965541869247311, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "39e003ae-f03b-43cc-b4a7-886d8bdef520", "metadata": {"aucs": [0.9993570632946911, 0.9946028245578444, 0.9957026729216577], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "49100059-ae14-45f4-b33d-8a9cc42c2924", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.5 + 0.5 * (self.evaluations / self.budget)  # Dynamic crossover probability\n        F = 0.5 + np.random.rand() * 0.3  # Targeted adjustment of Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 12)))  # Enhanced adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        if res.success and res.fun < func(solution): # Improved local search condition\n            self.evaluations += res.nfev\n            return res.x\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced convergence through targeted differential mutation scaling factor and improved local search conditions.", "configspace": "", "generation": 8, "fitness": 0.9960759097651827, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "39e003ae-f03b-43cc-b4a7-886d8bdef520", "metadata": {"aucs": [0.9979222318160462, 0.9946028245578444, 0.9957026729216577], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "f08bdb06-23fe-4f1e-b0b6-ed3ac335e71f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.6 + 0.4 * (self.evaluations / self.budget)  # Dynamic crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 12)))  # Enhanced adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced adaptive periodicity by dynamically adjusting crossover probability based on evaluations to improve convergence.", "configspace": "", "generation": 8, "fitness": 0.996554186064546, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "39e003ae-f03b-43cc-b4a7-886d8bdef520", "metadata": {"aucs": [0.9993570607141361, 0.9946028245578444, 0.9957026729216577], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "2adff4f7-9344-4ef9-9c52-9be8a82270c4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.8  # Fixed higher crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 12)))  # Enhanced adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 25})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Increase local search iteration limit to enhance solution refinement capability.", "configspace": "", "generation": 9, "fitness": 0.9968717658305589, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3f75f1f2-70e3-4484-9905-986fb7946fe4", "metadata": {"aucs": [0.9993570607141361, 0.9948783431598855, 0.9963798936176552], "final_y": [0.16485607222766785, 0.16485636954475336, 0.16485614737681342]}, "mutation_prompt": null}
{"id": "4aeeb63b-2bf5-4ff3-ba5e-f20edffbed03", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.7 + 0.3 * (1 - self.evaluations / self.budget)  # Adaptive crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 12)))  # Enhanced adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 30})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Introduce adaptive crossover probability and refine local search to enhance convergence precision.", "configspace": "", "generation": 9, "fitness": 0.9964500825464576, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3f75f1f2-70e3-4484-9905-986fb7946fe4", "metadata": {"aucs": [0.9993570942613507, 0.9933319657065424, 0.9966611876714798], "final_y": [0.16485607222766785, 0.16485589580039595, 0.16485592303774066]}, "mutation_prompt": null}
{"id": "13c15f43-bdc4-468e-aee2-6b105dc326b0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.5 + np.random.rand() * 0.5  # Fine-tuned adaptive crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 12)))  # Enhanced adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 20})  # Limit iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Fine-tune adaptive crossover probability for improved exploration-exploitation balance.", "configspace": "", "generation": 9, "fitness": 0.9920407963055692, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3f75f1f2-70e3-4484-9905-986fb7946fe4", "metadata": {"aucs": [0.9827917043292198, 0.9969507909698323, 0.9963798936176552], "final_y": [0.16485607222766785, 0.16485589580039595, 0.16485614737681342]}, "mutation_prompt": null}
{"id": "7138885d-be31-4379-9e19-00c89b3244a5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Increased crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 12)))  # Enhanced adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 30})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Adapt crossover and local search for enhanced solution diversity and convergence.", "configspace": "", "generation": 9, "fitness": 0.9975625929496127, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.998 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3f75f1f2-70e3-4484-9905-986fb7946fe4", "metadata": {"aucs": [0.9993570942613507, 0.9969507909698323, 0.9963798936176552], "final_y": [0.16485607222766785, 0.16485589580039595, 0.16485614737681342]}, "mutation_prompt": null}
{"id": "ab4600bb-ede6-4f8a-b40e-c9fe9b7b30ee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.8  # Fixed higher crossover probability\n        F = 0.5 + 0.1 * np.random.rand()  # More consistent adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)  # Use adaptive F\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (10 + int(self.evaluations / self.budget * 10)))  # Refined adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 20})  # Limit iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Optimize search by adaptive mutation scaling and improved periodicity constraints.", "configspace": "", "generation": 9, "fitness": 0.9970028190028729, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3f75f1f2-70e3-4484-9905-986fb7946fe4", "metadata": {"aucs": [0.9976777724211314, 0.9969507909698323, 0.9963798936176552], "final_y": [0.16485607222766785, 0.16485589580039595, 0.16485614737681342]}, "mutation_prompt": null}
{"id": "66e3e1b3-68f1-4b30-b971-9957d21b4c51", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.95  # Increased crossover probability\n        F = 0.5 + 0.2 * (self.evaluations / self.budget)  # Adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (5 + int(self.evaluations / self.budget * 10)))  # Enhanced adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhance exploration-exploitation balance by integrating adaptive mutation and improved periodicity enforcement.", "configspace": "", "generation": 10, "fitness": 0.9957981998528723, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7138885d-be31-4379-9e19-00c89b3244a5", "metadata": {"aucs": [0.9937128753070107, 0.995943328868242, 0.9977383953833642], "final_y": [0.1648560239231741, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "2c3e8bef-7c30-48a9-a5cd-9d191e18d412", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9\n        F = 0.8  # Changed to a fixed value for more consistent exploitation\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)  # Apply consistent scaling with F\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 12)))\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 30})\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhance exploration and exploitation balance by adapting mutation strategies and optimizing the selection process.", "configspace": "", "generation": 10, "fitness": 0.9907703404766034, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7138885d-be31-4379-9e19-00c89b3244a5", "metadata": {"aucs": [0.9760844340495226, 0.9976138514334482, 0.9986127359468396], "final_y": [0.16485607222766785, 0.16485625264306614, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "671ff95c-e94d-4057-9f48-439e1f765eea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Increased crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 12)))  # Enhanced adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 40})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Emphasize local refinement by increasing L-BFGS-B local search iterations, enhancing local solution precision.", "configspace": "", "generation": 10, "fitness": 0.9970194859064772, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7138885d-be31-4379-9e19-00c89b3244a5", "metadata": {"aucs": [0.994100426764505, 0.9983452950080869, 0.9986127359468396], "final_y": [0.164855852893673, 0.16485611688633428, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "64bd70a5-c945-4215-a9df-27f5eb2419f8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Increased crossover probability\n        F = 0.5 + np.random.rand() * 0.5  # Adaptive Differential weight with full range\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)  # Apply the adaptive F here\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 12)))  # Enhanced adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 30})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "HybridOptimization enhanced with adaptive differential weight adjustment for improved convergence and exploration balance.", "configspace": "", "generation": 10, "fitness": 0.9972407729935768, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7138885d-be31-4379-9e19-00c89b3244a5", "metadata": {"aucs": [0.9971662541656491, 0.995943328868242, 0.9986127359468396], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "6b602a75-552d-485b-b355-3629dc447b39", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.6 + 0.3 * np.random.rand()  # Changed line: Introduce adaptive crossover probability\n        F = 0.6 + np.random.rand() * 0.4  # Reduced adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + np.random.rand() * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 12)))  # Enhanced adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 30})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Introduce adaptive crossover probability in Differential Evolution to balance exploration and exploitation.", "configspace": "", "generation": 10, "fitness": 0.9924492555212489, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7138885d-be31-4379-9e19-00c89b3244a5", "metadata": {"aucs": [0.9827917017486649, 0.995943328868242, 0.9986127359468396], "final_y": [0.16485607222766785, 0.16485617387406792, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "f526e819-cc2a-4a8d-a97a-b554a23070cd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.95  # Changed crossover probability\n        F = 0.4 + np.random.rand() * 0.6  # Adjusted adaptive Differential weight\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = 1 + int(self.evaluations / self.budget * self.dim / 2)  # Adjusted periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Improved HybridOptimization with dynamic periodicity and enhanced local search for better convergence.", "configspace": "", "generation": 11, "fitness": 0.9971877466415657, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "64bd70a5-c945-4215-a9df-27f5eb2419f8", "metadata": {"aucs": [0.9982404294727196, 0.9947760005633904, 0.9985468098885869], "final_y": [0.16485607222766785, 0.164856055854262, 0.16485632737889844]}, "mutation_prompt": null}
{"id": "8620a96f-c819-48e6-a994-87e43d643326", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Increased crossover probability\n        F = 0.5 + np.random.rand() * 0.5  # Adaptive Differential weight with full range\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)  # Apply the adaptive F here\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        # Enhanced adaptive periodicity with finer control over period length\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 15)))\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 30})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "HybridOptimization with enhanced periodicity adaptation, focusing on dynamic period length adjustments for better solution convergence.", "configspace": "", "generation": 11, "fitness": 0.9977903322459301, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.998 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "64bd70a5-c945-4215-a9df-27f5eb2419f8", "metadata": {"aucs": [0.9971662541656491, 0.9969618001339755, 0.9992429424381654], "final_y": [0.16485607222766785, 0.1648562762921244, 0.1648561186542773]}, "mutation_prompt": null}
{"id": "99acd773-5703-4666-b5ab-71da211a50b0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Increased crossover probability\n        F = 0.5 + np.random.rand() * 0.5  # Adaptive Differential weight with full range\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)  # Apply the adaptive F here\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (6 + int(self.evaluations / self.budget * 14)))  # Enhanced adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 30})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = min(20, self.budget // 5)  # Dynamic population resizing\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced HybridOptimization with dynamic population resizing and adaptive periodicity enforcement for improved convergence.", "configspace": "", "generation": 11, "fitness": 0.9951234197925749, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.995 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "64bd70a5-c945-4215-a9df-27f5eb2419f8", "metadata": {"aucs": [0.9971662541656491, 0.9933308083713666, 0.9948731968407093], "final_y": [0.16485607222766785, 0.16485617387406792, 0.16485613050636005]}, "mutation_prompt": null}
{"id": "cac3d7c2-9159-42ab-bb28-a991af7fe836", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9\n        F = 0.3 + np.random.rand() * 0.7  # Expanded adaptive Differential weight range\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (6 + int(self.evaluations / self.budget * 14)))  # Modified adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        neighborhoods = np.random.uniform(-0.05, 0.05, solution.shape)  # Added neighborhood exploration\n        perturbed_solution = np.clip(solution + neighborhoods, bounds.lb, bounds.ub)\n        res = minimize(func, perturbed_solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 30})\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced HybridOptimization using adaptive periodicity scaling and a neighborhood search mechanism for improved solution diversity and convergence.", "configspace": "", "generation": 11, "fitness": 0.9958698929742874, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "64bd70a5-c945-4215-a9df-27f5eb2419f8", "metadata": {"aucs": [0.9990181397786576, 0.9969618001339755, 0.991629739010229], "final_y": [0.16485610186246147, 0.1648562762921244, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "63040f68-b810-4932-a2f1-0156bf9740ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Increased crossover probability\n        F = 0.5 + np.random.rand() * 0.5  # Adaptive Differential weight with full range\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)  # Apply the adaptive F here\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (6 + int(self.evaluations / self.budget * 12)))  # Enhanced adaptive periodicity\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 30})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "HybridOptimization refined with enhanced adaptive periodicity for improved convergence in multilayer optimization.", "configspace": "", "generation": 11, "fitness": 0.9963337564014062, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "64bd70a5-c945-4215-a9df-27f5eb2419f8", "metadata": {"aucs": [0.9971662722295338, 0.9969618001339755, 0.9948731968407093], "final_y": [0.16485607222766785, 0.1648562762921244, 0.16485613050636005]}, "mutation_prompt": null}
{"id": "5900ab22-e1b3-4811-8645-c2ababbcd0e1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.8 + 0.2 * np.sin(np.pi * self.evaluations / self.budget)  # Dynamic crossover probability\n        F = 0.5 + np.random.rand() * 0.5  # Adaptive Differential weight with full range\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)  # Apply the adaptive F here\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        # Enhanced adaptive periodicity with finer control over period length\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 15)))\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 30})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced diversity through dynamic crossover probability adaptation in differential evolution to improve exploration.", "configspace": "", "generation": 12, "fitness": 0.9967138304428894, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8620a96f-c819-48e6-a994-87e43d643326", "metadata": {"aucs": [0.9971662541656491, 0.9946160511993842, 0.9983591859636348], "final_y": [0.16485607222766785, 0.16485655164885238, 0.16485624824072564]}, "mutation_prompt": null}
{"id": "203e8081-4336-4ae8-b2b6-32fc8aae3b67", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Increased crossover probability\n        F = 0.5 + np.random.rand() * 0.5  # Adaptive Differential weight with full range\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)  # Apply the adaptive F here\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        # Enhanced adaptive periodicity with finer control over period length\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 15)))\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget - 5:  # Refined local search cutoff\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 30})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced HybridOptimization with refined local search cutoff for improved convergence efficiency.", "configspace": "", "generation": 12, "fitness": 0.9970094551325598, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8620a96f-c819-48e6-a994-87e43d643326", "metadata": {"aucs": [0.9971662541656491, 0.9946160511993842, 0.9992460600326457], "final_y": [0.16485607222766785, 0.16485655164885238, 0.164856624873307]}, "mutation_prompt": null}
{"id": "9c743d9e-fbea-4f52-b5dd-8bd296cd0eff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Increased crossover probability\n        F = 0.5 + np.random.rand() * 0.5  # Adaptive Differential weight with full range\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)  # Apply the adaptive F here\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        # Enhanced adaptive periodicity with finer control over period length\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 15)))\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced local search by increasing L-BFGS-B iterations for better fine-tuning of near-optimal solutions.", "configspace": "", "generation": 12, "fitness": 0.9970104813328393, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8620a96f-c819-48e6-a994-87e43d643326", "metadata": {"aucs": [0.9971662541656491, 0.9946160511993842, 0.9992491386334845], "final_y": [0.16485607222766785, 0.16485655164885238, 0.1648564024263225]}, "mutation_prompt": null}
{"id": "8d889129-6bce-4757-ab81-6bb02066da49", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9  # Increased crossover probability\n        F = 0.5 + np.random.rand() * 0.5  # Adaptive Differential weight with full range\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)  # Apply the adaptive F here\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)  # Encourage periodicity\n            if np.random.rand() < 0.8 and func(trial) < func(population[i]):  # Adaptive acceptance probability\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        # Enhanced adaptive periodicity with finer control over period length\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 15)))\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 30})  # Increased iterations\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Introducing adaptive control for trial solutions' acceptance to further enhance convergence speed.", "configspace": "", "generation": 12, "fitness": 0.9969095250160828, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8620a96f-c819-48e6-a994-87e43d643326", "metadata": {"aucs": [0.9981547198850159, 0.9933277951305867, 0.9992460600326457], "final_y": [0.16485607222766785, 0.16485617387406792, 0.164856624873307]}, "mutation_prompt": null}
{"id": "1d5ff8c4-229f-469d-9e3e-98561f338ac9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        mid_point = (lb + ub) / 2\n        range_span = (ub - lb) / 2\n        original = mid_point + np.random.uniform(-range_span * np.random.rand(), range_span * np.random.rand(), (size, self.dim))\n        opposite = ub - (original - lb)\n        choice = np.random.rand(size, self.dim) < 0.5\n        return np.where(choice, original, opposite)\n\n    def differential_evolution(self, population, func, bounds):\n        CR = 0.9 - 0.4 * (self.evaluations / self.budget)  # Variable crossover probability\n        F = 0.3 + 0.7 * (1 - self.evaluations / self.budget)  # Variable F to balance exploration-exploitation\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(len(population)) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant_vector = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant_vector, population[i])\n            trial = self.apply_periodicity(trial, bounds)\n            if func(trial) < func(population[i]):\n                new_population[i] = trial\n                self.evaluations += 1\n        return new_population\n\n    def apply_periodicity(self, solution, bounds):\n        period_length = max(1, self.dim // (8 + int(self.evaluations / self.budget * 15)))\n        for i in range(self.dim):\n            solution[i] = solution[i % period_length]\n        return np.clip(solution, bounds.lb, bounds.ub)\n\n    def local_search(self, solution, func, bounds):\n        if self.evaluations >= self.budget:\n            return solution\n        res = minimize(func, solution, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 30})\n        self.evaluations += res.nfev\n        return res.x if res.success else solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        population = self.quasi_oppositional_initialization(bounds.lb, bounds.ub, population_size)\n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                population[i] = self.local_search(population[i], func, bounds)\n        best_idx = np.argmin([func(ind) for ind in population])\n        return population[best_idx]", "name": "HybridOptimization", "description": "Enhanced HybridOptimization with variable CR and F based on evaluations to improve convergence exploration-exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.9908387165652185, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8620a96f-c819-48e6-a994-87e43d643326", "metadata": {"aucs": [0.9786540384636254, 0.9946160511993842, 0.9992460600326457], "final_y": [0.16485607222766785, 0.16485655164885238, 0.164856624873307]}, "mutation_prompt": null}
