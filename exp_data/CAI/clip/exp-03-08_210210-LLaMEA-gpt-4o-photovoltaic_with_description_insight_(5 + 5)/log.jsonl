{"id": "d6b20a5d-49c7-43c2-a364-5554140bea74", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for exploration and local gradient-based optimization for refinement, with adaptive layer growth to manage complexity.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 53, in __call__\n  File \"<string>\", line 21, in _mutation\nNameError: name 'lb' is not defined\n.", "error": "NameError(\"name 'lb' is not defined\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 53, in __call__\n  File \"<string>\", line 21, in _mutation\nNameError: name 'lb' is not defined\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "ffb1afe1-96c1-49f8-8bce-d27e2e0ebbea", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * self.dim\n        self.f = 0.8  # DE mutation factor\n        self.cr = 0.9  # DE crossover probability\n        self.local_search_steps = 5\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, pop, lb, ub):\n        new_pop = np.copy(pop)\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n            indices = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            fitness_trial = func(trial) - self.robustness_factor * np.std(trial)\n            self.eval_count += 1\n            if fitness_trial < func(pop[i]):\n                new_pop[i] = trial\n        return new_pop\n\n    def local_search(self, func, individual, lb, ub):\n        best = np.copy(individual)\n        best_fitness = func(best)\n        for _ in range(self.local_search_steps):\n            if self.eval_count >= self.budget:\n                break\n            candidate = best + np.random.normal(0, 0.1, size=self.dim)\n            candidate = np.clip(candidate, lb, ub)\n            fitness_candidate = func(candidate) - self.robustness_factor * np.std(candidate)\n            self.eval_count += 1\n            if fitness_candidate < best_fitness:\n                best, best_fitness = candidate, fitness_candidate\n        return best\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        self.eval_count = 0\n\n        while self.eval_count < self.budget:\n            pop = self.differential_evolution(func, pop, lb, ub)\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                pop[i] = self.local_search(func, pop[i], lb, ub)\n                layer_expansion = min(self.dim, int(self.eval_count / self.budget * self.dim))\n                pop[i][:layer_expansion] = self.local_search(func, pop[i][:layer_expansion], lb[:layer_expansion], ub[:layer_expansion])\n\n        best_idx = np.argmin([func(individual) for individual in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic combining Differential Evolution (DE) for global exploration and Local Search for refinement, with dynamic layer expansion and robustness metrics for optimizing high-dimensional noisy black-box functions.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 57, in __call__\n  File \"<string>\", line 37, in local_search\nValueError: operands could not be broadcast together with shapes (2,) (10,) \n.", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 57, in __call__\n  File \"<string>\", line 37, in local_search\nValueError: operands could not be broadcast together with shapes (2,) (10,) \n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "10cdfc2f-bf44-4a96-89aa-6d47dbfcf080", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.f = 0.5  # DE scaling factor\n        self.cr = 0.9  # Crossover probability\n        self.num_evals = 0  # Track the number of function evaluations\n\n    def differential_evolution(self, pop, bounds, func):\n        for i in range(len(pop)):\n            idxs = [idx for idx in range(len(pop)) if idx != i]\n            a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.f * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            if self.num_evals < self.budget:\n                trial_fitness = func(trial)\n                self.num_evals += 1\n                if trial_fitness > func(pop[i]):\n                    pop[i] = trial\n        return pop\n\n    def local_search(self, solution, bounds, func, complexity_increment):\n        perturbed_solution = solution.copy()\n        complexity = complexity_increment\n        while complexity < self.dim:\n            layer_indices = np.random.choice(self.dim, complexity, replace=False)\n            perturbation = np.random.uniform(-0.05, 0.05, size=complexity)\n            perturbed_solution[layer_indices] += perturbation\n            perturbed_solution = np.clip(perturbed_solution, bounds.lb, bounds.ub)\n            if self.num_evals < self.budget:\n                perturbed_fitness = func(perturbed_solution)\n                self.num_evals += 1\n                if perturbed_fitness > func(solution):\n                    solution = perturbed_solution.copy()\n            complexity += complexity_increment\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        pop_fitness = np.array([func(indiv) for indiv in pop])\n        self.num_evals += self.population_size\n\n        while self.num_evals < self.budget:\n            pop = self.differential_evolution(pop, bounds, func)\n            best_idx = np.argmax(pop_fitness)\n            best_solution = pop[best_idx]\n            best_solution = self.local_search(best_solution, bounds, func, complexity_increment=5)\n            if self.num_evals >= self.budget:\n                break\n\n        best_idx = np.argmax(pop_fitness)\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for global exploration with a custom local search that adapts layer optimization complexity dynamically to refine solutions.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 57, in __call__\n  File \"<string>\", line 37, in local_search\nValueError: operands could not be broadcast together with shapes (2,) (10,) \n.", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 57, in __call__\n  File \"<string>\", line 37, in local_search\nValueError: operands could not be broadcast together with shapes (2,) (10,) \n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "9a5626fb-4c6d-42f6-ae1f-98c49063723a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.CR = 0.9\n        self.F = 0.8\n        self.evaluations = 0\n    \n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        population = np.random.rand(self.pop_size, self.dim)\n        population = bounds.lb + (bounds.ub - bounds.lb) * population\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += self.pop_size\n        \n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = population[idxs]\n                mutant = np.clip(a + self.F * (b - c), bounds.lb, bounds.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def local_refinement(self, func, x0, bounds):\n        # Local optimization using L-BFGS-B\n        result = minimize(func, x0, method='L-BFGS-B', bounds=[(l, u) for l, u in zip(bounds.lb, bounds.ub)])\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        if self.evaluations < self.budget:\n            refined_solution, refined_fitness = self.local_refinement(func, best_solution, bounds)\n            if refined_fitness < best_fitness:\n                best_solution, best_fitness = refined_solution, refined_fitness\n        return best_solution", "name": "HybridOptimizer", "description": "The algorithm combines Differential Evolution for global search with a local gradient-based optimizer, utilizing adaptive layer growth and modular role preservation to efficiently solve high-dimensional noisy optimization problems.", "configspace": "", "generation": 0, "fitness": 0.8282923639136798, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.019. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8018498897936357, 0.8370999992855056, 0.845927202661898], "final_y": [0.145815078080763, 0.12759667853814116, 0.12878514495237603]}, "mutation_prompt": null}
{"id": "756512a0-bd09-4a46-91e6-90ac5e4433d7", "solution": "import numpy as np\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.global_pop_size = 50\n        self.local_refinement_steps = 10\n        self.dim_increment = max(dim // 10, 1)\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, bounds, pop_size, max_iter):\n        lb, ub = bounds.lb, bounds.ub\n        pop = lb + (ub - lb) * np.random.rand(pop_size, len(lb))\n        fitness = np.apply_along_axis(func, 1, pop)\n\n        for iteration in range(max_iter):\n            if len(fitness) >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                trial = np.where(np.random.rand(len(lb)) < 0.9, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if len(fitness) >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i], fitness[i] = trial, trial_fitness\n\n        return pop, fitness\n\n    def local_search(self, func, solution, bounds):\n        best_solution = solution\n        best_fitness = func(solution)\n        step_size = (bounds.ub - bounds.lb) / 100.0\n\n        for _ in range(self.local_refinement_steps):\n            neighbors = [solution + step_size * np.random.randn(self.dim) for _ in range(5)]\n            for neighbor in neighbors:\n                if bounds.lb <= neighbor.all() <= bounds.ub:\n                    neighbor_fitness = func(neighbor)\n                    if neighbor_fitness < best_fitness:\n                        best_solution, best_fitness = neighbor, neighbor_fitness\n        return best_solution, best_fitness\n\n    def __call__(self, func):\n        bounds = func.bounds\n        current_dim = self.dim_increment\n        best_solution = None\n        best_fitness = float('inf')\n\n        while current_dim <= self.dim:\n            pop, fitness = self.differential_evolution(func, bounds, self.global_pop_size, self.budget // 10)\n            for solution in pop:\n                if len(fitness) >= self.budget:\n                    break\n                refined_solution, refined_fitness = self.local_search(func, solution, bounds)\n                if refined_fitness < best_fitness:\n                    best_solution, best_fitness = refined_solution, refined_fitness\n\n            current_dim += self.dim_increment\n\n        return best_solution", "name": "PhotonicOptimizer", "description": "A hybrid metaheuristic combining Differential Evolution for global exploration with a local search refinement phase, incorporating adaptive layer expansion and robustness metrics to efficiently tackle noisy, high-dimensional optimization problems.", "configspace": "", "generation": 0, "fitness": 0.7995412716771816, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.018. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": null, "metadata": {"aucs": [0.774773245635916, 0.8172039386114534, 0.8066466307841753], "final_y": [0.15419598580607174, 0.14225366052859933, 0.14293641416552938]}, "mutation_prompt": null}
{"id": "297e90b1-1216-4a87-8685-69dc0c5ca65a", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * self.dim\n        self.f = 0.8  # DE mutation factor\n        self.cr = 0.9  # DE crossover probability\n        self.local_search_steps = 5\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, pop, lb, ub):\n        new_pop = np.copy(pop)\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n            indices = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            fitness_trial = func(trial) - self.robustness_factor * np.std(trial)\n            self.eval_count += 1\n            if fitness_trial < func(pop[i]):\n                new_pop[i] = trial\n        return new_pop\n\n    def local_search(self, func, individual, lb, ub):\n        best = np.copy(individual)\n        best_fitness = func(best)\n        for _ in range(self.local_search_steps):\n            if self.eval_count >= self.budget:\n                break\n            candidate = best + np.random.normal(0, 0.1, size=self.dim)\n            candidate = np.clip(candidate, lb, ub)\n            fitness_candidate = func(candidate) - self.robustness_factor * np.std(candidate)\n            self.eval_count += 1\n            if fitness_candidate < best_fitness:\n                best, best_fitness = candidate, fitness_candidate\n        return best\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        self.eval_count = 0\n\n        while self.eval_count < self.budget:\n            pop = self.differential_evolution(func, pop, lb, ub)\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                pop[i] = self.local_search(func, pop[i], lb, ub)\n                layer_expansion = min(self.dim, int(self.eval_count / self.budget * self.dim))\n                expanded_lb = lb[:layer_expansion] if layer_expansion > 0 else lb\n                expanded_ub = ub[:layer_expansion] if layer_expansion > 0 else ub\n                pop[i][:layer_expansion] = self.local_search(func, pop[i][:layer_expansion], expanded_lb, expanded_ub)\n\n        best_idx = np.argmin([func(individual) for individual in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic combining Differential Evolution (DE) for global exploration and Local Search for refinement, with dynamic layer expansion and corrected dimensionality handling for optimizing high-dimensional noisy black-box functions.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')", "parent_id": "ffb1afe1-96c1-49f8-8bce-d27e2e0ebbea", "metadata": {}, "mutation_prompt": null}
{"id": "79742364-547d-4d70-9f04-24aed7b6d313", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.f = 0.5  # DE scaling factor\n        self.cr = 0.9  # Crossover probability\n        self.num_evals = 0  # Track the number of function evaluations\n\n    def differential_evolution(self, pop, bounds, func):\n        for i in range(len(pop)):\n            idxs = [idx for idx in range(len(pop)) if idx != i]\n            a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.f * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            if self.num_evals < self.budget:\n                trial_fitness = func(trial)\n                self.num_evals += 1\n                if trial_fitness > func(pop[i]):\n                    pop[i] = trial\n        return pop\n\n    def local_search(self, solution, bounds, func, complexity_increment):\n        perturbed_solution = solution.copy()\n        complexity = complexity_increment\n        while complexity < self.dim:\n            layer_indices = np.random.choice(self.dim, complexity, replace=False)\n            perturbation = np.random.uniform(-0.05, 0.05, size=complexity)\n            perturbed_solution[layer_indices] += perturbation  # Ensure broadcasting\n            perturbed_solution = np.clip(perturbed_solution, bounds.lb, bounds.ub)\n            if self.num_evals < self.budget:\n                perturbed_fitness = func(perturbed_solution)\n                self.num_evals += 1\n                if perturbed_fitness > func(solution):\n                    solution = perturbed_solution.copy()\n            complexity += complexity_increment\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        pop_fitness = np.array([func(indiv) for indiv in pop])\n        self.num_evals += self.population_size\n\n        while self.num_evals < self.budget:\n            pop = self.differential_evolution(pop, bounds, func)\n            best_idx = np.argmax(pop_fitness)\n            best_solution = pop[best_idx]\n            best_solution = self.local_search(best_solution, bounds, func, complexity_increment=5)\n            if self.num_evals >= self.budget:\n                break\n\n        best_idx = np.argmax(pop_fitness)\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for global exploration with a custom local search that adapts layer optimization complexity dynamically to refine solutions, ensuring correct broadcasting during local search perturbations.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')", "parent_id": "10cdfc2f-bf44-4a96-89aa-6d47dbfcf080", "metadata": {}, "mutation_prompt": null}
{"id": "c0eb338d-240f-476e-affa-0e2946a84988", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.f = 0.5  # DE scaling factor\n        self.cr = 0.9  # Crossover probability\n        self.num_evals = 0  # Track the number of function evaluations\n\n    def differential_evolution(self, pop, bounds, func):\n        for i in range(len(pop)):\n            idxs = [idx for idx in range(len(pop)) if idx != i]\n            a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.f * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            if self.num_evals < self.budget:\n                trial_fitness = func(trial)\n                self.num_evals += 1\n                if trial_fitness > func(pop[i]):\n                    pop[i] = trial\n        return pop\n\n    def local_search(self, solution, bounds, func, complexity_increment):\n        perturbed_solution = solution.copy()\n        complexity = complexity_increment\n        while complexity < self.dim:\n            layer_indices = np.random.choice(self.dim, complexity, replace=False)\n            for index in layer_indices:  # Line change 1\n                perturbation = np.random.uniform(-0.05, 0.05)  # Line change 2\n                perturbed_solution[index] += perturbation\n            perturbed_solution = np.clip(perturbed_solution, bounds.lb, bounds.ub)\n            if self.num_evals < self.budget:\n                perturbed_fitness = func(perturbed_solution)\n                self.num_evals += 1\n                if perturbed_fitness > func(solution):\n                    solution = perturbed_solution.copy()\n            complexity += complexity_increment\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        pop_fitness = np.array([func(indiv) for indiv in pop])\n        self.num_evals += self.population_size\n\n        while self.num_evals < self.budget:\n            pop = self.differential_evolution(pop, bounds, func)\n            best_idx = np.argmax(pop_fitness)\n            best_solution = pop[best_idx]\n            best_solution = self.local_search(best_solution, bounds, func, complexity_increment=5)\n            if self.num_evals >= self.budget:\n                break\n\n        best_idx = np.argmax(pop_fitness)\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for global exploration with a custom local search that refines solutions by correctly handling layer-wise perturbation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')", "parent_id": "10cdfc2f-bf44-4a96-89aa-6d47dbfcf080", "metadata": {}, "mutation_prompt": null}
{"id": "a8a842dd-51e4-41d4-adc2-620d8a91cca2", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.f = 0.5  # DE scaling factor\n        self.cr = 0.9  # Crossover probability\n        self.num_evals = 0  # Track the number of function evaluations\n\n    def differential_evolution(self, pop, bounds, func):\n        for i in range(len(pop)):\n            idxs = [idx for idx in range(len(pop)) if idx != i]\n            a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.f * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            if self.num_evals < self.budget:\n                trial_fitness = func(trial)\n                self.num_evals += 1\n                if trial_fitness > func(pop[i]):\n                    pop[i] = trial\n        return pop\n\n    def local_search(self, solution, bounds, func, complexity_increment):\n        perturbed_solution = solution.copy()\n        complexity = complexity_increment\n        while complexity < self.dim:\n            layer_indices = np.random.choice(self.dim, min(complexity, self.dim), replace=False)  # Changed line\n            perturbation = np.random.uniform(-0.05, 0.05, size=complexity)\n            perturbed_solution[layer_indices] += perturbation\n            perturbed_solution = np.clip(perturbed_solution, bounds.lb, bounds.ub)\n            if self.num_evals < self.budget:\n                perturbed_fitness = func(perturbed_solution)\n                self.num_evals += 1\n                if perturbed_fitness > func(solution):\n                    solution = perturbed_solution.copy()\n            complexity += complexity_increment\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        pop_fitness = np.array([func(indiv) for indiv in pop])\n        self.num_evals += self.population_size\n\n        while self.num_evals < self.budget:\n            pop = self.differential_evolution(pop, bounds, func)\n            best_idx = np.argmax(pop_fitness)\n            best_solution = pop[best_idx]\n            best_solution = self.local_search(best_solution, bounds, func, complexity_increment=5)\n            if self.num_evals >= self.budget:\n                break\n\n        best_idx = np.argmax(pop_fitness)\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Improved layer selection strategy in local search for enhanced solution refinement.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')", "parent_id": "10cdfc2f-bf44-4a96-89aa-6d47dbfcf080", "metadata": {}, "mutation_prompt": null}
{"id": "22171ad4-51a8-44a6-890e-1b5e2a1cbf71", "solution": "import numpy as np\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.global_pop_size = 50\n        self.local_refinement_steps = 10\n        self.dim_increment = max(dim // 10, 1)\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, bounds, pop_size, max_iter):\n        lb, ub = bounds.lb, bounds.ub\n        pop = lb + (ub - lb) * np.random.rand(pop_size, len(lb))\n        fitness = np.apply_along_axis(func, 1, pop)\n\n        for iteration in range(max_iter):\n            if len(fitness) >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                trial = np.where(np.random.rand(len(lb)) < 0.9, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if len(fitness) >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i], fitness[i] = trial, trial_fitness\n\n        return pop, fitness\n\n    def local_search(self, func, solution, bounds):\n        best_solution = solution\n        best_fitness = func(solution)\n        step_size = (bounds.ub - bounds.lb) / 100.0\n\n        for _ in range(self.local_refinement_steps):\n            neighbors = [solution + step_size * np.random.randn(self.dim) for _ in range(5)]\n            for neighbor in neighbors:\n                if bounds.lb <= neighbor.all() <= bounds.ub:\n                    neighbor_fitness = func(neighbor)\n                    if neighbor_fitness < best_fitness:\n                        best_solution, best_fitness = neighbor, neighbor_fitness\n                        step_size *= 0.5  # Adaptive step size reduction for more precise refinement\n        return best_solution, best_fitness\n\n    def __call__(self, func):\n        bounds = func.bounds\n        current_dim = self.dim_increment\n        best_solution = None\n        best_fitness = float('inf')\n\n        while current_dim <= self.dim:\n            pop, fitness = self.differential_evolution(func, bounds, self.global_pop_size, self.budget // 10)\n            for solution in pop:\n                if len(fitness) >= self.budget:\n                    break\n                refined_solution, refined_fitness = self.local_search(func, solution, bounds)\n                if refined_fitness < best_fitness:\n                    best_solution, best_fitness = refined_solution, refined_fitness\n\n            current_dim += self.dim_increment\n\n        return best_solution", "name": "PhotonicOptimizer", "description": "Enhanced local search by introducing adaptive step size reduction for more precise refinement in high-dimensional optimization.", "configspace": "", "generation": 1, "fitness": 0.8116545043906424, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.005. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "756512a0-bd09-4a46-91e6-90ac5e4433d7", "metadata": {"aucs": [0.8190654946446334, 0.8087263613083637, 0.80717165721893], "final_y": [0.13252314410900634, 0.1446035067468564, 0.14380558686885603]}, "mutation_prompt": null}
{"id": "9a7c1ca8-4cce-407a-92dd-feb3a734268e", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * self.dim\n        self.f = 0.8  # DE mutation factor\n        self.cr = 0.9  # DE crossover probability\n        self.local_search_steps = 5\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, pop, lb, ub):\n        new_pop = np.copy(pop)\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n            indices = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            fitness_trial = func(trial) - self.robustness_factor * np.std(trial)\n            self.eval_count += 1\n            if fitness_trial < func(pop[i]):\n                new_pop[i] = trial\n        return new_pop\n\n    def local_search(self, func, individual, lb, ub):\n        best = np.copy(individual)\n        best_fitness = func(best)\n        for _ in range(self.local_search_steps):\n            if self.eval_count >= self.budget:\n                break\n            candidate = best + np.random.normal(0, 0.1, size=self.dim)\n            candidate = np.clip(candidate, lb, ub)\n            fitness_candidate = func(candidate) - self.robustness_factor * np.std(candidate)\n            self.eval_count += 1\n            if fitness_candidate < best_fitness:\n                best, best_fitness = candidate, fitness_candidate\n        return best\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        self.eval_count = 0\n\n        while self.eval_count < self.budget:\n            pop = self.differential_evolution(func, pop, lb, ub)\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                pop[i] = self.local_search(func, pop[i], lb, ub)\n                layer_expansion = min(self.dim, int(self.eval_count / self.budget * self.dim))\n                pop[i][:layer_expansion] = self.local_search(func, pop[i][:layer_expansion], lb[:layer_expansion], ub[:layer_expansion])\n\n        best_idx = np.argmin([func(individual) for individual in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic combining Differential Evolution and Local Search, with dynamic layer expansion and robustness metrics for high-dimensional noisy black-box optimization, refining layer expansion logic to ensure correct dimensionality.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')", "parent_id": "ffb1afe1-96c1-49f8-8bce-d27e2e0ebbea", "metadata": {}, "mutation_prompt": null}
{"id": "35bccd59-540b-4e16-ae0a-15e20a0ca0db", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * self.dim\n        self.f = 0.8  # DE mutation factor\n        self.cr = 0.9  # DE crossover probability\n        self.local_search_steps = 5\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, pop, lb, ub):\n        new_pop = np.copy(pop)\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n            indices = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            fitness_trial = func(trial) - self.robustness_factor * np.std(trial)\n            self.eval_count += 1\n            if fitness_trial < func(pop[i]):\n                new_pop[i] = trial\n        return new_pop\n\n    def local_search(self, func, individual, lb, ub):\n        best = np.copy(individual)\n        best_fitness = func(best)\n        for _ in range(self.local_search_steps):\n            if self.eval_count >= self.budget:\n                break\n            candidate = best + np.random.normal(0, 0.1, size=self.dim)\n            candidate = np.clip(candidate, lb, ub)\n            fitness_candidate = func(candidate) - self.robustness_factor * np.std(candidate)\n            self.eval_count += 1\n            if fitness_candidate < best_fitness:\n                best, best_fitness = candidate, fitness_candidate\n        return best\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        self.eval_count = 0\n\n        while self.eval_count < self.budget:\n            pop = self.differential_evolution(func, pop, lb, ub)\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                pop[i] = self.local_search(func, pop[i], lb, ub)\n                layer_expansion = min(self.dim, int(self.eval_count / self.budget * self.dim))\n                if layer_expansion > 0:  # Change made here to ensure valid slicing\n                    pop[i][:layer_expansion] = self.local_search(func, pop[i][:layer_expansion], lb[:layer_expansion], ub[:layer_expansion])\n\n        best_idx = np.argmin([func(individual) for individual in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "An optimized hybrid metaheuristic combining DE and local search with improved modular layer expansion.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')", "parent_id": "ffb1afe1-96c1-49f8-8bce-d27e2e0ebbea", "metadata": {}, "mutation_prompt": null}
{"id": "9d372ad7-4bb8-42b0-aff5-2dd353b89061", "solution": "import numpy as np\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.global_pop_size = 50\n        self.local_refinement_steps = 10\n        self.dim_increment = max(dim // 10, 1)\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, bounds, pop_size, max_iter):\n        lb, ub = bounds.lb, bounds.ub\n        pop = lb + (ub - lb) * np.random.rand(pop_size, len(lb))\n        fitness = np.apply_along_axis(func, 1, pop)\n        \n        F = 0.8 # Initial mutation factor\n        CR = 0.9 # Initial crossover rate\n\n        for iteration in range(max_iter):\n            if len(fitness) >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = np.random.uniform(0.5, 1.0)  # Self-adaptive mutation factor\n                CR = np.random.uniform(0.7, 1.0)  # Self-adaptive crossover rate\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                trial = np.where(np.random.rand(len(lb)) < CR, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if len(fitness) >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i], fitness[i] = trial, trial_fitness\n\n        return pop, fitness\n\n    def local_search(self, func, solution, bounds):\n        best_solution = solution\n        best_fitness = func(solution)\n        step_size = (bounds.ub - bounds.lb) / 100.0\n\n        for _ in range(self.local_refinement_steps):\n            neighbors = [solution + step_size * np.random.randn(self.dim) for _ in range(5)]\n            for neighbor in neighbors:\n                if bounds.lb <= neighbor.all() <= bounds.ub:\n                    neighbor_fitness = func(neighbor)\n                    if neighbor_fitness < best_fitness:\n                        best_solution, best_fitness = neighbor, neighbor_fitness\n                        step_size *= 0.5  # Adaptive step size reduction for more precise refinement\n        return best_solution, best_fitness\n\n    def __call__(self, func):\n        bounds = func.bounds\n        current_dim = self.dim_increment\n        best_solution = None\n        best_fitness = float('inf')\n\n        while current_dim <= self.dim:\n            pop, fitness = self.differential_evolution(func, bounds, self.global_pop_size, self.budget // 10)\n            for solution in pop:\n                if len(fitness) >= self.budget:\n                    break\n                refined_solution, refined_fitness = self.local_search(func, solution, bounds)\n                if refined_fitness < best_fitness:\n                    best_solution, best_fitness = refined_solution, refined_fitness\n\n            current_dim += self.dim_increment\n\n        return best_solution", "name": "PhotonicOptimizer", "description": "Enhanced PhotonicOptimizer by incorporating a self-adaptive mutation factor and crossover rate in Differential Evolution for improved global search.", "configspace": "", "generation": 2, "fitness": 0.8042071047360745, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.011. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "22171ad4-51a8-44a6-890e-1b5e2a1cbf71", "metadata": {"aucs": [0.790505331555519, 0.8035057535788883, 0.8186102290738162], "final_y": [0.13672753446147767, 0.14196771490301197, 0.1356115376777962]}, "mutation_prompt": null}
{"id": "deb311b1-bb8a-40c4-b12d-609e56c5d799", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.CR = 0.9\n        self.F = 0.8\n        self.evaluations = 0\n    \n    def differential_evolution(self, func, bounds):\n        population = np.random.rand(self.pop_size, self.dim)\n        population = bounds.lb + (bounds.ub - bounds.lb) * population\n        fitness = np.array([self.evaluate_individual(func, ind) for ind in population])\n        self.evaluations += self.pop_size\n        \n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = population[idxs]\n                mutant = np.clip(a + self.F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < self.variable_crossover_rate(i, fitness)\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_fitness = self.evaluate_individual(func, trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=[(l, u) for l, u in zip(bounds.lb, bounds.ub)])\n        return result.x, result.fun\n\n    def evaluate_individual(self, func, ind):\n        perturbation = 0.001 * (np.random.rand(self.dim) - 0.5)\n        return func(ind) + func(ind + perturbation)\n    \n    def variable_crossover_rate(self, index, fitness):\n        return self.CR * (1 - (fitness[index] / (np.max(fitness) + 1e-8)))\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        if self.evaluations < self.budget:\n            refined_solution, refined_fitness = self.local_refinement(func, best_solution, bounds)\n            if refined_fitness < best_fitness:\n                best_solution, best_fitness = refined_solution, refined_fitness\n        return best_solution", "name": "HybridOptimizer", "description": "The algorithm enhances the existing hybrid strategy by integrating robustness metrics in the fitness evaluation and dynamically adjusting crossover probability to efficiently balance exploration and exploitation in noisy, high-dimensional optimization problems.", "configspace": "", "generation": 2, "fitness": 0.8167273649532181, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.020. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "9a5626fb-4c6d-42f6-ae1f-98c49063723a", "metadata": {"aucs": [0.8415014799775922, 0.7928207890074893, 0.8158598258745726], "final_y": [0.12638180026817047, 0.15540647188672307, 0.13904501701193317]}, "mutation_prompt": null}
{"id": "987a72e9-ae5a-451d-81e7-546d58ec0d31", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Incorporate boundary variables into mutation function to fix undefined variable error and improve performance.", "configspace": "", "generation": 2, "fitness": 0.844117433504563, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.017. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "d6b20a5d-49c7-43c2-a364-5554140bea74", "metadata": {"aucs": [0.8604618111398389, 0.8505316487227661, 0.8213588406510841], "final_y": [0.12395641956436876, 0.13481537692154621, 0.14173451873047327]}, "mutation_prompt": null}
{"id": "c889209d-b8b7-49cf-87b7-11fd93361207", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.CR = 0.9\n        self.F = 0.8\n        self.evaluations = 0\n    \n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        population = np.random.rand(self.pop_size, self.dim)\n        population = bounds.lb + (bounds.ub - bounds.lb) * population\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += self.pop_size\n        \n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                # Mutation with adaptive F\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = population[idxs]\n                self.F = 0.5 + 0.3 * np.random.rand()  # Adjusted mutation factor\n                mutant = np.clip(a + self.F * (b - c), bounds.lb, bounds.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def local_refinement(self, func, x0, bounds):\n        # Local optimization using L-BFGS-B\n        result = minimize(func, x0, method='L-BFGS-B', bounds=[(l, u) for l, u in zip(bounds.lb, bounds.ub)])\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        if self.evaluations < self.budget:\n            refined_solution, refined_fitness = self.local_refinement(func, best_solution, bounds)\n            if refined_fitness < best_fitness:\n                best_solution, best_fitness = refined_solution, refined_fitness\n        return best_solution", "name": "HybridOptimizer", "description": "The algorithm improves Differential Evolution by dynamically adjusting the mutation factor (F) based on the convergence rate for efficient exploration in high-dimensional noisy optimization problems.", "configspace": "", "generation": 3, "fitness": 0.8401858071209073, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.008. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "9a5626fb-4c6d-42f6-ae1f-98c49063723a", "metadata": {"aucs": [0.8378147147968175, 0.8503432003402099, 0.8323995062256945], "final_y": [0.13259396094434694, 0.11574814784617493, 0.12941259696451346]}, "mutation_prompt": null}
{"id": "1b91695c-3335-4867-af99-daf3d04c151c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.CR = 0.9\n        self.F = 0.8\n        self.evaluations = 0\n    \n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        population = np.random.rand(self.pop_size, self.dim)\n        population = bounds.lb + (bounds.ub - bounds.lb) * population\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += self.pop_size\n        \n        while self.evaluations < self.budget:\n            self.F = 0.5 + 0.3 * (1 - self.evaluations / self.budget)  # Change: Dynamic F adjustment\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = population[idxs]\n                mutant = np.clip(a + self.F * (b - c), bounds.lb, bounds.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def local_refinement(self, func, x0, bounds):\n        # Local optimization using L-BFGS-B\n        result = minimize(func, x0, method='L-BFGS-B', bounds=[(l, u) for l, u in zip(bounds.lb, bounds.ub)])\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        if self.evaluations < self.budget:\n            refined_solution, refined_fitness = self.local_refinement(func, best_solution, bounds)\n            if refined_fitness < best_fitness:\n                best_solution, best_fitness = refined_solution, refined_fitness\n        return best_solution", "name": "HybridOptimizer", "description": "Enhance global exploration by dynamically adjusting the mutation factor F based on convergence speed to improve performance in noisy high-dimensional optimization problems.", "configspace": "", "generation": 3, "fitness": 0.8601279604766133, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "9a5626fb-4c6d-42f6-ae1f-98c49063723a", "metadata": {"aucs": [0.8637641517110197, 0.8483345765639327, 0.8682851531548875], "final_y": [0.12142676304619959, 0.12897842627622003, 0.1235924922308681]}, "mutation_prompt": null}
{"id": "f547b812-d8ac-4485-9dde-86f249671288", "solution": "import numpy as np\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.global_pop_size = 50\n        self.local_refinement_steps = 10\n        self.dim_increment = max(dim // 10, 1)\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, bounds, pop_size, max_iter):\n        lb, ub = bounds.lb, bounds.ub\n        pop = lb + (ub - lb) * np.random.rand(pop_size, len(lb))\n        fitness = np.apply_along_axis(func, 1, pop)\n\n        for iteration in range(max_iter):\n            if len(fitness) >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                trial = np.where(np.random.rand(len(lb)) < 0.9, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if len(fitness) >= self.budget:\n                    break\n\n                # Stochastic acceptance for exploration\n                if trial_fitness < fitness[i] or np.random.rand() < 0.05:\n                    pop[i], fitness[i] = trial, trial_fitness\n\n        return pop, fitness\n\n    def local_search(self, func, solution, bounds):\n        best_solution = solution\n        best_fitness = func(solution)\n        step_size = (bounds.ub - bounds.lb) / 100.0\n\n        for _ in range(self.local_refinement_steps):\n            neighbors = [solution + step_size * np.random.randn(self.dim) for _ in range(5)]\n            for neighbor in neighbors:\n                if bounds.lb <= neighbor.all() <= bounds.ub:\n                    neighbor_fitness = func(neighbor)\n                    if neighbor_fitness < best_fitness:\n                        best_solution, best_fitness = neighbor, neighbor_fitness\n                        step_size *= 0.5  # Adaptive step size reduction for more precise refinement\n        return best_solution, best_fitness\n\n    def __call__(self, func):\n        bounds = func.bounds\n        current_dim = self.dim_increment\n        best_solution = None\n        best_fitness = float('inf')\n\n        while current_dim <= self.dim:\n            pop, fitness = self.differential_evolution(func, bounds, self.global_pop_size, self.budget // 10)\n            for solution in pop:\n                if len(fitness) >= self.budget:\n                    break\n                refined_solution, refined_fitness = self.local_search(func, solution, bounds)\n                if refined_fitness < best_fitness:\n                    best_solution, best_fitness = refined_solution, refined_fitness\n\n            current_dim += self.dim_increment\n\n        return best_solution", "name": "PhotonicOptimizer", "description": "Improved exploration and exploitation balance by integrating stochastic acceptance and diversity preservation in population update.", "configspace": "", "generation": 3, "fitness": 0.8013501636482708, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.801 with standard deviation 0.031. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "22171ad4-51a8-44a6-890e-1b5e2a1cbf71", "metadata": {"aucs": [0.7592038852428338, 0.8305223841403624, 0.8143242215616165], "final_y": [0.15618430426911067, 0.1345906642464757, 0.13047334905194274]}, "mutation_prompt": null}
{"id": "8e46d4d6-6cb9-4236-bea9-0f60bd903b13", "solution": "import numpy as np\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.global_pop_size = 50\n        self.local_refinement_steps = 10\n        self.dim_increment = max(dim // 10, 1)\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, bounds, pop_size, max_iter):\n        lb, ub = bounds.lb, bounds.ub\n        pop = lb + (ub - lb) * np.random.rand(pop_size, len(lb))\n        fitness = np.apply_along_axis(func, 1, pop)\n        F = 0.8  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        for iteration in range(max_iter):\n            if len(fitness) >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()  # Self-adaptive mutation factor\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                CR = 0.8 + 0.2 * np.random.rand()  # Self-adaptive crossover rate\n                trial = np.where(np.random.rand(len(lb)) < CR, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if len(fitness) >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i], fitness[i] = trial, trial_fitness\n\n        return pop, fitness\n\n    def local_search(self, func, solution, bounds):\n        best_solution = solution\n        best_fitness = func(solution)\n        step_size = (bounds.ub - bounds.lb) / 100.0\n        perturbation_scale = 0.05\n\n        for _ in range(self.local_refinement_steps):\n            neighbors = [solution + step_size * np.random.randn(self.dim) for _ in range(5)]\n            for neighbor in neighbors:\n                neighbor = np.clip(neighbor, bounds.lb, bounds.ub)  # Ensure within bounds\n                neighbor_fitness = func(neighbor)\n                if neighbor_fitness < best_fitness:\n                    best_solution, best_fitness = neighbor, neighbor_fitness\n                    step_size *= 0.5  # Adaptive step size reduction\n                # Perturbation analysis for robustness\n                if np.random.rand() < perturbation_scale:\n                    test_perturbation = neighbor + perturbation_scale * np.random.randn(self.dim)\n                    test_perturbation = np.clip(test_perturbation, bounds.lb, bounds.ub)\n                    test_fitness = func(test_perturbation)\n                    if test_fitness < best_fitness:\n                        best_solution, best_fitness = test_perturbation, test_fitness\n        return best_solution, best_fitness\n\n    def __call__(self, func):\n        bounds = func.bounds\n        current_dim = self.dim_increment\n        best_solution = None\n        best_fitness = float('inf')\n\n        while current_dim <= self.dim:\n            pop, fitness = self.differential_evolution(func, bounds, self.global_pop_size, self.budget // 10)\n            for solution in pop:\n                if len(fitness) >= self.budget:\n                    break\n                refined_solution, refined_fitness = self.local_search(func, solution, bounds)\n                if refined_fitness < best_fitness:\n                    best_solution, best_fitness = refined_solution, refined_fitness\n\n            current_dim += self.dim_increment\n\n        return best_solution", "name": "PhotonicOptimizer", "description": "Enhance global exploration with self-adaptive parameters and improve local search robustness using a perturbation analysis.", "configspace": "", "generation": 3, "fitness": 0.8094943415542372, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.009. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "22171ad4-51a8-44a6-890e-1b5e2a1cbf71", "metadata": {"aucs": [0.8126721137171306, 0.8188239593919615, 0.7969869515536195], "final_y": [0.13868786520877585, 0.14051009566658856, 0.14553394596061975]}, "mutation_prompt": null}
{"id": "91dd06fe-c754-43b4-a2ce-c6b7ecc14199", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Incorporate adaptive mutation factor to the mutation function for dynamic exploration.", "configspace": "", "generation": 3, "fitness": 0.8494358070442006, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.030. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "987a72e9-ae5a-451d-81e7-546d58ec0d31", "metadata": {"aucs": [0.859587013062876, 0.8081252793980878, 0.8805951286716381], "final_y": [0.13231383423396315, 0.14879109103059263, 0.11562708227383123]}, "mutation_prompt": null}
{"id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by adding stochastic variation to the mutation factor (F) for improved performance in noisy environments.", "configspace": "", "generation": 4, "fitness": 0.87147445234096, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.013. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "987a72e9-ae5a-451d-81e7-546d58ec0d31", "metadata": {"aucs": [0.8621642141076042, 0.8898760434739269, 0.8623830994413487], "final_y": [0.13230291199926159, 0.11430257077042005, 0.11499709662251312]}, "mutation_prompt": null}
{"id": "7a1de8b3-c89b-4b99-8d35-acc9ca80208e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.CR = 0.9\n        self.F = 0.8\n        self.evaluations = 0\n        self.layer_increase_steps = [10, 20, 32]  # Change: Adaptive layer increase\n    \n    def differential_evolution(self, func, bounds):\n        population = np.random.rand(self.pop_size, self.dim)\n        population = bounds.lb + (bounds.ub - bounds.lb) * population\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += self.pop_size\n        \n        while self.evaluations < self.budget:\n            self.F = 0.5 + 0.3 * (1 - 0.7 * self.evaluations / self.budget)  # Change: Adjust F more aggressively\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = population[idxs]\n                modularity_weights = self.detect_layer_modularity(b, c)  # Change: Modularity detection\n                mutant = np.clip(a + self.F * (b - c) * modularity_weights, bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n            \n            if self.evaluations < self.budget:\n                self.increase_layers(func)  # Change: Dynamic layer increase\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def detect_layer_modularity(self, b, c):\n        # Simplified modularity detection\n        modularity = np.where(np.abs(b - c) < 0.1, 0.5, 1.0)\n        return modularity\n\n    def increase_layers(self, func):\n        # Change: Simplified layer increase logic\n        for step in self.layer_increase_steps:\n            if self.evaluations < self.budget:\n                self.dim = step\n                break\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=[(l, u) for l, u in zip(bounds.lb, bounds.ub)])\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        if self.evaluations < self.budget:\n            refined_solution, refined_fitness = self.local_refinement(func, best_solution, bounds)\n            if refined_fitness < best_fitness:\n                best_solution, best_fitness = refined_solution, refined_fitness\n        return best_solution", "name": "HybridOptimizer", "description": "Integrate layer modularity detection and adaptive layer increase with dynamic F adjustment to enhance exploration and exploitation balance in noisy high-dimensional optimization tasks.", "configspace": "", "generation": 4, "fitness": 0.8235434797637565, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.006. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "1b91695c-3335-4867-af99-daf3d04c151c", "metadata": {"aucs": [0.8155945832673379, 0.8313035063618822, 0.8237323496620491], "final_y": [0.1380620222669786, 0.1305964352677954, 0.1290216275429834]}, "mutation_prompt": null}
{"id": "677d6a4c-b992-4ca2-ab2f-d5319c046729", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n                    self.f *= 1.05  # Incremental mutation rate adjustment\n\n                if self.evals >= self.budget:\n                    break\n\n            diversity = np.std(population)  # Diversity-driven selection\n            if diversity < 0.01:  # Re-initialize population if diversity is low\n                population = self._initialize_population(lb, ub)\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Integrate diversity-driven selection and incremental mutation rate adjustment to enhance exploration and exploitation balance in high-dimensional noisy optimization problems.", "configspace": "", "generation": 4, "fitness": 0.860142114830602, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.006. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "987a72e9-ae5a-451d-81e7-546d58ec0d31", "metadata": {"aucs": [0.8542010510066691, 0.8690369081119077, 0.8571883853732293], "final_y": [0.1268270653314142, 0.11579278556962713, 0.12330603494782066]}, "mutation_prompt": null}
{"id": "0ebca1a9-ce83-4a81-b3ba-2d8c61e09e5a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by adjusting population size dynamically based on evaluation progress.", "configspace": "", "generation": 4, "fitness": 0.855912694473271, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.029. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "91dd06fe-c754-43b4-a2ce-c6b7ecc14199", "metadata": {"aucs": [0.8160655539757351, 0.8836766772608554, 0.8679958521832226], "final_y": [0.1427436732031837, 0.11440523332392827, 0.11716880808480534]}, "mutation_prompt": null}
{"id": "a526cc74-0399-40c1-8791-631760b3fc1e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.evals = 0  # Evaluation count\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Initial differential weight\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _adapt_parameters(self, population, func):\n        fitness = np.array([func(ind) for ind in population])\n        mean_fitness = np.mean(fitness)\n        diversity = np.std(fitness)\n        self.cr = 0.8 + 0.2 * (diversity / mean_fitness)\n        self.f = 0.3 + 0.7 * (1 - diversity / mean_fitness)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            self._adapt_parameters(population, func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance HybridMetaheuristic by incorporating dynamic adaptation of crossover and mutation rates based on population diversity for improved exploration and exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.8511693556678211, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.851 with standard deviation 0.033. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "987a72e9-ae5a-451d-81e7-546d58ec0d31", "metadata": {"aucs": [0.874314666387711, 0.8741839566032918, 0.8050094440124608], "final_y": [0.1125315874467907, 0.12240690825931011, 0.14713611282060446]}, "mutation_prompt": null}
{"id": "184eeb15-3da7-48bc-b7b3-779d632be2ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.cr = 0.9\n        self.f = 0.5\n        self.evals = 0\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))\n        modular_scaling = 1 + 0.1 * (self.dim // 10)  # New scaling factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c]) * modular_scaling\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        learning_rate = 0.01  # New learning rate\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                trial = trial + learning_rate * (best_solution - trial)\n                trial = np.clip(trial, lb, ub)  # Ensure bounds are respected\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration-exploitation balance by integrating adaptive learning rates and modular structure recognition.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\").", "error": "TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\")", "parent_id": "0ebca1a9-ce83-4a81-b3ba-2d8c61e09e5a", "metadata": {}, "mutation_prompt": null}
{"id": "72b75382-e075-413a-a387-9997ab4f9164", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.CR = 0.9\n        self.F = 0.8\n        self.evaluations = 0\n    \n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        population = np.random.rand(self.pop_size, self.dim)\n        population = bounds.lb + (bounds.ub - bounds.lb) * population\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += self.pop_size\n        \n        while self.evaluations < self.budget:\n            self.F = 0.5 + 0.3 * (1 - self.evaluations / self.budget)  # Change: Dynamic F adjustment\n            self.CR = 0.7 + 0.2 * (1 - self.evaluations / self.budget)  # Change: Dynamic CR adjustment\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = population[idxs]\n                mutant = np.clip(a + self.F * (b - c), bounds.lb, bounds.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def local_refinement(self, func, x0, bounds):\n        # Local optimization using L-BFGS-B\n        result = minimize(func, x0, method='L-BFGS-B', bounds=[(l, u) for l, u in zip(bounds.lb, bounds.ub)])\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        if self.evaluations < self.budget:\n            refined_solution, refined_fitness = self.local_refinement(func, best_solution, bounds)\n            if refined_fitness < best_fitness:\n                best_solution, best_fitness = refined_solution, refined_fitness\n        return best_solution", "name": "HybridOptimizer", "description": "Enhance exploration and exploitation by dynamically adjusting the crossover rate (CR) based on convergence speed.", "configspace": "", "generation": 5, "fitness": 0.8431162819217825, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.014. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "1b91695c-3335-4867-af99-daf3d04c151c", "metadata": {"aucs": [0.8392205654450562, 0.8620548767553178, 0.8280734035649733], "final_y": [0.12545305574340992, 0.12494699946964594, 0.13227963391595932]}, "mutation_prompt": null}
{"id": "ca6753cf-2617-4fea-85b4-7b5e955bedad", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        if np.random.rand() < 0.5:  # Probabilistic selection of mutation strategies\n            mutant = population[a] + self.f * (population[b] - population[c])\n        else:\n            mutant = population[a] + self.f * (population[c] - population[b])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n                    self.f *= 1.05  # Incremental mutation rate adjustment\n\n                if self.evals >= self.budget:\n                    break\n\n            diversity = np.std(population)  # Diversity-driven selection\n            if diversity < 0.01:  # Re-initialize population if diversity is low\n                population = self._initialize_population(lb, ub)\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a probabilistic selection of mutation strategies to enhance diversity and adaptivity.", "configspace": "", "generation": 5, "fitness": 0.8146300614028118, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.015. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "677d6a4c-b992-4ca2-ab2f-d5319c046729", "metadata": {"aucs": [0.8201485905620551, 0.793597501311604, 0.8301440923347759], "final_y": [0.14002868441858762, 0.15901116267818372, 0.13476626095628785]}, "mutation_prompt": null}
{"id": "cdad852a-5597-47c6-b10d-93f7d273ca5d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Dynamic crossover probability based on evaluations\n        cr_dynamic = self.cr * (1 - self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < cr_dynamic\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by introducing a dynamic crossover probability (CR) to balance exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.8541918856592522, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.017. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "metadata": {"aucs": [0.8786468016591737, 0.8424034690743086, 0.8415253862442744], "final_y": [0.1175637870016869, 0.13475532970694926, 0.1256407667429673]}, "mutation_prompt": null}
{"id": "44262d5a-31a5-4468-8a34-01c582320bf8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n                    self.f *= 1.05  # Incremental mutation rate adjustment\n\n                if self.evals >= self.budget:\n                    break\n\n            diversity = np.std(population)  # Diversity-driven selection\n            if diversity < 0.01:  # Re-initialize population if diversity is low\n                population = self._initialize_population(lb, ub)\n            self.cr = min(0.9, diversity * 10)  # Adjust crossover probability based on diversity\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve the exploitation phase by adjusting the crossover probability based on population diversity.", "configspace": "", "generation": 5, "fitness": 0.8160140078171748, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.816 with standard deviation 0.054. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "677d6a4c-b992-4ca2-ab2f-d5319c046729", "metadata": {"aucs": [0.7400036853084662, 0.8535546368861788, 0.8544837012568791], "final_y": [0.17767096583001174, 0.1353091656494696, 0.12954783505590728]}, "mutation_prompt": null}
{"id": "acba7027-e925-4118-adea-9a009c4261a2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adjust crossover probability dynamically based on best score\n        self.cr = 0.9 * (1 - best_score / (best_score + 1))\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic adjustment to crossover probability based on current best score to enhance exploitation.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'best_score' is not defined\").", "error": "NameError(\"name 'best_score' is not defined\")", "parent_id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "metadata": {}, "mutation_prompt": null}
{"id": "fc995f73-2e24-4f1f-bd89-02ea41a9f827", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adaptive CR based on population diversity\n        diversity = np.mean(np.std(population, axis=0))\n        adaptive_cr = max(0.1, min(1.0, self.cr * (1 + 0.1 * diversity)))\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve exploration and exploitation by introducing adaptive crossover probability (CR) based on population diversity.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "metadata": {}, "mutation_prompt": null}
{"id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.99, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Optimize layer role preservation by tweaking the local refinement step's starting point to better acknowledge modular structure in solutions.", "configspace": "", "generation": 6, "fitness": 0.9004245782625578, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.900 with standard deviation 0.015. And the mean value of best solutions found was 0.114 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "0ebca1a9-ce83-4a81-b3ba-2d8c61e09e5a", "metadata": {"aucs": [0.9010884023304909, 0.9185055262960179, 0.8816798061611646], "final_y": [0.1123535478478741, 0.10961088632201743, 0.11936737091160643]}, "mutation_prompt": null}
{"id": "b73aba40-edd1-4fdd-90ae-1aafe7062059", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Dynamic crossover probability based on evaluations\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover probability (CR) based on convergence speed to balance exploration and exploitation better.", "configspace": "", "generation": 6, "fitness": 0.8608220765228026, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.014. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "metadata": {"aucs": [0.8610981973619523, 0.8783123234438452, 0.8430557087626106], "final_y": [0.13230291199926159, 0.12363227099790697, 0.13479717088846366]}, "mutation_prompt": null}
{"id": "4f24b478-b715-4066-855f-2a88713d668f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n                    self.f *= (1.05 + 0.05 * np.std(population))  # Incremental mutation rate adjustment\n\n                if self.evals >= self.budget:\n                    break\n\n            diversity = np.std(population)  # Diversity-driven selection\n            if diversity < 0.01:  # Re-initialize population if diversity is low\n                population = self._initialize_population(lb, ub)\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Boosts algorithm robustness by adjusting mutation rate incrementally based on population diversity.", "configspace": "", "generation": 6, "fitness": 0.8834310299192957, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.883 with standard deviation 0.019. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "677d6a4c-b992-4ca2-ab2f-d5319c046729", "metadata": {"aucs": [0.8587345517852281, 0.903439502135345, 0.888119035837314], "final_y": [0.11874623896654146, 0.11280560128133565, 0.11715331069114965]}, "mutation_prompt": null}
{"id": "c467b973-7e3c-4cad-bfc2-44c537160683", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n                    self.f *= (1.01 + 0.09 * np.std(population))  # Adjust mutation rate\n\n                if self.evals >= self.budget:\n                    break\n\n            diversity = np.std(population)  # Diversity-driven selection\n            if diversity < 0.01:  # Re-initialize population if diversity is low\n                population = self._initialize_population(lb, ub)\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance performance by fine-tuning the adaptive mutation rate mechanism to respond more dynamically to population diversity.", "configspace": "", "generation": 7, "fitness": 0.8430089416150919, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.061. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "4f24b478-b715-4066-855f-2a88713d668f", "metadata": {"aucs": [0.7577242554096693, 0.8787232412888896, 0.8925793281467168], "final_y": [0.12692950484237053, 0.11765310435649301, 0.11523426950575022]}, "mutation_prompt": null}
{"id": "d538fb58-8381-4808-930d-0e16bb2711ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Dynamic crossover probability based on evaluations\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            population_scores = [func(ind) for ind in population]\n            best_idx = np.argmin(population_scores)\n            worst_idx = np.argmax(population_scores)\n            d = np.linalg.norm(population[best_idx] - population[worst_idx])\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim and d > 0.1:  # Perform more frequent refinement\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance local refinement by conducting it more frequently based on the distance between the best and worst solutions.", "configspace": "", "generation": 7, "fitness": 0.8639908958142194, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.025. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "b73aba40-edd1-4fdd-90ae-1aafe7062059", "metadata": {"aucs": [0.8489010127069337, 0.8991772270918841, 0.8438944476438404], "final_y": [0.12714622914597795, 0.11707723262386827, 0.13378333374135687]}, "mutation_prompt": null}
{"id": "31219726-2b61-491d-a103-20c08fc8ec03", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        # Incorporate Gaussian perturbation to enhance diversity\n        mutant += np.random.normal(0, 0.1, size=self.dim) \n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Dynamic crossover probability based on evaluations\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the mutation diversity by incorporating a Gaussian perturbation to the mutant vector.", "configspace": "", "generation": 7, "fitness": 0.8601301157125167, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.014. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "b73aba40-edd1-4fdd-90ae-1aafe7062059", "metadata": {"aucs": [0.840714608865526, 0.8733918601086235, 0.8662838781634009], "final_y": [0.1266764053817595, 0.1233020955493902, 0.11874051483766968]}, "mutation_prompt": null}
{"id": "01055dd7-b6e1-42ff-83e4-a4f552bbc27e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhancing layer preservation by adapting local refinement starting point to boost solution modularity.", "configspace": "", "generation": 7, "fitness": 0.8655177573652127, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.023. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.8815292722325243, 0.8332689864243323, 0.8817550134387816], "final_y": [0.11406019785333421, 0.14099211055737637, 0.1196331673388944]}, "mutation_prompt": null}
{"id": "2a3050e2-53b1-45ef-86a9-cc41e0ad8787", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub): \n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Improved adaptive mutation scaling\n        adaptive_f = self.f + (0.2 * np.random.rand() - 0.1)\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(population, scores)\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance solution diversity by using a tournament selection mechanism and adaptive mutation scaling for improved exploration.", "configspace": "", "generation": 7, "fitness": 0.8874832449969204, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.887 with standard deviation 0.019. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b73aba40-edd1-4fdd-90ae-1aafe7062059", "metadata": {"aucs": [0.8603209907889252, 0.8983715677868771, 0.9037571764149588], "final_y": [0.12896247807868166, 0.11149418894288365, 0.11440568420096908]}, "mutation_prompt": null}
{"id": "e45d43a9-ad39-4889-8deb-4c23a76cdcae", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Dynamic crossover probability adjustment based on population diversity\n        diversity = np.std(np.asarray(self.population))\n        self.cr = max(0.1, 1 - diversity)\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by introducing dynamic crossover probability adjustment based on population diversity.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\").", "error": "AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\")", "parent_id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "metadata": {}, "mutation_prompt": null}
{"id": "5c44ec5e-60da-45c9-a918-f7ffbdaf8f4a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n                    self.f *= (1.05 + 0.05 * np.std(population))  # Incremental mutation rate adjustment\n\n                if self.evals >= self.budget:\n                    break\n\n            diversity = np.std(population)  # Diversity-driven selection\n            if diversity < 0.01:  # Re-initialize population if diversity is low\n                population = self._initialize_population(lb, ub)\n\n            self.population_size = int(max(5 * self.dim, self.population_size * (1 + 0.1 * np.std(population))))  # Dynamically adjust population size\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve diversity by dynamically adjusting population size based on convergence rate and diversity.  ", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 145 is out of bounds for axis 0 with size 100').", "error": "IndexError('index 145 is out of bounds for axis 0 with size 100')", "parent_id": "4f24b478-b715-4066-855f-2a88713d668f", "metadata": {}, "mutation_prompt": null}
{"id": "7c0cffcd-61be-4b30-a223-59b111b20f05", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    # Dynamically adjust crossover probability\n    def _update_crossover_rate(self, population):\n        diversity = np.std(population, axis=0).mean()\n        self.cr = 0.5 + 0.4 * (1 - diversity / np.max([diversity, 1e-5]))\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            self._update_crossover_rate(population)  # Update crossover probability\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve exploration by dynamically adjusting crossover probability based on population diversity.", "configspace": "", "generation": 8, "fitness": 0.8304828027650851, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.030. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "metadata": {"aucs": [0.8308773368608033, 0.7934153127927578, 0.8671557586416945], "final_y": [0.13436275413858567, 0.1601017669387199, 0.12386786449268328]}, "mutation_prompt": null}
{"id": "a0d44580-3f15-464b-897c-7ccfe82c1081", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.cr = 0.9\n        self.f = 0.5\n        self.evals = 0\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        diversity = np.std(population, axis=0).mean()  # Measure diversity\n        adaptive_f = self.f * (1 - (self.evals / self.budget)) * diversity  # Dynamic weighting\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance algorithm efficiency by incorporating dynamic weighting of exploration and exploitation based on solution diversity and convergence speed.", "configspace": "", "generation": 8, "fitness": 0.7668012291602179, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.767 with standard deviation 0.020. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "01055dd7-b6e1-42ff-83e4-a4f552bbc27e", "metadata": {"aucs": [0.7446046465223465, 0.7934153127927578, 0.7623837281655493], "final_y": [0.1619617580379672, 0.1601017669387199, 0.1550651084314263]}, "mutation_prompt": null}
{"id": "89017de2-0af8-4f05-a66b-dff1ff396204", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr * (1 - (self.evals / self.budget))  # Dynamic crossover probability\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.99, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by using a dynamic crossover probability that decreases as evaluations progress to balance exploration and exploitation.", "configspace": "", "generation": 8, "fitness": 0.8564011727583027, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.022. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.880612300449196, 0.8608818754352304, 0.8277093423904816], "final_y": [0.11707013798338561, 0.12866184645863143, 0.13484028333830977]}, "mutation_prompt": null}
{"id": "39c16239-528c-45f9-b3d5-9683e48fa1c6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adaptive crossover probability\n        self.cr = max(0.5, 1 - self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Integrate adaptive crossover probability to enhance exploration and exploitation balance in noisy environments.", "configspace": "", "generation": 9, "fitness": 0.856084918898549, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.032. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "metadata": {"aucs": [0.8263489527082508, 0.9004872661253703, 0.8414185378620258], "final_y": [0.13768201466286922, 0.11707215330687137, 0.13396429151381228]}, "mutation_prompt": null}
{"id": "cde5d679-c532-4336-8a60-07db38610c72", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  \n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  \n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        self.cr = 0.9 * (1 - self.evals / self.budget)  # Dynamic crossover probability\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B') \n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  \n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  \n                trial = self._crossover(target, mutant)\n                score = np.mean([func(trial + np.random.normal(0, 1e-3, self.dim)) for _ in range(3)])  # Noise-resilient evaluation\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover probability and incorporate a noise-resilient objective function evaluation to improve stability and exploration.  ", "configspace": "", "generation": 9, "fitness": 0.848486084457665, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.034. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "01055dd7-b6e1-42ff-83e4-a4f552bbc27e", "metadata": {"aucs": [0.8408614537155504, 0.8937090673596206, 0.8108877322978238], "final_y": [0.11466254997017433, 0.11440478850166191, 0.14901555914048603]}, "mutation_prompt": null}
{"id": "d29612e0-5160-4f5d-b35f-479a87c71181", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        adaptive_cr = self.cr * (1 - (self.evals / self.budget))  # Adaptive crossover rate\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.99, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by incorporating an adaptive crossover rate that decreases as evaluations progress.", "configspace": "", "generation": 9, "fitness": 0.8608835250265633, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.032. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.859714100923415, 0.9008900777432279, 0.8220463964130469], "final_y": [0.12542721014821145, 0.11531543503683184, 0.14324682531244126]}, "mutation_prompt": null}
{"id": "8bf542bb-7466-4efc-a7bc-281e99b8a992", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adaptive crossover probability based on evaluation progress\n        adaptive_cr = self.cr * (1 - 0.5 * (self.evals / self.budget))\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by implementing adaptive crossover probability based on evaluation progress for better balance between exploration and exploitation.", "configspace": "", "generation": 9, "fitness": 0.884471572297099, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.884 with standard deviation 0.019. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "01055dd7-b6e1-42ff-83e4-a4f552bbc27e", "metadata": {"aucs": [0.857594500303791, 0.8982164386371677, 0.8976037779503385], "final_y": [0.1251366442596208, 0.11707292865274799, 0.1153766410646837]}, "mutation_prompt": null}
{"id": "1b3e0514-182e-406a-9777-15d1c2eca6ad", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Changed line:\n        cross_points = np.random.rand(self.dim) < self.cr * (1 - (self.evals / self.budget))  # Adaptive crossover probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability to enhance exploration and convergence balance.", "configspace": "", "generation": 9, "fitness": 0.8720710319556266, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.007. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "01055dd7-b6e1-42ff-83e4-a4f552bbc27e", "metadata": {"aucs": [0.8639861407954663, 0.8702796696955483, 0.8819472853758656], "final_y": [0.12513733685935102, 0.12667577837178723, 0.11891833923036699]}, "mutation_prompt": null}
{"id": "3cc4f5bc-4ba7-4bb5-8eac-540fcd72c77c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.cr = 0.9\n        self.f = 0.5\n        self.evals = 0\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        pop_variance = np.var(population, axis=0).mean()  # Calculate population variance\n        adaptive_f = self.f * (1 - (self.evals / self.budget)) * (1 + pop_variance)  # Dynamic mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        pop_variance = np.var(population, axis=0).mean()  # Calculate population variance\n        adaptive_cr = self.cr * (1 - 0.5 * (self.evals / self.budget) * (1 + pop_variance))  # Dynamic crossover\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduces dynamic weighting of crossover and mutation based on evaluation progress and variance in population.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "8bf542bb-7466-4efc-a7bc-281e99b8a992", "metadata": {}, "mutation_prompt": null}
{"id": "21f3937f-1805-4e98-ae7a-69e8217979d6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.elite_fraction = 0.1  # Elite preservation fraction\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        diversity = np.std(population, axis=0).mean()  # Measure diversity\n        adaptive_f = self.f * (1 - (self.evals / self.budget)) * (0.5 + 0.5 * diversity)  # Enhanced adaptive mutation\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        diversity = np.std(self.population, axis=0).mean()  # Measure diversity\n        adaptive_cr = self.cr * (1 - 0.5 * (self.evals / self.budget)) * (0.5 + 0.5 * diversity)  # Enhanced adaptive crossover\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            elite_count = int(self.elite_fraction * self.population_size)\n            population_scores = np.array([func(ind) for ind in population])\n            elite_indices = np.argsort(population_scores)[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(self.population_size):\n                if i in elite_indices:  # Preserve elite\n                    continue\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n            self.population = population  # Keep track of current population for diversity calculation\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce elite preservation mechanism and adaptive mutation/crossover probabilities based on evaluation progress and population diversity to improve exploration and exploitation.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\").", "error": "AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\")", "parent_id": "8bf542bb-7466-4efc-a7bc-281e99b8a992", "metadata": {}, "mutation_prompt": null}
{"id": "9efcc28f-5dce-4adc-9439-2bce1ebc77ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adaptive crossover probability based on evaluation progress\n        adaptive_cr = self.cr * (1 - 0.7 * (self.evals / self.budget))  # Adjusted starting value to enhance performance\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by adjusting adaptive crossover start value for better exploration-exploitation tradeoff.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\").", "error": "AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\")", "parent_id": "8bf542bb-7466-4efc-a7bc-281e99b8a992", "metadata": {}, "mutation_prompt": null}
{"id": "91e39ca3-67d2-458d-bdfc-f1bb12af8597", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        self.cr = 0.9 * (1 - (self.evals / self.budget))  # Dynamic crossover probability\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.99, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Optimize crossover mechanism to dynamically adjust based on evaluation progress to balance exploration and exploitation.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\").", "error": "AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\")", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {}, "mutation_prompt": null}
{"id": "4b82d98b-4db1-4a4f-913d-80631c934bba", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget) * (1 + (1 - np.min([population[a], population[b], population[c]]) / self.budget)))  # Adjusted mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adaptive crossover probability based on evaluation progress\n        adaptive_cr = self.cr * (1 - 0.5 * (self.evals / self.budget))\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by adjusting adaptive mutation factor to consider both evaluation progress and current best score.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\").", "error": "AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\")", "parent_id": "8bf542bb-7466-4efc-a7bc-281e99b8a992", "metadata": {}, "mutation_prompt": null}
