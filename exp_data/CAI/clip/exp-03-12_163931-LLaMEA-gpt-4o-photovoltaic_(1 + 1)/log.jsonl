{"id": "2cdd3d2e-e5cc-45d6-b36e-59df0020a5ef", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.7 + 0.3 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Adaptive Swarm Gradient Descent (ASGD): Combines swarm intelligence and gradient estimation to explore and exploit search space efficiently.", "configspace": "", "generation": 0, "fitness": 0.8264407036439118, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.020. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8041324604654931, 0.8227266579918514, 0.8524629924743906], "final_y": [0.1412466306774247, 0.13112094863304602, 0.12789490696654837]}, "mutation_prompt": null}
{"id": "c76d01d1-e078-45ae-9194-6ffe7c745b72", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.7 + 0.3 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor + 0.5 * np.random.random()  # Modified line\n            social_coeff = 1.5 + 0.5 * np.random.random()  # Modified line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent (EASGD): Introduces dynamic adaptation of coefficients for improved convergence in diverse landscapes.", "configspace": "", "generation": 1, "fitness": 0.8423052646671767, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.842 with standard deviation 0.013. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "2cdd3d2e-e5cc-45d6-b36e-59df0020a5ef", "metadata": {"aucs": [0.8355694658723707, 0.8310354719009017, 0.860310856228258], "final_y": [0.12914429170556774, 0.1365085777235876, 0.13002247955441437]}, "mutation_prompt": null}
{"id": "81a99a3c-02c0-470b-bda5-85ca787661dc", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor  # Modified line\n            cognitive_coeff = 1.5 * adaptive_factor + 0.5 * np.random.random()  # Modified line\n            social_coeff = 1.5 + 0.5 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Improved Enhanced Adaptive Swarm Gradient Descent by fine-tuning inertia weight dynamics for better exploration-exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.8849467848663819, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.885 with standard deviation 0.021. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "c76d01d1-e078-45ae-9194-6ffe7c745b72", "metadata": {"aucs": [0.8554269242861123, 0.9042798788302668, 0.8951335514827664], "final_y": [0.12084066476221167, 0.11543216361074171, 0.11706459534714397]}, "mutation_prompt": null}
{"id": "462709c4-21d5-4b1f-8247-9dcf698925cd", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor + 0.5 * np.random.random()\n            social_coeff = 1.5 + 0.5 * np.random.random()\n\n            # Adjust population size dynamically (line modified)\n            self.population_size = int((10 + 2 * np.sqrt(self.dim)) * (1 + 0.2 * adaptive_factor))  # Modified line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced convergence speed by dynamically adjusting swarm size based on exploration-exploitation balance.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 16 is out of bounds for axis 0 with size 16').", "error": "IndexError('index 16 is out of bounds for axis 0 with size 16')", "parent_id": "81a99a3c-02c0-470b-bda5-85ca787661dc", "metadata": {}, "mutation_prompt": null}
{"id": "88c3fdf5-98d0-41b5-ba63-56fe6300e9a8", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))  # New line\n            cognitive_coeff = 1.5 * adaptive_factor * improvement_factor + 0.5 * np.random.random()  # Modified line\n            social_coeff = 1.5 * improvement_factor + 0.5 * np.random.random()  # Modified line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by dynamically adjusting cognitive and social coefficients based on global-best value improvement.", "configspace": "", "generation": 4, "fitness": 0.9173448194249137, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.917 with standard deviation 0.008. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "81a99a3c-02c0-470b-bda5-85ca787661dc", "metadata": {"aucs": [0.9076053788838035, 0.9166828270656117, 0.927746252325326], "final_y": [0.11386577461312097, 0.11196319901310936, 0.11166347508948093]}, "mutation_prompt": null}
{"id": "76a48ba4-3737-4dfa-95b1-96d3547dafa8", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            velocity_scale = 0.5 + 0.5 * np.sin(np.pi * evaluations / self.budget)  # New line for dynamic velocity scaling\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.5 * adaptive_factor * improvement_factor + 0.5 * np.random.random()\n            social_coeff = 1.5 * improvement_factor + 0.5 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = velocity_scale * (inertia_weight * self.velocity[i] +  # Line modified for velocity scaling\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by introducing dynamic velocity scaling based on evaluation progress to improve exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.8819308101499623, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.007. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "88c3fdf5-98d0-41b5-ba63-56fe6300e9a8", "metadata": {"aucs": [0.8823598110060481, 0.8728138862162281, 0.8906187332276106], "final_y": [0.1170971368431103, 0.12372250422029618, 0.11269288181916759]}, "mutation_prompt": null}
{"id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))  # New line\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()  # Modified line\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()  # Modified line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Improved Adaptive Swarm Gradient Descent by refining coefficients to balance exploration and exploitation more effectively.", "configspace": "", "generation": 6, "fitness": 0.9209725262665235, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.921 with standard deviation 0.008. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "88c3fdf5-98d0-41b5-ba63-56fe6300e9a8", "metadata": {"aucs": [0.9094558986244882, 0.923821390232025, 0.929640289943057], "final_y": [0.1137905248258071, 0.11184460106044503, 0.1116002140732063]}, "mutation_prompt": null}
{"id": "42f679b6-c638-4903-ac89-d01ee14671d8", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n        momentum = 0.5  # New line\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))  \n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()  \n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()  \n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (momentum * self.velocity[i] +  # Modified line\n                                    inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by incorporating momentum for better convergence and stability.", "configspace": "", "generation": 7, "fitness": 0.8265722620039863, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.003. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.824739647636802, 0.8239052106874561, 0.8310719276877007], "final_y": [0.14740340922533957, 0.14834307766395727, 0.14419927929844745]}, "mutation_prompt": null}
{"id": "69372aa0-d383-41d5-9070-cd8791ba2012", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.8 - 0.4 * adaptive_factor  # Modified line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                dynamic_velocity_bound = (ub - lb) * adaptive_factor  # New line\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                self.velocity[i] = np.clip(self.velocity[i], -dynamic_velocity_bound, dynamic_velocity_bound)  # Modified line\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced the balance of exploration and exploitation by adjusting inertia weight and introducing dynamic velocity bounds for improved convergence.", "configspace": "", "generation": 8, "fitness": 0.9111427174601152, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.911 with standard deviation 0.015. And the mean value of best solutions found was 0.113 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.8936265477967035, 0.9094459245329115, 0.9303556800507309], "final_y": [0.11547010632574062, 0.11339957734150086, 0.11030662700096916]}, "mutation_prompt": null}
{"id": "b73c46af-d3ce-4381-af82-400bd169d3b0", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            # Modified inertia weight to stabilize convergence over iterations\n            inertia_weight = 0.9 - 0.5 * (adaptive_factor ** 2)  \n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            # Increased randomness in social coefficient for better exploration\n            social_coeff = 1.7 * improvement_factor + 0.5 * np.random.random()  \n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by adjusting the inertia weight dynamically to stabilize convergence and improve performance.", "configspace": "", "generation": 9, "fitness": 0.9197344335132507, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.920 with standard deviation 0.006. And the mean value of best solutions found was 0.114 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.9175425056989864, 0.9131902530114664, 0.9284705418292992], "final_y": [0.11419545234615147, 0.11712536871080337, 0.11102565804583486]}, "mutation_prompt": null}
{"id": "1fb46fd2-c8ef-4c75-a05c-a49247026e71", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            mutation_factor = 0.1 * adaptive_factor  # New mutation factor line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    mutation_factor * np.random.randn(self.dim))  # Modified line\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by introducing a mutation factor to diversify search and prevent premature convergence.", "configspace": "", "generation": 10, "fitness": 0.9117811675362161, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.912 with standard deviation 0.010. And the mean value of best solutions found was 0.113 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.9105711120301838, 0.9005457914453517, 0.9242265991331127], "final_y": [0.11075499301112779, 0.11694270927318007, 0.11261457162870492]}, "mutation_prompt": null}
{"id": "2b29caea-724d-4d08-a787-126f345f7279", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best with a dynamic convergence factor\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value * (1 - 0.5 * adaptive_factor)  # Changed line\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced global best update by incorporating a dynamic convergence factor to accelerate convergence speed.", "configspace": "", "generation": 11, "fitness": 0.8576548494549735, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.858 with standard deviation 0.016. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.8455018347159833, 0.8467589935375119, 0.8807037201114251], "final_y": [0.1396036413835915, 0.14019334588828858, 0.12744451084149577]}, "mutation_prompt": null}
{"id": "3c52865c-2104-4117-89f5-b2e219ace9ca", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n            convergence_factor = 0.5 + 0.5 * adaptive_factor  # New line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i])) * convergence_factor  # Modified line\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced velocity update dynamics by incorporating an adaptive convergence factor to better balance exploration and exploitation.", "configspace": "", "generation": 12, "fitness": 0.9051862504854175, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.905 with standard deviation 0.014. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.908092260333132, 0.8868128865495495, 0.920653604573571], "final_y": [0.1168274126231914, 0.12568669639173402, 0.11454391403664799]}, "mutation_prompt": null}
{"id": "02353b01-aced-4008-9ebf-c85b1d058a25", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * (1 - global_best_value)  # Changed line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by dynamically adjusting inertia weight based on convergence progress.", "configspace": "", "generation": 13, "fitness": 0.8953509743072489, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.895 with standard deviation 0.015. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.891958759618633, 0.8786976791045849, 0.9153964841985288], "final_y": [0.11960348237746132, 0.12354412257994074, 0.11246388409497621]}, "mutation_prompt": null}
{"id": "5a4045d6-91a7-4514-87e8-2516d478be8a", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor * np.sin(np.pi * adaptive_factor) + 0.3 * np.random.random()  # Modified line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by introducing non-linear time-varying coefficients for improved balance between exploration and exploitation.", "configspace": "", "generation": 14, "fitness": 0.879905860291032, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.011. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.8778132433452988, 0.8680084996396784, 0.8938958378881188], "final_y": [0.11372871452834166, 0.12245441625465348, 0.1120925844369649]}, "mutation_prompt": null}
{"id": "46b90eea-fa70-4193-b8c8-1141a97152db", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            learning_rate = 0.5 + 0.5 * adaptive_factor  # New line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += learning_rate * self.velocity[i]  # Modified line\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by integrating a dynamic adaptive learning rate to balance exploration and exploitation more effectively.", "configspace": "", "generation": 15, "fitness": 0.9133681313176366, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.913 with standard deviation 0.011. And the mean value of best solutions found was 0.114 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.9136044969069245, 0.9001762726173494, 0.9263236244286357], "final_y": [0.11474479271718774, 0.11551449346564202, 0.11172744078133456]}, "mutation_prompt": null}
{"id": "13b9e4e8-51c1-4ca1-acfa-8f101a2cc92f", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * adaptive_factor  # Modified line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))  # New line\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()  # Modified line\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()  # Modified line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by modifying adaptive factors for better balance between exploration and exploitation.", "configspace": "", "generation": 16, "fitness": 0.8957647673849558, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.896 with standard deviation 0.014. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.8840646471173755, 0.8883868240614277, 0.9148428309760642], "final_y": [0.12218366390357482, 0.12233672952763985, 0.11280618626818883]}, "mutation_prompt": null}
{"id": "505e7e11-d0f4-4f18-8269-347c58ad8779", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = np.sin(np.pi * evaluations / (2 * self.budget))**2  # Modified line\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 2.0 * improvement_factor + 0.1 * np.random.random()  # Modified line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced adaptive factor calculation and dynamic tuning of cognitive and social coefficients to improve convergence.", "configspace": "", "generation": 17, "fitness": 0.8819396067130786, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.016. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.865621081409451, 0.8766594222273676, 0.9035383165024171], "final_y": [0.11980806075455586, 0.12128147578086856, 0.11329251850014155]}, "mutation_prompt": null}
{"id": "f4b53ab9-3bce-406e-97ed-8e66f8fe265b", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n\n            # Using a chaotic map instead of random\n            chaotic_seq = np.mod(4.0 * np.random.random() * (1 - np.random.random()), 1)  # New line\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * chaotic_seq  # Modified line\n            social_coeff = 1.7 * improvement_factor + 0.3 * chaotic_seq  # Modified line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by introducing Chaotic Maps for improved exploration and diversity.", "configspace": "", "generation": 18, "fitness": 0.8975767547922114, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.898 with standard deviation 0.013. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.9067560612931234, 0.9071969014549899, 0.8787773016285211], "final_y": [0.11350153618508207, 0.11546938445546362, 0.1258491136752543]}, "mutation_prompt": null}
{"id": "8214a8d1-f7cc-4e36-9d25-b12fe7a5627e", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.4 + 0.5 * adaptive_factor  # Modified line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))  # New line\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()  # Modified line\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()  # Modified line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by introducing a dynamic inertia weight adjustment to improve convergence stability.", "configspace": "", "generation": 19, "fitness": 0.8685929125804961, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.869 with standard deviation 0.009. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.8691344710067356, 0.8567189284250707, 0.879925338309682], "final_y": [0.12738743817461307, 0.12708617479337025, 0.12213483603353059]}, "mutation_prompt": null}
{"id": "54ec42f4-ea5d-43ea-81c8-9b8d6e9760d0", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                random_disturbance = np.random.normal(0, 0.1, self.dim)  # New line\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i])) + random_disturbance  # Modified line\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by incorporating random disturbance to the velocity for improved exploration.", "configspace": "", "generation": 20, "fitness": 0.912320183734574, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.912 with standard deviation 0.010. And the mean value of best solutions found was 0.113 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.9101067955974853, 0.9017767774777217, 0.9250769781285151], "final_y": [0.11134539840256485, 0.11650580113804099, 0.11226692471742672]}, "mutation_prompt": null}
{"id": "15bdd66b-a8a6-4fd3-8554-471cd10b159e", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            variance_factor = 1 + np.var(personal_best_value)  # New line for dynamic adjustment\n            cognitive_coeff = (1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()) * variance_factor  # Modified line\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Further improved Adaptive Swarm Gradient Descent by introducing a dynamic adjustment to the cognitive coefficient based on the variance of personal best values.", "configspace": "", "generation": 21, "fitness": 0.9175023997333503, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.918 with standard deviation 0.009. And the mean value of best solutions found was 0.113 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.9094483307229775, 0.9134087249067064, 0.9296501435703673], "final_y": [0.11378952155185995, 0.11370843843220824, 0.11160783077785041]}, "mutation_prompt": null}
{"id": "85112f74-e7b2-4dc0-8ab9-ed75716f2005", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.8 - 0.4 * adaptive_factor  # Modified line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.8 * adaptive_factor * improvement_factor + 0.3 * np.random.random()  # Modified line\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced the inertia weight and cognitive coefficient for adaptive convergence in AdaptiveSwarmGradientDescent.", "configspace": "", "generation": 22, "fitness": 0.9164596131918863, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.916 with standard deviation 0.009. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.90534588946338, 0.9162830229324839, 0.9277499271797951], "final_y": [0.11287700326607597, 0.11221130817123026, 0.11181770208558495]}, "mutation_prompt": null}
{"id": "639c651f-7d81-44fb-91e6-a6c026def614", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.4 * adaptive_factor  # Modified line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    adaptive_factor * np.random.uniform(-1, 1, self.dim))  # Modified line\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by introducing dynamic inertia weight adjustment and augmenting global exploration.", "configspace": "", "generation": 23, "fitness": 0.8948854640994744, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.895 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.8860652124694491, 0.8922408446042578, 0.9063503352247165], "final_y": [0.11869202230129339, 0.1170634955121912, 0.11339715109533588]}, "mutation_prompt": null}
{"id": "b684d97a-8c8e-48a0-b926-c3b3aedd9b13", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor * adaptive_factor + 0.3 * np.random.random()  # Modified line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by introducing dynamic social coefficient decay to further balance exploration and exploitation.", "configspace": "", "generation": 24, "fitness": 0.9068346822271298, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.907 with standard deviation 0.012. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.901782778946422, 0.8950266543555712, 0.9236946133793962], "final_y": [0.11765447780105076, 0.12126487062520974, 0.11274146507652394]}, "mutation_prompt": null}
{"id": "8cc8a067-cf82-473c-b82c-964e444fb48e", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * adaptive_factor  # Modified line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.4 * np.random.random()  # Modified line\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by dynamically adjusting inertia and coefficients relative to search progress to improve convergence.", "configspace": "", "generation": 25, "fitness": 0.9041167789599749, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.015. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.8905816753628915, 0.896196948309095, 0.9255717132079383], "final_y": [0.11498055536726648, 0.11879176756870913, 0.1116072783835288]}, "mutation_prompt": null}
{"id": "43239e67-6c2f-47a1-b01e-7445d5ba4e3e", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    mutation_strength = 0.1 * (ub - lb) * np.random.normal(size=self.dim)  # New line\n                    global_best = swarm[i] + mutation_strength  # Modified line\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by using adaptive mutation to perturb the global best for improved exploration.", "configspace": "", "generation": 26, "fitness": 0.8421748682508667, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.842 with standard deviation 0.023. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.8503572076989485, 0.8108224807889772, 0.8653449162646747], "final_y": [0.13897981057449305, 0.1537019730394379, 0.12411182450651437]}, "mutation_prompt": null}
{"id": "52ef2d6e-b20f-4dad-ba2f-b45870bf190c", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))  # New line\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()  # Modified line\n\n            # Dynamic social coefficient to enhance convergence speed\n            social_coeff = 1.7 * improvement_factor * (1 - evaluations / self.budget) + 0.3 * np.random.random()  # Changed line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Improved Adaptive Swarm Gradient Descent by adding a dynamic social coefficient to enhance convergence speed.", "configspace": "", "generation": 27, "fitness": 0.9068346822271298, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.907 with standard deviation 0.012. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.901782778946422, 0.8950266543555712, 0.9236946133793962], "final_y": [0.11765447780105076, 0.12126487062520974, 0.11274146507652394]}, "mutation_prompt": null}
{"id": "06eacaa4-8e1b-424f-8f7a-51f06d62337d", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by introducing dynamic boundary adjustment for improved solution diversity.", "configspace": "", "generation": 28, "fitness": 0.9211736248103141, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.921 with standard deviation 0.010. And the mean value of best solutions found was 0.113 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "ec5892d0-4278-47ce-8122-4e83d6e99aad", "metadata": {"aucs": [0.9081387285567372, 0.923017807375192, 0.9323643384990131], "final_y": [0.11499367922793746, 0.1123085703516955, 0.11038577725691467]}, "mutation_prompt": null}
{"id": "f6a6af68-d2e0-4581-b3a1-4662510c167d", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (0.95 * self.velocity[i] +  # Velocity decay\n                                    inertia_weight * (cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i])))\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Improved Adaptive Swarm Gradient Descent with velocity decay for enhanced convergence stability.", "configspace": "", "generation": 29, "fitness": 0.8679094128875136, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.012. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "06eacaa4-8e1b-424f-8f7a-51f06d62337d", "metadata": {"aucs": [0.8535755647720535, 0.8666504133925744, 0.8835022604979125], "final_y": [0.1303599023521651, 0.12731669638877008, 0.12416769452835541]}, "mutation_prompt": null}
{"id": "ee1282c4-4093-4850-8de3-adb1616766df", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.7 + 0.2 * adaptive_factor  # Changed range for inertia weight\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Improved Adaptive Swarm Gradient Descent by incorporating a dynamic inertia weight range for enhanced exploration and exploitation.", "configspace": "", "generation": 30, "fitness": 0.865257339347632, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.865 with standard deviation 0.005. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "06eacaa4-8e1b-424f-8f7a-51f06d62337d", "metadata": {"aucs": [0.8596976401906556, 0.8651759824797183, 0.8708983953725221], "final_y": [0.1221920296533544, 0.12743467615295634, 0.12498016084929087]}, "mutation_prompt": null}
{"id": "199e65e3-24e8-4868-9ea3-6c5a4786645e", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * adaptive_factor**2  # Adjusted line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with dynamic inertia weight adjustment for faster convergence.", "configspace": "", "generation": 31, "fitness": 0.9037076049562147, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.007. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "06eacaa4-8e1b-424f-8f7a-51f06d62337d", "metadata": {"aucs": [0.9011319102192885, 0.9135334684580664, 0.8964574361912889], "final_y": [0.11275630913385015, 0.11382143216687979, 0.1184652886703299]}, "mutation_prompt": null}
{"id": "1cf6bc43-9193-4f29-84dc-c7787f125fe7", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            diversity_factor = np.std(personal_best) / (np.mean(personal_best) + 1e-10)  # New line added\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor * diversity_factor + 0.3 * np.random.random()  # Modified line\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Swarm Gradient Descent with dynamic adaptive factors based on swarm diversity for improved convergence.", "configspace": "", "generation": 32, "fitness": 0.8992273066696678, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.899 with standard deviation 0.023. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "06eacaa4-8e1b-424f-8f7a-51f06d62337d", "metadata": {"aucs": [0.9085189849185302, 0.8679670897639952, 0.921195845326478], "final_y": [0.11270499581506577, 0.11947680988093423, 0.11258105081871217]}, "mutation_prompt": null}
{"id": "245f9410-0e00-490e-8bf6-60394838ba1b", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                # Nonlinear dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * np.sin(np.pi * evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * np.sin(np.pi * evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Improved Adaptive Swarm Gradient Descent by adding nonlinear dynamic boundary adjustment for enhanced exploration.", "configspace": "", "generation": 33, "fitness": 0.9203455591748398, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.920 with standard deviation 0.010. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "06eacaa4-8e1b-424f-8f7a-51f06d62337d", "metadata": {"aucs": [0.9107516127427814, 0.9164255986610816, 0.9338594661206562], "final_y": [0.11401134115211475, 0.11292979332727271, 0.11009582138648111]}, "mutation_prompt": null}
{"id": "3dc21dc1-4e5e-4e89-b76c-8d84a56fe535", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = (1.7 * adaptive_factor * improvement_factor + \n                               0.3 * np.random.random()) * (evaluations / self.budget)\n            social_coeff = (1.7 * improvement_factor + \n                            0.3 * np.random.random()) * (1 - evaluations / self.budget)\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduced adaptive learning rates for cognitive and social components to enhance convergence efficiency.", "configspace": "", "generation": 34, "fitness": 0.8928568373562921, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.893 with standard deviation 0.009. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "06eacaa4-8e1b-424f-8f7a-51f06d62337d", "metadata": {"aucs": [0.8978805929972533, 0.879781249888224, 0.900908669183399], "final_y": [0.12056315889455538, 0.12663617201962207, 0.11991544494811923]}, "mutation_prompt": null}
{"id": "6f2c4d6b-dfd5-4c60-9853-7163df323ba4", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.8 - 0.4 * adaptive_factor  # Modified line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.1 * (evaluations / self.budget)  # Modified line\n                dynamic_ub = ub - (ub - lb) * 0.1 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Refined Adaptive Swarm Gradient Descent with improved inertia weight and dynamic boundary mechanism for enhanced convergence.  ", "configspace": "", "generation": 35, "fitness": 0.9099253582018213, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.910 with standard deviation 0.016. And the mean value of best solutions found was 0.114 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "06eacaa4-8e1b-424f-8f7a-51f06d62337d", "metadata": {"aucs": [0.8927126968231404, 0.9053920298390554, 0.9316713479432681], "final_y": [0.11568013405705735, 0.11719859312289649, 0.11008436793117571]}, "mutation_prompt": null}
{"id": "6d53ec5c-f55b-4f34-818a-dbdf8f031fba", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.initial_population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.initial_population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.initial_population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            current_population_size = int(self.initial_population_size * adaptive_factor) + 1\n            for i in range(current_population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduced dynamic population size adjustment based on evaluation progress for improved exploration and exploitation trade-off.", "configspace": "", "generation": 36, "fitness": 0.9144982390997195, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.914 with standard deviation 0.011. And the mean value of best solutions found was 0.114 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "06eacaa4-8e1b-424f-8f7a-51f06d62337d", "metadata": {"aucs": [0.9003924422591393, 0.9154171515248118, 0.9276851235152074], "final_y": [0.11583429457073846, 0.11471188066394067, 0.11051842798451672]}, "mutation_prompt": null}
{"id": "6bdd6ee6-4ef1-48e1-be42-8e82dadbe35d", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = (1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()) * (evaluations / self.budget)\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduced adaptive cognitive coefficient scaling based on iteration proximity to budget completion for improved convergence precision.", "configspace": "", "generation": 37, "fitness": 0.9124455293445942, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.912 with standard deviation 0.013. And the mean value of best solutions found was 0.114 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "06eacaa4-8e1b-424f-8f7a-51f06d62337d", "metadata": {"aucs": [0.9160266299055816, 0.8949493314067063, 0.9263606267214944], "final_y": [0.1130551333202352, 0.11803934867921795, 0.11224364254276575]}, "mutation_prompt": null}
{"id": "a3cb4877-3f16-4b3c-a690-457874413c46", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n            momentum = 0.1  # New momentum term\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (momentum * self.velocity[i] +  # Changed line\n                                    inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with momentum-based velocity update for improved convergence speed.", "configspace": "", "generation": 38, "fitness": 0.8971513557599939, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.897 with standard deviation 0.010. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "06eacaa4-8e1b-424f-8f7a-51f06d62337d", "metadata": {"aucs": [0.8848377773450516, 0.8964238113779471, 0.9101924785569829], "final_y": [0.12219592192877904, 0.12082260071952322, 0.11114551189541155]}, "mutation_prompt": null}
{"id": "801eeed9-89bf-4e94-9ddf-bca63079ceb3", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * np.random.random()  # Added line for stochastic attraction-repulsion\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best)) # Modified line\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by integrating stochastic attraction-repulsion mechanism for better exploration-exploitation balance.", "configspace": "", "generation": 39, "fitness": 0.9220853092643351, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.922 with standard deviation 0.003. And the mean value of best solutions found was 0.113 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "06eacaa4-8e1b-424f-8f7a-51f06d62337d", "metadata": {"aucs": [0.9195025884757806, 0.9210147199908841, 0.9257386193263402], "final_y": [0.11289157503352043, 0.11243760479805298, 0.11294161401963265]}, "mutation_prompt": null}
{"id": "39d4cabe-8c47-43a7-bc9b-b4f40e4e063e", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            # Non-linear inertia weight for enhanced convergence\n            inertia_weight = 0.9 - 0.5 * np.sin(np.pi * adaptive_factor)\n            # Dynamic improvement factor for learning rates\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            # Adjusted cognitive and social coefficients\n            cognitive_coeff = 1.5 * adaptive_factor * improvement_factor + 0.5 * np.random.random()\n            social_coeff = 1.5 * improvement_factor + 0.5 * np.random.random()\n\n            repulsion_coeff = 0.5 * np.random.random()  # Added line for stochastic attraction-repulsion\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best)) # Modified line\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by introducing non-linear inertia and dynamic learning rates for improved convergence and diversity.", "configspace": "", "generation": 40, "fitness": 0.8820585116026974, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.016. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "801eeed9-89bf-4e94-9ddf-bca63079ceb3", "metadata": {"aucs": [0.8744592898020025, 0.9048708926118053, 0.8668453523942846], "final_y": [0.12032302023411745, 0.11771435076863757, 0.12600350440191277]}, "mutation_prompt": null}
{"id": "e87015e4-e63b-41e8-97a0-c66679541bed", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * np.random.random()  # Added line for stochastic attraction-repulsion\n            velocity_scaling = 0.5 * (1 - adaptive_factor)  # New line for dynamic velocity scaling\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = velocity_scaling * (  # Apply scaling to velocity\n                    inertia_weight * self.velocity[i] +\n                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                    social_coeff * r2 * (global_best - swarm[i]) -\n                    repulsion_coeff * (swarm[i] - global_best)) # Modified line\n\n                swarm[i] += self.velocity[i]\n                # Nonlinear boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * np.sin(evaluations / self.budget * np.pi) # Modified line\n                dynamic_ub = ub - (ub - lb) * 0.05 * np.sin(evaluations / self.budget * np.pi) # Modified line\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with dynamic velocity scaling and nonlinear boundary adjustments.", "configspace": "", "generation": 41, "fitness": 0.7654121609544359, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.765 with standard deviation 0.034. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "801eeed9-89bf-4e94-9ddf-bca63079ceb3", "metadata": {"aucs": [0.7665690176944259, 0.8060849928737583, 0.7235824722951235], "final_y": [0.1523908130950692, 0.14822174876401029, 0.17587595298130954]}, "mutation_prompt": null}
{"id": "412d09df-1c51-4f79-b7cd-d89adbfe4929", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * np.random.random()  # Added line for stochastic attraction-repulsion\n\n            for i in range(self.population_size):\n                r1, r2, r3 = np.random.random(self.dim), np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                # Added local learning\n                local_best = personal_best[np.argmin(np.linalg.norm(swarm - swarm[i], axis=1))]\n                self.velocity[i] += 0.2 * r3 * (local_best - swarm[i])  # Added line for local learning\n\n                swarm[i] += self.velocity[i]\n                \n                # Mutation mechanism\n                if np.random.random() < 0.1:\n                    swarm[i] += np.random.normal(0, 0.1, self.dim)  # Added line for mutation\n\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Improved Adaptive Swarm Gradient Descent by incorporating local learning and mutation mechanism for enhanced escape from local optima.", "configspace": "", "generation": 42, "fitness": 0.8727819312729513, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.039. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "801eeed9-89bf-4e94-9ddf-bca63079ceb3", "metadata": {"aucs": [0.8175682105167783, 0.9053426911586003, 0.8954348921434752], "final_y": [0.14873763449224742, 0.12021110007827773, 0.12238375845842986]}, "mutation_prompt": null}
{"id": "eeac36eb-33c9-4911-aed9-b7a10d7e70f2", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()  # Changed line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best)) # Modified line\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by incorporating dynamic repulsion coefficient scaling based on stagnation to improve diversity and convergence.", "configspace": "", "generation": 43, "fitness": 0.9287892422176149, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.929 with standard deviation 0.002. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "801eeed9-89bf-4e94-9ddf-bca63079ceb3", "metadata": {"aucs": [0.9310474600904789, 0.9256315271538493, 0.9296887394085169], "final_y": [0.11170812863891144, 0.1122109950255159, 0.11072993205103066]}, "mutation_prompt": null}
{"id": "96ab623e-0340-417f-ae1e-65785a657571", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Temporal local search\n                if evaluations % (self.population_size // 2) == 0:  # Changed line 1\n                    swarm[i] += 0.01 * np.random.normal(0, 1, self.dim)  # Changed line 2\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if np.random.random() < 0.05:  # Stochastic perturbation - Changed line 3\n                    swarm[i] = np.random.uniform(dynamic_lb, dynamic_ub, self.dim)  # Changed line 4\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with temporal local search and stochastic perturbation to improve exploration and exploitation balance.", "configspace": "", "generation": 44, "fitness": 0.8794420136441689, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.879 with standard deviation 0.027. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "eeac36eb-33c9-4911-aed9-b7a10d7e70f2", "metadata": {"aucs": [0.8416519965403688, 0.8936003554958407, 0.9030736888962967], "final_y": [0.14052015580678934, 0.12234265723427173, 0.11856327355517704]}, "mutation_prompt": null}
{"id": "994a626a-fee4-4fc2-ad72-a51c7b8da23e", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * adaptive_factor # Changed line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()  # Changed line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best)) # Modified line\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent by fine-tuning the inertia weight formula to balance exploration and exploitation more effectively.", "configspace": "", "generation": 45, "fitness": 0.9123506070327618, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.912 with standard deviation 0.019. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "eeac36eb-33c9-4911-aed9-b7a10d7e70f2", "metadata": {"aucs": [0.9354967731175586, 0.9117695152688199, 0.8897855327119067], "final_y": [0.11046711458194103, 0.11307724479236814, 0.12423184062657988]}, "mutation_prompt": null}
{"id": "f9702670-f7dc-4ace-a83c-dc0ad158a330", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb)  # New line\n                for i in range(self.population_size):  # New line\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)  # New line\n                    candidate = np.clip(candidate, lb, ub)  # New line\n                    candidate_value = func(candidate)  # New line\n                    evaluations += 1  # New line\n                    if candidate_value < global_best_value:  # New line\n                        global_best = candidate  # New line\n                        global_best_value = candidate_value  # New line\n                    if evaluations >= self.budget:  # New line\n                        break  # New line\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with a variable neighborhood search phase to exploit local optima and improve convergence accuracy.", "configspace": "", "generation": 46, "fitness": 0.933418819129397, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.933 with standard deviation 0.003. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "eeac36eb-33c9-4911-aed9-b7a10d7e70f2", "metadata": {"aucs": [0.9345105384844485, 0.9360881466555291, 0.9296577722482133], "final_y": [0.1107869242679238, 0.11204203414976377, 0.11253384427247581]}, "mutation_prompt": null}
{"id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor  # Modified line\n                for i in range(self.population_size):  # New line\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)  # Modified line\n                    candidate = np.clip(candidate, lb, ub)  # New line\n                    candidate_value = func(candidate)  # New line\n                    evaluations += 1  # New line\n                    if candidate_value < global_best_value:  # New line\n                        global_best = candidate  # New line\n                        global_best_value = candidate_value  # New line\n                    if evaluations >= self.budget:  # New line\n                        break  # New line\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with improved local search using adaptive local search radius for better exploration. ", "configspace": "", "generation": 47, "fitness": 0.9338423865169293, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.934 with standard deviation 0.003. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "f9702670-f7dc-4ace-a83c-dc0ad158a330", "metadata": {"aucs": [0.9376068543771249, 0.9333316085824774, 0.9305886965911855], "final_y": [0.11017133664207712, 0.11319290510107216, 0.1120373717667561]}, "mutation_prompt": null}
{"id": "3ceb919e-d1f4-4769-a8eb-902284ad4ca7", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                if np.random.rand() < 0.1:  # New line\n                    self.velocity[i] = np.zeros(self.dim)  # New line\n                \n                swarm[i] += self.velocity[i]\n                # Enhanced dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.1 * (evaluations / self.budget)  # Modified line\n                dynamic_ub = ub - (ub - lb) * 0.1 * (evaluations / self.budget)  # Modified line\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor  # Modified line\n                for i in range(self.population_size):  # New line\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)  # Modified line\n                    candidate = np.clip(candidate, lb, ub)  # New line\n                    candidate_value = func(candidate)  # New line\n                    evaluations += 1  # New line\n                    if candidate_value < global_best_value:  # New line\n                        global_best = candidate  # New line\n                        global_best_value = candidate_value  # New line\n                    if evaluations >= self.budget:  # New line\n                        break  # New line\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Improved Adaptive Swarm Gradient Descent with enhanced dynamic boundary adjustment and selective velocity reset to boost exploration.", "configspace": "", "generation": 48, "fitness": 0.9112682291062781, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.911 with standard deviation 0.013. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.9181048679440628, 0.8934261922565994, 0.9222736271181721], "final_y": [0.11342704827150374, 0.12151736130252899, 0.11017594224687399]}, "mutation_prompt": null}
{"id": "f0d88f1f-7b57-4f34-8d06-2955073888fc", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * adaptive_factor  # Modified line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with time-varying inertia weight for improved balance between exploration and exploitation.", "configspace": "", "generation": 49, "fitness": 0.9263584377457867, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.926 with standard deviation 0.008. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.9355235698936004, 0.9170810625843722, 0.9264706807593877], "final_y": [0.10995563116899287, 0.11224226192190867, 0.11186284504482913]}, "mutation_prompt": null}
{"id": "d1193a4d-a697-4121-934e-22c247f2c464", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.4 + 0.5 * adaptive_factor  # Modified line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n                # Local crossover enhancement\n                if evaluations < self.budget:  # New line\n                    crossover_idx = np.random.randint(self.population_size)  # New line\n                    crossover_candidate = 0.5 * (swarm[crossover_idx] + global_best)  # New line\n                    crossover_candidate = np.clip(crossover_candidate, lb, ub)  # New line\n                    crossover_value = func(crossover_candidate)  # New line\n                    evaluations += 1  # New line\n                    if crossover_value < global_best_value:  # New line\n                        global_best = crossover_candidate  # New line\n                        global_best_value = crossover_value  # New line\n                    if evaluations >= self.budget:  # New line\n                        break  # New line\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced exploration and exploitation balance via dynamic inertia weight adjustment and incorporation of local crossover.", "configspace": "", "generation": 50, "fitness": 0.9157902355052142, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.916 with standard deviation 0.011. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.9302574245288617, 0.9047906554607592, 0.9123226265260218], "final_y": [0.10973390658516746, 0.11261142606204466, 0.11248100778835901]}, "mutation_prompt": null}
{"id": "a41ae3b3-5bba-4db0-95f3-c9ba144a1ca0", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * np.mean(np.std(swarm, axis=0))  # Modified line\n                for i in range(self.population_size):  \n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)  \n                    candidate = np.clip(candidate, lb, ub)  \n                    candidate_value = func(candidate)  \n                    evaluations += 1  \n                    if candidate_value < global_best_value:  \n                        global_best = candidate  \n                        global_best_value = candidate_value  \n                    if evaluations >= self.budget:  \n                        break  \n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with dynamic neighborhood search radius based on swarm diversity for improved exploration-exploitation balance.", "configspace": "", "generation": 51, "fitness": 0.8952573579403814, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.895 with standard deviation 0.012. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.8788138442899588, 0.9046909225585088, 0.9022673069726768], "final_y": [0.1198084173702324, 0.1184689721645561, 0.11426278533889334]}, "mutation_prompt": null}
{"id": "79563273-d110-4f2d-90cf-f46352986c85", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase with adaptive mutation\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor\n                mutation_probability = 0.1 * (1 - improvement_factor)  # New line\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    if np.random.random() < mutation_probability:  # New line\n                        candidate += np.random.normal(0, adaptive_factor, self.dim)  # New line\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Adaptive Swarm Gradient Descent with adaptive mutation for enhanced exploration and exploitation balance.", "configspace": "", "generation": 52, "fitness": 0.9308604570561444, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.931 with standard deviation 0.002. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.9330421288510257, 0.9303884668979054, 0.929150775419502], "final_y": [0.10999244295566069, 0.11208637822559242, 0.11291473746664793]}, "mutation_prompt": null}
{"id": "8d3df83b-773b-427f-81fe-e4bf09c33606", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            # Adjusted line: Adaptive social coefficient addition\n            social_coeff = (1.7 + 0.3 * np.random.random()) * improvement_factor * (1 - adaptive_factor)\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introducing adaptive social coefficient to maintain diversity and enhance exploration capabilities.", "configspace": "", "generation": 53, "fitness": 0.902440571081702, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.902 with standard deviation 0.018. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.8784890173525913, 0.9085106465726852, 0.9203220493198296], "final_y": [0.12374378307387246, 0.11630877837143594, 0.11195606217475729]}, "mutation_prompt": null}
{"id": "d5e7ea6e-ec69-4eb0-80f4-79fe723e6382", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                dynamic_velocity_scaling = 1 + adaptive_factor / 2     # Modified line\n                self.velocity[i] = (dynamic_velocity_scaling * inertia_weight * self.velocity[i] +  # Modified line\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Enhanced boundary adaptation\n                boundary_adaptation_factor = 0.1 * (evaluations / self.budget)  # Modified line\n                dynamic_lb = lb + (ub - lb) * boundary_adaptation_factor  # Modified line\n                dynamic_ub = ub - (ub - lb) * boundary_adaptation_factor\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduced dynamic velocity scaling and enhanced boundary adaptation for improved exploration.", "configspace": "", "generation": 54, "fitness": 0.9255068527903298, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.926 with standard deviation 0.007. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.9327947888001993, 0.916331243576703, 0.9273945259940873], "final_y": [0.10984762305447482, 0.11182264871703607, 0.1114590295282426]}, "mutation_prompt": null}
{"id": "7da94c88-5018-4fbb-8b71-397d7a09c1e7", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.1 * (evaluations / self.budget)  # Modified line\n                dynamic_ub = ub - (ub - lb) * 0.1 * (evaluations / self.budget)  # Modified line\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.15 * (ub - lb) * adaptive_factor  # Modified line\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with dynamic neighborhood exploration and improved boundary adjustment for better diversity and convergence.", "configspace": "", "generation": 55, "fitness": 0.9241117056526155, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.924 with standard deviation 0.004. And the mean value of best solutions found was 0.113 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.9194654800549955, 0.928686851610101, 0.92418278529275], "final_y": [0.11281558125955204, 0.11424061501273586, 0.11237806256907279]}, "mutation_prompt": null}
{"id": "27dc8996-b09a-4fab-8952-9236481928d6", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Adaptive boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.1 * (evaluations / self.budget)  # Modified line\n                dynamic_ub = ub - (ub - lb) * 0.1 * (evaluations / self.budget)  # Modified line\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Diversity-preserving mechanism\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with improved convergence by incorporating a diversity-preserving mechanism and adaptive boundary adjustment.", "configspace": "", "generation": 56, "fitness": 0.93249671257867, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.932 with standard deviation 0.002. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.9337506286478877, 0.9336753258280955, 0.9300641832600266], "final_y": [0.10988809553539836, 0.11223300480395415, 0.112468152282022]}, "mutation_prompt": null}
{"id": "5fe47a1a-bd03-4a89-9be9-e5a110a76948", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            velocity_scale = 0.5 + 0.5 * np.random.random()  # Modified line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = velocity_scale * (  # Modified line\n                                    inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget) * adaptive_factor  # Modified line\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget) * adaptive_factor  # Modified line\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with improved convergence via adaptive velocity scaling and boundary adjustment.", "configspace": "", "generation": 57, "fitness": 0.9148697157340872, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.915 with standard deviation 0.020. And the mean value of best solutions found was 0.113 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.9242260659389097, 0.8864955488853447, 0.9338875323780071], "final_y": [0.11139151814246384, 0.11678188046185622, 0.11021443674874543]}, "mutation_prompt": null}
{"id": "2e9bc8d4-bd9e-4fe6-8ad6-2ac17cc9f485", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.normal(0, local_search_radius, self.dim)  # Modified line\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with improved local search through adaptive mutation strategy for better exploration.", "configspace": "", "generation": 58, "fitness": 0.8966111123498394, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.897 with standard deviation 0.014. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.8818976006908759, 0.9147591859309729, 0.8931765504276692], "final_y": [0.12002441869732339, 0.11391699050308535, 0.12349459651746542]}, "mutation_prompt": null}
{"id": "c6470162-7ab3-4823-bd52-af0a27e90a7d", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor + 0.3 * np.random.random() # Modified line\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Optimized Enhanced Adaptive Swarm Gradient Descent with improved exploration via dynamic cognitive coefficient adjustment.", "configspace": "", "generation": 59, "fitness": 0.9317357156605653, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.932 with standard deviation 0.001. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.9332276805034464, 0.93196484595059, 0.9300146205276598], "final_y": [0.11015943668419759, 0.11241397908035122, 0.11190576627213844]}, "mutation_prompt": null}
{"id": "db3190d5-c845-4386-bb7d-458c120a505a", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            # Changed inertia weight calculation\n            inertia_weight = np.random.uniform(0.7, 0.9) * adaptive_factor  \n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                # Adjust local search radius and search mechanism\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor  \n                for i in range(self.population_size):  \n                    candidate = global_best + np.random.laplace(0, local_search_radius, self.dim)  \n                    candidate = np.clip(candidate, lb, ub)  \n                    candidate_value = func(candidate)  \n                    evaluations += 1  \n                    if candidate_value < global_best_value:  \n                        global_best = candidate  \n                        global_best_value = candidate_value  \n                    if evaluations >= self.budget:  \n                        break  \n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Adaptive Swarm Gradient Descent with stochastic inertia and modified local search for enhanced convergence.", "configspace": "", "generation": 60, "fitness": 0.9164291946113945, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.916 with standard deviation 0.012. And the mean value of best solutions found was 0.114 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.9051685777703175, 0.910638026382377, 0.9334809796814894], "final_y": [0.11286208738608416, 0.11781501342787204, 0.11021516586894065]}, "mutation_prompt": null}
{"id": "d165398b-8194-4b00-a8ce-911045b4658f", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.7 * adaptive_factor  # Changed line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor  # Modified line\n                for i in range(self.population_size):  # New line\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)  # Modified line\n                    candidate = np.clip(candidate, lb, ub)  # New line\n                    candidate_value = func(candidate)  # New line\n                    evaluations += 1  # New line\n                    if candidate_value < global_best_value:  # New line\n                        global_best = candidate  # New line\n                        global_best_value = candidate_value  # New line\n                    if evaluations >= self.budget:  # New line\n                        break  # New line\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduced dynamic inertia weight adaptation for improved exploration-exploitation balance.", "configspace": "", "generation": 61, "fitness": 0.9326540873635536, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.933 with standard deviation 0.004. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.9382684094512231, 0.9322194191124165, 0.9274744335270211], "final_y": [0.10991637524724018, 0.11178568967598324, 0.11253456669427131]}, "mutation_prompt": null}
{"id": "069080a6-9d40-4c18-8cb1-6b2a99a6da12", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor + 0.05 * np.sin(evaluations)  # Modified line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate_mutation = np.random.normal(0, 0.1, self.dim) * adaptive_factor  # Modified line\n                    candidate += candidate_mutation  # Modified line\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with local search acceleration using adaptive mutation and momentum inertia for improved convergence.", "configspace": "", "generation": 62, "fitness": 0.917645926171257, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.918 with standard deviation 0.014. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.9343224980577278, 0.8990202473445913, 0.919595033111452], "final_y": [0.1100569869757293, 0.12299038392119532, 0.11168685059012418]}, "mutation_prompt": null}
{"id": "e1519183-61b2-4bdb-b2df-664f68c59170", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Change: Initialize swarm with larger variance if dimension is large\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Dynamic repulsion based on stagnation\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Dynamic boundary adjustment\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Variable neighborhood search phase\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor  # Modified line\n                for i in range(self.population_size):  # New line\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)  # Modified line\n                    candidate = np.clip(candidate, lb, ub)  # New line\n                    candidate_value = func(candidate)  # New line\n                    evaluations += 1  # New line\n                    if candidate_value < global_best_value:  # New line\n                        global_best = candidate  # New line\n                        global_best_value = candidate_value  # New line\n                    if evaluations >= self.budget:  # New line\n                        break  # New line\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "The algorithm enhances exploration by adapting the initial swarm distribution based on problem complexity, improving convergence speed.", "configspace": "", "generation": 63, "fitness": 0.9338922271836899, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.934 with standard deviation 0.003. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "2e1f8979-f477-4bb7-89b9-5f30763eb625", "metadata": {"aucs": [0.9376647661990296, 0.9330422942645203, 0.9309696210875199], "final_y": [0.11042536064127262, 0.1132311202128301, 0.1121053417530028]}, "mutation_prompt": null}
{"id": "2fe0945c-619c-45b3-a656-c90cf88d0ef5", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])  # Modified line\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    mutation_step = np.random.normal(0, 0.01, self.dim)  # New line\n                    candidate += mutation_step  # Modified line\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "The algorithm improves local search by dynamically adjusting exploration based on convergence trends and introducing a diversity-enhancing mutation step for better solution quality.", "configspace": "", "generation": 64, "fitness": 0.9168216218449851, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.917 with standard deviation 0.014. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "e1519183-61b2-4bdb-b2df-664f68c59170", "metadata": {"aucs": [0.932452323780694, 0.8977428513282124, 0.920269690426049], "final_y": [0.11103795577354747, 0.12327006712640387, 0.11247560966072456]}, "mutation_prompt": null}
{"id": "d7db681a-e523-4e90-bde6-0c5061b3e829", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor * (1 + 0.5 * (1 - improvement_factor)) + 0.3 * np.random.random() # Adjusted line\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "The algorithm enhances exploration and increases convergence speed by adjusting the cognitive coefficient based on stagnation, encouraging more diverse solutions.", "configspace": "", "generation": 65, "fitness": 0.9325062046030332, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.933 with standard deviation 0.001. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e1519183-61b2-4bdb-b2df-664f68c59170", "metadata": {"aucs": [0.9317920172017189, 0.9345006543144891, 0.9312259422928917], "final_y": [0.10995661735270412, 0.11195235866780451, 0.11191497596916344]}, "mutation_prompt": null}
{"id": "01b784ba-e55e-442e-aa80-521fe7ea9a26", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.2 * (ub - lb) * adaptive_factor  # Modified line\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.normal(0, local_search_radius, self.dim)  # Modified line\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "The algorithm enhances exploration by dynamically adjusting swarm influence and using a more robust local search mechanism.", "configspace": "", "generation": 66, "fitness": 0.9106108349483253, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.911 with standard deviation 0.009. And the mean value of best solutions found was 0.113 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e1519183-61b2-4bdb-b2df-664f68c59170", "metadata": {"aucs": [0.906601701540205, 0.9019749099759788, 0.923255893328792], "final_y": [0.11250695305199243, 0.11399196021770075, 0.1137334097717898]}, "mutation_prompt": null}
{"id": "ed07f3ef-9d51-4815-9900-2bab929d41fc", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.12 * (ub - lb) * adaptive_factor  # Change made here\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Improved adaptive exploration by increasing variable neighborhood search radius slightly, enhancing solution diversity and convergence.  ", "configspace": "", "generation": 67, "fitness": 0.9226022004746239, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.923 with standard deviation 0.014. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e1519183-61b2-4bdb-b2df-664f68c59170", "metadata": {"aucs": [0.9304959271310547, 0.9346879653583269, 0.9026227089344903], "final_y": [0.11040767381973093, 0.11196485723874727, 0.1125953717748388]}, "mutation_prompt": null}
{"id": "e96a9166-9095-4e23-87d9-7ce55dbdd1e5", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.03 * (evaluations / self.budget)  # Changed line\n                dynamic_ub = ub - (ub - lb) * 0.03 * (evaluations / self.budget)  # Changed line\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.12 * (ub - lb) * adaptive_factor  # Changed line\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "This algorithm leverages dynamic boundary adjustment and adaptive local search to enhance optimization performance in high-dimensional spaces.", "configspace": "", "generation": 68, "fitness": 0.9229512325025907, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.923 with standard deviation 0.014. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e1519183-61b2-4bdb-b2df-664f68c59170", "metadata": {"aucs": [0.9304960999614382, 0.9348104603242324, 0.9035471372221013], "final_y": [0.11040795925777336, 0.1118733026293478, 0.11230568354296477]}, "mutation_prompt": null}
{"id": "688843aa-4db9-47ad-b91b-d03902f781ee", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            # Change: Dynamic inertia based on success rate\n            successful_updates = np.sum(personal_best_value < global_best_value)\n            inertia_weight = 0.7 + 0.2 * (successful_updates / self.population_size)\n\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Change: Refine local search with adaptive radius\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "The algorithm introduces dynamic inertia weighting based on success rate and integrates a refined local search radius to improve exploration and exploitation balance.", "configspace": "", "generation": 69, "fitness": 0.9322405430298487, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.932 with standard deviation 0.002. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e1519183-61b2-4bdb-b2df-664f68c59170", "metadata": {"aucs": [0.934649106036827, 0.9290674145241269, 0.933005108528592], "final_y": [0.11008680080242561, 0.11184435774927903, 0.11212581616875938]}, "mutation_prompt": null}
{"id": "11e5c458-ae08-470b-b60c-f05089e6454e", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            # Change: Utilize personal best distribution for coefficients\n            personal_best_mean = np.mean(personal_best_value)\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor * (1 + 0.5 * (personal_best_mean / global_best_value)) + 0.3 * np.random.random()  # Modified line\n            social_coeff = 1.7 * improvement_factor * (1 + 0.5 * (global_best_value / personal_best_mean)) + 0.3 * np.random.random()  # Modified line\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Improved exploration and exploitation balance by adapting inertia and coefficients dynamically based on personal best value distribution.", "configspace": "", "generation": 70, "fitness": 0.9249109929824785, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.925 with standard deviation 0.007. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e1519183-61b2-4bdb-b2df-664f68c59170", "metadata": {"aucs": [0.9147084471284787, 0.9296516696579538, 0.9303728621610028], "final_y": [0.11288286618169008, 0.11248161854592498, 0.11207400301839354]}, "mutation_prompt": null}
{"id": "d9701cae-4db7-4b81-b454-0261514e8abf", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                # Introduce adaptive mutation\n                mutation_strength = 0.05 * (1 - evaluations / self.budget)  # New line\n                swarm[i] += np.random.normal(0, mutation_strength, self.dim)  # New line\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.1 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    candidate = global_best + np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce adaptive mutation in particle positions based on iteration progress to enhance exploration-exploitation balance.", "configspace": "", "generation": 71, "fitness": 0.9298644812139404, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.930 with standard deviation 0.007. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e1519183-61b2-4bdb-b2df-664f68c59170", "metadata": {"aucs": [0.9318224713242975, 0.9198660115282682, 0.9379049607892554], "final_y": [0.11070419743298054, 0.11337708846414951, 0.11120637956078983]}, "mutation_prompt": null}
{"id": "7b759e09-dea7-43a4-ad82-2ee36b331080", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor  # Modified line\n                weight_factor = 0.5 * improvement_factor + 0.5  # New line\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)  # Modified line\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "The algorithm introduces a weighted local search mechanism to balance exploration and exploitation dynamically based on current performance metrics.", "configspace": "", "generation": 72, "fitness": 0.9362184997323443, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.936 with standard deviation 0.006. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e1519183-61b2-4bdb-b2df-664f68c59170", "metadata": {"aucs": [0.9424762986890597, 0.937380374026656, 0.9287988264813174], "final_y": [0.10964534570375528, 0.11168546431985837, 0.11209842714680718]}, "mutation_prompt": null}
{"id": "70900d9e-0982-4029-8e78-e0e9952f3837", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * (adaptive_factor ** 2)  # Modified line\n                weight_factor = 0.4 * improvement_factor + 0.6  # Modified line\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)  # Modified line\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce a non-linear dynamic weighting scheme to enhance the balance between exploration and exploitation phases.", "configspace": "", "generation": 73, "fitness": 0.9361092779369623, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.936 with standard deviation 0.007. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "7b759e09-dea7-43a4-ad82-2ee36b331080", "metadata": {"aucs": [0.943769565146199, 0.9384506594639904, 0.9261076092006973], "final_y": [0.10982643820807869, 0.11164257849444847, 0.1120916505107542]}, "mutation_prompt": null}
{"id": "7e4b35d2-7c35-4214-8e2e-3390aad0e701", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                for i in range(self.population_size):\n                    mutation_probability = 0.1 * adaptive_factor  # New line\n                    weight_factor = 0.5 * improvement_factor + 0.5\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    if np.random.random() < mutation_probability:  # New line\n                        candidate = np.random.uniform(lb, ub, self.dim)  # New line\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced exploration by introducing mutation for escaping local optima and improved performance adjustment for better adaptability.", "configspace": "", "generation": 74, "fitness": 0.9349140507160295, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.006. And the mean value of best solutions found was 0.112 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "7b759e09-dea7-43a4-ad82-2ee36b331080", "metadata": {"aucs": [0.9287666011442741, 0.9434269291288564, 0.932548621874958], "final_y": [0.11379726810112833, 0.111922862629531, 0.1114264666983984]}, "mutation_prompt": null}
{"id": "417d8524-b341-43a4-a5fd-db2a997945d1", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                velocity_scale = 1 - 0.5 * (evaluations / self.budget)  # Modified line\n                self.velocity[i] = velocity_scale * (inertia_weight * self.velocity[i] +\n                                                     cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                                     social_coeff * r2 * (global_best - swarm[i]) -\n                                                     repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "This refined algorithm incorporates a dynamic velocity scaling factor for improved convergence by adjusting swarm movement based on evaluation progress.", "configspace": "", "generation": 75, "fitness": 0.9361371619881583, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.936 with standard deviation 0.007. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "7b759e09-dea7-43a4-ad82-2ee36b331080", "metadata": {"aucs": [0.9444019678361136, 0.9366038913586942, 0.927405626769667], "final_y": [0.10964798987035551, 0.1122150001051272, 0.1121824177927997]}, "mutation_prompt": null}
{"id": "7b0704f7-02b5-482a-a179-aef5ccfb8c37", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor + 0.2 * np.random.random()  # Modified line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    mutation_factor = 0.1 * np.random.normal()  # New line\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim) + mutation_factor  # Modified line\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "The algorithm incorporates a dynamic inertia weight and adaptive mutation to enhance exploration and exploitation based on performance metrics.", "configspace": "", "generation": 76, "fitness": 0.9060608233901513, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.906 with standard deviation 0.019. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "7b759e09-dea7-43a4-ad82-2ee36b331080", "metadata": {"aucs": [0.8863937165189946, 0.9004573561160686, 0.9313313975353906], "final_y": [0.12588328249451586, 0.11756248228630373, 0.112236143778493]}, "mutation_prompt": null}
{"id": "0207681a-1319-4fbc-9db7-0b1c9252b93a", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            # Changed line: Calculating repulsion_coeff based on global_best_value\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random() * (1 - global_best_value)\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "The algorithm fine-tunes the balance between exploration and exploitation by adjusting the repulsion coefficient based on the global best value, aiming to improve convergence speed and solution quality.", "configspace": "", "generation": 77, "fitness": 0.9286468465082245, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.929 with standard deviation 0.011. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "7b759e09-dea7-43a4-ad82-2ee36b331080", "metadata": {"aucs": [0.942321462907167, 0.9164651441558276, 0.927153932461679], "final_y": [0.11010843982600393, 0.11179160713710856, 0.11213982180869453]}, "mutation_prompt": null}
{"id": "6a726333-b551-44fd-bcb8-bd7745eb67fe", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor)  # New line\n                    candidate += np.random.normal(0, mutation_strength, self.dim)  # New line\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce an adaptive mutation operator to enhance diversity and escape local minima.", "configspace": "", "generation": 78, "fitness": 0.9393679098573445, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.003. And the mean value of best solutions found was 0.110 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "7b759e09-dea7-43a4-ad82-2ee36b331080", "metadata": {"aucs": [0.9433546238682409, 0.9398082072828867, 0.9349408984209062], "final_y": [0.10974688890411521, 0.1116805344911036, 0.11006220762552021]}, "mutation_prompt": null}
{"id": "4bfd7a3a-d3d3-466d-9670-a1b7a5518221", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                swarm_mean = np.mean(swarm, axis=0)  # New term\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best) +\n                                    0.1 * (swarm_mean - swarm[i]))  # New line\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor)  # New line\n                    candidate += np.random.normal(0, mutation_strength, self.dim)  # New line\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhance velocity update by adding a feedback term based on the distance to the mean swarm position.", "configspace": "", "generation": 79, "fitness": 0.9335614058693352, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.934 with standard deviation 0.006. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "6a726333-b551-44fd-bcb8-bd7745eb67fe", "metadata": {"aucs": [0.9409054330227087, 0.9338305784780927, 0.9259482061072044], "final_y": [0.10986846143445472, 0.11176119419997543, 0.11277052106821561]}, "mutation_prompt": null}
{"id": "f6cefcfd-4f2e-47e1-8ff9-89292b291c4e", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = (1.7 + adaptive_factor) * improvement_factor + 0.3 * np.random.random()  # Modified line\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor)\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce a dynamic cognitive coefficient scaling based on the improvement rate to enhance exploration.", "configspace": "", "generation": 80, "fitness": 0.9392372385588027, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.004. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "6a726333-b551-44fd-bcb8-bd7745eb67fe", "metadata": {"aucs": [0.9436481796890425, 0.9394271397988241, 0.9346363961885412], "final_y": [0.10996161351636102, 0.1118862827436431, 0.11012076689132033]}, "mutation_prompt": null}
{"id": "70074bef-046a-4fff-b513-5eac539ca991", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    candidate += np.random.normal(0, 0.01, self.dim)  # New randomness introduced here\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhance global exploration by introducing randomness in global best position to prevent premature convergence.", "configspace": "", "generation": 81, "fitness": 0.9393362489365819, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.003. And the mean value of best solutions found was 0.110 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "6a726333-b551-44fd-bcb8-bd7745eb67fe", "metadata": {"aucs": [0.9432688792535012, 0.9398003312716919, 0.9349395362845522], "final_y": [0.10966139911089123, 0.1117024133454596, 0.1100622649945312]}, "mutation_prompt": null}
{"id": "0c13f32b-14c8-429e-a018-6b3c52d5b40e", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor * (1 - evaluations / (2 * self.budget))  # Modified line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor)\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce a dynamic inertia weight decay to balance exploration and exploitation more effectively.", "configspace": "", "generation": 82, "fitness": 0.9387455642253099, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.004. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "6a726333-b551-44fd-bcb8-bd7745eb67fe", "metadata": {"aucs": [0.9430935149195016, 0.9390995690742181, 0.9340436086822098], "final_y": [0.11009667298987502, 0.11164808945107585, 0.1102546177741599]}, "mutation_prompt": null}
{"id": "a216515e-f69d-400a-86a4-99e9e40f9d49", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor ** 0.5)  # Modified line\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Implement a dynamic mutation strength reduction to enhance convergence towards the end of optimization.", "configspace": "", "generation": 83, "fitness": 0.939352909029228, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.003. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "6a726333-b551-44fd-bcb8-bd7745eb67fe", "metadata": {"aucs": [0.9433548750527824, 0.9398083357868332, 0.9348955162480685], "final_y": [0.10974681683446841, 0.1116811282480068, 0.1101859882458247]}, "mutation_prompt": null}
{"id": "7ca39c99-4251-45a8-88bd-3a2efe05d3cf", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2  # Changed line (nonlinear convergence factor)\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor)\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce a nonlinear convergence factor to improve global exploration and exploitation balance.  ", "configspace": "", "generation": 84, "fitness": 0.939626442731622, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.940 with standard deviation 0.002. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "6a726333-b551-44fd-bcb8-bd7745eb67fe", "metadata": {"aucs": [0.9413595226467301, 0.940722416809515, 0.9367973887386206], "final_y": [0.10997754682270455, 0.11190362829281741, 0.11007493103341137]}, "mutation_prompt": null}
{"id": "d40a351f-f654-4b7a-88b9-ce2418e15a7e", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2  # Changed line (nonlinear convergence factor)\n            inertia_weight = 0.9 - 0.5 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor * (1 - improvement_factor)  # Changed line\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor)\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce a dynamic adjustment of the local search radius to enhance fine-tuning of solutions.", "configspace": "", "generation": 85, "fitness": 0.9386046231903888, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.002. And the mean value of best solutions found was 0.110 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "7ca39c99-4251-45a8-88bd-3a2efe05d3cf", "metadata": {"aucs": [0.9406107707537038, 0.9396781577330708, 0.9355249410843918], "final_y": [0.10962588958575747, 0.1116439277182184, 0.1100779361798302]}, "mutation_prompt": null}
{"id": "11f80bf0-2a11-4e5d-933c-14f026a0000b", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2  # Changed line (nonlinear convergence factor)\n            inertia_weight = 0.9 - 0.55 * adaptive_factor  # Refined line (fine-tune inertia weight)\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor)\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Fine-tune the inertia weight and expand search space boundaries dynamically for improved exploration.", "configspace": "", "generation": 86, "fitness": 0.940094967564829, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.940 with standard deviation 0.003. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "7ca39c99-4251-45a8-88bd-3a2efe05d3cf", "metadata": {"aucs": [0.9434519378304842, 0.9399849274380627, 0.9368480374259405], "final_y": [0.1098458688179722, 0.11219811836757554, 0.10999741856898615]}, "mutation_prompt": null}
{"id": "6800c2ef-98c8-4d51-aaee-cb64ea62a830", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2  # Changed line (nonlinear convergence factor)\n            inertia_weight = 0.9 - 0.55 * adaptive_factor  # Refined line (fine-tune inertia weight)\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            sorted_indices = np.argsort(personal_best_value)  # Preserve a fraction of best solutions\n            for i in range(self.population_size):\n                if i < self.population_size * 0.1:  # Elitism: top 10% solutions are kept\n                    self.velocity[sorted_indices[i]] *= 0\n                    continue\n\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor)\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce elitism by preserving a fraction of the best solutions in each iteration to maintain diversity and exploit high-quality solutions.", "configspace": "", "generation": 87, "fitness": 0.9084518182975866, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.908 with standard deviation 0.017. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "11f80bf0-2a11-4e5d-933c-14f026a0000b", "metadata": {"aucs": [0.884992949359512, 0.9205194314170714, 0.9198430741161764], "final_y": [0.12515683168662073, 0.11240570587820409, 0.11216530036395211]}, "mutation_prompt": null}
{"id": "0bce2179-a9fa-41dc-92fb-484b2b415c2e", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2\n            inertia_weight = 0.9 - 0.55 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                levy_step = 0.01 * np.random.standard_cauchy(self.dim)  # Changed line (levy flight step)\n                swarm[i] += self.velocity[i] + levy_step  # Changed line (apply levy flight)\n\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor)\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce levy flight to enhance global exploration capability.", "configspace": "", "generation": 88, "fitness": 0.9025074940885761, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.903 with standard deviation 0.028. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "11f80bf0-2a11-4e5d-933c-14f026a0000b", "metadata": {"aucs": [0.8636067852979916, 0.9157635056220167, 0.9281521913457201], "final_y": [0.13083720978580216, 0.11631366200223292, 0.11209175767533885]}, "mutation_prompt": null}
{"id": "6a0f53f6-51d9-47f4-8305-278616a7ad24", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2  \n            inertia_weight = 0.9 - 0.55 * adaptive_factor  \n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                dynamic_mutation_multiplier = 1.0 + (0.5 - 0.5 * (evaluations / self.budget))  # Changed line\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor)\n                    candidate += np.random.normal(0, mutation_strength * dynamic_mutation_multiplier, self.dim)  # Changed line\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhance the local search phase by introducing a dynamic mutation multiplier based on evaluation progress to improve solution refinement.", "configspace": "", "generation": 89, "fitness": 0.9387073359352424, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.004. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "11f80bf0-2a11-4e5d-933c-14f026a0000b", "metadata": {"aucs": [0.9434572476910068, 0.9399850877437785, 0.932679672370942], "final_y": [0.1098335722984417, 0.11219811585819306, 0.11143400718050867]}, "mutation_prompt": null}
{"id": "88385dba-e493-4944-8cf7-3daaf8a8acea", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2  # Changed line (nonlinear convergence factor)\n            inertia_weight = 0.9 - 0.55 * adaptive_factor  # Refined line (fine-tune inertia weight)\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor) ** 1.5  # Changed line (nonlinear scaling for mutation)\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce a nonlinear scaling for mutation strength to enhance exploration during initial iterations.", "configspace": "", "generation": 90, "fitness": 0.9401028971252255, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.940 with standard deviation 0.003. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "11f80bf0-2a11-4e5d-933c-14f026a0000b", "metadata": {"aucs": [0.9434531974767416, 0.939985135619681, 0.9368703582792538], "final_y": [0.10984593951743571, 0.11219798014924753, 0.1099991414830499]}, "mutation_prompt": null}
{"id": "7a80d564-5093-444c-b9be-bbbbf45dad5d", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2  # Changed line (nonlinear convergence factor)\n            inertia_weight = 0.9 - 0.55 * adaptive_factor  # Refined line (fine-tune inertia weight)\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * np.sin(np.pi * evaluations / self.budget)  # Changed line (sinusoidal scaling for mutation)\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhance exploration by adjusting mutation strength scaling based on a sinusoidal function for improved diversity.", "configspace": "", "generation": 91, "fitness": 0.9391785851418687, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.004. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "88385dba-e493-4944-8cf7-3daaf8a8acea", "metadata": {"aucs": [0.9434527193867247, 0.9409868110189551, 0.9330962250199263], "final_y": [0.1098391443567418, 0.1116390841597289, 0.11023224950419463]}, "mutation_prompt": null}
{"id": "5bebfbf9-6274-43db-ad60-d37cd3014e9e", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2\n            inertia_weight = 0.9 - 0.55 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor)  # Changed line\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Refine the local search strategy by adjusting the random candidate's mutation range.", "configspace": "", "generation": 92, "fitness": 0.940094967564829, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.940 with standard deviation 0.003. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "88385dba-e493-4944-8cf7-3daaf8a8acea", "metadata": {"aucs": [0.9434519378304842, 0.9399849274380627, 0.9368480374259405], "final_y": [0.1098458688179722, 0.11219811836757554, 0.10999741856898615]}, "mutation_prompt": null}
{"id": "573c31fe-bd3b-47ed-a14b-8d1b852d4fa1", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2  # Changed line (nonlinear convergence factor)\n            inertia_weight = 0.9 - 0.55 * adaptive_factor  # Refined line (fine-tune inertia weight)\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random() * adaptive_factor  # Modified line (dynamic social influence adjustment)\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor) ** 1.5  # Changed line (nonlinear scaling for mutation)\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhance exploration phase by dynamically adjusting cognitive and social coefficients to balance exploration and exploitation.", "configspace": "", "generation": 93, "fitness": 0.9400222984664396, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.940 with standard deviation 0.002. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "88385dba-e493-4944-8cf7-3daaf8a8acea", "metadata": {"aucs": [0.9428112272857655, 0.9401949772357864, 0.9370606908777668], "final_y": [0.1099664101978044, 0.1116415890330249, 0.11016602846573831]}, "mutation_prompt": null}
{"id": "0a4f186f-6267-4e55-a445-629cc9aab50c", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2  # Changed line (nonlinear convergence factor)\n            inertia_weight = 0.9 - 0.55 * adaptive_factor  # Refined line (fine-tune inertia weight)\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor * (1 + 0.5 * (evaluations / self.budget))  # Changed line\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor) ** 1.5  # Changed line (nonlinear scaling for mutation)\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce an adaptive dynamic adjustment for local search radius to enhance precision in later iterations.", "configspace": "", "generation": 94, "fitness": 0.9385870138738902, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.004. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "88385dba-e493-4944-8cf7-3daaf8a8acea", "metadata": {"aucs": [0.9427867686591407, 0.9396360934741144, 0.9333381794884156], "final_y": [0.10988549160275707, 0.11163620506852145, 0.11142412549906178]}, "mutation_prompt": null}
{"id": "e8c51766-3f19-4105-912a-b296f8bcd3de", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2\n            inertia_weight = 0.9 - 0.55 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = (0.5 * personal_best[i] + 0.5 * swarm[i])  # Changed line (dynamic personal best update)\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor) ** 1.5\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce dynamic personal best memory update for enhanced adaptability to changing landscapes.", "configspace": "", "generation": 95, "fitness": 0.9401850198907917, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.940 with standard deviation 0.004. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "88385dba-e493-4944-8cf7-3daaf8a8acea", "metadata": {"aucs": [0.9447772982549313, 0.9405455880913002, 0.9352321733261435], "final_y": [0.10965459875992967, 0.11200532996765578, 0.11012976200427671]}, "mutation_prompt": null}
{"id": "62b055ec-3389-4834-b7de-05ff7e62780a", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2\n            inertia_weight = 0.9 - 0.55 * adaptive_factor + 0.1 * np.random.rand()  # Changed line\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = (0.5 * personal_best[i] + 0.5 * swarm[i])  # Changed line (dynamic personal best update)\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor) ** 1.5\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhance swarm diversity with dynamic inertia weight adjustment for improved exploration.", "configspace": "", "generation": 96, "fitness": 0.9039918094541585, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.036. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "e8c51766-3f19-4105-912a-b296f8bcd3de", "metadata": {"aucs": [0.8544472928986941, 0.9166893755499201, 0.9408387599138612], "final_y": [0.13563297261328344, 0.11623964784849872, 0.1096486036205101]}, "mutation_prompt": null}
{"id": "e60999db-cdd4-4408-b0cd-715eea089719", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2\n            inertia_weight = 0.9 - 0.55 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            \n            # Changed lines for dynamic coefficients\n            cognitive_coeff = (1.7 * adaptive_factor + 0.3 * np.random.random()) * improvement_factor\n            social_coeff = (1.7 * adaptive_factor + 0.3 * np.random.random()) * improvement_factor\n            \n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = (0.5 * personal_best[i] + 0.5 * swarm[i])  # Changed line (dynamic personal best update)\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor) ** 1.5\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce dynamic adaptability in both personal and global learning coefficients for enhanced convergence.", "configspace": "", "generation": 97, "fitness": 0.9343446716715859, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.934 with standard deviation 0.007. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e8c51766-3f19-4105-912a-b296f8bcd3de", "metadata": {"aucs": [0.9433036936598709, 0.9262889182134143, 0.9334414031414724], "final_y": [0.10979419798661083, 0.11168976401059305, 0.11004114522726338]}, "mutation_prompt": null}
{"id": "20e3f467-ca8d-40ab-a6e5-6739ae6b6365", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2\n            inertia_weight = 0.9 - 0.55 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    momentum = 0.9  # New line added for momentum term\n                    personal_best[i] = (momentum * personal_best[i] + (1 - momentum) * swarm[i])  # Modified line\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor) ** 1.5\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhance the swarm exploration by extending the dynamic personal best update mechanism to include a momentum term.", "configspace": "", "generation": 98, "fitness": 0.9392561897987209, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.004. And the mean value of best solutions found was 0.110 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e8c51766-3f19-4105-912a-b296f8bcd3de", "metadata": {"aucs": [0.9426677614073253, 0.9410443276655831, 0.9340564803232544], "final_y": [0.10967922015962883, 0.11163469224268474, 0.11009307585356143]}, "mutation_prompt": null}
{"id": "7c65eaba-5cd3-4f3f-a585-79720b5ad3a6", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2\n            inertia_weight = 0.9 - 0.55 * adaptive_factor\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                scaling_factor = 1 - (0.5 * evaluations / self.budget)  # Changed line (adaptive velocity scaling)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += scaling_factor * self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = (0.5 * personal_best[i] + 0.5 * swarm[i])  # Changed line (dynamic personal best update)\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor) ** 1.5\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce adaptive velocity scaling for smoother convergence and improved final solution accuracy.", "configspace": "", "generation": 99, "fitness": 0.9391987490925876, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.003. And the mean value of best solutions found was 0.111 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e8c51766-3f19-4105-912a-b296f8bcd3de", "metadata": {"aucs": [0.9434711257165236, 0.9378139618288361, 0.9363111597324029], "final_y": [0.1096861933622022, 0.1127151799378463, 0.11002309396690568]}, "mutation_prompt": null}
{"id": "423a5e42-fa4d-400c-8c30-a3806793dd4b", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        variance_factor = 0.1 if self.dim < 10 else 0.5\n        swarm = np.random.uniform(lb - variance_factor, ub + variance_factor, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - evaluations / self.budget) ** 2\n            inertia_weight = 0.9 - 0.55 * adaptive_factor * np.sin(np.pi * evaluations / self.budget)  # Changed line (dynamic inertia weight adjustment)\n            improvement_factor = np.exp(-global_best_value / (np.min(personal_best_value) + 1e-10))\n            cognitive_coeff = 1.7 * adaptive_factor * improvement_factor + 0.3 * np.random.random()\n            social_coeff = 1.7 * improvement_factor + 0.3 * np.random.random()\n\n            repulsion_coeff = 0.5 * (1 - improvement_factor) * np.random.random()\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) -\n                                    repulsion_coeff * (swarm[i] - global_best))\n\n                swarm[i] += self.velocity[i]\n                dynamic_lb = lb + (ub - lb) * 0.05 * (evaluations / self.budget)\n                dynamic_ub = ub - (ub - lb) * 0.05 * (evaluations / self.budget)\n                swarm[i] = np.clip(swarm[i], dynamic_lb, dynamic_ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = (0.5 * personal_best[i] + 0.5 * swarm[i])  # Changed line (dynamic personal best update)\n                    personal_best_value[i] = f_value\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget:\n                local_search_radius = 0.05 * (ub - lb) * adaptive_factor\n                weight_factor = 0.5 * improvement_factor + 0.5\n                for i in range(self.population_size):\n                    candidate = global_best + weight_factor * np.random.uniform(-local_search_radius, local_search_radius, self.dim)\n                    mutation_strength = 0.01 * (1 - adaptive_factor) ** 1.5\n                    candidate += np.random.normal(0, mutation_strength, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                    candidate_value = func(candidate)\n                    evaluations += 1\n                    if candidate_value < global_best_value:\n                        global_best = candidate\n                        global_best_value = candidate_value\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Introduce dynamic inertia weight adjustment using a sinusoidal function to enhance exploration and exploitation balance.", "configspace": "", "generation": 100, "fitness": 0.9400227008221805, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.940 with standard deviation 0.003. And the mean value of best solutions found was 0.110 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "e8c51766-3f19-4105-912a-b296f8bcd3de", "metadata": {"aucs": [0.9434347753474918, 0.9411476616336125, 0.9354856654854373], "final_y": [0.10964771879842738, 0.11163425595495113, 0.11010880013150537]}, "mutation_prompt": null}
