{"id": "611c8975-b742-44df-bbe3-681d586e296c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGSOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_prob = 0.9\n        self.population = None\n        self.bounds = None\n    \n    def _initialize_population(self, lb, ub):\n        self.population = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        quasi_oppositional = lb + ub - self.population\n        self.population = np.vstack((self.population, quasi_oppositional))\n    \n    def _evaluate_population(self, func):\n        return np.array([func(ind) for ind in self.population])\n    \n    def _mutate(self, target_idx):\n        indices = [idx for idx in range(self.population_size) if idx != target_idx]\n        a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n        mutant = a + self.mutation_factor * (b - c)\n        return np.clip(mutant, self.bounds[0], self.bounds[1])\n    \n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.crossover_prob\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n    \n    def _local_search(self, x0, func):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=self.bounds)\n        return result.x, result.fun\n    \n    def __call__(self, func):\n        self.bounds = np.array([func.bounds.lb, func.bounds.ub])\n        self._initialize_population(func.bounds.lb, func.bounds.ub)\n        eval_count = 0\n        \n        while eval_count < self.budget:\n            fitness = self._evaluate_population(func)\n            eval_count += len(fitness)\n            for i in range(self.population_size):\n                mutant = self._mutate(i)\n                trial = self._crossover(self.population[i], mutant)\n                trial_fit = func(trial)\n                eval_count += 1\n                if trial_fit < fitness[i]:\n                    self.population[i] = trial\n                    fitness[i] = trial_fit\n                if eval_count >= self.budget:\n                    break\n\n            # Local optimization on the best candidate\n            best_idx = np.argmin(fitness)\n            best_candidate = self.population[best_idx]\n            refined_candidate, refined_fit = self._local_search(best_candidate, func)\n            if refined_fit < fitness[best_idx]:\n                self.population[best_idx] = refined_candidate\n                fitness[best_idx] = refined_fit\n                eval_count += 1\n        \n        return self.population[np.argmin(fitness)]", "name": "HybridDEBFGSOptimizer", "description": "A hybrid global-local optimization algorithm combining Differential Evolution and BFGS, leveraging quasi-oppositional sampling and periodicity encouragement through tailored cost functions.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/scratch/hyin/LLaMEA/llamea/llamea.py\", line 194, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratch/hyin/LLaMEA/llamea/llamea.py\", line 288, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratch/hyin/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 149, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 61, in __call__\n  File \"<string>\", line 36, in _local_search\n  File \"/data/hyin/conda_envs/llm/lib/python3.11/site-packages/scipy/optimize/_minimize.py\", line 663, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/hyin/conda_envs/llm/lib/python3.11/site-packages/scipy/optimize/_minimize.py\", line 1043, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/hyin/conda_envs/llm/lib/python3.11/site-packages/scipy/optimize/_constraints.py\", line 429, in old_bound_to_new\n    lb, ub = zip(*bounds)\n    ^^^^^^\nValueError: too many values to unpack (expected 2)\n.", "error": "ValueError('too many values to unpack (expected 2)')Traceback (most recent call last):\n  File \"/scratch/hyin/LLaMEA/llamea/llamea.py\", line 194, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratch/hyin/LLaMEA/llamea/llamea.py\", line 288, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratch/hyin/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 149, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 61, in __call__\n  File \"<string>\", line 36, in _local_search\n  File \"/data/hyin/conda_envs/llm/lib/python3.11/site-packages/scipy/optimize/_minimize.py\", line 663, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/hyin/conda_envs/llm/lib/python3.11/site-packages/scipy/optimize/_minimize.py\", line 1043, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/hyin/conda_envs/llm/lib/python3.11/site-packages/scipy/optimize/_constraints.py\", line 429, in old_bound_to_new\n    lb, ub = zip(*bounds)\n    ^^^^^^\nValueError: too many values to unpack (expected 2)\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "4f4785f5-fbee-49df-9647-5f02769e4664", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic for population size\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.population = None\n        self.best_solution = None\n        self.best_cost = float('inf')\n        self.bounds = None\n\n    def initialize_population(self, lower_bound, upper_bound):\n        # Quasi-Oppositional Initialization\n        self.population = np.random.uniform(lower_bound, upper_bound, (self.population_size, self.dim))\n        self.population = np.concatenate((self.population, lower_bound + upper_bound - self.population[:self.population_size//2]), axis=0)\n\n    def evaluate_population(self, func):\n        costs = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmin(costs)\n        if costs[best_idx] < self.best_cost:\n            self.best_cost = costs[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n        return costs\n\n    def differential_evolution(self, func):\n        for _ in range(self.budget):\n            for i in range(self.population_size):\n                candidates = list(range(self.population_size))\n                candidates.remove(i)\n                a, b, c = self.population[np.random.choice(candidates, 3, replace=False)]\n                mutant_vector = np.clip(a + self.mutation_factor * (b - c), func.bounds.lb, func.bounds.ub)\n                crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n                trial_vector = np.where(crossover_mask, mutant_vector, self.population[i])\n                trial_cost = func(trial_vector)\n                if trial_cost < func(self.population[i]):\n                    self.population[i] = trial_vector\n                    if trial_cost < self.best_cost:\n                        self.best_cost = trial_cost\n                        self.best_solution = trial_vector.copy()\n\n    def local_search(self, func):\n        from scipy.optimize import minimize\n        res = minimize(func, self.best_solution, method='BFGS', bounds=[(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)])\n        if res.fun < self.best_cost:\n            self.best_cost = res.fun\n            self.best_solution = res.x\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(func.bounds.lb, func.bounds.ub)\n        self.budget -= self.population_size\n        self.evaluate_population(func)\n        self.differential_evolution(func)\n        if self.budget > 0:\n            self.local_search(func)\n        return self.best_solution, self.best_cost", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution with Quasi-Oppositional initialization and a Local Search phase to enhance exploration and exploitation in finding near-optimal periodic solutions for complex black box optimization problems.", "configspace": "", "generation": 0, "fitness": 0.6466546570528428, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.647 with standard deviation 0.007. And the mean value of best solutions found was 0.303 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": null, "metadata": {"aucs": [0.6387018861658197, 0.6563184182697681, 0.6449436667229405], "final_y": [0.3134298421743147, 0.3012389772291604, 0.29308253479726465]}, "mutation_prompt": null}
{"id": "ec573d9d-a466-428a-9b70-49debb43f0d6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PeriodicDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * dim\n        self.CR = 0.9  # Crossover probability\n        self.F = 0.8   # Differential weight\n        self.population = None\n        self.bounds = None\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n        # Encourage periodicity by initializing with sinusoidal patterns\n        for i in range(self.pop_size):\n            period = np.random.randint(1, self.dim // 2)\n            self.population[i] = np.sin(np.linspace(0, 2 * np.pi * period, self.dim)) * (ub - lb) / 2 + (lb + ub) / 2\n\n    def evaluate(self, func, x):\n        if self.evaluations >= self.budget:\n            raise Exception(\"Budget exhausted\")\n        self.evaluations += 1\n        return func(x)\n\n    def mutate(self, target_idx):\n        indices = list(range(self.pop_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n        return np.clip(mutant, self.bounds.lb, self.bounds.ub)\n\n    def crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def local_optimization(self, best_individual, func):\n        res = minimize(func, best_individual, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(i)\n                trial = self.crossover(target, mutant)\n                trial_score = self.evaluate(func, trial)\n                target_score = self.evaluate(func, target)\n\n                if trial_score < target_score:\n                    self.population[i] = trial\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        best_solution = trial\n\n                if self.evaluations >= self.budget:\n                    break\n\n        # Fine-tune the best solution found\n        if best_solution is not None:\n            best_solution = self.local_optimization(best_solution, func)\n\n        return best_solution", "name": "PeriodicDifferentialEvolution", "description": "This algorithm combines Differential Evolution with a periodicity-inducing strategy and local optimization to efficiently explore and fine-tune solutions for multilayer photonic structure optimization.", "configspace": "", "generation": 0, "fitness": 0.5928398282724903, "feedback": "The algorithm PeriodicDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.593 with standard deviation 0.022. And the mean value of best solutions found was 0.265 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": null, "metadata": {"aucs": [0.6172975957697536, 0.564755672509929, 0.5964662165377881], "final_y": [0.2897341567068329, 0.24657445253982535, 0.25916317953108414]}, "mutation_prompt": null}
{"id": "7e9433d9-9a30-456f-8a60-4ddc21c7b3ca", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def _initialize_population(self, size, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (size, self.dim))\n        # Symmetric initialization to enhance exploration\n        pop[:size//2] = lb + (ub - pop[size//2:])\n        return pop\n\n    def _differential_evolution(self, pop, func, bounds, F=0.8, CR=0.9):\n        size = len(pop)\n        for i in range(size):\n            # Randomly select three distinct vectors\n            indices = np.random.choice(np.delete(np.arange(size), i), 3, replace=False)\n            a, b, c = pop[indices]\n            # Mutation\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            # Recombination\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            # Selection\n            if func(trial) < func(pop[i]):\n                pop[i] = trial\n        return pop\n\n    def _local_search(self, solution, func, bounds):\n        # Local refinement using BFGS\n        res = minimize(func, solution, method='L-BFGS-B', bounds=np.vstack((bounds.lb, bounds.ub)).T)\n        return res.x\n\n    def _periodic_cost(self, solution):\n        # Custom cost function to encourage periodicity\n        period_length = self.dim // 2\n        periodic_part = solution[:period_length]\n        periodicity_penalty = np.sum((solution - np.tile(periodic_part, self.dim // period_length))**2)\n        return periodicity_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        num_generations = self.budget // population_size\n        pop = self._initialize_population(population_size, bounds)\n\n        for _ in range(num_generations):\n            pop = self._differential_evolution(pop, lambda x: func(x) + self._periodic_cost(x), bounds)\n            best_idx = np.argmin([func(ind) for ind in pop])\n            best_solution = pop[best_idx]\n            refined_solution = self._local_search(best_solution, func, bounds)\n            # Replace worst with refined solution to maintain diversity\n            worst_idx = np.argmax([func(ind) for ind in pop])\n            pop[worst_idx] = refined_solution\n\n        # Return the best found solution\n        best_idx = np.argmin([func(ind) for ind in pop])\n        return pop[best_idx]", "name": "HybridOptimization", "description": "Hybrid Global-Local Optimization Algorithm that combines Differential Evolution with a Local Search strategy to efficiently explore and exploit the search space, promoting periodicity through a tailored cost function and symmetric initialization to solve black box optimization problems.", "configspace": "", "generation": 0, "fitness": 0.9657477450252127, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.018. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.9740715196429836, 0.9820971030854199, 0.9410746123472347], "final_y": [0.16485675710728187, 0.16485893586067923, 0.16485607939286995]}, "mutation_prompt": null}
{"id": "c08a9492-2f16-4b98-aaa0-b3de4b9fdc8e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.current_evals = 0\n    \n    def __call__(self, func):\n        # Initialize population within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        self.current_evals += self.population_size\n        \n        while self.current_evals < self.budget:\n            # Differential Evolution mutation and crossover\n            for i in range(self.population_size):\n                idxs = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                \n                # Encourage periodicity by averaging layer thicknesses to nearby values\n                mutant = self._encourage_periodicity(mutant)\n                \n                # Crossover\n                crossover = np.where(np.random.rand(self.dim) < 0.9, mutant, population[i])\n                \n                # Evaluate new solution\n                new_fit = func(crossover)\n                self.current_evals += 1\n                \n                # Selection\n                if new_fit < fitness[i]:\n                    population[i] = crossover\n                    fitness[i] = new_fit\n                    \n                if self.current_evals >= self.budget:\n                    break\n            \n            # Local optimization with BFGS on best individual\n            best_idx = np.argmin(fitness)\n            res = minimize(func, population[best_idx], method='L-BFGS-B', bounds=list(zip(lb, ub)))\n            if res.fun < fitness[best_idx]:\n                population[best_idx] = res.x\n                fitness[best_idx] = res.fun\n                self.current_evals += res.nfev\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _encourage_periodicity(self, individual):\n        # Adjust layers to encourage periodicity by averaging thicknesses\n        period = 2  # Simple 2-period adjustment\n        for i in range(0, self.dim, period):\n            if i + 1 < self.dim:\n                avg = (individual[i] + individual[i+1]) / 2\n                individual[i], individual[i+1] = avg, avg\n        return individual", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic combining Differential Evolution with periodicity encouragement and local refinement via BFGS for optimizing multilayer structures.", "configspace": "", "generation": 0, "fitness": 0.976929684024415, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.012. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.9761605156710158, 0.9621551599240464, 0.9924733764781827], "final_y": [0.16485599497101355, 0.1648569774603369, 0.16485626849552248]}, "mutation_prompt": null}
{"id": "a18c5b11-6e97-45ea-a77c-e0a56aba64dc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PeriodicDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * dim\n        self.CR = 0.9  # Crossover probability\n        self.F = 0.8   # Differential weight\n        self.population = None\n        self.bounds = None\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n        for i in range(self.pop_size):\n            period = np.random.randint(1, self.dim // 2)\n            self.population[i] = np.sin(np.linspace(0, 2 * np.pi * period, self.dim)) * (ub - lb) / 2 + (lb + ub) / 2\n\n    def evaluate(self, func, x):\n        if self.evaluations >= self.budget:\n            raise Exception(\"Budget exhausted\")\n        self.evaluations += 1\n        return func(x)\n\n    def mutate(self, target_idx):\n        indices = list(range(self.pop_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n        # Encourage periodicity by applying a sine modulation\n        modulation = np.sin(np.arange(self.dim) * 2 * np.pi / self.dim)\n        mutant = mutant * modulation\n        return np.clip(mutant, self.bounds.lb, self.bounds.ub)\n\n    def crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def local_optimization(self, best_individual, func):\n        res = minimize(func, best_individual, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(i)\n                trial = self.crossover(target, mutant)\n                trial_score = self.evaluate(func, trial)\n                target_score = self.evaluate(func, target)\n\n                if trial_score < target_score:\n                    self.population[i] = trial\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        best_solution = trial\n\n                if self.evaluations >= self.budget:\n                    break\n\n        if best_solution is not None:\n            best_solution = self.local_optimization(best_solution, func)\n\n        return best_solution", "name": "PeriodicDifferentialEvolution", "description": "Enhanced Differential Evolution with adaptive periodicity encouragement and strategic mutation for improved exploration and solution refinement.", "configspace": "", "generation": 1, "fitness": 0.5670399098297982, "feedback": "The algorithm PeriodicDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.567 with standard deviation 0.115. And the mean value of best solutions found was 0.344 (0. is the best) with standard deviation 0.096.", "error": "", "parent_id": "ec573d9d-a466-428a-9b70-49debb43f0d6", "metadata": {"aucs": [0.40538102337217286, 0.6283702373129988, 0.667368468804223], "final_y": [0.47815060022762, 0.29123417485093017, 0.26142543275203667]}, "mutation_prompt": null}
{"id": "88dda011-14c5-4d65-a597-0e2349cff665", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PeriodicDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * dim\n        self.CR = 0.9  # Crossover probability\n        self.F = 0.8   # Differential weight\n        self.population = None\n        self.bounds = None\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n        # Encourage periodicity by initializing with sinusoidal patterns\n        for i in range(self.pop_size):\n            period = np.random.randint(1, self.dim // 2)\n            self.population[i] = np.sin(np.linspace(0, 2 * np.pi * period, self.dim)) * (ub - lb) / 2 + (lb + ub) / 2\n\n    def evaluate(self, func, x):\n        if self.evaluations >= self.budget:\n            raise Exception(\"Budget exhausted\")\n        self.evaluations += 1\n        return func(x)\n\n    def mutate(self, target_idx):\n        indices = list(range(self.pop_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n        return np.clip(mutant, self.bounds.lb, self.bounds.ub)\n\n    def crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def local_optimization(self, best_individual, func):\n        res = minimize(func, best_individual, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)), options={'ftol': 1e-10})\n        return res.x\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(i)\n                trial = self.crossover(target, mutant)\n                trial_score = self.evaluate(func, trial)\n                target_score = self.evaluate(func, target)\n\n                if trial_score < target_score:\n                    self.population[i] = trial\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        best_solution = trial\n\n                if self.evaluations >= self.budget:\n                    break\n\n        # Fine-tune the best solution found\n        if best_solution is not None:\n            best_solution = self.local_optimization(best_solution, func)\n\n        return best_solution", "name": "PeriodicDifferentialEvolution", "description": "Enhanced local optimization strategy by adjusting solver options for improved convergence in the PeriodicDifferentialEvolution algorithm.", "configspace": "", "generation": 1, "fitness": 0.6830538552850859, "feedback": "The algorithm PeriodicDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.683 with standard deviation 0.017. And the mean value of best solutions found was 0.238 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "ec573d9d-a466-428a-9b70-49debb43f0d6", "metadata": {"aucs": [0.6991866317288749, 0.6901188103061849, 0.6598561238201974], "final_y": [0.22393227216490164, 0.2367993981284523, 0.2545503481908521]}, "mutation_prompt": null}
{"id": "4c4bc085-f442-4bfe-b53a-c2cfc3c8dbc0", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic for population size\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.population = None\n        self.best_solution = None\n        self.best_cost = float('inf')\n        self.bounds = None\n\n    def initialize_population(self, lower_bound, upper_bound):\n        # Quasi-Oppositional Initialization\n        self.population = np.random.uniform(lower_bound, upper_bound, (self.population_size, self.dim))\n        self.population = np.concatenate((self.population, lower_bound + upper_bound - self.population[:self.population_size//2]), axis=0)\n\n    def evaluate_population(self, func):\n        costs = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmin(costs)\n        if costs[best_idx] < self.best_cost:\n            self.best_cost = costs[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n        return costs\n\n    def differential_evolution(self, func):\n        for _ in range(self.budget):\n            for i in range(self.population_size):\n                candidates = list(range(self.population_size))\n                candidates.remove(i)\n                a, b, c = self.population[np.random.choice(candidates, 3, replace=False)]\n                mutant_vector = np.clip(a + self.mutation_factor * (b - c), func.bounds.lb, func.bounds.ub)\n                crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n                trial_vector = np.where(crossover_mask, mutant_vector, self.population[i])\n                trial_cost = func(trial_vector)\n                if trial_cost < func(self.population[i]):\n                    self.population[i] = trial_vector\n                    if trial_cost < self.best_cost:\n                        self.best_cost = trial_cost\n                        self.best_solution = trial_vector.copy()\n\n    def local_search(self, func):\n        from scipy.optimize import minimize\n        res = minimize(func, self.best_solution, method='trust-constr', bounds=[(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)])\n        if res.fun < self.best_cost:\n            self.best_cost = res.fun\n            self.best_solution = res.x\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(func.bounds.lb, func.bounds.ub)\n        self.budget -= self.population_size\n        self.evaluate_population(func)\n        self.differential_evolution(func)\n        if self.budget > 0:\n            self.local_search(func)\n        return self.best_solution, self.best_cost", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search by employing 'trust-constr' method instead of 'BFGS' for improved solution refinement in complex optimization landscapes.", "configspace": "", "generation": 1, "fitness": 0.6398057347548637, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.640 with standard deviation 0.050. And the mean value of best solutions found was 0.284 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "4f4785f5-fbee-49df-9647-5f02769e4664", "metadata": {"aucs": [0.7039917622454757, 0.5827165955147551, 0.6327088465043602], "final_y": [0.26364718772286133, 0.27081548412251777, 0.3168080089689559]}, "mutation_prompt": null}
{"id": "c28a422c-8193-4f5b-9d03-647a4f895399", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def _initialize_population(self, size, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (size, self.dim))\n        pop[:size//2] = lb + (ub - pop[size//2:])\n        return pop\n\n    def _adaptive_mutation_factor(self, generation, max_generations):\n        return 0.5 + (0.9 - 0.5) * (1 - generation / max_generations)\n\n    def _crowding_distance(self, pop, func):\n        fitness = np.array([func(ind) for ind in pop])\n        order = np.argsort(fitness)\n        distances = np.zeros(len(pop))\n        for i in range(1, len(pop) - 1):\n            distances[order[i]] = fitness[order[i + 1]] - fitness[order[i - 1]]\n        return distances\n\n    def _differential_evolution(self, pop, func, bounds, generation, max_generations):\n        size = len(pop)\n        F = self._adaptive_mutation_factor(generation, max_generations)\n        CR = 0.9\n        for i in range(size):\n            indices = np.random.choice(np.delete(np.arange(size), i), 3, replace=False)\n            a, b, c = pop[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                pop[i] = trial\n        return pop\n\n    def _local_search(self, solution, func, bounds):\n        res = minimize(func, solution, method='L-BFGS-B', bounds=np.vstack((bounds.lb, bounds.ub)).T)\n        return res.x\n\n    def _periodic_cost(self, solution):\n        period_length = self.dim // 2\n        periodic_part = solution[:period_length]\n        periodicity_penalty = np.sum((solution - np.tile(periodic_part, self.dim // period_length))**2)\n        return periodicity_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        num_generations = self.budget // population_size\n        pop = self._initialize_population(population_size, bounds)\n\n        for generation in range(num_generations):\n            pop = self._differential_evolution(pop, lambda x: func(x) + self._periodic_cost(x), bounds, generation, num_generations)\n            best_idx = np.argmin([func(ind) for ind in pop])\n            best_solution = pop[best_idx]\n            refined_solution = self._local_search(best_solution, func, bounds)\n            worst_idx = np.argmax(self._crowding_distance(pop, func))\n            pop[worst_idx] = refined_solution\n\n        best_idx = np.argmin([func(ind) for ind in pop])\n        return pop[best_idx]", "name": "HybridOptimization", "description": "Enhanced HybridOptimization algorithm integrating adaptive mutation factor and crowding distance to improve diversity and convergence in multilayer optimization.", "configspace": "", "generation": 1, "fitness": 0.9514967994246533, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.951 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "7e9433d9-9a30-456f-8a60-4ddc21c7b3ca", "metadata": {"aucs": [0.9390236027499114, 0.9286211191609697, 0.9868456763630787], "final_y": [0.1818868392322217, 0.16485618972937288, 0.16485743573982503]}, "mutation_prompt": null}
{"id": "7789afbd-eef1-4770-98dc-6fb16c2c2ebf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PeriodicDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * dim\n        self.CR = 0.9  # Crossover probability\n        self.F = 0.8   # Differential weight\n        self.population = None\n        self.bounds = None\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n        # Encourage periodicity by initializing with sinusoidal patterns\n        for i in range(self.pop_size):\n            period = np.random.randint(1, self.dim // 2)\n            self.population[i] = np.sin(np.linspace(0, 2 * np.pi * period, self.dim)) * (ub - lb) / 2 + (lb + ub) / 2\n\n    def evaluate(self, func, x):\n        if self.evaluations >= self.budget:\n            raise Exception(\"Budget exhausted\")\n        self.evaluations += 1\n        return func(x)\n\n    def mutate(self, target_idx):\n        indices = list(range(self.pop_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        d = np.std(self.population, axis=0)  # Population diversity measure\n        F_adaptive = self.F * (1 + np.mean(d))  # Adaptive scaling factor\n        mutant = self.population[a] + F_adaptive * (self.population[b] - self.population[c])\n        return np.clip(mutant, self.bounds.lb, self.bounds.ub)\n\n    def crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def local_optimization(self, best_individual, func):\n        res = minimize(func, best_individual, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(i)\n                trial = self.crossover(target, mutant)\n                trial_score = self.evaluate(func, trial)\n                target_score = self.evaluate(func, target)\n\n                if trial_score < target_score:\n                    self.population[i] = trial\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        best_solution = trial\n\n                if self.evaluations >= self.budget:\n                    break\n\n        # Fine-tune the best solution found\n        if best_solution is not None:\n            best_solution = self.local_optimization(best_solution, func)\n\n        return best_solution", "name": "PeriodicDifferentialEvolution", "description": "Enhanced mutation strategy with adaptive F scaling based on population diversity to improve exploration and convergence.", "configspace": "", "generation": 1, "fitness": 0.5414344620333919, "feedback": "The algorithm PeriodicDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.541 with standard deviation 0.142. And the mean value of best solutions found was 0.357 (0. is the best) with standard deviation 0.098.", "error": "", "parent_id": "ec573d9d-a466-428a-9b70-49debb43f0d6", "metadata": {"aucs": [0.34384553764881953, 0.6108366817420344, 0.6696211667093218], "final_y": [0.4950280879183159, 0.3027928128939833, 0.2741521474568316]}, "mutation_prompt": null}
{"id": "755b68aa-2851-42f9-8585-56760c373d95", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def _initialize_population(self, size, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (size, self.dim))\n        pop[:size//2] = lb + (ub - pop[size//2:])\n        return pop\n\n    def _adaptive_mutation_factor(self, generation, max_generations):\n        return 0.7 + (0.9 - 0.7) * (1 - generation / max_generations)  # Adjusted mutation factor range\n\n    def _crowding_distance(self, pop, func):\n        fitness = np.array([func(ind) for ind in pop])\n        order = np.argsort(fitness)\n        distances = np.zeros(len(pop))\n        for i in range(1, len(pop) - 1):\n            distances[order[i]] = fitness[order[i + 1]] - fitness[order[i - 1]]\n        return distances\n\n    def _differential_evolution(self, pop, func, bounds, generation, max_generations):\n        size = len(pop)\n        F = self._adaptive_mutation_factor(generation, max_generations)\n        CR = 0.8  # Reduced crossover rate\n        for i in range(size):\n            indices = np.random.choice(np.delete(np.arange(size), i), 3, replace=False)\n            a, b, c = pop[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                pop[i] = trial\n        return pop\n\n    def _local_search(self, solution, func, bounds):\n        res = minimize(func, solution, method='L-BFGS-B', bounds=np.vstack((bounds.lb, bounds.ub)).T)\n        return res.x\n\n    def _periodic_cost(self, solution):\n        period_length = self.dim // 2\n        periodic_part = solution[:period_length]\n        periodicity_penalty = np.sum((solution - np.tile(periodic_part, self.dim // period_length))**2)\n        return periodicity_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        num_generations = self.budget // population_size\n        pop = self._initialize_population(population_size, bounds)\n        best_solution = None\n\n        for generation in range(num_generations):\n            pop = self._differential_evolution(pop, lambda x: func(x) + self._periodic_cost(x), bounds, generation, num_generations)\n            current_best_idx = np.argmin([func(ind) for ind in pop])\n            current_best_solution = pop[current_best_idx]\n            if best_solution is None or func(current_best_solution) < func(best_solution):\n                best_solution = current_best_solution  # Retain elitism\n            refined_solution = self._local_search(current_best_solution, func, bounds)\n            worst_idx = np.argmax(self._crowding_distance(pop, func))\n            pop[worst_idx] = refined_solution\n\n        return best_solution", "name": "HybridOptimization", "description": "Improved HybridOptimization algorithm by introducing elitism to retain the best solutions and adjusted differential evolution parameters for enhanced performance in multilayer optimization.", "configspace": "", "generation": 2, "fitness": 0.9369496233450595, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.937 with standard deviation 0.003. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "c28a422c-8193-4f5b-9d03-647a4f895399", "metadata": {"aucs": [0.9390948432589171, 0.9385670525173025, 0.9331869742589589], "final_y": [0.1818868392322217, 0.18187826708463484, 0.16485917242967174]}, "mutation_prompt": null}
{"id": "c34f6045-d872-44f4-b09c-a5e40a23ab89", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def _initialize_population(self, size, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (size, self.dim))\n        # Symmetric initialization to enhance exploration\n        pop[:size//2] = lb + (ub - pop[size//2:])\n        return pop\n\n    def _differential_evolution(self, pop, func, bounds, F=0.8, CR=0.9):\n        size = len(pop)\n        for i in range(size):\n            # Randomly select three distinct vectors\n            indices = np.random.choice(np.delete(np.arange(size), i), 3, replace=False)\n            a, b, c = pop[indices]\n            # Adaptive Mutation\n            F = 0.5 + 0.3 * np.random.rand()\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            # Adaptive Recombination\n            CR = 0.5 + 0.4 * np.random.rand()\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            # Selection\n            if func(trial) < func(pop[i]):\n                pop[i] = trial\n        return pop\n\n    def _local_search(self, solution, func, bounds):\n        # Local refinement using BFGS\n        res = minimize(func, solution, method='L-BFGS-B', bounds=np.vstack((bounds.lb, bounds.ub)).T)\n        return res.x\n\n    def _periodic_cost(self, solution):\n        # Custom cost function to encourage periodicity\n        period_length = self.dim // 2\n        periodic_part = solution[:period_length]\n        periodicity_penalty = np.sum((solution - np.tile(periodic_part, self.dim // period_length))**2)\n        return periodicity_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        num_generations = self.budget // population_size\n        pop = self._initialize_population(population_size, bounds)\n\n        for _ in range(num_generations):\n            pop = self._differential_evolution(pop, lambda x: func(x) + self._periodic_cost(x), bounds)\n            best_idx = np.argmin([func(ind) for ind in pop])\n            best_solution = pop[best_idx]\n            refined_solution = self._local_search(best_solution, func, bounds)\n            # Replace worst with refined solution to maintain diversity\n            worst_idx = np.argmax([func(ind) for ind in pop])\n            pop[worst_idx] = refined_solution\n\n        # Return the best found solution\n        best_idx = np.argmin([func(ind) for ind in pop])\n        return pop[best_idx]", "name": "HybridOptimization", "description": "Enhanced HybridOptimization algorithm with adaptive F and CR parameters in Differential Evolution for improved diversity and convergence.", "configspace": "", "generation": 2, "fitness": 0.9176821433270539, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.918 with standard deviation 0.027. And the mean value of best solutions found was 0.188 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "7e9433d9-9a30-456f-8a60-4ddc21c7b3ca", "metadata": {"aucs": [0.9391178879225027, 0.9349515174128995, 0.8789770246457597], "final_y": [0.1818868392322217, 0.18187968060544402, 0.20044717677401724]}, "mutation_prompt": null}
{"id": "6fb3b9bd-28b3-47ce-9c92-02d1e0f8130e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PeriodicDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * dim\n        self.CR = 0.9  # Crossover probability\n        self.F = 0.8   # Differential weight\n        self.population = None\n        self.bounds = None\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.rand(self.pop_size, self.dim) * (ub - lb) + lb\n        # Encourage periodicity by initializing with sinusoidal patterns\n        for i in range(self.pop_size):\n            period = np.random.randint(1, self.dim // 2)\n            self.population[i] = np.sin(np.linspace(0, 2 * np.pi * period, self.dim)) * (ub - lb) / 2 + (lb + ub) / 2\n\n    def evaluate(self, func, x):\n        if self.evaluations >= self.budget:\n            raise Exception(\"Budget exhausted\")\n        self.evaluations += 1\n        return func(x)\n\n    def mutate(self, target_idx):\n        indices = list(range(self.pop_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n        return np.clip(mutant, self.bounds.lb, self.bounds.ub)\n\n    def crossover(self, target, mutant):\n        # Adapt CR based on evaluations\n        self.CR = 0.9 - 0.9 * (self.evaluations / self.budget)\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def local_optimization(self, best_individual, func):\n        res = minimize(func, best_individual, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)), options={'ftol': 1e-10})\n        return res.x\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(self.bounds.lb, self.bounds.ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(i)\n                trial = self.crossover(target, mutant)\n                trial_score = self.evaluate(func, trial)\n                target_score = self.evaluate(func, target)\n\n                if trial_score < target_score:\n                    self.population[i] = trial\n                    if trial_score < best_score:\n                        best_score = trial_score\n                        best_solution = trial\n\n                if self.evaluations >= self.budget:\n                    break\n\n        # Fine-tune the best solution found\n        if best_solution is not None:\n            best_solution = self.local_optimization(best_solution, func)\n\n        return best_solution", "name": "PeriodicDifferentialEvolution", "description": "Integrates adaptive crossover probability in PeriodicDifferentialEvolution to enhance exploration and prevent premature convergence.", "configspace": "", "generation": 2, "fitness": 0.6063894998344181, "feedback": "The algorithm PeriodicDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.606 with standard deviation 0.017. And the mean value of best solutions found was 0.276 (0. is the best) with standard deviation 0.025.", "error": "", "parent_id": "88dda011-14c5-4d65-a597-0e2349cff665", "metadata": {"aucs": [0.6030744470207751, 0.628562697657699, 0.5875313548247802], "final_y": [0.24448813853906137, 0.2797937479129936, 0.3047091131405053]}, "mutation_prompt": null}
{"id": "9900b132-639d-42db-a8c2-8a548f728d54", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.current_evals = 0\n    \n    def __call__(self, func):\n        # Initialize population within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        self.current_evals += self.population_size\n        \n        while self.current_evals < self.budget:\n            # Differential Evolution mutation and crossover\n            for i in range(self.population_size):\n                idxs = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F = 0.5 + 0.3 * (self.budget - self.current_evals) / self.budget  # Adaptive mutation factor\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                # Encourage periodicity by averaging layer thicknesses to nearby values\n                mutant = self._encourage_periodicity(mutant)\n                \n                # Crossover\n                crossover = np.where(np.random.rand(self.dim) < 0.9, mutant, population[i])\n                \n                # Evaluate new solution\n                new_fit = func(crossover)\n                self.current_evals += 1\n                \n                # Selection\n                if new_fit < fitness[i]:\n                    population[i] = crossover\n                    fitness[i] = new_fit\n                    \n                if self.current_evals >= self.budget:\n                    break\n            \n            # Local optimization with BFGS on best individual\n            best_idx = np.argmin(fitness)\n            res = minimize(func, population[best_idx], method='L-BFGS-B', bounds=list(zip(lb, ub)))\n            if res.fun < fitness[best_idx]:\n                population[best_idx] = res.x\n                fitness[best_idx] = res.fun\n                self.current_evals += res.nfev\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _encourage_periodicity(self, individual):\n        # Adjust layers to encourage periodicity by averaging thicknesses\n        period = 2  # Simple 2-period adjustment\n        for i in range(0, self.dim, period):\n            if i + 1 < self.dim:\n                avg = (individual[i] + individual[i+1]) / 2\n                individual[i], individual[i+1] = avg, avg\n        return individual", "name": "HybridMetaheuristicOptimizer", "description": "Improved solution exploration by increasing population diversity and adjusting DE mutation factor adaptively to enhance convergence.", "configspace": "", "generation": 2, "fitness": 0.7602485095249536, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.760 with standard deviation 0.142. And the mean value of best solutions found was 0.237 (0. is the best) with standard deviation 0.062.", "error": "", "parent_id": "c08a9492-2f16-4b98-aaa0-b3de4b9fdc8e", "metadata": {"aucs": [0.9578391745805739, 0.6904914572688997, 0.6324148967253873], "final_y": [0.1648568938315682, 0.22863208741872987, 0.3168080089689559]}, "mutation_prompt": null}
{"id": "190a599d-052c-482f-ad9e-0f3e5bfe18b3", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Heuristic for population size\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.population = None\n        self.best_solution = None\n        self.best_cost = float('inf')\n        self.bounds = None\n\n    def initialize_population(self, lower_bound, upper_bound):\n        # Quasi-Oppositional Initialization\n        self.population = np.random.uniform(lower_bound, upper_bound, (self.population_size, self.dim))\n        self.population = np.concatenate((self.population, lower_bound + upper_bound - self.population[:self.population_size//2]), axis=0)\n\n    def evaluate_population(self, func):\n        costs = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmin(costs)\n        if costs[best_idx] < self.best_cost:\n            self.best_cost = costs[best_idx]\n            self.best_solution = self.population[best_idx].copy()\n        return costs\n\n    def differential_evolution(self, func):\n        for _ in range(self.budget):\n            for i in range(self.population_size):\n                candidates = list(range(self.population_size))\n                candidates.remove(i)\n                a, b, c = self.population[np.random.choice(candidates, 3, replace=False)]\n                mutant_vector = np.clip(a + 0.9 * (b - c), func.bounds.lb, func.bounds.ub)  # Adjusted mutation factor\n                crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n                trial_vector = np.where(crossover_mask, mutant_vector, self.population[i])\n                trial_cost = func(trial_vector)\n                if trial_cost < func(self.population[i]):\n                    self.population[i] = trial_vector\n                    if trial_cost < self.best_cost:\n                        self.best_cost = trial_cost\n                        self.best_solution = trial_vector.copy()\n\n    def local_search(self, func):\n        from scipy.optimize import minimize\n        res = minimize(func, self.best_solution, method='BFGS', bounds=[(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)])\n        if res.fun < self.best_cost:\n            self.best_cost = res.fun\n            self.best_solution = res.x\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        self.initialize_population(func.bounds.lb, func.bounds.ub)\n        self.budget -= self.population_size\n        self.evaluate_population(func)\n        self.differential_evolution(func)\n        if self.budget > 0:\n            self.local_search(func)\n        return self.best_solution, self.best_cost", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation factor adaptation in Differential Evolution for improved exploration and convergence.", "configspace": "", "generation": 2, "fitness": 0.6161038645487622, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.616 with standard deviation 0.030. And the mean value of best solutions found was 0.323 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "4f4785f5-fbee-49df-9647-5f02769e4664", "metadata": {"aucs": [0.6414788407471527, 0.5744178561737466, 0.6324148967253873], "final_y": [0.3071101538379436, 0.34600246746530494, 0.3168080089689559]}, "mutation_prompt": null}
{"id": "c3b94854-bd51-4df5-ba3a-06ed5b36038f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def _initialize_population(self, size, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (size, self.dim))\n        pop[:size//2] = lb + (ub - pop[size//2:])\n        return pop\n\n    def _adaptive_mutation_factor(self, generation, max_generations):\n        return 0.7 + (0.9 - 0.7) * (1 - generation / max_generations)  # Adjusted mutation factor range\n\n    def _crowding_distance(self, pop, func):\n        fitness = np.array([func(ind) for ind in pop])\n        order = np.argsort(fitness)\n        distances = np.zeros(len(pop))\n        for i in range(1, len(pop) - 1):\n            distances[order[i]] = fitness[order[i + 1]] - fitness[order[i - 1]]\n        return distances\n\n    def _differential_evolution(self, pop, func, bounds, generation, max_generations):\n        size = len(pop)\n        F = self._adaptive_mutation_factor(generation, max_generations)\n        CR = 0.5 + 0.3 * (generation / max_generations)  # Introduced dynamic crossover rate\n        for i in range(size):\n            indices = np.random.choice(np.delete(np.arange(size), i), 3, replace=False)\n            a, b, c = pop[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                pop[i] = trial\n        return pop\n\n    def _local_search(self, solution, func, bounds):\n        res = minimize(func, solution, method='L-BFGS-B', bounds=np.vstack((bounds.lb, bounds.ub)).T)\n        return res.x\n\n    def _periodic_cost(self, solution):\n        period_length = self.dim // 2\n        periodic_part = solution[:period_length]\n        periodicity_penalty = np.sum((solution - np.tile(periodic_part, self.dim // period_length))**2)\n        return periodicity_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        num_generations = self.budget // population_size\n        pop = self._initialize_population(population_size, bounds)\n        best_solution = None\n\n        for generation in range(num_generations):\n            pop = self._differential_evolution(pop, lambda x: func(x) + self._periodic_cost(x), bounds, generation, num_generations)\n            current_best_idx = np.argmin([func(ind) for ind in pop])\n            current_best_solution = pop[current_best_idx]\n            if best_solution is None or func(current_best_solution) < func(best_solution):\n                best_solution = current_best_solution  # Retain elitism\n            refined_solution = self._local_search(current_best_solution, func, bounds)\n            worst_idx = np.argmax(self._crowding_distance(pop, func))\n            pop[worst_idx] = refined_solution\n\n        return best_solution", "name": "HybridOptimization", "description": "Enhanced the crossover strategy by introducing a dynamic crossover rate to improve diversity in the search process.", "configspace": "", "generation": 3, "fitness": 0.8990529448487828, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.899 with standard deviation 0.088. And the mean value of best solutions found was 0.196 (0. is the best) with standard deviation 0.033.", "error": "", "parent_id": "755b68aa-2851-42f9-8585-56760c373d95", "metadata": {"aucs": [0.9391038327572963, 0.9808397545957166, 0.7772152471933356], "final_y": [0.1818868392322217, 0.1648567243809923, 0.24257563538856264]}, "mutation_prompt": null}
{"id": "2af5f37f-f6c5-4235-8875-f2e1f27012b3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def _initialize_population(self, size, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (size, self.dim))\n        pop[:size//2] = lb + (ub - pop[size//2:])\n        return pop\n\n    def _differential_evolution(self, pop, func, bounds, F=0.8, CR=0.9):\n        size = len(pop)\n        for i in range(size):\n            indices = np.random.choice(np.delete(np.arange(size), i), 3, replace=False)\n            a, b, c = pop[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                pop[i] = trial\n        return pop\n\n    def _adaptive_local_search(self, solution, func, bounds):\n        res = minimize(func, solution, method='L-BFGS-B', bounds=np.vstack((bounds.lb, bounds.ub)).T)\n        return res.x\n\n    def _adaptive_periodic_cost(self, solution):\n        adaptive_period_length = max(1, self.dim // 4)\n        periodic_part = solution[:adaptive_period_length]\n        periodicity_penalty = np.sum((solution - np.tile(periodic_part, self.dim // adaptive_period_length))**2)\n        return periodicity_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        num_generations = self.budget // population_size\n        pop = self._initialize_population(population_size, bounds)\n\n        for _ in range(num_generations):\n            pop = self._differential_evolution(pop, lambda x: func(x) + self._adaptive_periodic_cost(x), bounds)\n            best_idx = np.argmin([func(ind) for ind in pop])\n            best_solution = pop[best_idx]\n            refined_solution = self._adaptive_local_search(best_solution, func, bounds)\n            worst_idx = np.argmax([func(ind) for ind in pop])\n            pop[worst_idx] = refined_solution\n\n        best_idx = np.argmin([func(ind) for ind in pop])\n        return pop[best_idx]", "name": "HybridOptimization", "description": "Enhanced HybridOptimization algorithm combining Differential Evolution with adaptive local search using L-BFGS-B and improved periodicity enforcement through adaptive period length for efficient multilayer optimization.", "configspace": "", "generation": 3, "fitness": 0.9638908724166723, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.022. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "7e9433d9-9a30-456f-8a60-4ddc21c7b3ca", "metadata": {"aucs": [0.9701425370198189, 0.9346689858548555, 0.9868610943753424], "final_y": [0.16485913850972733, 0.18187968060544402, 0.16485743573982503]}, "mutation_prompt": null}
{"id": "d935d5f9-4697-4442-a354-79b08b2f124d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def _initialize_population(self, size, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        # Enhanced initialization using Sobol sequences for better coverage\n        sampler = Sobol(d=self.dim, scramble=True)\n        pop = lb + (ub - lb) * sampler.random(size)\n        pop[:size // 2] = lb + (ub - pop[size // 2:])\n        return pop\n\n    def _differential_evolution(self, pop, func, bounds, F=0.8, CR=0.9):\n        size = len(pop)\n        for i in range(size):\n            indices = np.random.choice(np.delete(np.arange(size), i), 3, replace=False)\n            a, b, c = pop[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                pop[i] = trial\n        return pop\n\n    def _local_search(self, solution, func, bounds):\n        res = minimize(func, solution, method='L-BFGS-B', bounds=np.vstack((bounds.lb, bounds.ub)).T)\n        return res.x\n\n    def _periodic_cost(self, solution):\n        period_length = self.dim // 2\n        periodic_part = solution[:period_length]\n        periodicity_penalty = np.sum((solution - np.tile(periodic_part, self.dim // period_length))**2)\n        return periodicity_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        num_generations = self.budget // population_size\n        pop = self._initialize_population(population_size, bounds)\n\n        for gen in range(num_generations):\n            pop = self._differential_evolution(pop, lambda x: func(x) + self._periodic_cost(x), bounds)\n            if gen % 5 == 0:  # Adaptive local search invocation\n                best_idx = np.argmin([func(ind) for ind in pop])\n                best_solution = pop[best_idx]\n                refined_solution = self._local_search(best_solution, func, bounds)\n                worst_idx = np.argmax([func(ind) for ind in pop])\n                pop[worst_idx] = refined_solution\n\n        best_idx = np.argmin([func(ind) for ind in pop])\n        return pop[best_idx]", "name": "HybridOptimization", "description": "Improved HybridOptimization algorithm with enhanced initialization using Sobol sequences and adaptive local search invocation for efficient exploration and refinement.", "configspace": "", "generation": 3, "fitness": 0.903851240693213, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.046. And the mean value of best solutions found was 0.190 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "7e9433d9-9a30-456f-8a60-4ddc21c7b3ca", "metadata": {"aucs": [0.9384445275839914, 0.9339073144202967, 0.839201880075351], "final_y": [0.1818812917836905, 0.18188104395377258, 0.20725640443260218]}, "mutation_prompt": null}
{"id": "8b7b7819-66ad-4e73-a187-060bd968ef9a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def _initialize_population(self, size, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (size, self.dim))\n        pop[:size//2] = lb + (ub - pop[size//2:])\n        return pop\n\n    def _adaptive_mutation_factor(self, generation, max_generations):\n        return 0.7 + (0.9 - 0.7) * (1 - generation / max_generations)  # Adjusted mutation factor range\n\n    def _crowding_distance(self, pop, func):\n        fitness = np.array([func(ind) for ind in pop])\n        order = np.argsort(fitness)\n        distances = np.zeros(len(pop))\n        for i in range(1, len(pop) - 1):\n            distances[order[i]] = fitness[order[i + 1]] - fitness[order[i - 1]]\n        return distances\n\n    def _differential_evolution(self, pop, func, bounds, generation, max_generations):\n        size = len(pop)\n        F = self._adaptive_mutation_factor(generation, max_generations)\n        CR = 0.6 + (0.9 - 0.6) * (generation / max_generations)  # Dynamic crossover rate\n        for i in range(size):\n            indices = np.random.choice(np.delete(np.arange(size), i), 3, replace=False)\n            a, b, c = pop[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                pop[i] = trial\n        return pop\n\n    def _local_search(self, solution, func, bounds):\n        res = minimize(func, solution, method='L-BFGS-B', bounds=np.vstack((bounds.lb, bounds.ub)).T)\n        return res.x\n\n    def _periodic_cost(self, solution):\n        period_length = self.dim // 2\n        periodic_part = solution[:period_length]\n        periodicity_penalty = np.sum((solution - np.tile(periodic_part, self.dim // period_length))**2)\n        return periodicity_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        num_generations = self.budget // population_size\n        pop = self._initialize_population(population_size, bounds)\n        best_solution = None\n\n        for generation in range(num_generations):\n            pop = self._differential_evolution(pop, lambda x: func(x) + self._periodic_cost(x), bounds, generation, num_generations)\n            current_best_idx = np.argmin([func(ind) for ind in pop])\n            current_best_solution = pop[current_best_idx]\n            if best_solution is None or func(current_best_solution) < func(best_solution):\n                best_solution = current_best_solution  # Retain elitism\n            refined_solution = self._local_search(current_best_solution, func, bounds)\n            worst_idx = np.argmax(self._crowding_distance(pop, func))\n            pop[worst_idx] = refined_solution\n\n        return best_solution", "name": "HybridOptimization", "description": "Enhanced HybridOptimization algorithm by refining crossover rate strategy to better balance exploration and exploitation in multilayer optimization.", "configspace": "", "generation": 3, "fitness": 0.9353332326570033, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.042. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "755b68aa-2851-42f9-8585-56760c373d95", "metadata": {"aucs": [0.8840173157613918, 0.9350505616456578, 0.98693182056396], "final_y": [0.20044794283525613, 0.18187968060544402, 0.16485743573982503]}, "mutation_prompt": null}
{"id": "f9b9b2cb-acd5-4c13-bc3a-506d83256872", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def _initialize_population(self, size, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (size, self.dim))\n        pop[:size//2] = lb + (ub - pop[size//2:])\n        return pop\n\n    def _adaptive_mutation_factor(self, generation, max_generations):\n        return 0.7 + (0.9 - 0.7) * (1 - generation / max_generations)\n\n    def _adaptive_crossover_rate(self, generation, max_generations):\n        return 0.9 - 0.5 * (generation / max_generations)  # New adaptive crossover rate\n\n    def _crowding_distance(self, pop, func):\n        fitness = np.array([func(ind) for ind in pop])\n        order = np.argsort(fitness)\n        distances = np.zeros(len(pop))\n        for i in range(1, len(pop) - 1):\n            distances[order[i]] = fitness[order[i + 1]] - fitness[order[i - 1]]\n        return distances\n\n    def _differential_evolution(self, pop, func, bounds, generation, max_generations):\n        size = len(pop)\n        F = self._adaptive_mutation_factor(generation, max_generations)\n        CR = self._adaptive_crossover_rate(generation, max_generations)  # Updated crossover rate\n        for i in range(size):\n            indices = np.random.choice(np.delete(np.arange(size), i), 3, replace=False)\n            a, b, c = pop[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                pop[i] = trial\n        return pop\n\n    def _local_search(self, solution, func, bounds):\n        res = minimize(func, solution, method='L-BFGS-B', bounds=np.vstack((bounds.lb, bounds.ub)).T)\n        return res.x\n\n    def _periodic_cost(self, solution):\n        period_length = self.dim // 2\n        periodic_part = solution[:period_length]\n        periodicity_penalty = np.sum((solution - np.tile(periodic_part, self.dim // period_length))**2)\n        return periodicity_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        num_generations = self.budget // population_size\n        pop = self._initialize_population(population_size, bounds)\n        best_solution = None\n\n        for generation in range(num_generations):\n            pop = self._differential_evolution(pop, lambda x: func(x) + self._periodic_cost(x), bounds, generation, num_generations)\n            current_best_idx = np.argmin([func(ind) for ind in pop])\n            current_best_solution = pop[current_best_idx]\n            if best_solution is None or func(current_best_solution) < func(best_solution):\n                best_solution = current_best_solution\n            refined_solution = self._local_search(current_best_solution, func, bounds)\n            worst_idx = np.argmax(self._crowding_distance(pop, func))\n            pop[worst_idx] = refined_solution\n\n        return best_solution", "name": "HybridOptimization", "description": "Enhanced HybridOptimization algorithm introduces adaptive crossover rate to balance exploration and exploitation, while maintaining elitism and periodicity encouragement.", "configspace": "", "generation": 3, "fitness": 0.917077923979485, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.917 with standard deviation 0.099. And the mean value of best solutions found was 0.191 (0. is the best) with standard deviation 0.037.", "error": "", "parent_id": "755b68aa-2851-42f9-8585-56760c373d95", "metadata": {"aucs": [0.9872218773177482, 0.7771257858170691, 0.9868861088036375], "final_y": [0.16485671163114923, 0.24257726458391793, 0.16485743573982503]}, "mutation_prompt": null}
{"id": "0a580511-1164-4b08-86fb-a1394cc15a50", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def _initialize_population(self, size, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (size, self.dim))\n        pop[:size//2] = lb + (ub - pop[size//2:])\n        return pop\n\n    def _adaptive_mutation_factor(self, generation, max_generations):\n        return 0.7 + (0.9 - 0.7) * (1 - generation / max_generations)  # Adjusted mutation factor range\n\n    def _crowding_distance(self, pop, func):\n        fitness = np.array([func(ind) for ind in pop])\n        order = np.argsort(fitness)\n        distances = np.zeros(len(pop))\n        for i in range(1, len(pop) - 1):\n            distances[order[i]] = fitness[order[i + 1]] - fitness[order[i - 1]]\n        return distances\n\n    def _differential_evolution(self, pop, func, bounds, generation, max_generations):\n        size = len(pop)\n        F = self._adaptive_mutation_factor(generation, max_generations)\n        CR = 0.8  # Reduced crossover rate\n        for i in range(size):\n            indices = np.random.choice(np.delete(np.arange(size), i), 3, replace=False)\n            a, b, c = pop[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                pop[i] = trial\n        return pop\n\n    def _local_search(self, solution, func, bounds):\n        res = minimize(func, solution, method='L-BFGS-B', bounds=np.vstack((bounds.lb, bounds.ub)).T)\n        return res.x\n\n    def _periodic_cost(self, solution):\n        period_length = self.dim // 2\n        periodic_part = solution[:period_length]\n        periodicity_penalty = np.sum((solution - np.tile(periodic_part, self.dim // period_length))**2)\n        return periodicity_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20 + (generation // 10)  # Adjust population size dynamically\n        num_generations = self.budget // population_size\n        pop = self._initialize_population(population_size, bounds)\n        best_solution = None\n\n        for generation in range(num_generations):\n            pop = self._differential_evolution(pop, lambda x: func(x) + self._periodic_cost(x), bounds, generation, num_generations)\n            current_best_idx = np.argmin([func(ind) for ind in pop])\n            current_best_solution = pop[current_best_idx]\n            if best_solution is None or func(current_best_solution) < func(best_solution):\n                best_solution = current_best_solution  # Retain elitism\n            refined_solution = self._local_search(current_best_solution, func, bounds)\n            worst_idx = np.argmax(self._crowding_distance(pop, func))\n            pop[worst_idx] = refined_solution\n\n        return best_solution", "name": "HybridOptimization", "description": "Introduced dynamic population size adjustment in the Differential Evolution phase to improve convergence efficiency within the strict code change limits.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"cannot access local variable 'generation' where it is not associated with a value\").", "error": "UnboundLocalError(\"cannot access local variable 'generation' where it is not associated with a value\")", "parent_id": "755b68aa-2851-42f9-8585-56760c373d95", "metadata": {}, "mutation_prompt": null}
{"id": "6c101e77-0111-49c7-8e88-8ef7b3646dad", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def _initialize_population(self, size, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (size, self.dim))\n        pop[:size//2] = lb + (ub - pop[size//2:])\n        return pop\n\n    def _differential_evolution(self, pop, func, bounds, F=0.8, CR=0.95):  # Changed CR from 0.9 to 0.95\n        size = len(pop)\n        for i in range(size):\n            indices = np.random.choice(np.delete(np.arange(size), i), 3, replace=False)\n            a, b, c = pop[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                pop[i] = trial\n        return pop\n\n    def _adaptive_local_search(self, solution, func, bounds):\n        res = minimize(func, solution, method='L-BFGS-B', bounds=np.vstack((bounds.lb, bounds.ub)).T)\n        return res.x\n\n    def _adaptive_periodic_cost(self, solution):\n        adaptive_period_length = max(1, self.dim // 4)\n        periodic_part = solution[:adaptive_period_length]\n        periodicity_penalty = np.sum((solution - np.tile(periodic_part, self.dim // adaptive_period_length))**2)\n        return periodicity_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        num_generations = self.budget // population_size\n        pop = self._initialize_population(population_size, bounds)\n\n        for _ in range(num_generations):\n            pop = self._differential_evolution(pop, lambda x: func(x) + self._adaptive_periodic_cost(x), bounds)\n            best_idx = np.argmin([func(ind) for ind in pop])\n            best_solution = pop[best_idx]\n            refined_solution = self._adaptive_local_search(best_solution, func, bounds)\n            worst_idx = np.argmax([func(ind) for ind in pop])\n            pop[worst_idx] = refined_solution\n\n        best_idx = np.argmin([func(ind) for ind in pop])\n        return pop[best_idx]", "name": "HybridOptimization", "description": "Improved HybridOptimization by tuning the differential evolution's crossover rate to enhance exploration and convergence in multilayer optimization.", "configspace": "", "generation": 4, "fitness": 0.9628365187531243, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.963 with standard deviation 0.022. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2af5f37f-f6c5-4235-8875-f2e1f27012b3", "metadata": {"aucs": [0.9726456358594219, 0.9317185327010171, 0.9841453876989334], "final_y": [0.16485704562655945, 0.18187968060544402, 0.16485743573982503]}, "mutation_prompt": null}
{"id": "90aa1738-e117-44ac-9da3-a32e04dfe494", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def _initialize_population(self, size, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (size, self.dim))\n        # Symmetric initialization to enhance exploration\n        pop[:size//2] = lb + (ub - pop[size//2:])\n        return pop\n\n    def _differential_evolution(self, pop, func, bounds, F=0.8, CR=0.9):\n        size = len(pop)\n        elite = pop[np.argmin([func(ind) for ind in pop])]  # Elitism: retain the best individual\n        for i in range(size):\n            # Randomly select three distinct vectors\n            indices = np.random.choice(np.delete(np.arange(size), i), 3, replace=False)\n            a, b, c = pop[indices]\n            # Mutation\n            F_adaptive = 0.5 + 0.5 * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n            # Recombination\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            # Selection\n            if func(trial) < func(pop[i]):\n                pop[i] = trial\n        pop[np.argmax([func(ind) for ind in pop])] = elite  # Incorporate elite individual back into population\n        return pop\n\n    def _local_search(self, solution, func, bounds):\n        # Local refinement using BFGS\n        res = minimize(func, solution, method='L-BFGS-B', bounds=np.vstack((bounds.lb, bounds.ub)).T)\n        return res.x\n\n    def _periodic_cost(self, solution):\n        # Custom cost function to encourage periodicity\n        period_length = self.dim // 2\n        periodic_part = solution[:period_length]\n        periodicity_penalty = np.sum((solution - np.tile(periodic_part, self.dim // period_length))**2)\n        return periodicity_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        num_generations = self.budget // population_size\n        pop = self._initialize_population(population_size, bounds)\n\n        for _ in range(num_generations):\n            pop = self._differential_evolution(pop, lambda x: func(x) + self._periodic_cost(x), bounds)\n            best_idx = np.argmin([func(ind) for ind in pop])\n            best_solution = pop[best_idx]\n            refined_solution = self._local_search(best_solution, func, bounds)\n            # Replace worst with refined solution to maintain diversity\n            worst_idx = np.argmax([func(ind) for ind in pop])\n            pop[worst_idx] = refined_solution\n\n        # Return the best found solution\n        best_idx = np.argmin([func(ind) for ind in pop])\n        return pop[best_idx]", "name": "HybridOptimization", "description": "Improved HybridOptimization algorithm with adaptive mutation factor and incorporation of elitism to enhance convergence and diversity in multilayer optimization.", "configspace": "", "generation": 4, "fitness": 0.9489275712953694, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.949 with standard deviation 0.014. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "7e9433d9-9a30-456f-8a60-4ddc21c7b3ca", "metadata": {"aucs": [0.936896117722902, 0.9682899817063537, 0.9415966144568525], "final_y": [0.1818868392322217, 0.1648571566973186, 0.16485599308187604]}, "mutation_prompt": null}
{"id": "95f89ec8-5c0d-4c03-95e5-18c4006a11a4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.current_evals = 0\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        self.current_evals += self.population_size\n        \n        while self.current_evals < self.budget:\n            for i in range(self.population_size):\n                idxs = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                # Dynamic mutation factor for variation\n                F = 0.5 + 0.3 * np.random.rand()\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                # Adaptive periodicity encouragement\n                mutant = self._encourage_periodicity(mutant, self.current_evals / self.budget)\n                \n                crossover = np.where(np.random.rand(self.dim) < 0.9, mutant, population[i])\n                \n                new_fit = func(crossover)\n                self.current_evals += 1\n                \n                if new_fit < fitness[i]:\n                    population[i] = crossover\n                    fitness[i] = new_fit\n                    \n                if self.current_evals >= self.budget:\n                    break\n            \n            best_idx = np.argmin(fitness)\n            res = minimize(func, population[best_idx], method='L-BFGS-B', bounds=list(zip(lb, ub)))\n            if res.fun < fitness[best_idx]:\n                population[best_idx] = res.x\n                fitness[best_idx] = res.fun\n                self.current_evals += res.nfev\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _encourage_periodicity(self, individual, progress):\n        period = 2 + int(progress * 3)\n        for i in range(0, self.dim, period):\n            if i + 1 < self.dim:\n                avg = (individual[i] + individual[i+1]) / 2\n                individual[i], individual[i+1] = avg, avg\n        return individual", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced HybridMetaheuristicOptimizer with adaptive periodicity encouragement and dynamic mutation factor for improved convergence and solution quality.", "configspace": "", "generation": 4, "fitness": 0.9781728564447598, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c08a9492-2f16-4b98-aaa0-b3de4b9fdc8e", "metadata": {"aucs": [0.9809024575739425, 0.9878156560124384, 0.9658004557478986], "final_y": [0.16486154281099408, 0.16486110310223723, 0.1648582246854482]}, "mutation_prompt": null}
{"id": "6d5cc353-6485-4163-9597-13dac6ae372d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.current_evals = 0\n    \n    def __call__(self, func):\n        # Initialize population within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        self.current_evals += self.population_size\n        \n        while self.current_evals < self.budget:\n            # Differential Evolution mutation and crossover\n            for i in range(self.population_size):\n                idxs = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                \n                # Encourage periodicity by averaging layer thicknesses to nearby values\n                mutant = self._encourage_periodicity(mutant)\n                \n                # Crossover\n                crossover = np.where(np.random.rand(self.dim) < 0.9, mutant, population[i])\n                \n                # Evaluate new solution\n                new_fit = func(crossover)\n                self.current_evals += 1\n                \n                # Selection\n                if new_fit < fitness[i]:\n                    population[i] = crossover\n                    fitness[i] = new_fit\n                    \n                if self.current_evals >= self.budget:\n                    break\n            \n            # Local optimization with BFGS on best individual\n            best_idx = np.argmin(fitness)\n            res = minimize(func, population[best_idx], method='L-BFGS-B', bounds=list(zip(lb, ub)))\n            if res.fun < fitness[best_idx]:\n                population[best_idx] = res.x\n                fitness[best_idx] = res.fun\n                self.current_evals += res.nfev\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _encourage_periodicity(self, individual):\n        # Adjust layers to encourage periodicity by averaging thicknesses\n        period = np.random.choice([2, 4], p=[0.7, 0.3])  # Dynamic period length\n        for i in range(0, self.dim, period):\n            if i + 1 < self.dim:\n                avg = np.mean(individual[i:i+period])\n                individual[i:i+period] = avg\n        return individual", "name": "HybridMetaheuristicOptimizer", "description": "An improved hybrid metaheuristic optimizer for multilayer structures combining Differential Evolution with dynamic periodicity encouragement and BFGS refinement.", "configspace": "", "generation": 4, "fitness": 0.9912449067420783, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c08a9492-2f16-4b98-aaa0-b3de4b9fdc8e", "metadata": {"aucs": [0.985901100564483, 0.9948598389823808, 0.9929737806793709], "final_y": [0.16486177492427523, 0.16485929346456007, 0.16485625082386435]}, "mutation_prompt": null}
{"id": "77e459f7-a05d-4ed5-acba-1135e6e12197", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def _initialize_population(self, size, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (size, self.dim))\n        # Symmetric initialization to enhance exploration\n        pop[:size//2] = lb + (ub - pop[size//2:])\n        return pop\n\n    def _differential_evolution(self, pop, func, bounds, F=0.8, CR=0.9):\n        size = len(pop)\n        for i in range(size):\n            # Randomly select three distinct vectors\n            indices = np.random.choice(np.delete(np.arange(size), i), 3, replace=False)\n            a, b, c = pop[indices]\n            # Mutation\n            F = 0.5 + (0.5 * i / size)  # Adaptive mutation factor\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            # Recombination\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            # Selection\n            if func(trial) < func(pop[i]):\n                pop[i] = trial\n        return pop\n\n    def _local_search(self, solution, func, bounds):\n        # Local refinement using BFGS\n        res = minimize(func, solution, method='L-BFGS-B', bounds=np.vstack((bounds.lb, bounds.ub)).T)\n        return res.x\n\n    def _periodic_cost(self, solution):\n        # Custom cost function to encourage periodicity\n        period_length = self.dim // 2\n        periodic_part = solution[:period_length]\n        periodicity_penalty = np.sum((solution - np.tile(periodic_part, self.dim // period_length))**2)\n        return periodicity_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        num_generations = self.budget // population_size\n        pop = self._initialize_population(population_size, bounds)\n\n        for _ in range(num_generations):\n            pop = self._differential_evolution(pop, lambda x: func(x) + self._periodic_cost(x), bounds)\n            best_idx = np.argmin([func(ind) for ind in pop])\n            best_solution = pop[best_idx]\n            refined_solution = self._local_search(best_solution, func, bounds)\n            # Replace worst with refined solution to maintain diversity\n            worst_idx = np.argmax([func(ind) for ind in pop])\n            pop[worst_idx] = refined_solution\n\n        # Return the best found solution\n        best_idx = np.argmin([func(ind) for ind in pop])\n        return pop[best_idx]", "name": "HybridOptimization", "description": "Enhanced HybridOptimization with adaptive mutation factor and dynamic population size adjustment to improve exploration and convergence efficiency.", "configspace": "", "generation": 5, "fitness": 0.9547685176951219, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.955 with standard deviation 0.030. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7e9433d9-9a30-456f-8a60-4ddc21c7b3ca", "metadata": {"aucs": [0.9797269544864355, 0.9717248084552625, 0.912853790143668], "final_y": [0.16485687594161436, 0.16485757286954883, 0.16485619030193477]}, "mutation_prompt": null}
{"id": "8a1080a6-cd32-4b5a-a43c-19288283932a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def _initialize_population(self, size, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        pop = np.random.uniform(lb, ub, (size, self.dim))\n        pop[:size//2] = lb + (ub - pop[size//2:])\n        return pop\n\n    def _differential_evolution(self, pop, func, bounds, F=0.8, CR=0.9):\n        size = len(pop)\n        for i in range(size):\n            indices = np.random.choice(np.delete(np.arange(size), i), 3, replace=False)\n            a, b, c = pop[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                pop[i] = trial\n        return pop\n\n    def _adaptive_local_search(self, solution, func, bounds):\n        res = minimize(func, solution, method='L-BFGS-B', bounds=np.vstack((bounds.lb, bounds.ub)).T)\n        return res.x\n\n    def _adaptive_periodic_cost(self, solution):\n        adaptive_period_length = max(1, self.dim // 4)\n        periodic_part = solution[:adaptive_period_length]\n        periodicity_penalty = np.sum((solution - np.tile(periodic_part, self.dim // adaptive_period_length))**2)\n        return periodicity_penalty\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = 20\n        num_generations = self.budget // population_size\n        pop = self._initialize_population(population_size, bounds)\n\n        for gen in range(num_generations):\n            # Adjust the mutation factor F based on the generation number\n            F = 0.9 - 0.5 * (gen / num_generations)\n            pop = self._differential_evolution(pop, lambda x: func(x) + self._adaptive_periodic_cost(x), bounds, F=F)\n            best_idx = np.argmin([func(ind) for ind in pop])\n            best_solution = pop[best_idx]\n            refined_solution = self._adaptive_local_search(best_solution, func, bounds)\n            worst_idx = np.argmax([func(ind) for ind in pop])\n            pop[worst_idx] = refined_solution\n\n        best_idx = np.argmin([func(ind) for ind in pop])\n        return pop[best_idx]", "name": "HybridOptimization", "description": "Further refined Enhanced HybridOptimization algorithm by introducing a strategic mutation factor adjustment to enhance solution quality and convergence.", "configspace": "", "generation": 5, "fitness": 0.9585108789336783, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.028. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2af5f37f-f6c5-4235-8875-f2e1f27012b3", "metadata": {"aucs": [0.9625314545228848, 0.9904692765176429, 0.9225319057605068], "final_y": [0.16486050631430904, 0.16485597280084607, 0.18187838066932527]}, "mutation_prompt": null}
{"id": "d7335d30-00f8-44b1-9bae-35d509b81108", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.current_evals = 0\n    \n    def __call__(self, func):\n        # Initialize population within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        self.current_evals += self.population_size\n        \n        while self.current_evals < self.budget:\n            # Differential Evolution mutation and crossover\n            for i in range(self.population_size):\n                idxs = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.9 * (b - c), lb, ub)  # Modified mutation factor\n                \n                # Encourage periodicity by averaging layer thicknesses to nearby values\n                mutant = self._encourage_periodicity(mutant)\n                \n                # Crossover\n                crossover = np.where(np.random.rand(self.dim) < 0.9, mutant, population[i])\n                \n                # Evaluate new solution\n                new_fit = func(crossover)\n                self.current_evals += 1\n                \n                # Selection\n                if new_fit < fitness[i]:\n                    population[i] = crossover\n                    fitness[i] = new_fit\n                    \n                if self.current_evals >= self.budget:\n                    break\n            \n            # Local optimization with BFGS on best individual\n            best_idx = np.argmin(fitness)\n            res = minimize(func, population[best_idx], method='L-BFGS-B', bounds=list(zip(lb, ub)))\n            if res.fun < fitness[best_idx]:\n                population[best_idx] = res.x\n                fitness[best_idx] = res.fun\n                self.current_evals += res.nfev\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _encourage_periodicity(self, individual):\n        # Adjust layers to encourage periodicity by averaging thicknesses\n        period = np.random.choice([2, 4], p=[0.7, 0.3])  # Dynamic period length\n        for i in range(0, self.dim, period):\n            if i + 1 < self.dim:\n                avg = np.mean(individual[i:i+period])\n                individual[i:i+period] = avg\n        return individual", "name": "HybridMetaheuristicOptimizer", "description": "Fine-tuned HybridMetaheuristicOptimizer by modifying the mutation factor for enhanced exploration.", "configspace": "", "generation": 5, "fitness": 0.983107898981797, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6d5cc353-6485-4163-9597-13dac6ae372d", "metadata": {"aucs": [0.9846478208232848, 0.9808222016893294, 0.9838536744327768], "final_y": [0.16485701387411933, 0.1648572520546132, 0.1648573801003861]}, "mutation_prompt": null}
{"id": "5923bd04-88df-4d8f-9c9f-68da26fdc235", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.current_evals = 0\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        self.current_evals += self.population_size\n        \n        elite = population[np.argmin(fitness)]  # Track best solution so far\n\n        while self.current_evals < self.budget:\n            for i in range(self.population_size):\n                idxs = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                \n                mutant = self._encourage_periodicity(mutant)\n                \n                crossover_rate = 0.9 if self.current_evals < self.budget * 0.5 else 0.6\n                crossover = np.where(np.random.rand(self.dim) < crossover_rate, mutant, population[i])\n                \n                new_fit = func(crossover)\n                self.current_evals += 1\n                \n                if new_fit < fitness[i]:\n                    population[i] = crossover\n                    fitness[i] = new_fit\n                    \n                if self.current_evals >= self.budget:\n                    break\n            \n            best_idx = np.argmin(fitness)\n            res = minimize(func, population[best_idx], method='L-BFGS-B', bounds=list(zip(lb, ub)))\n            if res.fun < fitness[best_idx]:\n                population[best_idx] = res.x\n                fitness[best_idx] = res.fun\n                self.current_evals += res.nfev\n\n            # Update elite solution\n            if fitness[best_idx] < func(elite):\n                elite = population[best_idx]\n        \n        return elite, func(elite)\n\n    def _encourage_periodicity(self, individual):\n        period = np.random.choice([2, 4], p=[0.7, 0.3])\n        for i in range(0, self.dim, period):\n            if i + 1 < self.dim:\n                avg = np.mean(individual[i:i+period])\n                individual[i:i+period] = avg\n        return individual", "name": "HybridMetaheuristicOptimizer", "description": "Optimized HybridMetaheuristicOptimizer with dynamic crossover rate and elitism, enhancing exploration and convergence for multilayer structures.", "configspace": "", "generation": 5, "fitness": 0.9857218703630016, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6d5cc353-6485-4163-9597-13dac6ae372d", "metadata": {"aucs": [0.993348907913546, 0.972856395936051, 0.9909603072394082], "final_y": [0.16485794546791288, 0.16485775393173874, 0.16486171565828556]}, "mutation_prompt": null}
{"id": "df8afa0a-8da1-4622-a5e3-2c26c521efc8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.current_evals = 0\n    \n    def __call__(self, func):\n        # Initialize population within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        self.current_evals += self.population_size\n        \n        while self.current_evals < self.budget:\n            mutation_factor = 0.8 + 0.2 * (1 - self.current_evals / self.budget)  # Adaptive mutation factor\n            # Differential Evolution mutation and crossover\n            for i in range(self.population_size):\n                idxs = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + mutation_factor * (b - c), lb, ub)\n                \n                # Encourage periodicity by averaging layer thicknesses to nearby values\n                mutant = self._encourage_periodicity(mutant)\n                \n                # Intelligent Crossover to improve diversity\n                crossover_prob = 0.9 - 0.3 * (self.current_evals / self.budget)\n                crossover = np.where(np.random.rand(self.dim) < crossover_prob, mutant, population[i])\n                \n                # Evaluate new solution\n                new_fit = func(crossover)\n                self.current_evals += 1\n                \n                # Selection\n                if new_fit < fitness[i]:\n                    population[i] = crossover\n                    fitness[i] = new_fit\n                    \n                if self.current_evals >= self.budget:\n                    break\n            \n            # Local optimization with BFGS on best individual\n            best_idx = np.argmin(fitness)\n            res = minimize(func, population[best_idx], method='L-BFGS-B', bounds=list(zip(lb, ub)))\n            if res.fun < fitness[best_idx]:\n                population[best_idx] = res.x\n                fitness[best_idx] = res.fun\n                self.current_evals += res.nfev\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def _encourage_periodicity(self, individual):\n        # Adjust layers to encourage periodicity by averaging thicknesses\n        period = np.random.choice([2, 4], p=[0.7, 0.3])  # Dynamic period length\n        for i in range(0, self.dim, period):\n            if i + 1 < self.dim:\n                avg = np.mean(individual[i:i+period])\n                individual[i:i+period] = avg\n        return individual", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic optimizer using adaptive mutation factor and intelligent crossover to improve convergence speed and solution quality.", "configspace": "", "generation": 5, "fitness": 0.9838350989807608, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6d5cc353-6485-4163-9597-13dac6ae372d", "metadata": {"aucs": [0.9820166211706335, 0.9774519239148272, 0.9920367518568216], "final_y": [0.16485791495603286, 0.1648560627479222, 0.1648594174020641]}, "mutation_prompt": null}
