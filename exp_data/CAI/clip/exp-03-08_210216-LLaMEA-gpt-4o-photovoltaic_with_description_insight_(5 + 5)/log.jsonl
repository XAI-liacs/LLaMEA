{"id": "2e1960ec-d5c4-4aaa-bb71-aef0a4607a65", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n      \n    def differential_evolution_step(self, population, func, bounds, F=0.8, CR=0.9):\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            indices = np.random.choice(len(population), 3, replace=False)\n            a, b, c = population[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial_fitness = func(trial)\n            self.evaluations += 1\n            if trial_fitness < func(population[i]):\n                new_population[i] = trial\n        return new_population\n\n    def local_refinement(self, candidate, func, bounds):\n        if self.evaluations >= self.budget:\n            return candidate\n        result = minimize(func, candidate, method='Nelder-Mead', bounds=bounds,\n                          options={'maxiter': 10, 'disp': False})\n        self.evaluations += result.nfev\n        return result.x if result.success else candidate\n\n    def progressive_layer_optimization(self, func, bounds, max_layers):\n        population_size = 10\n        current_layers = 10\n        population = np.random.uniform(bounds.lb, bounds.ub, (population_size, self.dim))\n\n        while self.evaluations < self.budget and current_layers <= max_layers:\n            population = self.differential_evolution_step(population, func, bounds)\n            best_candidate = min(population, key=func)\n            self.evaluations += 1\n            best_candidate = self.local_refinement(best_candidate, func, bounds)\n            current_layers += 1\n            self.dim = current_layers * 2  # Assuming each layer adds a new dimension pair\n\n        return best_candidate\n\n    def __call__(self, func):\n        bounds = func.bounds\n        max_layers = self.dim // 2\n        best_solution = self.progressive_layer_optimization(func, bounds, max_layers)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for global exploration and Nelder-Mead for local refinement, enhanced with progressive layer introduction and adaptive robustness measures to optimize multilayer photonic structures.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\n  File \"<string>\", line 49, in progressive_layer_optimization\nUnboundLocalError: local variable 'best_candidate' referenced before assignment\n.", "error": "UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\n  File \"<string>\", line 49, in progressive_layer_optimization\nUnboundLocalError: local variable 'best_candidate' referenced before assignment\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "1a18194f-a226-4d65-954f-8d1a1c00a410", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_budget = 0\n        self.pop_size = min(50, dim * 5)  # Population size for DE\n        self.population = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        self.fitness = np.full(self.pop_size, np.inf)\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def differential_evolution(self, func):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            mutant_vector = np.clip(a + self.mutation_factor * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.crossover_rate\n            trial_vector = np.where(crossover, mutant_vector, self.population[i])\n            trial_fitness = self.evaluate(func, trial_vector)\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial_vector\n                self.fitness[i] = trial_fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial_vector\n\n    def local_search(self, func, solution):\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        new_solution = np.clip(solution + perturbation, func.bounds.lb, func.bounds.ub)\n        new_fitness = self.evaluate(func, new_solution)\n        if new_fitness < self.best_fitness:\n            self.best_solution = new_solution\n            self.best_fitness = new_fitness\n\n    def evaluate(self, func, solution):\n        if self.current_budget < self.budget:\n            self.current_budget += 1\n            return func(solution)\n        return np.inf\n\n    def __call__(self, func):\n        layers_step = max(2, self.dim // 10)\n        for current_dim in range(layers_step, self.dim + 1, layers_step):\n            self.population = np.random.uniform(-1, 1, (self.pop_size, current_dim))\n            self.fitness = np.full(self.pop_size, np.inf)\n            self.best_solution = None\n            self.best_fitness = np.inf\n            while self.current_budget < self.budget:\n                self.differential_evolution(func)\n                if self.best_solution is not None:\n                    self.local_search(func, self.best_solution)\n            if self.current_budget >= self.budget:\n                break\n        return self.best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm that combines Differential Evolution for global search with a local search strategy, designed to adaptively balance exploration and exploitation while incrementally increasing problem complexity for efficient layer-based optimization.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 53, in __call__\n  File \"<string>\", line 20, in differential_evolution\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 2169, in clip\n    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n    return bound(*args, **kwds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/_methods.py\", line 99, in _clip\n    return um.clip(a, min, max, out=out, **kwargs)\nValueError: operands could not be broadcast together with shapes (2,) (10,) (10,) \n.", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) (10,) ')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 53, in __call__\n  File \"<string>\", line 20, in differential_evolution\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 2169, in clip\n    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n    return bound(*args, **kwds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/_methods.py\", line 99, in _clip\n    return um.clip(a, min, max, out=out, **kwargs)\nValueError: operands could not be broadcast together with shapes (2,) (10,) (10,) \n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "6a9c60a4-a768-46a2-a626-3d12fb074f42", "solution": "import numpy as np\nfrom cma import CMAEvolutionStrategy\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.de_mutation_factor = 0.8\n        self.de_crossover_prob = 0.9\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, pop, bounds):\n        new_pop = np.copy(pop)\n        for i in range(len(pop)):\n            if self.current_evaluations >= self.budget:\n                break\n            # Randomly select three other individuals\n            indices = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[indices]\n            # Create a mutant vector\n            mutant = a + self.de_mutation_factor * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            # Crossover\n            crossover_mask = np.random.rand(self.dim) < self.de_crossover_prob\n            offspring = np.where(crossover_mask, mutant, pop[i])\n            # Selection\n            if self.current_evaluations < self.budget:\n                if func(offspring) < func(pop[i]):\n                    new_pop[i] = offspring\n                self.current_evaluations += 1\n        return new_pop\n\n    def cma_es_refinement(self, func, x0, bounds):\n        if self.current_evaluations >= self.budget:\n            return x0\n        es = CMAEvolutionStrategy(x0, 0.5, {'bounds': [bounds.lb, bounds.ub], 'popsize': self.population_size})\n        while not es.stop() and self.current_evaluations < self.budget:\n            solutions = es.ask()\n            fitnesses = [func(sol) for sol in solutions]\n            self.current_evaluations += len(solutions)\n            es.tell(solutions, fitnesses)\n            es.disp()\n        return es.result[0]\n\n    def adaptive_layer_refinement(self, func, bounds):\n        layers = 10\n        best_solution = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n        while layers <= self.dim and self.current_evaluations < self.budget:\n            # DE phase\n            pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, layers))\n            pop = self.differential_evolution(func, pop, bounds)\n            best_solution = pop[np.argmin([func(ind) for ind in pop])]\n            \n            # CMA-ES phase\n            best_solution = self.cma_es_refinement(func, best_solution, bounds)\n            \n            # Increase complexity by adding layers\n            layers = min(layers + 5, self.dim)\n        \n        return best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_layer_refinement(func, bounds)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic optimization algorithm combining Differential Evolution (DE) and Covariance Matrix Adaptation Evolution Strategy (CMA-ES) with adaptive layer-wise refinement for efficient exploration and exploitation in high-dimensional noisy optimization problems.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 124, in evaluatePhotonic\n    exec(code, globals())\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'cma'\n.", "error": "ModuleNotFoundError(\"No module named 'cma'\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 124, in evaluatePhotonic\n    exec(code, globals())\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'cma'\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "b0b05721-45c9-40cd-a045-ac41d5856f9e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HALO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8  # Differential evolution scale factor\n        self.CR = 0.9 # Crossover probability\n        self.pop = None\n        self.func_evals = 0\n\n    def _initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.pop])\n        self.func_evals += self.population_size\n        return fitness\n\n    def _differential_evolution_step(self, fitness, lb, ub):\n        next_pop = np.empty_like(self.pop)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.pop[indices]\n            mutant = np.clip(a + self.F * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            trial_fitness = func(trial)\n            self.func_evals += 1\n            if trial_fitness < fitness[i]:\n                next_pop[i] = trial\n            else:\n                next_pop[i] = self.pop[i]\n        self.pop = next_pop\n\n    def _local_refinement(self, func, lb, ub):\n        best_idx = np.argmin(self._evaluate_population(func))\n        best_solution = self.pop[best_idx]\n        result = minimize(func, best_solution, bounds=[(lb[i], ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        if result.success and func(result.x) < func(best_solution):\n            self.pop[best_idx] = result.x\n            self.func_evals += 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(lb, ub)\n        fitness = self._evaluate_population(func)\n\n        while self.func_evals < self.budget:\n            self._differential_evolution_step(fitness, lb, ub)\n            if self.func_evals < self.budget // 2:\n                self._local_refinement(func, lb, ub)\n            fitness = self._evaluate_population(func)\n\n        best_idx = np.argmin(fitness)\n        return self.pop[best_idx]", "name": "HALO", "description": "Hybrid Adaptive Layered Optimization (HALO) combines differential evolution for global exploration with local refinement and adaptive layer addition to efficiently tackle high-dimensional noisy black box problems in photovoltaics.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\n  File \"<string>\", line 32, in _differential_evolution_step\nNameError: name 'func' is not defined\n.", "error": "NameError(\"name 'func' is not defined\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\n  File \"<string>\", line 32, in _differential_evolution_step\nNameError: name 'func' is not defined\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "8d7214f7-7643-4e56-a74f-11a1d4ac8e8e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Placeholder for modular structure detection (not fully implemented)\n        # In a real-case scenario, this method would analyze the layers and adapt strategies\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        # Gradually increase the dimensionality of the problem\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            # Local search on best individual\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            # Adapt dimensionality (if implementing a gradual increase)\n            self.dim = self.adapt_dimensionality(self.dim, target_dim=func.dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Hybrid adaptive metaheuristic leveraging Differential Evolution (DE) for global exploration and a local search phase with adaptive tuning, combined with layer-wise modular structure detection and dynamic dimensionality adaptation.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 70, in __call__\nAttributeError: 'ioh.iohcpp.problem.RealSingleObjectiveWrappedProbl' object has no attribute 'dim'\n.", "error": "AttributeError(\"'ioh.iohcpp.problem.RealSingleObjectiveWrappedProbl' object has no attribute 'dim'\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 70, in __call__\nAttributeError: 'ioh.iohcpp.problem.RealSingleObjectiveWrappedProbl' object has no attribute 'dim'\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "0943bab8-56af-45e0-96b1-b3aaaf9e8ab3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n      \n    def differential_evolution_step(self, population, func, bounds, F=0.8, CR=0.9):\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            indices = np.random.choice(len(population), 3, replace=False)\n            a, b, c = population[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial_fitness = func(trial)\n            self.evaluations += 1\n            if trial_fitness < func(population[i]):\n                new_population[i] = trial\n        return new_population\n\n    def local_refinement(self, candidate, func, bounds):\n        if self.evaluations >= self.budget:\n            return candidate\n        result = minimize(func, candidate, method='Nelder-Mead', bounds=bounds,\n                          options={'maxiter': 10, 'disp': False})\n        self.evaluations += result.nfev\n        return result.x if result.success else candidate\n\n    def progressive_layer_optimization(self, func, bounds, max_layers):\n        population_size = 10\n        current_layers = 10\n        population = np.random.uniform(bounds.lb, bounds.ub, (population_size, self.dim))\n\n        while self.evaluations < self.budget and current_layers <= max_layers:\n            population = self.differential_evolution_step(population, func, bounds)\n            best_candidate = min(population, key=func)  # Ensures assignment is defined\n            self.evaluations += 1\n            best_candidate = self.local_refinement(best_candidate, func, bounds)\n            current_layers += 1\n            self.dim = current_layers * 2  # Assuming each layer adds a new dimension pair\n\n        return best_candidate\n\n    def __call__(self, func):\n        bounds = func.bounds\n        max_layers = self.dim // 2\n        best_solution = self.progressive_layer_optimization(func, bounds, max_layers)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic that merges Differential Evolution with Nelder-Mead and includes a fix for best_candidate assignment in the progressive layer optimization strategy for photonic structure optimization.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\")", "parent_id": "2e1960ec-d5c4-4aaa-bb71-aef0a4607a65", "metadata": {}, "mutation_prompt": null}
{"id": "65c8084f-d25a-4c9e-8060-a59df01c80ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n      \n    def differential_evolution_step(self, population, func, bounds, F=0.8, CR=0.9):\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            indices = np.random.choice(len(population), 3, replace=False)\n            a, b, c = population[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial_fitness = func(trial)\n            self.evaluations += 1\n            if trial_fitness < func(population[i]):\n                new_population[i] = trial\n        return new_population\n\n    def local_refinement(self, candidate, func, bounds):\n        if self.evaluations >= self.budget:\n            return candidate\n        result = minimize(func, candidate, method='Nelder-Mead', bounds=bounds,\n                          options={'maxiter': 10, 'disp': False})\n        self.evaluations += result.nfev\n        return result.x if result.success else candidate\n\n    def progressive_layer_optimization(self, func, bounds, max_layers):\n        population_size = 10\n        current_layers = 10\n        population = np.random.uniform(bounds.lb, bounds.ub, (population_size, self.dim))\n\n        while self.evaluations < self.budget and current_layers <= max_layers:\n            population = self.differential_evolution_step(population, func, bounds)\n            best_candidate = min(population, key=func, default=population[0])\n            self.evaluations += 1\n            best_candidate = self.local_refinement(best_candidate, func, bounds)\n            current_layers += 1\n            self.dim = current_layers * 2  # Assuming each layer adds a new dimension pair\n\n        return best_candidate\n\n    def __call__(self, func):\n        bounds = func.bounds\n        max_layers = self.dim // 2\n        best_solution = self.progressive_layer_optimization(func, bounds, max_layers)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for global exploration and Nelder-Mead for local refinement, enhanced with progressive layer introduction, adaptive robustness measures, and corrected best candidate assignment to optimize multilayer photonic structures.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\")", "parent_id": "2e1960ec-d5c4-4aaa-bb71-aef0a4607a65", "metadata": {}, "mutation_prompt": null}
{"id": "e398e40c-bd8b-4bb4-bfff-a29abca961da", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_budget = 0\n        self.pop_size = min(50, dim * 5)  # Population size for DE\n        self.population = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        self.fitness = np.full(self.pop_size, np.inf)\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def differential_evolution(self, func):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            # Fix dimensionality handling in DE mutation\n            mutant_vector = np.clip(a + self.mutation_factor * (b - c)[:self.dim], func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.crossover_rate\n            trial_vector = np.where(crossover, mutant_vector, self.population[i])\n            trial_fitness = self.evaluate(func, trial_vector)\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial_vector\n                self.fitness[i] = trial_fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial_vector\n\n    def local_search(self, func, solution):\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        new_solution = np.clip(solution + perturbation, func.bounds.lb, func.bounds.ub)\n        new_fitness = self.evaluate(func, new_solution)\n        if new_fitness < self.best_fitness:\n            self.best_solution = new_solution\n            self.best_fitness = new_fitness\n\n    def evaluate(self, func, solution):\n        if self.current_budget < self.budget:\n            self.current_budget += 1\n            return func(solution)\n        return np.inf\n\n    def __call__(self, func):\n        layers_step = max(2, self.dim // 10)\n        for current_dim in range(layers_step, self.dim + 1, layers_step):\n            self.population = np.random.uniform(-1, 1, (self.pop_size, current_dim))\n            self.fitness = np.full(self.pop_size, np.inf)\n            self.best_solution = None\n            self.best_fitness = np.inf\n            while self.current_budget < self.budget:\n                self.differential_evolution(func)\n                if self.best_solution is not None:\n                    self.local_search(func, self.best_solution)\n            if self.current_budget >= self.budget:\n                break\n        return self.best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic optimization algorithm refining dimensionality handling in DE mutation to improve performance in high-dimensional noisy optimization.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (10,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) (10,) ')", "parent_id": "1a18194f-a226-4d65-954f-8d1a1c00a410", "metadata": {}, "mutation_prompt": null}
{"id": "bc54f4dd-b05a-4826-88fd-288b62776948", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HALO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8  # Differential evolution scale factor\n        self.CR = 0.9 # Crossover probability\n        self.pop = None\n        self.func_evals = 0\n\n    def _initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.pop])\n        self.func_evals += self.population_size\n        return fitness\n\n    def _differential_evolution_step(self, fitness, lb, ub, func):  # Added 'func' as a parameter\n        next_pop = np.empty_like(self.pop)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.pop[indices]\n            mutant = np.clip(a + self.F * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            trial_fitness = func(trial)\n            self.func_evals += 1\n            if trial_fitness < fitness[i]:\n                next_pop[i] = trial\n            else:\n                next_pop[i] = self.pop[i]\n        self.pop = next_pop\n\n    def _local_refinement(self, func, lb, ub):\n        best_idx = np.argmin(self._evaluate_population(func))\n        best_solution = self.pop[best_idx]\n        result = minimize(func, best_solution, bounds=[(lb[i], ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        if result.success and func(result.x) < func(best_solution):\n            self.pop[best_idx] = result.x\n            self.func_evals += 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(lb, ub)\n        fitness = self._evaluate_population(func)\n\n        while self.func_evals < self.budget:\n            self._differential_evolution_step(fitness, lb, ub, func)\n            if self.func_evals < self.budget // 2:\n                self._local_refinement(func, lb, ub)\n            fitness = self._evaluate_population(func)\n\n        best_idx = np.argmin(fitness)\n        return self.pop[best_idx]", "name": "HALO", "description": "Enhanced robustness by correcting the incorrect use of a local variable, ensuring `func` is used in the differential evolution step to evaluate trial solutions.", "configspace": "", "generation": 1, "fitness": 0.8433676956735398, "feedback": "The algorithm HALO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.012. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "b0b05721-45c9-40cd-a045-ac41d5856f9e", "metadata": {"aucs": [0.828459990053326, 0.8436557437318404, 0.8579873532354527], "final_y": [0.1391576355867108, 0.13919517771094259, 0.13063785618548807]}, "mutation_prompt": null}
{"id": "9944bff1-f45d-4548-bd40-3fd1f5ced2e2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved HybridMetaheuristicOptimizer by fixing a dimensionality attribute access error and enhancing the modular structure detection mechanism.", "configspace": "", "generation": 1, "fitness": 0.8298340928247585, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.037. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "8d7214f7-7643-4e56-a74f-11a1d4ac8e8e", "metadata": {"aucs": [0.8267690064704629, 0.8764662020463148, 0.786267069957498], "final_y": [0.1306471634751013, 0.11743860950629603, 0.11531763794593508]}, "mutation_prompt": null}
{"id": "47f51c6a-eac1-45db-b8cf-77b940470ddb", "solution": "import numpy as np\nfrom cma import CMAEvolutionStrategy\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.de_mutation_factor = 0.8\n        self.de_crossover_prob = 0.9\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, pop, bounds):\n        new_pop = np.copy(pop)\n        for i in range(len(pop)):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[indices]\n            mutant = a + self.de_mutation_factor * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            crossover_mask = np.random.rand(self.dim) < self.de_crossover_prob\n            offspring = np.where(crossover_mask, mutant, pop[i])\n            if self.current_evaluations < self.budget:\n                # Noise-handling: evaluate the offspring multiple times and take the mean\n                if np.mean([func(offspring) for _ in range(3)]) < func(pop[i]):\n                    new_pop[i] = offspring\n                self.current_evaluations += 1\n        return new_pop\n\n    def cma_es_refinement(self, func, x0, bounds):\n        if self.current_evaluations >= self.budget:\n            return x0\n        es = CMAEvolutionStrategy(x0, 0.5, {'bounds': [bounds.lb, bounds.ub], 'popsize': self.population_size})\n        while not es.stop() and self.current_evaluations < self.budget:\n            solutions = es.ask()\n            fitnesses = [func(sol) for sol in solutions]\n            self.current_evaluations += len(solutions)\n            es.tell(solutions, fitnesses)\n            es.disp()\n        return es.result[0]\n\n    def adaptive_layer_refinement(self, func, bounds):\n        layers = 10\n        best_solution = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n        while layers <= self.dim and self.current_evaluations < self.budget:\n            pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, layers))\n            pop = self.differential_evolution(func, pop, bounds)\n            best_solution = pop[np.argmin([func(ind) for ind in pop])]\n            best_solution = self.cma_es_refinement(func, best_solution, bounds)\n            layers = min(layers + 5, self.dim)\n        \n        return best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_layer_refinement(func, bounds)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic optimization algorithm combining DE and CMA-ES with adaptive layer-wise refinement, enhanced by incorporating a noise-handling technique to improve stability in noisy environments.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'cma'\").", "error": "ModuleNotFoundError(\"No module named 'cma'\")", "parent_id": "6a9c60a4-a768-46a2-a626-3d12fb074f42", "metadata": {}, "mutation_prompt": null}
{"id": "2733a23f-d9d5-419b-bf4f-9a46a5013e1c", "solution": "import numpy as np\nfrom cma import CMAEvolutionStrategy\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.de_mutation_factor = 0.8\n        self.de_crossover_prob = 0.95  # Adjusted crossover probability\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, pop, bounds):\n        new_pop = np.copy(pop)\n        for i in range(len(pop)):\n            if self.current_evaluations >= self.budget:\n                break\n            # Randomly select three other individuals\n            indices = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[indices]\n            # Create a mutant vector\n            mutant = a + self.de_mutation_factor * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            # Crossover\n            crossover_mask = np.random.rand(self.dim) < self.de_crossover_prob\n            offspring = np.where(crossover_mask, mutant, pop[i])\n            # Selection\n            if self.current_evaluations < self.budget:\n                if func(offspring) < func(pop[i]):\n                    new_pop[i] = offspring\n                self.current_evaluations += 1\n        return new_pop\n\n    def cma_es_refinement(self, func, x0, bounds):\n        if self.current_evaluations >= self.budget:\n            return x0\n        es = CMAEvolutionStrategy(x0, 0.5, {'bounds': [bounds.lb, bounds.ub], 'popsize': self.population_size})\n        while not es.stop() and self.current_evaluations < self.budget:\n            solutions = es.ask()\n            fitnesses = [func(sol) for sol in solutions]\n            self.current_evaluations += len(solutions)\n            es.tell(solutions, fitnesses)\n            es.disp()\n        return es.result[0]\n\n    def adaptive_layer_refinement(self, func, bounds):\n        layers = 5  # Start with fewer layers for gradual introduction\n        best_solution = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n        while layers <= self.dim and self.current_evaluations < self.budget:\n            # DE phase\n            pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, layers))\n            pop = self.differential_evolution(func, pop, bounds)\n            best_solution = pop[np.argmin([func(ind) for ind in pop])]\n            \n            # CMA-ES phase\n            best_solution = self.cma_es_refinement(func, best_solution, bounds)\n            \n            # Increase complexity by adding layers\n            layers = min(layers + 5, self.dim)\n        \n        return best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_layer_refinement(func, bounds)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Refined the adaptive layer refinement strategy to include a gradual introduction of layers and adjusted the DE crossover probability for enhanced diversity in solution space.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'cma'\").", "error": "ModuleNotFoundError(\"No module named 'cma'\")", "parent_id": "6a9c60a4-a768-46a2-a626-3d12fb074f42", "metadata": {}, "mutation_prompt": null}
{"id": "97e75dcc-77fd-49ba-8923-e66316831202", "solution": "import numpy as np\nfrom cma import CMAEvolutionStrategy\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.de_mutation_factor = 0.8\n        self.de_crossover_prob = 0.9\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, pop, bounds):\n        new_pop = np.copy(pop)\n        for i in range(len(pop)):\n            if self.current_evaluations >= self.budget:\n                break\n            # Randomly select three other individuals\n            indices = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[indices]\n            # Create a mutant vector\n            mutant = a + self.de_mutation_factor * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            # Crossover\n            crossover_mask = np.random.rand(self.dim) < self.de_crossover_prob\n            offspring = np.where(crossover_mask, mutant, pop[i])\n            # Selection\n            if self.current_evaluations < self.budget:\n                if func(offspring) < func(pop[i]):\n                    new_pop[i] = offspring\n                self.current_evaluations += 1\n        return new_pop\n\n    def cma_es_refinement(self, func, x0, bounds):\n        if self.current_evaluations >= self.budget:\n            return x0\n        es = CMAEvolutionStrategy(x0, 0.5, {'bounds': [bounds.lb, bounds.ub], 'popsize': self.population_size})\n        while not es.stop() and self.current_evaluations < self.budget:\n            solutions = es.ask()\n            fitnesses = [func(sol) for sol in solutions]\n            self.current_evaluations += len(solutions)\n            es.tell(solutions, fitnesses)\n            es.disp()\n        return es.result[0]\n\n    def adaptive_layer_refinement(self, func, bounds):\n        layers = 10\n        best_solution = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n        best_solution = func(best_solution)  # Pre-evaluate the initial best solution\n        while layers <= self.dim and self.current_evaluations < self.budget:\n            # DE phase\n            pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, layers))\n            pop = self.differential_evolution(func, pop, bounds)\n            best_solution = pop[np.argmin([func(ind) for ind in pop])]\n            \n            # CMA-ES phase\n            best_solution = self.cma_es_refinement(func, best_solution, bounds)\n            \n            # Increase complexity by adding layers\n            layers = min(layers + 5, self.dim)\n        \n        return best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_layer_refinement(func, bounds)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive layer refinement with pre-evaluation of the initial best solution to ensure efficient layer-based optimization.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'cma'\").", "error": "ModuleNotFoundError(\"No module named 'cma'\")", "parent_id": "6a9c60a4-a768-46a2-a626-3d12fb074f42", "metadata": {}, "mutation_prompt": null}
{"id": "b679a835-9afd-44ad-ae6e-f8cff1f46aeb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution_step(self, population, func, bounds, F=0.8, CR=0.9):\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            indices = np.random.choice(len(population), 3, replace=False)\n            a, b, c = population[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial_fitness = func(trial)\n            self.evaluations += 1\n            if trial_fitness < func(population[i]):\n                new_population[i] = trial\n        return new_population\n\n    def local_refinement(self, candidate, func, bounds):\n        if self.evaluations >= self.budget:\n            return candidate\n        result = minimize(func, candidate, method='Nelder-Mead', bounds=bounds,\n                          options={'maxiter': 10, 'disp': False})\n        self.evaluations += result.nfev\n        return result.x if result.success else candidate\n\n    def progressive_layer_optimization(self, func, bounds, max_layers):\n        population_size = 10\n        current_layers = 10\n        population = np.random.uniform(bounds.lb, bounds.ub, (population_size, self.dim))\n\n        while self.evaluations < self.budget and current_layers <= max_layers:\n            population = self.differential_evolution_step(population, func, bounds)\n            best_candidate = min(population, key=func)\n            self.evaluations += 1\n            best_candidate = self.local_refinement(best_candidate, func, bounds)\n            current_layers += 1\n            self.dim = current_layers * 2  # Assuming each layer adds a new dimension pair\n\n        return best_candidate\n\n    def __call__(self, func):\n        bounds = func.bounds\n        max_layers = self.dim // 2\n        if self.evaluations < self.budget:\n            best_solution = self.progressive_layer_optimization(func, bounds, max_layers)\n        else:\n            best_solution = None\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic optimizer through improved initialization and error handling, aiming for efficient exploration and robustness in multilayer photonic structure optimization.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\")", "parent_id": "2e1960ec-d5c4-4aaa-bb71-aef0a4607a65", "metadata": {}, "mutation_prompt": null}
{"id": "3d76cd6d-3fee-409b-99b1-5eb99069665f", "solution": "import numpy as np\n\ntry:\n    from cma import CMAEvolutionStrategy\nexcept ModuleNotFoundError:\n    CMAEvolutionStrategy = None\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.de_mutation_factor = 0.8\n        self.de_crossover_prob = 0.9\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, pop, bounds):\n        new_pop = np.copy(pop)\n        for i in range(len(pop)):\n            if self.current_evaluations >= self.budget:\n                break\n            # Randomly select three other individuals\n            indices = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[indices]\n            # Create a mutant vector\n            mutant = a + self.de_mutation_factor * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            # Crossover\n            crossover_mask = np.random.rand(self.dim) < self.de_crossover_prob\n            offspring = np.where(crossover_mask, mutant, pop[i])\n            # Selection\n            if self.current_evaluations < self.budget:\n                if func(offspring) < func(pop[i]):\n                    new_pop[i] = offspring\n                self.current_evaluations += 1\n        return new_pop\n\n    def cma_es_refinement(self, func, x0, bounds):\n        if self.current_evaluations >= self.budget or CMAEvolutionStrategy is None:\n            return x0\n        es = CMAEvolutionStrategy(x0, 0.5, {'bounds': [bounds.lb, bounds.ub], 'popsize': self.population_size})\n        while not es.stop() and self.current_evaluations < self.budget:\n            solutions = es.ask()\n            fitnesses = [func(sol) for sol in solutions]\n            self.current_evaluations += len(solutions)\n            es.tell(solutions, fitnesses)\n            es.disp()\n        return es.result[0]\n\n    def adaptive_layer_refinement(self, func, bounds):\n        layers = 10\n        best_solution = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n        while layers <= self.dim and self.current_evaluations < self.budget:\n            # DE phase\n            pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, layers))\n            pop = self.differential_evolution(func, pop, bounds)\n            best_solution = pop[np.argmin([func(ind) for ind in pop])]\n            \n            # CMA-ES phase\n            best_solution = self.cma_es_refinement(func, best_solution, bounds)\n            \n            # Increase complexity by adding layers\n            layers = min(layers + 5, self.dim)\n        \n        return best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_layer_refinement(func, bounds)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved robustness by adding a check for the presence of the 'cma' module and handling the exception gracefully.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\")", "parent_id": "6a9c60a4-a768-46a2-a626-3d12fb074f42", "metadata": {}, "mutation_prompt": null}
{"id": "c28182f6-f09f-4c70-beaa-fb37a9fb2555", "solution": "import numpy as np\nfrom cma import CMAEvolutionStrategy\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.de_mutation_factor = 0.8\n        self.de_crossover_prob = 0.9\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, pop, bounds):\n        new_pop = np.copy(pop)\n        for i in range(len(pop)):\n            if self.current_evaluations >= self.budget:\n                break\n            # Randomly select three other individuals\n            indices = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[indices]\n            # Create a mutant vector\n            mutant = a + self.de_mutation_factor * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            # Crossover\n            crossover_mask = np.random.rand(self.dim) < self.de_crossover_prob\n            offspring = np.where(crossover_mask, mutant, pop[i])\n            # Selection\n            if self.current_evaluations < self.budget:\n                if func(offspring) < func(pop[i]):\n                    new_pop[i] = offspring\n                self.current_evaluations += 1\n        return new_pop\n\n    def cma_es_refinement(self, func, x0, bounds):\n        if self.current_evaluations >= self.budget:\n            return x0\n        es = CMAEvolutionStrategy(x0, 0.4, {'bounds': [bounds.lb, bounds.ub], 'popsize': self.population_size * 2})\n        while not es.stop() and self.current_evaluations < self.budget:\n            solutions = es.ask()\n            fitnesses = [func(sol) for sol in solutions]\n            self.current_evaluations += len(solutions)\n            es.tell(solutions, fitnesses)\n        return es.result[0]\n\n    def adaptive_layer_refinement(self, func, bounds):\n        layers = 10\n        best_solution = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n        while layers <= self.dim and self.current_evaluations < self.budget:\n            # DE phase\n            pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, layers))\n            pop = self.differential_evolution(func, pop, bounds)\n            best_solution = pop[np.argmin([func(ind) for ind in pop])]\n            \n            # CMA-ES phase\n            best_solution = self.cma_es_refinement(func, best_solution, bounds)\n            \n            # Increase complexity by adding layers\n            layers = min(layers + 5, self.dim)\n        \n        return best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_layer_refinement(func, bounds)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Hybrid optimization algorithm combining DE and CMA-ES with enhanced layer-wise refinement and adaptive exploration-exploitation balance for high-dimensional noisy problems.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'cma'\").", "error": "ModuleNotFoundError(\"No module named 'cma'\")", "parent_id": "6a9c60a4-a768-46a2-a626-3d12fb074f42", "metadata": {}, "mutation_prompt": null}
{"id": "4addc517-43a5-4e36-8afc-338930d503bd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution_step(self, population, func, bounds, F=0.8, CR=0.9):\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            indices = np.random.choice(len(population), 3, replace=False)\n            a, b, c = population[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial_fitness = func(trial)\n            self.evaluations += 1\n            if trial_fitness < func(population[i]):\n                new_population[i] = trial\n        return new_population\n\n    def local_refinement(self, candidate, func, bounds):\n        if self.evaluations >= self.budget:\n            return candidate\n        result = minimize(func, candidate, method='Nelder-Mead', bounds=bounds,\n                          options={'maxiter': 10, 'disp': False})\n        self.evaluations += result.nfev\n        return result.x if result.success else candidate\n\n    def progressive_layer_optimization(self, func, bounds, max_layers):\n        population_size = 10\n        current_layers = 10\n        population = np.random.uniform(bounds.lb, bounds.ub, (population_size, self.dim))\n\n        while self.evaluations < self.budget and current_layers <= max_layers:\n            population = self.differential_evolution_step(population, func, bounds)\n            best_candidate = min(population, key=lambda ind: func(ind))\n            self.evaluations += 1\n            best_candidate = self.local_refinement(best_candidate, func, bounds)\n            self.dim = min(current_layers * 2 + 2, max_layers * 2)  # Enhance layer increment logic\n            current_layers += 1\n\n        return best_candidate\n\n    def __call__(self, func):\n        bounds = func.bounds\n        max_layers = self.dim // 2\n        best_solution = self.progressive_layer_optimization(func, bounds, max_layers)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Refined HybridMetaheuristicOptimizer with fixed best candidate initialization and enhanced progressive layer adjustment.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\")", "parent_id": "2e1960ec-d5c4-4aaa-bb71-aef0a4607a65", "metadata": {}, "mutation_prompt": null}
{"id": "39318d8e-c48b-4dc2-ae98-c8e634e7484c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        layer_bounds = [(max(lb, val - 0.1), min(ub, val + 0.1)) for lb, ub, val in zip(bounds.lb, bounds.ub, individual_denorm)]\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=layer_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved layer-based optimization by refining local search strategy using adaptive bounds.", "configspace": "", "generation": 3, "fitness": 0.8357854598217216, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.836 with standard deviation 0.058. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "9944bff1-f45d-4548-bd40-3fd1f5ced2e2", "metadata": {"aucs": [0.755115721391527, 0.8639380610132642, 0.8883025970603735], "final_y": [0.16482810497946265, 0.11855424214845334, 0.11252984710218072]}, "mutation_prompt": null}
{"id": "c29a8655-e0c2-478a-a78a-a42a0b8c9cf2", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass HALO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8  # Differential evolution scale factor\n        self.CR = 0.9 # Crossover probability\n        self.pop = None\n        self.func_evals = 0\n\n    def _initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb - 0.1 * (ub - lb), ub + 0.1 * (ub - lb), (self.population_size, self.dim))  # Diversified initialization\n\n    def _evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.pop])\n        self.func_evals += self.population_size\n        return fitness\n\n    def _differential_evolution_step(self, fitness, lb, ub, func):  # Added 'func' as a parameter\n        next_pop = np.empty_like(self.pop)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.pop[indices]\n            mutant = np.clip(a + self.F * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            trial_fitness = func(trial)\n            self.func_evals += 1\n            if trial_fitness < fitness[i]:\n                next_pop[i] = trial\n            else:\n                next_pop[i] = self.pop[i]\n        self.pop = next_pop\n\n    def _local_refinement(self, func, lb, ub):\n        best_idx = np.argmin(self._evaluate_population(func))\n        best_solution = self.pop[best_idx]\n        result = differential_evolution(func, bounds=[(lb[i], ub[i]) for i in range(self.dim)], maxiter=10, polish=False)  # Using DE for noisy local refinement\n        if result.success and func(result.x) < func(best_solution):\n            self.pop[best_idx] = result.x\n            self.func_evals += 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(lb, ub)\n        fitness = self._evaluate_population(func)\n\n        while self.func_evals < self.budget:\n            self._differential_evolution_step(fitness, lb, ub, func)\n            if self.func_evals < self.budget // 2:\n                self._local_refinement(func, lb, ub)\n            fitness = self._evaluate_population(func)\n\n        best_idx = np.argmin(fitness)\n        return self.pop[best_idx]", "name": "HALO", "description": "Improved HALO by enhancing the local refinement phase using a more suitable optimization method for noisy functions and adjusting the population initialization to ensure diverse solutions.", "configspace": "", "generation": 3, "fitness": 0.8099425693445585, "feedback": "The algorithm HALO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.021. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "bc54f4dd-b05a-4826-88fd-288b62776948", "metadata": {"aucs": [0.8397333848135758, 0.7955905846744983, 0.7945037385456013], "final_y": [0.12445868302892416, 0.1288586721003424, 0.14919383729303382]}, "mutation_prompt": null}
{"id": "851e79e2-8729-4158-8426-137fcd918c8a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.9  # Increased DE mutation factor for better exploration\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        best_idx_overall = np.argmin(self.scores)\n        best_score_overall = self.scores[best_idx_overall]\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if best_score_overall > self.scores[best_idx]:\n                best_score_overall = self.scores[best_idx]\n                best_idx_overall = best_idx\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_solution = bounds.lb + self.population[best_idx_overall] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx_overall]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced Differential Evolution by adjusting mutation factor and incorporating elitism to improve convergence and solution quality.", "configspace": "", "generation": 3, "fitness": 0.8733299996636849, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.022. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "9944bff1-f45d-4548-bd40-3fd1f5ced2e2", "metadata": {"aucs": [0.8453267136633534, 0.8743949853325829, 0.9002682999951185], "final_y": [0.1256755350269676, 0.11490043736864297, 0.10999641391330417]}, "mutation_prompt": null}
{"id": "d695641e-b382-43e2-ab74-a79de37e2abd", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass HALO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30 + dim  # Dynamic population size based on problem dimension\n        self.F = 0.8\n        self.CR = 0.7 + 0.2 * (dim / budget)  # Adaptive crossover probability\n        self.pop = None\n        self.func_evals = 0\n\n    def _initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))  # More focused initialization\n\n    def _evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.pop])\n        self.func_evals += self.population_size\n        return fitness\n\n    def _differential_evolution_step(self, fitness, lb, ub, func):\n        next_pop = np.empty_like(self.pop)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.pop[indices]\n            mutant = np.clip(a + self.F * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            trial_fitness = func(trial)\n            self.func_evals += 1\n            if trial_fitness < fitness[i]:\n                next_pop[i] = trial\n            else:\n                next_pop[i] = self.pop[i]\n        self.pop = next_pop\n\n    def _local_refinement(self, func, lb, ub):\n        best_idx = np.argmin(self._evaluate_population(func))\n        best_solution = self.pop[best_idx]\n        result = differential_evolution(func, bounds=[(lb[i], ub[i]) for i in range(self.dim)], maxiter=10, polish=False)\n        if result.success and func(result.x) < func(best_solution):\n            self.pop[best_idx] = result.x\n            self.func_evals += 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(lb, ub)\n        fitness = self._evaluate_population(func)\n\n        while self.func_evals < self.budget:\n            self._differential_evolution_step(fitness, lb, ub, func)\n            if self.func_evals < self.budget // 2:\n                self._local_refinement(func, lb, ub)\n            fitness = self._evaluate_population(func)\n\n        best_idx = np.argmin(fitness)\n        return self.pop[best_idx]", "name": "HALO", "description": "Enhanced HALO by introducing a dynamic population size and adaptive crossover rate to improve exploration and exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.8023677611577469, "feedback": "The algorithm HALO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.005. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "c29a8655-e0c2-478a-a78a-a42a0b8c9cf2", "metadata": {"aucs": [0.808689117283021, 0.7967023692093704, 0.8017117969808496], "final_y": [0.1285937053735462, 0.14587762493708045, 0.13548532582431028]}, "mutation_prompt": null}
{"id": "0403ab62-e57e-407e-9436-4deaca37f1ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        # Adaptive mutation factor and crossover probability\n        self.F = 0.5 + 0.3 * np.random.rand()\n        self.CR = 0.8 + 0.1 * np.random.rand()\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        layer_bounds = [(max(lb, val - 0.1), min(ub, val + 0.1)) for lb, ub, val in zip(bounds.lb, bounds.ub, individual_denorm)]\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=layer_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def robustness_check(self, func, best_solution, bounds):\n        perturbation = np.random.normal(0, 0.01, size=best_solution.shape)\n        perturbed_solution = best_solution + perturbation\n        perturbed_solution = np.clip(perturbed_solution, bounds.lb, bounds.ub)\n        return func(perturbed_solution)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n                # Perturbation-based robustness check\n                perturb_score = self.robustness_check(func, refined_solution, bounds)\n                if perturb_score < refined_score:\n                    self.scores[best_idx] = perturb_score\n            \n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced the optimization by incorporating adaptive mutation and crossover in DE and introducing a perturbation-based robustness check.", "configspace": "", "generation": 4, "fitness": 0.7751564441183038, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.010. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "39318d8e-c48b-4dc2-ae98-c8e634e7484c", "metadata": {"aucs": [0.7773143123297447, 0.786646665664334, 0.7615083543608323], "final_y": [0.15012870946666845, 0.1574961695205792, 0.16547492986276247]}, "mutation_prompt": null}
{"id": "629faf6c-ebea-4616-9055-b901df6fa7bb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HALO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8  # Differential evolution scale factor\n        self.CR = 0.5  # Changed from 0.9 to 0.5 for adaptive crossover probability\n        self.pop = None\n        self.func_evals = 0\n\n    def _initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.pop])\n        self.func_evals += self.population_size\n        return fitness\n\n    def _differential_evolution_step(self, fitness, lb, ub, func):  # Added 'func' as a parameter\n        next_pop = np.empty_like(self.pop)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.pop[indices]\n            mutant = np.clip(a + self.F * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            trial_fitness = func(trial)\n            self.func_evals += 1\n            if trial_fitness < fitness[i]:\n                next_pop[i] = trial\n            else:\n                next_pop[i] = self.pop[i]\n        self.pop = next_pop\n\n    def _local_refinement(self, func, lb, ub):\n        best_idx = np.argmin(self._evaluate_population(func))\n        best_solution = self.pop[best_idx]\n        result = minimize(func, best_solution, bounds=[(lb[i], ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        if result.success and func(result.x) < func(best_solution):\n            self.pop[best_idx] = result.x\n            self.func_evals += 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(lb, ub)\n        fitness = self._evaluate_population(func)\n\n        while self.func_evals < self.budget:\n            self._differential_evolution_step(fitness, lb, ub, func)\n            if self.func_evals < self.budget // 2:\n                self._local_refinement(func, lb, ub)\n            fitness = self._evaluate_population(func)\n\n        best_idx = np.argmin(fitness)\n        return self.pop[best_idx]", "name": "HALO", "description": "Enhanced HALO by introducing an adaptive crossover probability to balance exploration and exploitation dynamically.", "configspace": "", "generation": 4, "fitness": 0.8296720921365358, "feedback": "The algorithm HALO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.031. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "bc54f4dd-b05a-4826-88fd-288b62776948", "metadata": {"aucs": [0.8727094629508108, 0.8156150010486138, 0.8006918124101828], "final_y": [0.12037362367284243, 0.13229063307229305, 0.1481169059808729]}, "mutation_prompt": null}
{"id": "b605ea0f-0e24-48d3-beeb-b87fa6aa8587", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass HALO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.85  # Differential evolution scale factor (adjusted)\n        self.CR = 0.9 # Crossover probability\n        self.pop = None\n        self.func_evals = 0\n\n    def _initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb - 0.1 * (ub - lb), ub + 0.1 * (ub - lb), (self.population_size, self.dim))  # Diversified initialization\n\n    def _evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.pop])\n        self.func_evals += self.population_size\n        return fitness\n\n    def _differential_evolution_step(self, fitness, lb, ub, func):  # Added 'func' as a parameter\n        next_pop = np.empty_like(self.pop)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.pop[indices]\n            mutant = np.clip(a + self.F * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            trial_fitness = func(trial)\n            self.func_evals += 1\n            if trial_fitness < fitness[i]:\n                next_pop[i] = trial\n            else:\n                next_pop[i] = self.pop[i]\n        self.pop = next_pop\n\n    def _local_refinement(self, func, lb, ub):\n        best_idx = np.argmin(self._evaluate_population(func))\n        best_solution = self.pop[best_idx]\n        result = differential_evolution(func, bounds=[(lb[i], ub[i]) for i in range(self.dim)], maxiter=10, polish=False)  # Using DE for noisy local refinement\n        if result.success and func(result.x) < func(best_solution):\n            self.pop[best_idx] = result.x\n            self.func_evals += 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(lb, ub)\n        fitness = self._evaluate_population(func)\n\n        while self.func_evals < self.budget:\n            self._differential_evolution_step(fitness, lb, ub, func)\n            if self.func_evals < self.budget // 2:\n                self._local_refinement(func, lb, ub)\n            fitness = self._evaluate_population(func)\n\n        best_idx = np.argmin(fitness)\n        return self.pop[best_idx]", "name": "HALO", "description": "Improved the HALO algorithm by adjusting the differential evolution scale factor to enhance exploration capabilities.", "configspace": "", "generation": 4, "fitness": 0.8036241498944247, "feedback": "The algorithm HALO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.010. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "c29a8655-e0c2-478a-a78a-a42a0b8c9cf2", "metadata": {"aucs": [0.7928660502598305, 0.8014203941920448, 0.8165860052313987], "final_y": [0.14185857368928623, 0.14227739783683002, 0.13825670479951246]}, "mutation_prompt": null}
{"id": "427b11eb-9a60-47bb-ba56-c1e4dbc5ffb1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.5 + np.random.rand() * 0.5  # Adaptive DE mutation factor\n        self.CR = 0.8 + np.random.rand() * 0.2  # Adaptive DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        layer_bounds = [(max(lb, val - 0.1), min(ub, val + 0.1)) for lb, ub, val in zip(bounds.lb, bounds.ub, individual_denorm)]\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=layer_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                with ThreadPoolExecutor() as executor:\n                    future = executor.submit(self.local_search, func, best_individual, bounds)\n                    refined_solution, refined_score = future.result()\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved exploration-exploitation balance by adaptive mutation and crossover rates and introducing parallel local searches.", "configspace": "", "generation": 4, "fitness": 0.771125153754345, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.771 with standard deviation 0.018. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "39318d8e-c48b-4dc2-ae98-c8e634e7484c", "metadata": {"aucs": [0.7597882862497648, 0.7968690946556014, 0.7567180803576687], "final_y": [0.1578784966371678, 0.14185127619609494, 0.15374384957458465]}, "mutation_prompt": null}
{"id": "bb4be052-2c83-4180-bb67-91c769f2648c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.9  # Increased DE mutation factor for better exploration\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n                self.F = min(1.0, self.F + 0.05)  # Dynamic adaptation of F\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        best_idx_overall = np.argmin(self.scores)\n        best_score_overall = self.scores[best_idx_overall]\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if best_score_overall > self.scores[best_idx]:\n                best_score_overall = self.scores[best_idx]\n                best_idx_overall = best_idx\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_solution = bounds.lb + self.population[best_idx_overall] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx_overall]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation strategy by introducing dynamic adaptation based on success rates of past generations.", "configspace": "", "generation": 5, "fitness": 0.8082739477424529, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.022. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "851e79e2-8729-4158-8426-137fcd918c8a", "metadata": {"aucs": [0.7784414689499943, 0.8314602843886792, 0.8149200898886855], "final_y": [0.15502763482363413, 0.133176384216512, 0.1367218892451637]}, "mutation_prompt": null}
{"id": "096d5182-2304-406d-b357-3c84644c6022", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(min(100, 10 * dim), dim)  # Adaptive population size\n        self.scores = np.full(self.population.shape[0], np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.population.shape[0]):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.population.shape[0]):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population.shape[0]) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        layer_bounds = [(max(lb, val - 0.1), min(ub, val + 0.1)) for lb, ub, val in zip(bounds.lb, bounds.ub, individual_denorm)]\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=layer_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration by introducing an adaptive population size based on the current evaluation budget.", "configspace": "", "generation": 5, "fitness": 0.8025149115550398, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.026. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "39318d8e-c48b-4dc2-ae98-c8e634e7484c", "metadata": {"aucs": [0.7977780831778373, 0.7738997291025193, 0.8358669223847632], "final_y": [0.14676533191087082, 0.1538454819032281, 0.13130247615310464]}, "mutation_prompt": null}
{"id": "f027c278-2265-49dc-8141-33ff4eb7b3ba", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.population)  # Adapt mutation factor\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced robustness and exploration through adaptive population diversity control and improved local search integration.", "configspace": "", "generation": 5, "fitness": 0.8588856755515214, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.009. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "9944bff1-f45d-4548-bd40-3fd1f5ced2e2", "metadata": {"aucs": [0.8711350754895022, 0.8572521802743732, 0.8482697708906887], "final_y": [0.1154596403241469, 0.12278307239214503, 0.126996708347776]}, "mutation_prompt": null}
{"id": "0c0f9a0f-4cf4-4d1f-a541-b2209743ef52", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.3  # Adapt F\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration by integrating adaptive F and CR parameters into differential evolution for better convergence.", "configspace": "", "generation": 5, "fitness": 0.8469688921982318, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.847 with standard deviation 0.018. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "9944bff1-f45d-4548-bd40-3fd1f5ced2e2", "metadata": {"aucs": [0.8426589101749061, 0.8702603474266233, 0.8279874189931662], "final_y": [0.12665380362850298, 0.11757503386724621, 0.13670563605805452]}, "mutation_prompt": null}
{"id": "8a5a6e45-eb10-4784-b79e-31781c9668a8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HALO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8  # Differential evolution scale factor\n        self.CR = 0.5  # Changed from 0.9 to 0.5 for adaptive crossover probability\n        self.pop = None\n        self.func_evals = 0\n\n    def _initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.pop])\n        self.func_evals += self.population_size\n        return fitness\n\n    def _differential_evolution_step(self, fitness, lb, ub, func):\n        next_pop = np.empty_like(self.pop)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.pop[indices]\n            self.F = 0.5 + 0.3 * (1 - self.func_evals / self.budget)  # Adaptive mutation scaling\n            mutant = np.clip(a + self.F * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            trial_fitness = func(trial)\n            self.func_evals += 1\n            if trial_fitness < fitness[i]:\n                next_pop[i] = trial\n            else:\n                next_pop[i] = self.pop[i]\n        self.pop = next_pop\n\n    def _local_refinement(self, func, lb, ub):\n        if self.func_evals < (3 * self.budget) // 4:  # Adjusted local refinement frequency\n            best_idx = np.argmin(self._evaluate_population(func))\n            best_solution = self.pop[best_idx]\n            result = minimize(func, best_solution, bounds=[(lb[i], ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n            if result.success and func(result.x) < func(best_solution):\n                self.pop[best_idx] = result.x\n                self.func_evals += 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(lb, ub)\n        fitness = self._evaluate_population(func)\n\n        while self.func_evals < self.budget:\n            self._differential_evolution_step(fitness, lb, ub, func)\n            fitness = self._evaluate_population(func)\n\n        best_idx = np.argmin(fitness)\n        return self.pop[best_idx]", "name": "HALO", "description": "Enhanced HALO by adjusting local refinement frequency and introducing adaptive mutation scaling for improved exploration-exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.8180500383792353, "feedback": "The algorithm HALO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.011. And the mean value of best solutions found was 0.143 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "629faf6c-ebea-4616-9055-b901df6fa7bb", "metadata": {"aucs": [0.8026359294137815, 0.8233679546722035, 0.828146231051721], "final_y": [0.1513343159289644, 0.13331122496725223, 0.1433044930385009]}, "mutation_prompt": null}
{"id": "960fcbb3-78a4-4978-9d3e-b6b7c21a4b79", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.9  # Increased DE mutation factor for better exploration\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        adaptive_bounds = list(zip(bounds.lb * 0.9, bounds.ub * 1.1))  # Modified line\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=adaptive_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        best_idx_overall = np.argmin(self.scores)\n        best_score_overall = self.scores[best_idx_overall]\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if best_score_overall > self.scores[best_idx]:\n                best_score_overall = self.scores[best_idx]\n                best_idx_overall = best_idx\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_solution = bounds.lb + self.population[best_idx_overall] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx_overall]", "name": "HybridMetaheuristicOptimizer", "description": "Improved solution quality by enhancing the local search step with adaptive bounds scaling.", "configspace": "", "generation": 6, "fitness": 0.8049208371043092, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.805 with standard deviation 0.036. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "851e79e2-8729-4158-8426-137fcd918c8a", "metadata": {"aucs": [0.8545021955417702, 0.7919098067848231, 0.7683505089863345], "final_y": [0.12492293036687496, 0.15150708413727154, 0.15695249389771604]}, "mutation_prompt": null}
{"id": "de6e0f31-abc1-4fe1-8914-9f7953cd998e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8\n        self.CR = 0.9\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                denorm = self.denormalize(self.population[i], func.bounds)\n                self.scores[i] = func(denorm)\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.5\n            self.CR = 0.7 + np.random.rand() * 0.3\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = self.denormalize(trial, bounds)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def denormalize(self, individual, bounds):\n        return bounds.lb + individual * (bounds.ub - bounds.lb)\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = self.denormalize(individual, bounds)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n    \n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim and self.evaluations % 50 == 0:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = self.denormalize(self.population[best_idx], bounds)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Integrating adaptive mutation and crossover with modularity detection and staged dimensionality growth for robust solution convergence.", "configspace": "", "generation": 6, "fitness": 0.8235285890672507, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.030. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "0c0f9a0f-4cf4-4d1f-a541-b2209743ef52", "metadata": {"aucs": [0.8618939235029561, 0.8212768045266877, 0.7874150391721083], "final_y": [0.11716513883326907, 0.13753668833967347, 0.1567498778894313]}, "mutation_prompt": null}
{"id": "d4aafe8b-0047-440d-898a-22640d8767db", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.7 + 0.2 * np.random.rand()  # Dynamically adjust DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        layer_bounds = [(max(lb, val - 0.1), min(ub, val + 0.1)) for lb, ub, val in zip(bounds.lb, bounds.ub, individual_denorm)]\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=layer_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.005:  # Enhanced variance threshold\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced differential evolution by dynamically adjusting crossover probability and integrating variance-based segment detection.", "configspace": "", "generation": 6, "fitness": 0.7751827501947238, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.019. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "39318d8e-c48b-4dc2-ae98-c8e634e7484c", "metadata": {"aucs": [0.792123456653679, 0.7491992509654856, 0.7842255429650069], "final_y": [0.1452594112217216, 0.16766396069332623, 0.14835540673490322]}, "mutation_prompt": null}
{"id": "d360863d-44e9-44f6-8716-3ffb2abb833d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        layer_bounds = [(max(lb, val - 0.1), min(ub, val + 0.1)) for lb, ub, val in zip(bounds.lb, bounds.ub, individual_denorm)]\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=layer_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.005:  # Adjusted threshold for better detection\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def adapt_parameters(self):\n        self.F = max(0.5, min(1.0, self.F + 0.1 * np.random.randn()))  # Adaptive F adjustment\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            self.adapt_parameters()  # Integrate adaptive parameter adjustment\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Refined strategy integrating adaptive control for DE parameters and enhanced modular structure detection.", "configspace": "", "generation": 6, "fitness": 0.7655612781885038, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.766 with standard deviation 0.005. And the mean value of best solutions found was 0.155 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "39318d8e-c48b-4dc2-ae98-c8e634e7484c", "metadata": {"aucs": [0.7679577113583833, 0.7587089560063773, 0.7700171672007508], "final_y": [0.15172176385096303, 0.1541110296294529, 0.15863525459471406]}, "mutation_prompt": null}
{"id": "cc361482-a458-4210-9e1c-a7fd8fc1812c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.3  # Adapt F\n            learning_rate = 1.0 - (self.evaluations / self.budget)  # Adaptive learning rate\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + learning_rate * (trial * (bounds.ub - bounds.lb))  # Apply learning rate\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Integrate adaptive learning rate in differential evolution to enhance convergence speed and solution quality.", "configspace": "", "generation": 6, "fitness": 0.7800150251542356, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.051. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "0c0f9a0f-4cf4-4d1f-a541-b2209743ef52", "metadata": {"aucs": [0.8502237368420615, 0.7584023886279003, 0.7314189499927448], "final_y": [0.1233159760076189, 0.14372225012068018, 0.1732193506914651]}, "mutation_prompt": null}
{"id": "831fed62-fe1b-4519-ad06-6d2125ee0ff2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # Adjusted DE mutation factor for better balance\n        self.CR = 0.6  # Adjusted crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            F_dynamic = np.random.uniform(0.5, self.F)  # Dynamic mutation factor\n            mutant = np.clip(a + F_dynamic * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        best_idx_overall = np.argmin(self.scores)\n        best_score_overall = self.scores[best_idx_overall]\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if best_score_overall > self.scores[best_idx]:\n                best_score_overall = self.scores[best_idx]\n                best_idx_overall = best_idx\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_solution = bounds.lb + self.population[best_idx_overall] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx_overall]", "name": "HybridMetaheuristicOptimizer", "description": "Improved balance between exploration and exploitation by introducing adaptive mutation and crossover rates, and enhancing local search efficiency.", "configspace": "", "generation": 7, "fitness": 0.8404741799068348, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.031. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "851e79e2-8729-4158-8426-137fcd918c8a", "metadata": {"aucs": [0.8816047820359298, 0.8315655695838102, 0.8082521881007646], "final_y": [0.1168125245196121, 0.13391259713782944, 0.14235737415155614]}, "mutation_prompt": null}
{"id": "7fe638c6-bcfc-416d-970a-7e1ebc65193f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.population, axis=0).mean()  # Adapt mutation factor\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation factor adaptation by dynamically adjusting based on population diversity to improve solution quality.", "configspace": "", "generation": 7, "fitness": 0.8631482091362642, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.026. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "f027c278-2265-49dc-8141-33ff4eb7b3ba", "metadata": {"aucs": [0.8272115684178611, 0.8864357171224745, 0.8757973418684568], "final_y": [0.13317575732393339, 0.11602974249774578, 0.1193655799694725]}, "mutation_prompt": null}
{"id": "851304dc-c239-4932-939b-af80186738e9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population, axis=0).mean()\n        self.F = 0.5 + 0.3 * (1 - diversity)  # Adaptive mutation factor based on diversity\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        layer_bounds = [(max(lb, val - 0.1), min(ub, val + 0.1)) for lb, ub, val in zip(bounds.lb, bounds.ub, individual_denorm)]\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=layer_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved exploration by introducing adaptive mutation factor in differential evolution based on population diversity.", "configspace": "", "generation": 7, "fitness": 0.8100377356659433, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.056. And the mean value of best solutions found was 0.143 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "39318d8e-c48b-4dc2-ae98-c8e634e7484c", "metadata": {"aucs": [0.7552983072733397, 0.7879614935191466, 0.8868534062053438], "final_y": [0.16502089772180706, 0.15076654550788904, 0.1117618770381279]}, "mutation_prompt": null}
{"id": "9e135b1c-daa4-4e7d-838d-405f60372df8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.3  # Adapt F\n            self.CR = 0.8 + 0.2 * np.random.rand()  # Dynamic CR\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)  # Adaptive pop size\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced balance between exploration and exploitation by introducing adaptive population size and dynamic crossover strategy.", "configspace": "", "generation": 7, "fitness": 0.8655725553651363, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.000. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "0c0f9a0f-4cf4-4d1f-a541-b2209743ef52", "metadata": {"aucs": [0.8657189690369282, 0.8653183368499274, 0.8656803602085534], "final_y": [0.11682198099812935, 0.12133065013858513, 0.11562445351998007]}, "mutation_prompt": null}
{"id": "dffd9294-b860-473c-9b22-12aa95252f70", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.population)  # Adapt mutation factor\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search by adjusting method to 'trust-constr' for improved convergence.", "configspace": "", "generation": 7, "fitness": 0.8503128970749403, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.850 with standard deviation 0.022. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "f027c278-2265-49dc-8141-33ff4eb7b3ba", "metadata": {"aucs": [0.8212644763472812, 0.8730075385495576, 0.8566666763279824], "final_y": [0.13302422200110064, 0.11622917806710553, 0.12360794847044576]}, "mutation_prompt": null}
{"id": "34c9a65b-eb30-4a5b-b2f6-1c8a8c27b1c0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.population)  # Adapt mutation factor\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced adaptive crossover probability based on population diversity to enhance solution quality.", "configspace": "", "generation": 8, "fitness": 0.8767628949658518, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.877 with standard deviation 0.005. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "dffd9294-b860-473c-9b22-12aa95252f70", "metadata": {"aucs": [0.8698955277260721, 0.8823906159745766, 0.8780025411969066], "final_y": [0.11827487887720545, 0.11706912808476244, 0.11488973560673499]}, "mutation_prompt": null}
{"id": "1936ee1d-2e94-44de-ae30-56bcb52b9b65", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.4 * np.std(self.population)  # Adapt mutation factor\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved mutation factor adaptation for more effective exploration and exploitation balance.", "configspace": "", "generation": 8, "fitness": 0.8470861194225533, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.847 with standard deviation 0.025. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "f027c278-2265-49dc-8141-33ff4eb7b3ba", "metadata": {"aucs": [0.8498388019519638, 0.8155364433620383, 0.8758831129536578], "final_y": [0.12638665389089476, 0.12884206498737483, 0.11936609820085842]}, "mutation_prompt": null}
{"id": "d4373a9e-8327-4321-886e-1eb15ce94a14", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.9  # Increased DE mutation factor for better exploration\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        elite_idx = np.argmin(self.scores)  # Added line to preserve top solutions\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget or i == elite_idx:\n                break  # Ensure elite solution is preserved\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        best_idx_overall = np.argmin(self.scores)\n        best_score_overall = self.scores[best_idx_overall]\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if best_score_overall > self.scores[best_idx]:\n                best_score_overall = self.scores[best_idx]\n                best_idx_overall = best_idx\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_solution = bounds.lb + self.population[best_idx_overall] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx_overall]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced elitism by preserving top solutions across generations to improve convergence speed.", "configspace": "", "generation": 8, "fitness": 0.8394478437434452, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.839 with standard deviation 0.036. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "851e79e2-8729-4158-8426-137fcd918c8a", "metadata": {"aucs": [0.8016627408146298, 0.8295170168362191, 0.8871637735794865], "final_y": [0.13953098700030486, 0.13503510077223835, 0.11176883220266087]}, "mutation_prompt": null}
{"id": "4f6960b6-82ba-448a-bfd0-86083ee189e8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.4  # Increase F range\n            self.CR = 0.7 + 0.3 * np.random.rand()  # Adjust CR range\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='Powell', bounds=list(zip(bounds.lb, bounds.ub)))  # Use Powell method\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)  # Adaptive pop size\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive local search integration and fine-tuned mutation strategy to improve exploration and exploitation balance.", "configspace": "", "generation": 8, "fitness": 0.8819648251438771, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.014. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "9e135b1c-daa4-4e7d-838d-405f60372df8", "metadata": {"aucs": [0.8633157573307395, 0.886121333592357, 0.8964573845085346], "final_y": [0.11741242719283895, 0.11602711837802027, 0.11163410876986402]}, "mutation_prompt": null}
{"id": "1ef07468-72b7-48f9-b7c7-8f43e54b34e9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.4 * np.std(self.population, axis=0).mean()  # Adapt mutation factor\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation factor adaptation and refined local search by adjusting based on population diversity and using alternative optimization method for improved solution quality.", "configspace": "", "generation": 8, "fitness": 0.8726561831850502, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.013. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "7fe638c6-bcfc-416d-970a-7e1ebc65193f", "metadata": {"aucs": [0.8866066199833994, 0.8546594870681706, 0.8767024425035805], "final_y": [0.11196015421074679, 0.12100481420908493, 0.11531789099840839]}, "mutation_prompt": null}
{"id": "07c2c964-0a44-438b-b597-cfab0d1911b2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.4 + np.random.rand() * 0.5  # Modify F range for more adaptability\n            self.CR = 0.6 + 0.4 * np.random.rand()  # Adjust CR range for dynamic crossover\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='Powell', bounds=list(zip(bounds.lb, bounds.ub)))  # Use Powell method\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)  # Adaptive pop size\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced adaptive mutation factor adjustment and dynamic crossover to enhance exploration-exploitation balance.", "configspace": "", "generation": 9, "fitness": 0.8403775200391812, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.026. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "4f6960b6-82ba-448a-bfd0-86083ee189e8", "metadata": {"aucs": [0.8044258661562401, 0.8629557464538627, 0.8537509475074408], "final_y": [0.13537633672592342, 0.1200453776448539, 0.12313319551216406]}, "mutation_prompt": null}
{"id": "be9226cd-dce8-40d0-ba00-9c0fb573ce41", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.4 * np.std(self.population, axis=0).mean()  # Adapt mutation factor\n            self.CR = np.mean(self.scores) / (np.std(self.scores) + 1e-8)  # Dynamic crossover strategy\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation factor adaptation and local search integration by introducing a dynamic crossover strategy based on current evolution success to improve solution quality.", "configspace": "", "generation": 9, "fitness": 0.8573506701725883, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.015. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "1ef07468-72b7-48f9-b7c7-8f43e54b34e9", "metadata": {"aucs": [0.8369479260866394, 0.8615353059219873, 0.8735687785091378], "final_y": [0.11889317441579672, 0.11836876513578143, 0.11562211855157722]}, "mutation_prompt": null}
{"id": "6f4c1805-1246-45b4-a6c1-cda9e25bfc82", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.scores)  # Adapt mutation factor based on fitness variance\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved convergence by introducing adaptive F and CR based on fitness variance.", "configspace": "", "generation": 9, "fitness": 0.8811960294783878, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.881 with standard deviation 0.014. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "34c9a65b-eb30-4a5b-b2f6-1c8a8c27b1c0", "metadata": {"aucs": [0.86210139586791, 0.886201743530128, 0.8952849490371253], "final_y": [0.12047515982366708, 0.11280370858096356, 0.11162669748550225]}, "mutation_prompt": null}
{"id": "888af7bb-9a4c-4433-8d7f-1a60cf2bcd9a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.3  # Adapt F\n            self.CR = 0.8 + 0.2 * np.random.rand()  # Dynamic CR\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c) + 0.1 * (np.random.rand(self.dim) - 0.5), 0, 1)  # Added diversity\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)  # Adaptive pop size\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved exploration by modifying the mutation strategy to consider more diverse candidate solutions.", "configspace": "", "generation": 9, "fitness": 0.8430406181881457, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.021. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "9e135b1c-daa4-4e7d-838d-405f60372df8", "metadata": {"aucs": [0.8254755332670043, 0.8307064028517137, 0.8729399184457192], "final_y": [0.13594241033076915, 0.13391165390511006, 0.11936340833562509]}, "mutation_prompt": null}
{"id": "31d80c54-c460-427b-9d89-7314aed88413", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            top_indices = self.scores.argsort()[:max(1, self.pop_size // 10)]\n            top_diversity = np.std(self.population[top_indices], axis=0).mean()  # Diversity in top individuals\n            self.F = 0.5 + 0.4 * top_diversity  # Adapt mutation factor based on top diversity\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if np.random.rand() < 0.7:  # Probabilistic trust region approach\n            perturbed_individual = individual_denorm + np.random.uniform(-0.01, 0.01, size=self.dim)\n            perturbed_individual = np.clip(perturbed_individual, bounds.lb, bounds.ub)\n            perturbed_score = func(perturbed_individual)\n            if perturbed_score < result.fun:\n                return perturbed_individual, perturbed_score\n        return (result.x, result.fun) if result.success else (individual_denorm, func(individual_denorm))\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation factor adaptation by incorporating diversity within top individuals and refined local search using a probabilistic trust region approach. ", "configspace": "", "generation": 9, "fitness": 0.8519126388256691, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.029. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "1ef07468-72b7-48f9-b7c7-8f43e54b34e9", "metadata": {"aucs": [0.8578462959080333, 0.8841301089329308, 0.8137615116360432], "final_y": [0.12132363915430355, 0.11602564839288765, 0.13130462670352216]}, "mutation_prompt": null}
