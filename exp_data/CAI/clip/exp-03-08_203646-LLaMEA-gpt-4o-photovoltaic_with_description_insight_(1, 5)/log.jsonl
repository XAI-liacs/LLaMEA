{"id": "61f130e2-b8d3-4693-a9e0-528211b26e74", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm that combines Differential Evolution (DE) for global search with a local search strategy, incorporating layer modularity detection and robustness metrics, gradually increasing complexity to optimize multilayered photonic structures.", "configspace": "", "generation": 0, "fitness": 0.7876070307361857, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.788 with standard deviation 0.025. And the mean value of best solutions found was 0.149 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": null, "metadata": {"aucs": [0.7520163374528934, 0.803116244873461, 0.8076885098822026], "final_y": [0.16488723910552805, 0.1397156956496598, 0.1415408448762916]}, "mutation_prompt": null}
{"id": "010c2bb3-a858-4a1b-8482-8feb511e2e65", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic that improves search efficiency by adjusting the mutation factor dynamically based on population diversity.", "configspace": "", "generation": 1, "fitness": 0.7149944991511855, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.715 with standard deviation 0.054. And the mean value of best solutions found was 0.175 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "61f130e2-b8d3-4693-a9e0-528211b26e74", "metadata": {"aucs": [0.7912044668731412, 0.6701120701252603, 0.6836669604551551], "final_y": [0.14920055428084433, 0.20786468114489565, 0.16781345950320103]}, "mutation_prompt": null}
{"id": "85f62359-72ea-47c5-adc0-6c4972607695", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR * (1 - self.evaluations / self.budget)\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic combining DE with adaptive crossover probability for improved exploration and exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.6997522399133219, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.700 with standard deviation 0.051. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "61f130e2-b8d3-4693-a9e0-528211b26e74", "metadata": {"aucs": [0.771280166670643, 0.6613516104264628, 0.6666249426428601], "final_y": [0.14475320829271665, 0.19373420917995898, 0.1934458438585368]}, "mutation_prompt": null}
{"id": "830b3993-19f1-4887-8403-370e6d6a35b9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = F * (0.5 + np.random.rand()/2)  # Adaptive mutation factor\n                mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n                CR_adaptive = CR * (0.5 + np.random.rand()/2)  # Adaptive crossover probability\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic algorithm utilizing adaptive mutation and crossover strategies to optimize multilayered photonic structures.", "configspace": "", "generation": 1, "fitness": 0.7042113579663757, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.704 with standard deviation 0.057. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.032.", "error": "", "parent_id": "61f130e2-b8d3-4693-a9e0-528211b26e74", "metadata": {"aucs": [0.7852833799419121, 0.6644810535698966, 0.6628696403873184], "final_y": [0.1334587532635957, 0.18898050329291338, 0.20825406498252785]}, "mutation_prompt": null}
{"id": "029220a1-fc68-4669-b771-4068ac67ca16", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.base_F = 0.5  # Base mutation factor\n        self.base_CR = 0.9  # Base crossover probability\n        self.evaluations = 0\n        self.layer_increment = 5  # Increment layers in steps\n        self.current_layers = self.layer_increment\n\n    def adaptive_differential_evolution(self, func, bounds, pop_size=50):\n        F = self.base_F + np.random.rand() * 0.3  # Adaptive F\n        CR = self.base_CR - np.random.rand() * 0.1  # Adaptive CR\n        pop = np.random.rand(pop_size, self.current_layers) * (bounds.ub[:self.current_layers] - bounds.lb[:self.current_layers]) + bounds.lb[:self.current_layers]\n        best_idx = np.argmin([func(np.pad(ind, (0, self.dim - self.current_layers), 'constant')) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb[:self.current_layers], bounds.ub[:self.current_layers])\n                cross_points = np.random.rand(self.current_layers) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.current_layers)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(np.pad(trial, (0, self.dim - self.current_layers), 'constant'))\n                self.evaluations += 1\n                if f < func(np.pad(pop[i], (0, self.dim - self.current_layers), 'constant')):\n                    pop[i] = trial\n                    if f < func(np.pad(best, (0, self.dim - self.current_layers), 'constant')):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Increase complexity gradually\n            if self.evaluations % (self.budget // 4) == 0 and self.current_layers < self.dim:\n                self.current_layers = min(self.current_layers + self.layer_increment, self.dim)\n                pop = np.pad(pop, ((0, 0), (0, self.layer_increment)), 'constant')\n        return np.pad(best, (0, self.dim - self.current_layers), 'constant')\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.adaptive_differential_evolution(func, bounds, self.pop_size)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhancing the hybrid metaheuristic by introducing adaptive DE parameters and layered complexity control for improved exploration and exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.6770206163702049, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.677 with standard deviation 0.005. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "61f130e2-b8d3-4693-a9e0-528211b26e74", "metadata": {"aucs": [0.6770424331908649, 0.6827983654563118, 0.6712210504634384], "final_y": [0.20238956327700197, 0.19845775728223347, 0.20469669319689576]}, "mutation_prompt": null}
{"id": "0b6703f3-8aca-42d4-935b-339f5972eaaf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = F * (1 - self.evaluations/self.budget)  # Adaptive mutation factor\n                mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        options = {'maxiter': 100}  # Limit iterations for efficiency\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options=options)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced HybridMetaheuristic by incorporating adaptive mutation factor and improving local search efficiency.", "configspace": "", "generation": 1, "fitness": 0.7082803283034652, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.708 with standard deviation 0.042. And the mean value of best solutions found was 0.181 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "61f130e2-b8d3-4693-a9e0-528211b26e74", "metadata": {"aucs": [0.7666014151130248, 0.6852798115349374, 0.6729597582624336], "final_y": [0.14976152446058721, 0.19960235370017343, 0.19487595642296107]}, "mutation_prompt": null}
{"id": "40b586c6-0ecb-479f-8bbb-5bcf843dd57f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                pop_size = min(100, int(pop_size * 1.1))\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with adaptive population size based on convergence to improve search efficiency.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 54 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 54 is out of bounds for axis 0 with size 50')", "parent_id": "010c2bb3-a858-4a1b-8482-8feb511e2e65", "metadata": {}, "mutation_prompt": null}
{"id": "bf7a7110-f729-4f0f-a6c0-cc112cdd1f55", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n\n            for i in range(pop_size):\n                if self.evaluations >= self.budget * 0.7:\n                    break  # Reserve budget for local search\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n        return best\n\n    def local_search(self, func, x0, bounds):\n        if self.evaluations < self.budget:\n            res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n            self.evaluations += res.nfev\n            return res.x\n        return x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Multi-Phase Hybrid Metaheuristic that balances exploration and exploitation by dynamically allocating resources between global and local search phases. ", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 53 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 53 is out of bounds for axis 0 with size 50')", "parent_id": "010c2bb3-a858-4a1b-8482-8feb511e2e65", "metadata": {}, "mutation_prompt": null}
{"id": "9fc0d0bb-102b-48fb-ad52-bc70eb560434", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor and crossover probability based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())  # This line is changed\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic that adjusts both mutation factor and crossover probability dynamically based on population diversity.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 50 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 50 is out of bounds for axis 0 with size 50')", "parent_id": "010c2bb3-a858-4a1b-8482-8feb511e2e65", "metadata": {}, "mutation_prompt": null}
{"id": "7fa5b21f-16ea-424f-86da-3b8969362266", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.3 * (diversity / (bounds.ub - bounds.lb).mean())  # Adjust crossover probability\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with adaptive crossover probability to improve convergence by balancing exploration and exploitation.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 54 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 54 is out of bounds for axis 0 with size 50')", "parent_id": "010c2bb3-a858-4a1b-8482-8feb511e2e65", "metadata": {}, "mutation_prompt": null}
{"id": "4abae252-8667-482a-83f2-5a14bf07ad34", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n\n            # Change: Introduce dynamic crossover probability adjustment\n            CR = 0.9 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improved Hybrid Metaheuristic that leverages a dynamic crossover probability to enhance convergence speed.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 52 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 52 is out of bounds for axis 0 with size 50')", "parent_id": "010c2bb3-a858-4a1b-8482-8feb511e2e65", "metadata": {}, "mutation_prompt": null}
{"id": "51b4c9b6-8b3a-49b5-bbfb-e35cf1d3f2ab", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.4 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                pop_size = min(100, int(pop_size * 1.1))\n                pop = np.clip(pop[:pop_size], bounds.lb, bounds.ub)  # Ensure population size is consistent\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='trust-constr')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with adaptive diversity control and improved local search for high-dimensional noisy optimization.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 50 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 50 is out of bounds for axis 0 with size 50')", "parent_id": "40b586c6-0ecb-479f-8bbb-5bcf843dd57f", "metadata": {}, "mutation_prompt": null}
{"id": "9d244628-b0a7-4fae-b834-906c5768f3b3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Adjust mutation based on diversity and adaptively adjust pop_size\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            pop_size = min(100, max(20, int(pop_size * (1 + 0.1 * np.sin(2 * np.pi * self.evaluations / self.budget)))))\n            for i in range(pop_size):\n                idxs = np.delete(np.arange(pop_size), i)\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + 0.01 * np.linalg.norm(trial - pop[i])  # Add robustness metric\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced adaptive hybrid algorithm with modular evolution and robustness integration to efficiently tackle high-dimensional noisy optimization.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 50 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 50 is out of bounds for axis 0 with size 50')", "parent_id": "40b586c6-0ecb-479f-8bbb-5bcf843dd57f", "metadata": {}, "mutation_prompt": null}
{"id": "ec7b3147-6a9f-4107-ae45-9ccf6e6b1c98", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                pop_size = min(100, int(pop_size * 1.1))\n                pop = np.resize(pop, (pop_size, self.dim))  # Ensure population size matches\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance robustness by adjusting adaptive population size and ensuring valid index selection to improve search efficiency.", "configspace": "", "generation": 3, "fitness": 0.7919452691988559, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.792 with standard deviation 0.010. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "40b586c6-0ecb-479f-8bbb-5bcf843dd57f", "metadata": {"aucs": [0.7788357874047149, 0.7955467078736779, 0.8014533123181746], "final_y": [0.15894296940566877, 0.14548159236012215, 0.14845543689201657]}, "mutation_prompt": null}
{"id": "1fb79c96-a765-4134-b1ac-f98f3a57932e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Adaptive Hybrid Metaheuristic using error-checking to prevent index out-of-bounds errors during adaptive population size adjustments.", "configspace": "", "generation": 3, "fitness": 0.8067758487705835, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.807 with standard deviation 0.009. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "40b586c6-0ecb-479f-8bbb-5bcf843dd57f", "metadata": {"aucs": [0.7941804333367506, 0.8128167847205127, 0.8133303282544874], "final_y": [0.15343584868840154, 0.14642658806671427, 0.14356989157864308]}, "mutation_prompt": null}
{"id": "1a6239ec-0686-43c5-b43f-6d4461bc64c7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        layer_penalty = 0.01  # Adjust factor for layer complexity\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                # Introduce layer complexity penalty to handle large dimensions\n                f = func(trial) + layer_penalty * np.sum(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    best = trial if f < func(best) else best\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Refined Adaptive Hybrid Metaheuristic with layer-wise strategy adjustment for improved exploration and exploitation.", "configspace": "", "generation": 3, "fitness": 0.7736576734776666, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.032. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "40b586c6-0ecb-479f-8bbb-5bcf843dd57f", "metadata": {"aucs": [0.7284971518517924, 0.7945901820562783, 0.7978856865249286], "final_y": [0.16564209015523956, 0.1601017669387199, 0.1540759922093271]}, "mutation_prompt": null}
{"id": "0b6d63e8-bc70-4bee-970b-f958a18778dc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Gradually increase complexity\n            if self.evaluations % (self.budget // 5) == 0 and self.dim < self.max_layers:\n                self.dim += 1\n                pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Adaptive Hybrid Metaheuristic with diversity-based population adjustment and layer-wise incremental optimization.", "configspace": "", "generation": 4, "fitness": 0.7765358303296748, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.777 with standard deviation 0.031. And the mean value of best solutions found was 0.156 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "1fb79c96-a765-4134-b1ac-f98f3a57932e", "metadata": {"aucs": [0.7374267044297326, 0.8127480002084723, 0.7794327863508194], "final_y": [0.1765165504702093, 0.14527631652940276, 0.14586079666260943]}, "mutation_prompt": null}
{"id": "10eddac9-15ba-430e-af6d-3c26c7c84251", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n            if np.random.rand() < 0.3:\n                perturbation = np.random.rand(self.dim) * 0.01 * (bounds.ub - bounds.lb)\n                best = np.clip(best + perturbation, bounds.lb, bounds.ub)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Adaptive Hybrid Metaheuristic with stochastic layer addition and robustness checks for improved high-dimensional convergence.", "configspace": "", "generation": 4, "fitness": 0.7815132119449678, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.782 with standard deviation 0.015. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "1fb79c96-a765-4134-b1ac-f98f3a57932e", "metadata": {"aucs": [0.7676920441051772, 0.8022606033070092, 0.7745869884227173], "final_y": [0.15768563464921503, 0.15610711728714, 0.16112914143213508]}, "mutation_prompt": null}
{"id": "bf050df7-6b0c-4497-8f3a-8d627848bd9b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Adaptive Hybrid Metaheuristic with improved exploration through adaptive crossover probability based on population diversity.", "configspace": "", "generation": 4, "fitness": 0.799245862623717, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.799 with standard deviation 0.029. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "1fb79c96-a765-4134-b1ac-f98f3a57932e", "metadata": {"aucs": [0.7632710059738621, 0.8003593985362869, 0.8341071833610019], "final_y": [0.16132977376601132, 0.15738782579498878, 0.1389706991270142]}, "mutation_prompt": null}
{"id": "5b70ad29-7f0d-4c21-904c-cb171e990582", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Initial crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                # Dynamic crossover probability\n                CR = 0.8 + 0.2 * diversity / (bounds.ub - bounds.lb).mean()\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                # Perturbation for robustness\n                trial += np.random.normal(0, 0.01, self.dim) * (bounds.ub - bounds.lb)\n                \n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improved Adaptive Hybrid Metaheuristic with dynamic updating of crossover probability and use of perturbation for robust solutions.", "configspace": "", "generation": 4, "fitness": 0.7731053042757058, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.773 with standard deviation 0.029. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "1fb79c96-a765-4134-b1ac-f98f3a57932e", "metadata": {"aucs": [0.7316330308611205, 0.795358817808497, 0.7923240641574998], "final_y": [0.1640886547207716, 0.1597003476285308, 0.14973962096668247]}, "mutation_prompt": null}
{"id": "81d1e3d8-b9d2-4a76-bcf8-cfeae3d6ef28", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Introduce layer complexity increase\n            if self.evaluations % (self.budget // 5) == 0:\n                self.max_layers = min(self.max_layers + 4, self.dim)\n                pop = pop[:, :self.max_layers]\n\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improved Adaptive Hybrid Metaheuristic using diversity awareness and graduated layer complexity for enhanced optimization efficiency.", "configspace": "", "generation": 4, "fitness": 0.7970018510193649, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.010. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "1fb79c96-a765-4134-b1ac-f98f3a57932e", "metadata": {"aucs": [0.7993097605228953, 0.8073506571477802, 0.7843451353874191], "final_y": [0.1485240386004767, 0.14991641934456112, 0.13762253275004555]}, "mutation_prompt": null}
{"id": "acb663c3-50eb-457c-9c17-437bf8856a2b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom joblib import Parallel, delayed\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = Parallel(n_jobs=2)(delayed(minimize)(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B') for _ in range(2))\n        return min(res, key=lambda r: r.fun).x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improved local search with parallelism to refine solutions more efficiently.", "configspace": "", "generation": 5, "fitness": 0.7909442842815776, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.791 with standard deviation 0.035. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "bf050df7-6b0c-4497-8f3a-8d627848bd9b", "metadata": {"aucs": [0.758328107390094, 0.8402799082821678, 0.7742248371724713], "final_y": [0.15096362659332885, 0.12730531673998646, 0.16280303336299207]}, "mutation_prompt": null}
{"id": "bd0864e0-c2c7-4266-831a-8c1478cf6851", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n        self.convergence_threshold = 1e-6  # Added convergence threshold\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        if np.abs(f - func(best)) < self.convergence_threshold:  # Early stopping criterion\n                            return best\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Adaptive Hybrid Metaheuristic with improved early stopping by introducing a convergence threshold based on solution improvement. ", "configspace": "", "generation": 5, "fitness": 0.7804229400274142, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.014. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "bf050df7-6b0c-4497-8f3a-8d627848bd9b", "metadata": {"aucs": [0.7649057599091185, 0.7991751115665657, 0.7771879486065585], "final_y": [0.16243097947096563, 0.1572719644896139, 0.15677852026881733]}, "mutation_prompt": null}
{"id": "ed6023a9-2e39-4efe-b7a0-e2c3eb0f1859", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            # Gradient-based adjustment for mutation factor\n            F = 0.5 + 0.3 * np.tanh(1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improved Adaptive Hybrid Metaheuristic with gradient-based mutation factor adjustment for enhanced convergence.", "configspace": "", "generation": 5, "fitness": 0.7863518689869148, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.011. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "bf050df7-6b0c-4497-8f3a-8d627848bd9b", "metadata": {"aucs": [0.7841310396158554, 0.800571346553624, 0.7743532207912651], "final_y": [0.1405601028237191, 0.15325275876417888, 0.14881000050841953]}, "mutation_prompt": null}
{"id": "026d82eb-c0fa-40de-8ebe-6ace0069cd9e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Adaptive Hybrid Metaheuristic with improved exploitation through refined local search integration.", "configspace": "", "generation": 5, "fitness": 0.8012184920377852, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.801 with standard deviation 0.025. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "bf050df7-6b0c-4497-8f3a-8d627848bd9b", "metadata": {"aucs": [0.7667920901359123, 0.8266981696364898, 0.8101652163409536], "final_y": [0.14388589287997555, 0.14807821651548048, 0.15318846078854886]}, "mutation_prompt": null}
{"id": "94dd43b6-8e3c-4fbb-84e3-d15c8b4987ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.6 + 0.2 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())  # Modified line\n            \n            CR = 0.85 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean())  # Modified line\n            \n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget * 0.9:  # Modified line\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Optimized HybridMetaheuristic with enhanced local search transition and adaptive mutation control to balance exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.7929728795732766, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.793 with standard deviation 0.020. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "bf050df7-6b0c-4497-8f3a-8d627848bd9b", "metadata": {"aucs": [0.7725062437574907, 0.8204250953274792, 0.7859872996348598], "final_y": [0.15992448889865818, 0.14586997239218402, 0.15391934771664084]}, "mutation_prompt": null}
{"id": "f28c3338-502a-472d-8417-715a6c993f9f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n\n            # Gradually increase the number of active layers during optimization\n            active_layers = min(self.max_layers, int(self.dim * (self.evaluations / self.budget)))\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb[:active_layers], bounds.ub[:active_layers])\n                cross_points = np.random.rand(active_layers) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, active_layers)] = True\n                trial = np.where(cross_points, mutant, pop[i][:active_layers])\n                trial_full = np.copy(pop[i])\n                trial_full[:active_layers] = trial\n                f = func(trial_full)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i][:active_layers] = trial\n                    if f < func(best):\n                        best[:active_layers] = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        if self.evaluations < self.budget * 0.8:\n            res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        else:\n            res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='Powell', options={'maxiter': 25})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with dynamic layer adjustment and adaptive local search to improve solution robustness and convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (0,) (0,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (0,) (0,) ')", "parent_id": "026d82eb-c0fa-40de-8ebe-6ace0069cd9e", "metadata": {}, "mutation_prompt": null}
{"id": "3b0cb1ab-a858-4e87-bb25-bc3463ebb9ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                F_dynamic = F * (1.5 if i % 2 == 0 else 0.5)  # Dynamic scaling\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})  # Increased iterations\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Adaptive Hybrid Metaheuristic with dynamic mutation factor scaling and improved local search integration.", "configspace": "", "generation": 6, "fitness": 0.775262498391306, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.022. And the mean value of best solutions found was 0.155 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "026d82eb-c0fa-40de-8ebe-6ace0069cd9e", "metadata": {"aucs": [0.7458106055462994, 0.8003696220175254, 0.7796072676100929], "final_y": [0.1616864553659132, 0.15168741416737164, 0.15302642032521419]}, "mutation_prompt": null}
{"id": "18d4170a-5786-4114-978a-cd4647344ed0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c, d = pop[np.random.choice(idxs, 4, replace=False)]  # Changed mutation strategy\n                mutant = np.clip(a + F * (b - c) + F * (c - d), bounds.lb, bounds.ub)  # Modified mutation formula\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Modified mutation strategy in differential evolution to improve exploration in high-dimensional spaces.", "configspace": "", "generation": 6, "fitness": 0.7750135812048873, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.019. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "026d82eb-c0fa-40de-8ebe-6ace0069cd9e", "metadata": {"aucs": [0.749550588016039, 0.7945901820562783, 0.7808999735423446], "final_y": [0.16076743634052826, 0.1601017669387199, 0.15851021339605575]}, "mutation_prompt": null}
{"id": "f3d0ea23-79fb-4e49-86a4-b7c690b134a3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Integrate adaptive population size based on fitness variance\n            fitness_values = np.array([func(ind) for ind in pop])\n            fitness_variance = np.var(fitness_values)\n            dynamic_pop_size = int(pop_size * (1.0 + fitness_variance / 10))\n            if dynamic_pop_size != pop_size:\n                pop_size = dynamic_pop_size\n                pop = pop[:pop_size]  # Truncate or expand population\n\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with improved exploration through adaptive population diversity control.", "configspace": "", "generation": 6, "fitness": 0.7579501480301335, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.758 with standard deviation 0.040. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "026d82eb-c0fa-40de-8ebe-6ace0069cd9e", "metadata": {"aucs": [0.70288461328546, 0.7960304675404535, 0.7749353632644869], "final_y": [0.17625472680794574, 0.1585246362450734, 0.1613939960150983]}, "mutation_prompt": null}
{"id": "6f7c8ea6-0f5e-45b0-b10f-1c96c72e8011", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Gradually increase layer complexity during optimization\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Integrate adaptive layer complexity and robustness into the HybridMetaheuristic for enhanced optimization.", "configspace": "", "generation": 6, "fitness": 0.796219180876367, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.796 with standard deviation 0.027. And the mean value of best solutions found was 0.157 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "026d82eb-c0fa-40de-8ebe-6ace0069cd9e", "metadata": {"aucs": [0.763868306366906, 0.7945901820562783, 0.8301990542059169], "final_y": [0.1656675561020723, 0.1601017669387199, 0.14564866908927065]}, "mutation_prompt": null}
{"id": "75115e1d-e2f2-48d7-a80f-15aad42f41ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.4 + 0.4 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())  # Adjusted mutation factor\n            CR = 0.8 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean())  # Adjusted crossover probability\n\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.2))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                mutant += np.random.normal(0, 0.01, size=mutant.shape)  # Add Gaussian noise for robustness\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(np.clip(trial, bounds.lb, bounds.ub))\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance HybridMetaheuristic by incorporating Gaussian noise robustness and dynamic diversity-based parameter tuning.", "configspace": "", "generation": 7, "fitness": 0.7811721184921501, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.017. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "6f7c8ea6-0f5e-45b0-b10f-1c96c72e8011", "metadata": {"aucs": [0.7703138313979284, 0.8048043937328915, 0.7683981303456302], "final_y": [0.16195046786087663, 0.15236756301317933, 0.16045640428318786]}, "mutation_prompt": null}
{"id": "ad4ff954-d13c-4f76-8e64-427990d35ecd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + 0.1 * (best - a), bounds.lb, bounds.ub)  # Added gradient-based mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Gradually increase layer complexity during optimization\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance diversity and robustness through adaptive mutation based on gradient information.", "configspace": "", "generation": 7, "fitness": 0.805766131290274, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.027. And the mean value of best solutions found was 0.149 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "6f7c8ea6-0f5e-45b0-b10f-1c96c72e8011", "metadata": {"aucs": [0.7769389728191475, 0.8426188494999877, 0.7977405715516867], "final_y": [0.1600700378862655, 0.13112991491237735, 0.1558951373065388]}, "mutation_prompt": null}
{"id": "90653747-19c9-409d-b085-7485889a4f67", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            adaptive_F = 0.5 + 0.4 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.15))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance exploration and exploitation by dynamically adjusting mutation and crossover strategies and employing a more detailed local search.", "configspace": "", "generation": 7, "fitness": 0.7798506723053406, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.014. And the mean value of best solutions found was 0.156 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "6f7c8ea6-0f5e-45b0-b10f-1c96c72e8011", "metadata": {"aucs": [0.7617628003436508, 0.7953061640564089, 0.7824830525159623], "final_y": [0.14574857347778025, 0.15835176617564806, 0.1628722068455556]}, "mutation_prompt": null}
{"id": "7cce6b32-ea02-4fe5-a89d-62f675fbd591", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on progress\n            F = 0.5 + 0.4 * (1 - self.evaluations / self.budget)\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (np.mean(np.std(pop, axis=0)) / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Gradually increase layer complexity during optimization\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance balance between exploration and exploitation by adjusting mutation factor dynamically based on progress.", "configspace": "", "generation": 7, "fitness": 0.7875822702785568, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.788 with standard deviation 0.005. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "6f7c8ea6-0f5e-45b0-b10f-1c96c72e8011", "metadata": {"aucs": [0.7816758704435185, 0.7945901820562783, 0.7864807583358732], "final_y": [0.14712600399187525, 0.1601017669387199, 0.14879363928552913]}, "mutation_prompt": null}
{"id": "13bce746-05f9-408b-b969-9f26944bfac3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Gradually increase layer complexity during optimization\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance the local search strategy by increasing the maximum iterations for more refined solution exploration.", "configspace": "", "generation": 7, "fitness": 0.8182666116719548, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.024. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "6f7c8ea6-0f5e-45b0-b10f-1c96c72e8011", "metadata": {"aucs": [0.7978010014444475, 0.8049769389748882, 0.8520218945965287], "final_y": [0.15356647002621326, 0.15242916901008374, 0.12865252411817163]}, "mutation_prompt": null}
{"id": "8dfdb313-0a21-4623-97ff-d8257a18c070", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            fitness_values = np.array([func(ind) for ind in pop])\n            F = 0.5 + 0.3 * (1.0 - np.var(fitness_values) / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Gradually increase layer complexity during optimization\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance mutation strategy by dynamically adjusting mutation factor using population fitness variance for improved exploration.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'diversity' is not defined\").", "error": "NameError(\"name 'diversity' is not defined\")", "parent_id": "13bce746-05f9-408b-b969-9f26944bfac3", "metadata": {}, "mutation_prompt": null}
{"id": "5ffe721f-a92e-49f6-aab6-41dce595dcd0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        options = {'maxiter': 150}  # Increased max iterations for local search\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options=options)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic that combines adaptive differential evolution with adaptive local search for efficient exploration and fine-tuning of solutions.", "configspace": "", "generation": 8, "fitness": 0.789858423118393, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.790 with standard deviation 0.020. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "13bce746-05f9-408b-b969-9f26944bfac3", "metadata": {"aucs": [0.7638497623705476, 0.8138212092253283, 0.7919042977593034], "final_y": [0.14749004561540835, 0.14175595718563538, 0.14780979852864917]}, "mutation_prompt": null}
{"id": "cb8745f4-8a08-4195-bc02-f0c0e654f374", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        max_iter = int(50 + 50 * (self.evaluations / self.budget))  # Adaptive iteration adjustment\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': max_iter})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce adaptive local search with dynamically adjusted iterations to enhance the refinement process.", "configspace": "", "generation": 8, "fitness": 0.7745514609613288, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.020. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "13bce746-05f9-408b-b969-9f26944bfac3", "metadata": {"aucs": [0.7507520559540407, 0.7995909706157959, 0.7733113563141498], "final_y": [0.1655818386886967, 0.14521669918665037, 0.16369827877925625]}, "mutation_prompt": null}
{"id": "464ed3d2-8e9f-4c00-bf5a-77c1d99f6373", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Adaptive adjustment of layer complexity\n            if self.evaluations % (self.budget // 5) == 0:  \n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Implement adaptive layer complexity scaling and parameter tuning for optimized search efficiency.", "configspace": "", "generation": 8, "fitness": 0.7979602467811691, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.008. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "13bce746-05f9-408b-b969-9f26944bfac3", "metadata": {"aucs": [0.7900909279457584, 0.7948737114775133, 0.8089161009202354], "final_y": [0.15223073526217323, 0.1593461460015163, 0.14738337404910085]}, "mutation_prompt": null}
{"id": "051c1c9e-e708-4b28-991b-764cc542f375", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Gradually increase layer complexity during optimization\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x + np.random.normal(0, 0.01, size=self.dim)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce an adaptive mutation factor in the local search to enhance local refinement.", "configspace": "", "generation": 8, "fitness": 0.781416453008375, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.022. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "13bce746-05f9-408b-b969-9f26944bfac3", "metadata": {"aucs": [0.7533576166147147, 0.8074371252421282, 0.7834546171682824], "final_y": [0.16720985140140876, 0.1481047329429115, 0.15974141709910716]}, "mutation_prompt": null}
