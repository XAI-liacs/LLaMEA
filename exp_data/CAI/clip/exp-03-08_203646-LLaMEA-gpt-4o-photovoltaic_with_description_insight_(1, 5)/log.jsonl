{"id": "61f130e2-b8d3-4693-a9e0-528211b26e74", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm that combines Differential Evolution (DE) for global search with a local search strategy, incorporating layer modularity detection and robustness metrics, gradually increasing complexity to optimize multilayered photonic structures.", "configspace": "", "generation": 0, "fitness": 0.7876070307361857, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.788 with standard deviation 0.025. And the mean value of best solutions found was 0.149 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": null, "metadata": {"aucs": [0.7520163374528934, 0.803116244873461, 0.8076885098822026], "final_y": [0.16488723910552805, 0.1397156956496598, 0.1415408448762916]}, "mutation_prompt": null}
{"id": "010c2bb3-a858-4a1b-8482-8feb511e2e65", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic that improves search efficiency by adjusting the mutation factor dynamically based on population diversity.", "configspace": "", "generation": 1, "fitness": 0.7149944991511855, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.715 with standard deviation 0.054. And the mean value of best solutions found was 0.175 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "61f130e2-b8d3-4693-a9e0-528211b26e74", "metadata": {"aucs": [0.7912044668731412, 0.6701120701252603, 0.6836669604551551], "final_y": [0.14920055428084433, 0.20786468114489565, 0.16781345950320103]}, "mutation_prompt": null}
{"id": "85f62359-72ea-47c5-adc0-6c4972607695", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR * (1 - self.evaluations / self.budget)\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic combining DE with adaptive crossover probability for improved exploration and exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.6997522399133219, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.700 with standard deviation 0.051. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "61f130e2-b8d3-4693-a9e0-528211b26e74", "metadata": {"aucs": [0.771280166670643, 0.6613516104264628, 0.6666249426428601], "final_y": [0.14475320829271665, 0.19373420917995898, 0.1934458438585368]}, "mutation_prompt": null}
{"id": "830b3993-19f1-4887-8403-370e6d6a35b9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = F * (0.5 + np.random.rand()/2)  # Adaptive mutation factor\n                mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n                CR_adaptive = CR * (0.5 + np.random.rand()/2)  # Adaptive crossover probability\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic algorithm utilizing adaptive mutation and crossover strategies to optimize multilayered photonic structures.", "configspace": "", "generation": 1, "fitness": 0.7042113579663757, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.704 with standard deviation 0.057. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.032.", "error": "", "parent_id": "61f130e2-b8d3-4693-a9e0-528211b26e74", "metadata": {"aucs": [0.7852833799419121, 0.6644810535698966, 0.6628696403873184], "final_y": [0.1334587532635957, 0.18898050329291338, 0.20825406498252785]}, "mutation_prompt": null}
{"id": "029220a1-fc68-4669-b771-4068ac67ca16", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.base_F = 0.5  # Base mutation factor\n        self.base_CR = 0.9  # Base crossover probability\n        self.evaluations = 0\n        self.layer_increment = 5  # Increment layers in steps\n        self.current_layers = self.layer_increment\n\n    def adaptive_differential_evolution(self, func, bounds, pop_size=50):\n        F = self.base_F + np.random.rand() * 0.3  # Adaptive F\n        CR = self.base_CR - np.random.rand() * 0.1  # Adaptive CR\n        pop = np.random.rand(pop_size, self.current_layers) * (bounds.ub[:self.current_layers] - bounds.lb[:self.current_layers]) + bounds.lb[:self.current_layers]\n        best_idx = np.argmin([func(np.pad(ind, (0, self.dim - self.current_layers), 'constant')) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb[:self.current_layers], bounds.ub[:self.current_layers])\n                cross_points = np.random.rand(self.current_layers) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.current_layers)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(np.pad(trial, (0, self.dim - self.current_layers), 'constant'))\n                self.evaluations += 1\n                if f < func(np.pad(pop[i], (0, self.dim - self.current_layers), 'constant')):\n                    pop[i] = trial\n                    if f < func(np.pad(best, (0, self.dim - self.current_layers), 'constant')):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Increase complexity gradually\n            if self.evaluations % (self.budget // 4) == 0 and self.current_layers < self.dim:\n                self.current_layers = min(self.current_layers + self.layer_increment, self.dim)\n                pop = np.pad(pop, ((0, 0), (0, self.layer_increment)), 'constant')\n        return np.pad(best, (0, self.dim - self.current_layers), 'constant')\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.adaptive_differential_evolution(func, bounds, self.pop_size)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhancing the hybrid metaheuristic by introducing adaptive DE parameters and layered complexity control for improved exploration and exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.6770206163702049, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.677 with standard deviation 0.005. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "61f130e2-b8d3-4693-a9e0-528211b26e74", "metadata": {"aucs": [0.6770424331908649, 0.6827983654563118, 0.6712210504634384], "final_y": [0.20238956327700197, 0.19845775728223347, 0.20469669319689576]}, "mutation_prompt": null}
{"id": "0b6703f3-8aca-42d4-935b-339f5972eaaf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = F * (1 - self.evaluations/self.budget)  # Adaptive mutation factor\n                mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        options = {'maxiter': 100}  # Limit iterations for efficiency\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options=options)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced HybridMetaheuristic by incorporating adaptive mutation factor and improving local search efficiency.", "configspace": "", "generation": 1, "fitness": 0.7082803283034652, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.708 with standard deviation 0.042. And the mean value of best solutions found was 0.181 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "61f130e2-b8d3-4693-a9e0-528211b26e74", "metadata": {"aucs": [0.7666014151130248, 0.6852798115349374, 0.6729597582624336], "final_y": [0.14976152446058721, 0.19960235370017343, 0.19487595642296107]}, "mutation_prompt": null}
{"id": "40b586c6-0ecb-479f-8bbb-5bcf843dd57f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                pop_size = min(100, int(pop_size * 1.1))\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with adaptive population size based on convergence to improve search efficiency.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 54 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 54 is out of bounds for axis 0 with size 50')", "parent_id": "010c2bb3-a858-4a1b-8482-8feb511e2e65", "metadata": {}, "mutation_prompt": null}
{"id": "bf7a7110-f729-4f0f-a6c0-cc112cdd1f55", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n\n            for i in range(pop_size):\n                if self.evaluations >= self.budget * 0.7:\n                    break  # Reserve budget for local search\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n        return best\n\n    def local_search(self, func, x0, bounds):\n        if self.evaluations < self.budget:\n            res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n            self.evaluations += res.nfev\n            return res.x\n        return x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Multi-Phase Hybrid Metaheuristic that balances exploration and exploitation by dynamically allocating resources between global and local search phases. ", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 53 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 53 is out of bounds for axis 0 with size 50')", "parent_id": "010c2bb3-a858-4a1b-8482-8feb511e2e65", "metadata": {}, "mutation_prompt": null}
{"id": "9fc0d0bb-102b-48fb-ad52-bc70eb560434", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor and crossover probability based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())  # This line is changed\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic that adjusts both mutation factor and crossover probability dynamically based on population diversity.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 50 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 50 is out of bounds for axis 0 with size 50')", "parent_id": "010c2bb3-a858-4a1b-8482-8feb511e2e65", "metadata": {}, "mutation_prompt": null}
{"id": "7fa5b21f-16ea-424f-86da-3b8969362266", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.3 * (diversity / (bounds.ub - bounds.lb).mean())  # Adjust crossover probability\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with adaptive crossover probability to improve convergence by balancing exploration and exploitation.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 54 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 54 is out of bounds for axis 0 with size 50')", "parent_id": "010c2bb3-a858-4a1b-8482-8feb511e2e65", "metadata": {}, "mutation_prompt": null}
{"id": "4abae252-8667-482a-83f2-5a14bf07ad34", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n\n            # Change: Introduce dynamic crossover probability adjustment\n            CR = 0.9 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improved Hybrid Metaheuristic that leverages a dynamic crossover probability to enhance convergence speed.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 52 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 52 is out of bounds for axis 0 with size 50')", "parent_id": "010c2bb3-a858-4a1b-8482-8feb511e2e65", "metadata": {}, "mutation_prompt": null}
{"id": "51b4c9b6-8b3a-49b5-bbfb-e35cf1d3f2ab", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.4 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                pop_size = min(100, int(pop_size * 1.1))\n                pop = np.clip(pop[:pop_size], bounds.lb, bounds.ub)  # Ensure population size is consistent\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='trust-constr')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with adaptive diversity control and improved local search for high-dimensional noisy optimization.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 50 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 50 is out of bounds for axis 0 with size 50')", "parent_id": "40b586c6-0ecb-479f-8bbb-5bcf843dd57f", "metadata": {}, "mutation_prompt": null}
{"id": "9d244628-b0a7-4fae-b834-906c5768f3b3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Adjust mutation based on diversity and adaptively adjust pop_size\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            pop_size = min(100, max(20, int(pop_size * (1 + 0.1 * np.sin(2 * np.pi * self.evaluations / self.budget)))))\n            for i in range(pop_size):\n                idxs = np.delete(np.arange(pop_size), i)\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + 0.01 * np.linalg.norm(trial - pop[i])  # Add robustness metric\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced adaptive hybrid algorithm with modular evolution and robustness integration to efficiently tackle high-dimensional noisy optimization.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 50 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 50 is out of bounds for axis 0 with size 50')", "parent_id": "40b586c6-0ecb-479f-8bbb-5bcf843dd57f", "metadata": {}, "mutation_prompt": null}
{"id": "ec7b3147-6a9f-4107-ae45-9ccf6e6b1c98", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                pop_size = min(100, int(pop_size * 1.1))\n                pop = np.resize(pop, (pop_size, self.dim))  # Ensure population size matches\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance robustness by adjusting adaptive population size and ensuring valid index selection to improve search efficiency.", "configspace": "", "generation": 3, "fitness": 0.7919452691988559, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.792 with standard deviation 0.010. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "40b586c6-0ecb-479f-8bbb-5bcf843dd57f", "metadata": {"aucs": [0.7788357874047149, 0.7955467078736779, 0.8014533123181746], "final_y": [0.15894296940566877, 0.14548159236012215, 0.14845543689201657]}, "mutation_prompt": null}
{"id": "1fb79c96-a765-4134-b1ac-f98f3a57932e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Adaptive Hybrid Metaheuristic using error-checking to prevent index out-of-bounds errors during adaptive population size adjustments.", "configspace": "", "generation": 3, "fitness": 0.8067758487705835, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.807 with standard deviation 0.009. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "40b586c6-0ecb-479f-8bbb-5bcf843dd57f", "metadata": {"aucs": [0.7941804333367506, 0.8128167847205127, 0.8133303282544874], "final_y": [0.15343584868840154, 0.14642658806671427, 0.14356989157864308]}, "mutation_prompt": null}
{"id": "1a6239ec-0686-43c5-b43f-6d4461bc64c7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        layer_penalty = 0.01  # Adjust factor for layer complexity\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                # Introduce layer complexity penalty to handle large dimensions\n                f = func(trial) + layer_penalty * np.sum(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    best = trial if f < func(best) else best\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Refined Adaptive Hybrid Metaheuristic with layer-wise strategy adjustment for improved exploration and exploitation.", "configspace": "", "generation": 3, "fitness": 0.7736576734776666, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.032. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "40b586c6-0ecb-479f-8bbb-5bcf843dd57f", "metadata": {"aucs": [0.7284971518517924, 0.7945901820562783, 0.7978856865249286], "final_y": [0.16564209015523956, 0.1601017669387199, 0.1540759922093271]}, "mutation_prompt": null}
{"id": "0b6d63e8-bc70-4bee-970b-f958a18778dc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Gradually increase complexity\n            if self.evaluations % (self.budget // 5) == 0 and self.dim < self.max_layers:\n                self.dim += 1\n                pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Adaptive Hybrid Metaheuristic with diversity-based population adjustment and layer-wise incremental optimization.", "configspace": "", "generation": 4, "fitness": 0.7765358303296748, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.777 with standard deviation 0.031. And the mean value of best solutions found was 0.156 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "1fb79c96-a765-4134-b1ac-f98f3a57932e", "metadata": {"aucs": [0.7374267044297326, 0.8127480002084723, 0.7794327863508194], "final_y": [0.1765165504702093, 0.14527631652940276, 0.14586079666260943]}, "mutation_prompt": null}
{"id": "10eddac9-15ba-430e-af6d-3c26c7c84251", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n            if np.random.rand() < 0.3:\n                perturbation = np.random.rand(self.dim) * 0.01 * (bounds.ub - bounds.lb)\n                best = np.clip(best + perturbation, bounds.lb, bounds.ub)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Adaptive Hybrid Metaheuristic with stochastic layer addition and robustness checks for improved high-dimensional convergence.", "configspace": "", "generation": 4, "fitness": 0.7815132119449678, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.782 with standard deviation 0.015. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "1fb79c96-a765-4134-b1ac-f98f3a57932e", "metadata": {"aucs": [0.7676920441051772, 0.8022606033070092, 0.7745869884227173], "final_y": [0.15768563464921503, 0.15610711728714, 0.16112914143213508]}, "mutation_prompt": null}
{"id": "bf050df7-6b0c-4497-8f3a-8d627848bd9b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Adaptive Hybrid Metaheuristic with improved exploration through adaptive crossover probability based on population diversity.", "configspace": "", "generation": 4, "fitness": 0.799245862623717, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.799 with standard deviation 0.029. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "1fb79c96-a765-4134-b1ac-f98f3a57932e", "metadata": {"aucs": [0.7632710059738621, 0.8003593985362869, 0.8341071833610019], "final_y": [0.16132977376601132, 0.15738782579498878, 0.1389706991270142]}, "mutation_prompt": null}
{"id": "5b70ad29-7f0d-4c21-904c-cb171e990582", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Initial crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                # Dynamic crossover probability\n                CR = 0.8 + 0.2 * diversity / (bounds.ub - bounds.lb).mean()\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                \n                # Perturbation for robustness\n                trial += np.random.normal(0, 0.01, self.dim) * (bounds.ub - bounds.lb)\n                \n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improved Adaptive Hybrid Metaheuristic with dynamic updating of crossover probability and use of perturbation for robust solutions.", "configspace": "", "generation": 4, "fitness": 0.7731053042757058, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.773 with standard deviation 0.029. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "1fb79c96-a765-4134-b1ac-f98f3a57932e", "metadata": {"aucs": [0.7316330308611205, 0.795358817808497, 0.7923240641574998], "final_y": [0.1640886547207716, 0.1597003476285308, 0.14973962096668247]}, "mutation_prompt": null}
{"id": "81d1e3d8-b9d2-4a76-bcf8-cfeae3d6ef28", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Introduce layer complexity increase\n            if self.evaluations % (self.budget // 5) == 0:\n                self.max_layers = min(self.max_layers + 4, self.dim)\n                pop = pop[:, :self.max_layers]\n\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improved Adaptive Hybrid Metaheuristic using diversity awareness and graduated layer complexity for enhanced optimization efficiency.", "configspace": "", "generation": 4, "fitness": 0.7970018510193649, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.010. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "1fb79c96-a765-4134-b1ac-f98f3a57932e", "metadata": {"aucs": [0.7993097605228953, 0.8073506571477802, 0.7843451353874191], "final_y": [0.1485240386004767, 0.14991641934456112, 0.13762253275004555]}, "mutation_prompt": null}
{"id": "acb663c3-50eb-457c-9c17-437bf8856a2b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom joblib import Parallel, delayed\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = Parallel(n_jobs=2)(delayed(minimize)(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B') for _ in range(2))\n        return min(res, key=lambda r: r.fun).x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improved local search with parallelism to refine solutions more efficiently.", "configspace": "", "generation": 5, "fitness": 0.7909442842815776, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.791 with standard deviation 0.035. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "bf050df7-6b0c-4497-8f3a-8d627848bd9b", "metadata": {"aucs": [0.758328107390094, 0.8402799082821678, 0.7742248371724713], "final_y": [0.15096362659332885, 0.12730531673998646, 0.16280303336299207]}, "mutation_prompt": null}
{"id": "bd0864e0-c2c7-4266-831a-8c1478cf6851", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n        self.convergence_threshold = 1e-6  # Added convergence threshold\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        if np.abs(f - func(best)) < self.convergence_threshold:  # Early stopping criterion\n                            return best\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Adaptive Hybrid Metaheuristic with improved early stopping by introducing a convergence threshold based on solution improvement. ", "configspace": "", "generation": 5, "fitness": 0.7804229400274142, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.014. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "bf050df7-6b0c-4497-8f3a-8d627848bd9b", "metadata": {"aucs": [0.7649057599091185, 0.7991751115665657, 0.7771879486065585], "final_y": [0.16243097947096563, 0.1572719644896139, 0.15677852026881733]}, "mutation_prompt": null}
{"id": "ed6023a9-2e39-4efe-b7a0-e2c3eb0f1859", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            # Gradient-based adjustment for mutation factor\n            F = 0.5 + 0.3 * np.tanh(1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improved Adaptive Hybrid Metaheuristic with gradient-based mutation factor adjustment for enhanced convergence.", "configspace": "", "generation": 5, "fitness": 0.7863518689869148, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.011. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "bf050df7-6b0c-4497-8f3a-8d627848bd9b", "metadata": {"aucs": [0.7841310396158554, 0.800571346553624, 0.7743532207912651], "final_y": [0.1405601028237191, 0.15325275876417888, 0.14881000050841953]}, "mutation_prompt": null}
{"id": "026d82eb-c0fa-40de-8ebe-6ace0069cd9e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Adaptive Hybrid Metaheuristic with improved exploitation through refined local search integration.", "configspace": "", "generation": 5, "fitness": 0.8012184920377852, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.801 with standard deviation 0.025. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "bf050df7-6b0c-4497-8f3a-8d627848bd9b", "metadata": {"aucs": [0.7667920901359123, 0.8266981696364898, 0.8101652163409536], "final_y": [0.14388589287997555, 0.14807821651548048, 0.15318846078854886]}, "mutation_prompt": null}
{"id": "94dd43b6-8e3c-4fbb-84e3-d15c8b4987ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.6 + 0.2 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())  # Modified line\n            \n            CR = 0.85 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean())  # Modified line\n            \n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget * 0.9:  # Modified line\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Optimized HybridMetaheuristic with enhanced local search transition and adaptive mutation control to balance exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.7929728795732766, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.793 with standard deviation 0.020. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "bf050df7-6b0c-4497-8f3a-8d627848bd9b", "metadata": {"aucs": [0.7725062437574907, 0.8204250953274792, 0.7859872996348598], "final_y": [0.15992448889865818, 0.14586997239218402, 0.15391934771664084]}, "mutation_prompt": null}
{"id": "f28c3338-502a-472d-8417-715a6c993f9f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n\n            # Gradually increase the number of active layers during optimization\n            active_layers = min(self.max_layers, int(self.dim * (self.evaluations / self.budget)))\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb[:active_layers], bounds.ub[:active_layers])\n                cross_points = np.random.rand(active_layers) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, active_layers)] = True\n                trial = np.where(cross_points, mutant, pop[i][:active_layers])\n                trial_full = np.copy(pop[i])\n                trial_full[:active_layers] = trial\n                f = func(trial_full)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i][:active_layers] = trial\n                    if f < func(best):\n                        best[:active_layers] = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        if self.evaluations < self.budget * 0.8:\n            res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        else:\n            res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='Powell', options={'maxiter': 25})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with dynamic layer adjustment and adaptive local search to improve solution robustness and convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (0,) (0,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (0,) (0,) ')", "parent_id": "026d82eb-c0fa-40de-8ebe-6ace0069cd9e", "metadata": {}, "mutation_prompt": null}
{"id": "3b0cb1ab-a858-4e87-bb25-bc3463ebb9ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                F_dynamic = F * (1.5 if i % 2 == 0 else 0.5)  # Dynamic scaling\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})  # Increased iterations\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Adaptive Hybrid Metaheuristic with dynamic mutation factor scaling and improved local search integration.", "configspace": "", "generation": 6, "fitness": 0.775262498391306, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.022. And the mean value of best solutions found was 0.155 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "026d82eb-c0fa-40de-8ebe-6ace0069cd9e", "metadata": {"aucs": [0.7458106055462994, 0.8003696220175254, 0.7796072676100929], "final_y": [0.1616864553659132, 0.15168741416737164, 0.15302642032521419]}, "mutation_prompt": null}
{"id": "18d4170a-5786-4114-978a-cd4647344ed0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c, d = pop[np.random.choice(idxs, 4, replace=False)]  # Changed mutation strategy\n                mutant = np.clip(a + F * (b - c) + F * (c - d), bounds.lb, bounds.ub)  # Modified mutation formula\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Modified mutation strategy in differential evolution to improve exploration in high-dimensional spaces.", "configspace": "", "generation": 6, "fitness": 0.7750135812048873, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.019. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "026d82eb-c0fa-40de-8ebe-6ace0069cd9e", "metadata": {"aucs": [0.749550588016039, 0.7945901820562783, 0.7808999735423446], "final_y": [0.16076743634052826, 0.1601017669387199, 0.15851021339605575]}, "mutation_prompt": null}
{"id": "f3d0ea23-79fb-4e49-86a4-b7c690b134a3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Integrate adaptive population size based on fitness variance\n            fitness_values = np.array([func(ind) for ind in pop])\n            fitness_variance = np.var(fitness_values)\n            dynamic_pop_size = int(pop_size * (1.0 + fitness_variance / 10))\n            if dynamic_pop_size != pop_size:\n                pop_size = dynamic_pop_size\n                pop = pop[:pop_size]  # Truncate or expand population\n\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with improved exploration through adaptive population diversity control.", "configspace": "", "generation": 6, "fitness": 0.7579501480301335, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.758 with standard deviation 0.040. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "026d82eb-c0fa-40de-8ebe-6ace0069cd9e", "metadata": {"aucs": [0.70288461328546, 0.7960304675404535, 0.7749353632644869], "final_y": [0.17625472680794574, 0.1585246362450734, 0.1613939960150983]}, "mutation_prompt": null}
{"id": "6f7c8ea6-0f5e-45b0-b10f-1c96c72e8011", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Gradually increase layer complexity during optimization\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Integrate adaptive layer complexity and robustness into the HybridMetaheuristic for enhanced optimization.", "configspace": "", "generation": 6, "fitness": 0.796219180876367, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.796 with standard deviation 0.027. And the mean value of best solutions found was 0.157 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "026d82eb-c0fa-40de-8ebe-6ace0069cd9e", "metadata": {"aucs": [0.763868306366906, 0.7945901820562783, 0.8301990542059169], "final_y": [0.1656675561020723, 0.1601017669387199, 0.14564866908927065]}, "mutation_prompt": null}
{"id": "75115e1d-e2f2-48d7-a80f-15aad42f41ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.4 + 0.4 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())  # Adjusted mutation factor\n            CR = 0.8 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean())  # Adjusted crossover probability\n\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.2))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                mutant += np.random.normal(0, 0.01, size=mutant.shape)  # Add Gaussian noise for robustness\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(np.clip(trial, bounds.lb, bounds.ub))\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance HybridMetaheuristic by incorporating Gaussian noise robustness and dynamic diversity-based parameter tuning.", "configspace": "", "generation": 7, "fitness": 0.7811721184921501, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.017. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "6f7c8ea6-0f5e-45b0-b10f-1c96c72e8011", "metadata": {"aucs": [0.7703138313979284, 0.8048043937328915, 0.7683981303456302], "final_y": [0.16195046786087663, 0.15236756301317933, 0.16045640428318786]}, "mutation_prompt": null}
{"id": "ad4ff954-d13c-4f76-8e64-427990d35ecd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + 0.1 * (best - a), bounds.lb, bounds.ub)  # Added gradient-based mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Gradually increase layer complexity during optimization\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance diversity and robustness through adaptive mutation based on gradient information.", "configspace": "", "generation": 7, "fitness": 0.805766131290274, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.027. And the mean value of best solutions found was 0.149 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "6f7c8ea6-0f5e-45b0-b10f-1c96c72e8011", "metadata": {"aucs": [0.7769389728191475, 0.8426188494999877, 0.7977405715516867], "final_y": [0.1600700378862655, 0.13112991491237735, 0.1558951373065388]}, "mutation_prompt": null}
{"id": "90653747-19c9-409d-b085-7485889a4f67", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            adaptive_F = 0.5 + 0.4 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.15))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance exploration and exploitation by dynamically adjusting mutation and crossover strategies and employing a more detailed local search.", "configspace": "", "generation": 7, "fitness": 0.7798506723053406, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.014. And the mean value of best solutions found was 0.156 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "6f7c8ea6-0f5e-45b0-b10f-1c96c72e8011", "metadata": {"aucs": [0.7617628003436508, 0.7953061640564089, 0.7824830525159623], "final_y": [0.14574857347778025, 0.15835176617564806, 0.1628722068455556]}, "mutation_prompt": null}
{"id": "7cce6b32-ea02-4fe5-a89d-62f675fbd591", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on progress\n            F = 0.5 + 0.4 * (1 - self.evaluations / self.budget)\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (np.mean(np.std(pop, axis=0)) / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Gradually increase layer complexity during optimization\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance balance between exploration and exploitation by adjusting mutation factor dynamically based on progress.", "configspace": "", "generation": 7, "fitness": 0.7875822702785568, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.788 with standard deviation 0.005. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "6f7c8ea6-0f5e-45b0-b10f-1c96c72e8011", "metadata": {"aucs": [0.7816758704435185, 0.7945901820562783, 0.7864807583358732], "final_y": [0.14712600399187525, 0.1601017669387199, 0.14879363928552913]}, "mutation_prompt": null}
{"id": "13bce746-05f9-408b-b969-9f26944bfac3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Gradually increase layer complexity during optimization\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance the local search strategy by increasing the maximum iterations for more refined solution exploration.", "configspace": "", "generation": 7, "fitness": 0.8182666116719548, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.024. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "6f7c8ea6-0f5e-45b0-b10f-1c96c72e8011", "metadata": {"aucs": [0.7978010014444475, 0.8049769389748882, 0.8520218945965287], "final_y": [0.15356647002621326, 0.15242916901008374, 0.12865252411817163]}, "mutation_prompt": null}
{"id": "8dfdb313-0a21-4623-97ff-d8257a18c070", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            fitness_values = np.array([func(ind) for ind in pop])\n            F = 0.5 + 0.3 * (1.0 - np.var(fitness_values) / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Gradually increase layer complexity during optimization\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance mutation strategy by dynamically adjusting mutation factor using population fitness variance for improved exploration.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'diversity' is not defined\").", "error": "NameError(\"name 'diversity' is not defined\")", "parent_id": "13bce746-05f9-408b-b969-9f26944bfac3", "metadata": {}, "mutation_prompt": null}
{"id": "5ffe721f-a92e-49f6-aab6-41dce595dcd0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        options = {'maxiter': 150}  # Increased max iterations for local search\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options=options)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic that combines adaptive differential evolution with adaptive local search for efficient exploration and fine-tuning of solutions.", "configspace": "", "generation": 8, "fitness": 0.789858423118393, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.790 with standard deviation 0.020. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "13bce746-05f9-408b-b969-9f26944bfac3", "metadata": {"aucs": [0.7638497623705476, 0.8138212092253283, 0.7919042977593034], "final_y": [0.14749004561540835, 0.14175595718563538, 0.14780979852864917]}, "mutation_prompt": null}
{"id": "cb8745f4-8a08-4195-bc02-f0c0e654f374", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        max_iter = int(50 + 50 * (self.evaluations / self.budget))  # Adaptive iteration adjustment\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': max_iter})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce adaptive local search with dynamically adjusted iterations to enhance the refinement process.", "configspace": "", "generation": 8, "fitness": 0.7745514609613288, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.020. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "13bce746-05f9-408b-b969-9f26944bfac3", "metadata": {"aucs": [0.7507520559540407, 0.7995909706157959, 0.7733113563141498], "final_y": [0.1655818386886967, 0.14521669918665037, 0.16369827877925625]}, "mutation_prompt": null}
{"id": "464ed3d2-8e9f-4c00-bf5a-77c1d99f6373", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Adaptive adjustment of layer complexity\n            if self.evaluations % (self.budget // 5) == 0:  \n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Implement adaptive layer complexity scaling and parameter tuning for optimized search efficiency.", "configspace": "", "generation": 8, "fitness": 0.7979602467811691, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.008. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "13bce746-05f9-408b-b969-9f26944bfac3", "metadata": {"aucs": [0.7900909279457584, 0.7948737114775133, 0.8089161009202354], "final_y": [0.15223073526217323, 0.1593461460015163, 0.14738337404910085]}, "mutation_prompt": null}
{"id": "051c1c9e-e708-4b28-991b-764cc542f375", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            # Dynamically adjust the mutation factor based on diversity\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive crossover probability based on diversity\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            \n            # Adaptive population size\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Gradually increase layer complexity during optimization\n            self.dim = min(self.dim + 1, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x + np.random.normal(0, 0.01, size=self.dim)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce an adaptive mutation factor in the local search to enhance local refinement.", "configspace": "", "generation": 8, "fitness": 0.781416453008375, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.022. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "13bce746-05f9-408b-b969-9f26944bfac3", "metadata": {"aucs": [0.7533576166147147, 0.8074371252421282, 0.7834546171682824], "final_y": [0.16720985140140876, 0.1481047329429115, 0.15974141709910716]}, "mutation_prompt": null}
{"id": "1cbb4f1e-730a-490d-bb2f-86eb09fd7dc9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            # Adjust mutation factor dynamically based on diversity\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean()) + np.random.uniform(-0.1, 0.1)\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Adaptive adjustment of layer complexity\n            if self.evaluations % (self.budget // 5) == 0:  \n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Implement a dynamic adjustment strategy for the mutation factor to enhance convergence.", "configspace": "", "generation": 9, "fitness": 0.7776178031809305, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.012. And the mean value of best solutions found was 0.157 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "464ed3d2-8e9f-4c00-bf5a-77c1d99f6373", "metadata": {"aucs": [0.7650578822905109, 0.7945901820562783, 0.7732053451960026], "final_y": [0.16308179266615896, 0.1601017669387199, 0.14797280414666758]}, "mutation_prompt": null}
{"id": "09da7d6b-4541-4c0e-9207-e832bd4e7756", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.4 + 0.6 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())  # Changed line\n            CR = 0.8 - 0.3 * (diversity / (bounds.ub - bounds.lb).mean())  # Changed line\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(120, int(pop_size * 1.2))  # Changed line\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Adaptive adjustment of layer complexity\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance population diversity and adaptive parameter tuning for improved exploration and convergence.", "configspace": "", "generation": 9, "fitness": 0.7976092890056615, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.007. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "464ed3d2-8e9f-4c00-bf5a-77c1d99f6373", "metadata": {"aucs": [0.7908918671435929, 0.7945901820562783, 0.8073458178171135], "final_y": [0.15280144868526147, 0.1601017669387199, 0.1435384207997128]}, "mutation_prompt": null}
{"id": "fc08683e-8475-40f1-94e7-453503f50265", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        elite = best.copy()  # Retain an elite solution\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                        elite = trial if func(trial) < func(elite) else elite  # Update elite\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:  \n                self.dim = min(self.dim + 2, self.max_layers)\n        return elite\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance adaptive complexity scaling and introduce elite retention for improved convergence.", "configspace": "", "generation": 9, "fitness": 0.7826910111420545, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.030. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "464ed3d2-8e9f-4c00-bf5a-77c1d99f6373", "metadata": {"aucs": [0.74091241287386, 0.7960999244748757, 0.8110606960774278], "final_y": [0.16750518371047307, 0.15825508478363948, 0.1519317727176891]}, "mutation_prompt": null}
{"id": "fe60cd78-7bc4-4968-888b-01c672594c06", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c + 0.1 * (best - a)), bounds.lb, bounds.ub)  # Added perturbation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Adaptive adjustment of layer complexity\n            if self.evaluations % (self.budget // 5) == 0:  \n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Refine mutation strategy in differential evolution to enhance diversity and convergence.", "configspace": "", "generation": 9, "fitness": 0.8052822429398598, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.805 with standard deviation 0.009. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "464ed3d2-8e9f-4c00-bf5a-77c1d99f6373", "metadata": {"aucs": [0.8121530112181174, 0.8109334424660926, 0.7927602751353696], "final_y": [0.14755838356071038, 0.15001013017613862, 0.155196968662057]}, "mutation_prompt": null}
{"id": "1bbc773b-a7e3-4569-bf9f-4444ab9d3da6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.5 + (0.4 * self.evaluations / self.budget)  # Adaptive crossover probability\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Adaptive adjustment of layer complexity\n            if self.evaluations % (self.budget // 5) == 0:  \n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce an adaptive crossover probability based on the evaluation count to enhance convergence.", "configspace": "", "generation": 9, "fitness": 0.7780570116245661, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.017. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "464ed3d2-8e9f-4c00-bf5a-77c1d99f6373", "metadata": {"aucs": [0.759135024275182, 0.8013192620042606, 0.7737167485942554], "final_y": [0.15440548967708934, 0.15724310586065782, 0.16381513390495928]}, "mutation_prompt": null}
{"id": "1fd3b039-43f6-4df3-babf-a08d464093b7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean())  # Adjust mutation factor dynamically\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean())  # Adjust crossover probability\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01  # Add penalty for high variability\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:  \n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce dynamic adjustment of the mutation factor and integrate a penalty for solutions with high variability to improve convergence stability.", "configspace": "", "generation": 10, "fitness": 0.8026176263470696, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.033. And the mean value of best solutions found was 0.150 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "fe60cd78-7bc4-4968-888b-01c672594c06", "metadata": {"aucs": [0.7575989555918565, 0.8137009011137976, 0.8365530223355546], "final_y": [0.16997274030911613, 0.13796773835202292, 0.14337965255702123]}, "mutation_prompt": null}
{"id": "91e27b93-7c33-43af-870c-4c209352cf03", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.4 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())  # Line 1 changed\n            CR = 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(120, int(pop_size * 1.2))  # Line 2 changed\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c + 0.15 * (best - a)), bounds.lb, bounds.ub)  # Line 3 changed\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:  \n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance differential evolution with adaptive mutation and exploration dynamics.", "configspace": "", "generation": 10, "fitness": 0.7676649588781457, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.768 with standard deviation 0.027. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "fe60cd78-7bc4-4968-888b-01c672594c06", "metadata": {"aucs": [0.7388971018961448, 0.8030239577905175, 0.7610738169477751], "final_y": [0.17066001510876716, 0.1535116003243131, 0.17296403364081692]}, "mutation_prompt": null}
{"id": "ef359864-c1f7-439f-accb-96cbdc15e5e7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = 0.9 - 0.3 * (diversity / (bounds.ub - bounds.lb).mean())  # Changed CR adaptation\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.15))  # Adjusted population growth rate\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c + 0.1 * (best - a)), bounds.lb, bounds.ub)  # Added perturbation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Adaptive adjustment of layer complexity\n            if self.evaluations % (self.budget // 5) == 0:  \n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance adaptation strategy in differential evolution by refining exploration based on population diversity metrics.", "configspace": "", "generation": 10, "fitness": 0.7922267800098549, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.792 with standard deviation 0.026. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "fe60cd78-7bc4-4968-888b-01c672594c06", "metadata": {"aucs": [0.7554703353208254, 0.8109666122432413, 0.810243392465498], "final_y": [0.13460509439822377, 0.15015694861519424, 0.14857863855995357]}, "mutation_prompt": null}
{"id": "a7b083b5-21ea-4a88-96f2-0014e3c3ba5f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())\n            CR = max(0.1, 0.9 - 0.5 * (diversity / (bounds.ub - bounds.lb).mean()))  # Adjusted crossover rate\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * (1.1 + 0.1 * np.random.rand())))  # Dynamic pop size change\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                noise = 0.1 * np.random.randn(*a.shape)  # Added adaptive noise handling\n                mutant = np.clip(a + F * (b - c + 0.1 * (best - a)) + noise, bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:  \n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance exploration-exploitation balance by dynamically adjusting population size and crossover rate and incorporating adaptive noise handling.", "configspace": "", "generation": 10, "fitness": 0.7833648071377323, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.014. And the mean value of best solutions found was 0.157 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "fe60cd78-7bc4-4968-888b-01c672594c06", "metadata": {"aucs": [0.7669098674292538, 0.8009642217691028, 0.7822203322148401], "final_y": [0.16086172187594172, 0.15657999023970692, 0.1547372455903786]}, "mutation_prompt": null}
{"id": "f9103daa-8387-4939-86e2-57093eb87936", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.4 * (1.0 - diversity / (bounds.ub - bounds.lb).mean())  # Modified line: changed multiplier\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean())  # Modified line: changed multiplier\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c + 0.1 * (best - a)), bounds.lb, bounds.ub)  # Added perturbation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial)\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            # Adaptive adjustment of layer complexity\n            if self.evaluations % (self.budget // 4) == 0:  # Modified line: changed divisor\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance exploration-exploitation balance in differential evolution by fine-tuning mutation and crossover rates and improving layer adaptation.", "configspace": "", "generation": 10, "fitness": 0.7810350435765153, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.015. And the mean value of best solutions found was 0.157 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "fe60cd78-7bc4-4968-888b-01c672594c06", "metadata": {"aucs": [0.7612405097250765, 0.7970337133760066, 0.7848309076284626], "final_y": [0.15611399322155262, 0.15733705342184423, 0.15651286305188827]}, "mutation_prompt": null}
{"id": "54d4ad72-858a-4482-b90f-b79c522fdac0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean())  \n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean())  \n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01 \n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:  \n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 50 if np.var(x0) < 0.02 else 0})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover based on population diversity and apply selective local search only when necessary to refine the solution.", "configspace": "", "generation": 11, "fitness": 0.7882017186450286, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.788 with standard deviation 0.026. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "1fd3b039-43f6-4df3-babf-a08d464093b7", "metadata": {"aucs": [0.770882080914765, 0.8242974172200349, 0.7694256578002858], "final_y": [0.15977792263892432, 0.14748001652455334, 0.1655143778175765]}, "mutation_prompt": null}
{"id": "74b177c5-b2a6-4b2e-8e8e-4a4ee480a3b0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)  # Stochastic perturbation\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce a stochastic perturbation to the mutation factor to enhance exploration and resilience to local optima.", "configspace": "", "generation": 11, "fitness": 0.7975662646171147, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.012. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "1fd3b039-43f6-4df3-babf-a08d464093b7", "metadata": {"aucs": [0.7840891236575579, 0.7945901820562783, 0.8140194881375078], "final_y": [0.15496986960726356, 0.1601017669387199, 0.1403799078976975]}, "mutation_prompt": null}
{"id": "e587122b-f470-412c-a3c8-359d31d468cc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean())  # Adjust mutation factor dynamically\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean())  # Adjust crossover probability\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * (1.1 + 0.1 * (diversity / (bounds.ub - bounds.lb).mean()))))  # Adaptive population size\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01  # Add penalty for high variability\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:  \n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 200})  # Increase maxiter\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce adaptive population size and enhance local search integration for improved convergence and stability.", "configspace": "", "generation": 11, "fitness": 0.7925345213622972, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.793 with standard deviation 0.007. And the mean value of best solutions found was 0.156 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "1fd3b039-43f6-4df3-babf-a08d464093b7", "metadata": {"aucs": [0.7902135954954304, 0.8026021373671602, 0.7847878312243013], "final_y": [0.15004614661139704, 0.15541553422252175, 0.1631113767633544]}, "mutation_prompt": null}
{"id": "6fea5776-ea0c-4e5c-aadd-c5daaafca092", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            avg_fit = np.mean([func(ind) for ind in pop])  # Calculate average fitness\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean())  # Adjust mutation factor dynamically\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean())  # Adjust crossover probability\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1 * (avg_fit / (bounds.ub - bounds.lb).mean()) ))  # Adjust population size\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01  # Add penalty for high variability\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:  \n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improve convergence by dynamically adjusting population size and mutation factor based on average fitness.", "configspace": "", "generation": 11, "fitness": 0.7498616980850459, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.750 with standard deviation 0.050. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "1fd3b039-43f6-4df3-babf-a08d464093b7", "metadata": {"aucs": [0.6832645948112197, 0.8027348862128137, 0.7635856132311043], "final_y": [0.20726632345844398, 0.15584933888861707, 0.16652690485138066]}, "mutation_prompt": null}
{"id": "eccee948-59f8-4e94-9599-c492466f0252", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean())  # Adjust mutation factor dynamically\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean())  # Adjust crossover probability\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                penalty_factor = 0.01 + 0.01 * np.random.rand()  # Add stochastic component to penalty\n                f = func(trial) + np.var(trial) * penalty_factor\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:  \n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce a stochastic component to dynamically adjust the penalty factor during differential evolution to increase exploration capabilities.", "configspace": "", "generation": 11, "fitness": 0.7808946855143993, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.017. And the mean value of best solutions found was 0.157 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "1fd3b039-43f6-4df3-babf-a08d464093b7", "metadata": {"aucs": [0.7603311421144955, 0.8022417818861399, 0.7801111325425627], "final_y": [0.15708283904843467, 0.15310453196178675, 0.16082540162634906]}, "mutation_prompt": null}
{"id": "f9ae4d33-9b03-4bcb-b1b1-7c0a73eff25d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.25 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)  # Refined stochastic perturbation\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean())\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(150, int(pop_size * 1.1))  # Increased cap on population size\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce adaptive population resizing and refine stochastic perturbation for enhanced exploration and local optima resilience.", "configspace": "", "generation": 12, "fitness": 0.7733631314252202, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.773 with standard deviation 0.025. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "74b177c5-b2a6-4b2e-8e8e-4a4ee480a3b0", "metadata": {"aucs": [0.7376974820982483, 0.7945901820562783, 0.787801730121134], "final_y": [0.17924064183162647, 0.1601017669387199, 0.1617603628877432]}, "mutation_prompt": null}
{"id": "b378b54f-9191-4e32-9bb9-2141a9c87573", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)  # Stochastic perturbation\n            CR = 0.9 - 0.2 * (diversity / (bounds.ub - bounds.lb).mean())  # Dynamic adjustment\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce dynamic adjustment to both mutation factor and crossover probability based on diversity to enhance adaptability.", "configspace": "", "generation": 12, "fitness": 0.7778016021915577, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.014. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "74b177c5-b2a6-4b2e-8e8e-4a4ee480a3b0", "metadata": {"aucs": [0.7678406972697147, 0.7972663409972172, 0.7682977683077411], "final_y": [0.1501387328029431, 0.1569406389538176, 0.1661136526365855]}, "mutation_prompt": null}
{"id": "c6a73973-4e2c-4690-a39f-5fec62bf1a0e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)  # Stochastic perturbation\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)  # Adaptive perturbation\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability adjustments based on population convergence to improve exploration and exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.7852167267562535, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.785 with standard deviation 0.023. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "74b177c5-b2a6-4b2e-8e8e-4a4ee480a3b0", "metadata": {"aucs": [0.7537599326901807, 0.7945901820562783, 0.8073000655223014], "final_y": [0.16419321527657826, 0.1601017669387199, 0.13671217143934433]}, "mutation_prompt": null}
{"id": "6ae4d3d2-97a2-4f92-8d6b-0f830ce7e7fe", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)  # Stochastic perturbation\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.02)  # Adaptive control\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce adaptive control of the crossover probability using diversity to improve refinement accuracy.", "configspace": "", "generation": 12, "fitness": 0.7944202783038463, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.026. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "74b177c5-b2a6-4b2e-8e8e-4a4ee480a3b0", "metadata": {"aucs": [0.798189105353563, 0.823997912610201, 0.7610738169477751], "final_y": [0.1528526806734828, 0.14780061943099376, 0.17296403364081692]}, "mutation_prompt": null}
{"id": "2c33eb51-7c91-433f-81a2-5c7d211df149", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)  # Stochastic perturbation\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.01)  # Adaptive adjustment\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Adjust crossover probability adaptively based on diversity to enhance exploration and exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.7861433916758451, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.013. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "74b177c5-b2a6-4b2e-8e8e-4a4ee480a3b0", "metadata": {"aucs": [0.7673446735115648, 0.7945901820562783, 0.7964953194596921], "final_y": [0.1531918318679386, 0.1601017669387199, 0.14209473937018358]}, "mutation_prompt": null}
{"id": "96ab1063-a69c-4b07-bfca-1a1f40aba8ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n        self.f_history = []  # Historical performance\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            self.f_history.append(func(best))\n            if len(self.f_history) > 1:\n                F = 0.5 + 0.3 * np.sign(self.f_history[-1] - self.f_history[-2])  # Adaptive control based on history\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.02)  # Adaptive control\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce adaptive mutation factor based on historical performance to enhance search efficiency.", "configspace": "", "generation": 13, "fitness": 0.788558655038251, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.789 with standard deviation 0.013. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "6ae4d3d2-97a2-4f92-8d6b-0f830ce7e7fe", "metadata": {"aucs": [0.80053822472127, 0.7945901820562783, 0.7705475583372047], "final_y": [0.14885894514020925, 0.1601017669387199, 0.16460096466325302]}, "mutation_prompt": null}
{"id": "618770a2-2320-4c85-a0b9-c31931c5c935", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)  # Stochastic perturbation\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.02)  # Adaptive control\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + np.random.normal(0, 0.01, self.dim), bounds.lb, bounds.ub)  # Modified Line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Refine the mutation strategy with hybrid differential evolution and Gaussian perturbation for improved exploration.", "configspace": "", "generation": 13, "fitness": 0.7925289856179548, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.793 with standard deviation 0.025. And the mean value of best solutions found was 0.156 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "6ae4d3d2-97a2-4f92-8d6b-0f830ce7e7fe", "metadata": {"aucs": [0.771562020485829, 0.8278051067084866, 0.7782198296595492], "final_y": [0.16099984201874573, 0.14626317099397468, 0.15962693890145718]}, "mutation_prompt": null}
{"id": "21e2038b-1ee4-49df-8c6e-d36530d3ef5e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)  # Stochastic perturbation\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.02)  # Adaptive control\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = np.mean([func(trial) for _ in range(3)]) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce noise reduction in evaluation by averaging function outcomes over multiple trials for improved robustness.", "configspace": "", "generation": 13, "fitness": 0.7643032364828249, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.764 with standard deviation 0.027. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "6ae4d3d2-97a2-4f92-8d6b-0f830ce7e7fe", "metadata": {"aucs": [0.7324547516114105, 0.79830257051367, 0.7621523873233942], "final_y": [0.17363478662557752, 0.15326936832639226, 0.16737687911937638]}, "mutation_prompt": null}
{"id": "fd1343ca-a454-43d2-937d-65ba24080412", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)  # Stochastic perturbation\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.02)  # Adaptive control\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * (1.1 if diversity > 0.1 else 0.9)))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                else:\n                    pop = pop[:new_pop_size]\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce a dynamic population size that adapts based on convergence progress to improve exploration-exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.7854584179285533, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.785 with standard deviation 0.008. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "6ae4d3d2-97a2-4f92-8d6b-0f830ce7e7fe", "metadata": {"aucs": [0.776324631900927, 0.7965434008560413, 0.7835072210286914], "final_y": [0.1554610162632647, 0.1556912950633531, 0.1619377799809878]}, "mutation_prompt": null}
{"id": "1f7cf2bc-7bab-4a36-931f-56caa834d947", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        prev_best_val = func(best)\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            convergence_speed = abs(func(best) - prev_best_val)\n            prev_best_val = func(best)\n            F = 0.5 + 0.3 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)\n            CR = 0.9 - 0.5 * (convergence_speed / diversity) + np.random.normal(0, 0.02)\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance adaptive measures by dynamically adjusting mutation and crossover based on convergence speed and diversity.", "configspace": "", "generation": 13, "fitness": 0.7831148972184735, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.015. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "6ae4d3d2-97a2-4f92-8d6b-0f830ce7e7fe", "metadata": {"aucs": [0.7695013801323815, 0.8037224618307276, 0.7761208496923113], "final_y": [0.1632042764377133, 0.15658960134409172, 0.1418874678508132]}, "mutation_prompt": null}
{"id": "fc010b35-7be8-49d3-a596-59b4e60446b3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.02)\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                F_layer = np.random.rand(self.dim) * 0.1  # Adaptive per-layer mutation factor\n                mutant = np.clip(a + F_layer * (b - c) + np.random.normal(0, 0.01, self.dim), bounds.lb, bounds.ub)  # Modified Line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce an adaptive layer-wise mutation strategy to enhance exploitation while maintaining diversity in large-scale optimization.", "configspace": "", "generation": 14, "fitness": 0.770033385759258, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.770 with standard deviation 0.026. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "618770a2-2320-4c85-a0b9-c31931c5c935", "metadata": {"aucs": [0.7414008947672475, 0.8047066841287004, 0.7639925783818259], "final_y": [0.17448738954394638, 0.15450804010206298, 0.16609101970511686]}, "mutation_prompt": null}
{"id": "1185ab96-1aee-41e0-96e8-f4bf2c555ccd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.6  # Adjusted mutation factor\n        self.CR = 0.85  # Adjusted crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.6, CR=0.85):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.3 * (diversity / (bounds.ub - bounds.lb).mean())  # Enhanced\n            if self.evaluations % (self.budget // 8) == 0:\n                new_pop_size = min(120, int(pop_size * 1.2))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + np.random.normal(0, 0.02, self.dim), bounds.lb, bounds.ub)  # Adjusted\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)  # Adjust layer increase\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150})  # Increased iterations\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Integrate adaptive layer-wise perturbation and hierarchical local search to enhance exploration and solution robustness.", "configspace": "", "generation": 14, "fitness": 0.7776813501986298, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.014. And the mean value of best solutions found was 0.161 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "618770a2-2320-4c85-a0b9-c31931c5c935", "metadata": {"aucs": [0.7730376367569597, 0.7966471129814553, 0.7633593008574744], "final_y": [0.15802404453740793, 0.15862470222505987, 0.16783553839522247]}, "mutation_prompt": null}
{"id": "fec93fd8-c773-48f2-983b-1b7b54d9ea0e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean())  # Change Line: Dynamic adjustment of F\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.02)  # Adaptive control\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + np.random.normal(0, 0.01, self.dim), bounds.lb, bounds.ub)  # Modified Line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance exploration by dynamically adjusting mutation factor based on population diversity.", "configspace": "", "generation": 14, "fitness": 0.7661039026897342, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.766 with standard deviation 0.026. And the mean value of best solutions found was 0.169 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "618770a2-2320-4c85-a0b9-c31931c5c935", "metadata": {"aucs": [0.7340444566042177, 0.7973523013868138, 0.766914950078171], "final_y": [0.17828998838795806, 0.15816595410003054, 0.16992776045607327]}, "mutation_prompt": null}
{"id": "b39bf1c7-98f9-4edb-bdd5-6ec2c1487e52", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def levy_flight(self, L=1.5):\n        u = np.random.normal(0, 1, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / (np.abs(v) ** (1 / L))\n        return step\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * np.random.rand()  # Adaptive mutation factor\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.02)  \n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * 1.1))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + self.levy_flight(), bounds.lb, bounds.ub)  # Modified Line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance exploration by introducing Levy flights into differential evolution and adjust mutation factor adaptively.", "configspace": "", "generation": 14, "fitness": 0.7856251356889189, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.014. And the mean value of best solutions found was 0.157 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "618770a2-2320-4c85-a0b9-c31931c5c935", "metadata": {"aucs": [0.7718139530068988, 0.8042159949685086, 0.7808454590913492], "final_y": [0.1562057208672789, 0.15383399475182213, 0.16195942112253148]}, "mutation_prompt": null}
{"id": "d317cdf5-2665-4c07-8bf3-21dbe6420ba4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)  # Stochastic perturbation\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.02)  # Adaptive control\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * (1.1 + 0.1 * (diversity / (bounds.ub - bounds.lb).mean()))))  # Modified Line\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + np.random.normal(0, 0.01, self.dim), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce diversity control by adapting the population size more dynamically during the evolutionary process.", "configspace": "", "generation": 14, "fitness": 0.7940460471641207, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.022. And the mean value of best solutions found was 0.150 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "618770a2-2320-4c85-a0b9-c31931c5c935", "metadata": {"aucs": [0.7734329364713648, 0.8237832470530628, 0.7849219579679343], "final_y": [0.15471221624893017, 0.13865931159373124, 0.15753322133026693]}, "mutation_prompt": null}
{"id": "1c61583f-370b-49b2-82b8-f18589ea516c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.6 + 0.1 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)  # Enhanced mutation factor\n            CR = 0.95 - 0.3 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.02)  # Adjusted crossover probability\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * (1.1 + 0.1 * (diversity / (bounds.ub - bounds.lb).mean()))))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + np.random.normal(0, 0.01, self.dim), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance exploration by modifying mutation strategy and adaptively adjusting crossover probability based on diversity.", "configspace": "", "generation": 15, "fitness": 0.7833146023335841, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.015. And the mean value of best solutions found was 0.161 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "d317cdf5-2665-4c07-8bf3-21dbe6420ba4", "metadata": {"aucs": [0.7628328387634337, 0.7945901820562783, 0.79252078618104], "final_y": [0.16722281873630485, 0.1601017669387199, 0.15582859256731973]}, "mutation_prompt": null}
{"id": "912aeecd-2bf5-4eed-a4be-7aa65d32506a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()) * np.var(pop) + np.random.normal(0, 0.05)  # Adjusted Line\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.02)  # Adaptive control\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * (1.1 + 0.1 * (diversity / (bounds.ub - bounds.lb).mean()))))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + np.random.normal(0, 0.01, self.dim), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Adjust the mutation factor dynamically based on diversity and population variance for enhanced exploration.", "configspace": "", "generation": 15, "fitness": 0.7837137296799126, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.044. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "d317cdf5-2665-4c07-8bf3-21dbe6420ba4", "metadata": {"aucs": [0.724445499200292, 0.797411685287548, 0.8292840045518978], "final_y": [0.17784878506889779, 0.15510746305942014, 0.14579197532447008]}, "mutation_prompt": null}
{"id": "ba0f4101-adc9-41ff-acc5-fe95b0b898eb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.05)  # Stochastic perturbation\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.02)  # Adaptive control\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * (1.1 + 0.1 * (diversity / (bounds.ub - bounds.lb).mean()))))  # Modified Line\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + np.random.normal(0, 0.01, self.dim) + 0.1 * (np.random.rand(self.dim) - 0.5), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance mutation in DE by incorporating self-adaptive random walk for more diversity.", "configspace": "", "generation": 15, "fitness": 0.77126998056946, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.771 with standard deviation 0.025. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "d317cdf5-2665-4c07-8bf3-21dbe6420ba4", "metadata": {"aucs": [0.7369308935248987, 0.7945901820562783, 0.7822888661272026], "final_y": [0.1763844714361157, 0.1601017669387199, 0.1604602274886584]}, "mutation_prompt": null}
{"id": "18833075-d5dc-4982-8cb9-f540c60a244c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.5 + 0.1 * np.sin(self.evaluations / self.budget * np.pi)  # Dynamic adaptation\n            CR = 0.9 - 0.4 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.02)  # Adaptive control\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(100, int(pop_size * (1.1 + 0.1 * (diversity / (bounds.ub - bounds.lb).mean()))))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + np.random.normal(0, 0.01, self.dim), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce a dynamic adaptation mechanism for mutation factor F and optimize layer gradient initialization.", "configspace": "", "generation": 15, "fitness": 0.7758336621420954, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.776 with standard deviation 0.017. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "d317cdf5-2665-4c07-8bf3-21dbe6420ba4", "metadata": {"aucs": [0.7532788148180914, 0.7945901820562783, 0.7796319895519161], "final_y": [0.15811069699305824, 0.1601017669387199, 0.15514364529850666]}, "mutation_prompt": null}
{"id": "c13dbcf2-dbf6-48d4-86b2-a5b4e4166580", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.6 + 0.15 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.03)  # Enhanced Line\n            CR = 0.9 - 0.3 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.01)  # Enhanced Line\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(120, int(pop_size * (1.2 + 0.1 * (diversity / (bounds.ub - bounds.lb).mean()))))  # Enhanced Line\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='TNC', options={'maxiter': 100})  # Enhanced Line\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Refine diversity control by enhancing mutation strategy and local search integration in HybridMetaheuristic.", "configspace": "", "generation": 15, "fitness": 0.784562691594818, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.785 with standard deviation 0.011. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "d317cdf5-2665-4c07-8bf3-21dbe6420ba4", "metadata": {"aucs": [0.7770263685252463, 0.80055941752105, 0.7761022887381573], "final_y": [0.15635866015158162, 0.15708688253757896, 0.16628710300105298]}, "mutation_prompt": null}
{"id": "2a42bd11-ad3c-4b39-91b6-97930fb7098f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.6 + 0.15 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.03)\n            CR = max(0.1, 0.9 - 0.3 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.01))  # Enhanced Line\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(120, int(pop_size * (1.2 + 0.1 * (diversity / (bounds.ub - bounds.lb).mean()))))\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='TNC', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improve the diversity control by dynamically adjusting the crossover probability based on variance.", "configspace": "", "generation": 16, "fitness": 0.7752690827778524, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.015. And the mean value of best solutions found was 0.162 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "c13dbcf2-dbf6-48d4-86b2-a5b4e4166580", "metadata": {"aucs": [0.7588217619127173, 0.7946184699491288, 0.7723670164717114], "final_y": [0.16087082238517225, 0.15995070385265664, 0.16578031749884192]}, "mutation_prompt": null}
{"id": "77955668-eb28-4cfe-8f4c-de22cf94d6a0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        pop += np.random.normal(0, 0.02, pop.shape)  # Enhanced Line\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.6 + 0.15 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.03)  # Enhanced Line\n            CR = 0.9 - 0.3 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.01)  # Enhanced Line\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(120, int(pop_size * (1.2 + 0.1 * (diversity / (bounds.ub - bounds.lb).mean()))))  # Enhanced Line\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='TNC', options={'maxiter': 100})  # Enhanced Line\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improve population initialization by introducing a Gaussian perturbation for better diversity in HybridMetaheuristic.", "configspace": "", "generation": 16, "fitness": 0.7768587173311449, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.777 with standard deviation 0.016. And the mean value of best solutions found was 0.163 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "c13dbcf2-dbf6-48d4-86b2-a5b4e4166580", "metadata": {"aucs": [0.7565193217358119, 0.7945901820562783, 0.7794666482013443], "final_y": [0.16727304731558412, 0.1601017669387199, 0.16189366800473992]}, "mutation_prompt": null}
{"id": "54652c3b-b92e-404b-b194-f38352dfffe8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)  # Adjusted Line\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)  # Adjusted Line\n            adapt_factor = np.random.uniform(0.9, 1.1)  # New Line\n            new_pop_size = int(pop_size * adapt_factor)  # New Line\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='TNC', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance HybridMetaheuristic by redefining mutation strategy and implementing an adaptive population size mechanism for improved exploration-exploitation balance.", "configspace": "", "generation": 16, "fitness": 0.7958988475579364, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.796 with standard deviation 0.013. And the mean value of best solutions found was 0.157 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "c13dbcf2-dbf6-48d4-86b2-a5b4e4166580", "metadata": {"aucs": [0.7807075174080729, 0.7945901820562783, 0.8123988432094578], "final_y": [0.16304658776510916, 0.1601017669387199, 0.14741651264815336]}, "mutation_prompt": null}
{"id": "ebab9644-4ab9-4cd8-b349-5585133c3990", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.6 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean())  # Enhanced Line\n            CR = 0.8 - 0.2 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.01)  # Enhanced Line\n            if self.evaluations % (self.budget // 8) == 0:  # Enhanced Line\n                new_pop_size = min(150, int(pop_size * (1.3 + 0.1 * (diversity / (bounds.ub - bounds.lb).mean()))))  # Enhanced Line\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 4) == 0:  # Enhanced Line\n                self.dim = min(self.dim + 3, self.max_layers)  # Enhanced Line\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150})  # Enhanced Line\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance adaptability by introducing layer modularity mapping, adaptive metrics, and progressive dimensional increment in the HybridMetaheuristic.", "configspace": "", "generation": 16, "fitness": 0.7710993175088635, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.771 with standard deviation 0.017. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "c13dbcf2-dbf6-48d4-86b2-a5b4e4166580", "metadata": {"aucs": [0.7576339535225374, 0.7945901820562783, 0.7610738169477751], "final_y": [0.16193491569729712, 0.1601017669387199, 0.17296403364081692]}, "mutation_prompt": null}
{"id": "ffda8f81-fd71-46f7-a53b-19c6271a505b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = 0.6 + 0.15 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.03)  # Enhanced Line\n            CR = 0.9 - 0.3 * (diversity / (bounds.ub - bounds.lb).mean()) + np.random.normal(0, 0.01)  # Enhanced Line\n            if self.evaluations % (self.budget // 10) == 0:\n                new_pop_size = min(120, int(pop_size * (1.2 + 0.1 * (diversity / (bounds.ub - bounds.lb).mean()))))  # Enhanced Line\n                if new_pop_size > pop_size:\n                    additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                    pop = np.vstack((pop, additional_pop))\n                pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.02  # Modified line\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='TNC', options={'maxiter': 100})  # Enhanced Line\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improve robustness by incorporating variance scaling in trial evaluation during mutation strategy.", "configspace": "", "generation": 16, "fitness": 0.7867023125529341, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.787 with standard deviation 0.010. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "c13dbcf2-dbf6-48d4-86b2-a5b4e4166580", "metadata": {"aucs": [0.7730069971082998, 0.7974672453274936, 0.7896326952230088], "final_y": [0.14671239391007962, 0.15829896235951724, 0.15103288548359872]}, "mutation_prompt": null}
{"id": "2f0c3ec7-1df0-42ad-8162-04d76361dc99", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.3 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)  # Adjusted Line\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)  # Adjusted Line\n            adapt_factor = np.random.uniform(0.9, 1.1)  # New Line\n            new_pop_size = int(pop_size * adapt_factor)  # New Line\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='TNC', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Fine-tune the mutation factor by dynamically adjusting F based on population diversity and fitness improvement.", "configspace": "", "generation": 17, "fitness": 0.7738223373407894, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.015. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "54652c3b-b92e-404b-b194-f38352dfffe8", "metadata": {"aucs": [0.7629706026588304, 0.7945901820562783, 0.7639062273072595], "final_y": [0.16774915767335274, 0.1601017669387199, 0.14619336686983697]}, "mutation_prompt": null}
{"id": "cc8ad8a2-3497-4ddf-adba-3292df331036", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.9, 1.1)\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + int(self.max_layers * 0.1), self.max_layers)  # Adjusted Line\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 100})  # Adjusted Line\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Implement adaptive layer increment strategy and enhance local search for improved convergence.", "configspace": "", "generation": 17, "fitness": 0.787978773369926, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.788 with standard deviation 0.007. And the mean value of best solutions found was 0.157 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "54652c3b-b92e-404b-b194-f38352dfffe8", "metadata": {"aucs": [0.7782958550704949, 0.7945901820562783, 0.7910502829830047], "final_y": [0.1585523903888839, 0.1601017669387199, 0.1532144007000691]}, "mutation_prompt": null}
{"id": "cb3ad52a-7fd7-4d16-84a6-a2b256ae2fe3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.8, 1.2)  # Modified Line\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005  # Modified Line\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:  # Modified Line\n                self.dim = min(self.dim + 3, self.max_layers)  # Modified Line\n                \n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150})  # Modified Line\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introducing adaptive differential evolution with noise reduction and enhanced local search to improve convergence and solution accuracy.", "configspace": "", "generation": 17, "fitness": 0.788349021664945, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.788 with standard deviation 0.006. And the mean value of best solutions found was 0.157 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "54652c3b-b92e-404b-b194-f38352dfffe8", "metadata": {"aucs": [0.7801539714176418, 0.7945901820562783, 0.7903029115209146], "final_y": [0.15184404097825677, 0.1601017669387199, 0.15868306765651197]}, "mutation_prompt": null}
{"id": "3fac3faf-7f6e-4c0d-832d-92f26e0133ef", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.95, 1.15)  # Adjusted Line\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005  # Adjusted Line\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)  # Adjusted Line\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='TNC', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced modular adaptation and noise handling in hybrid metaheuristic for robust solar cell optimization.", "configspace": "", "generation": 17, "fitness": 0.782397869752507, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.782 with standard deviation 0.011. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "54652c3b-b92e-404b-b194-f38352dfffe8", "metadata": {"aucs": [0.7841373283498839, 0.7945901820562783, 0.7684660988513589], "final_y": [0.15159133394866453, 0.1601017669387199, 0.1650832376008401]}, "mutation_prompt": null}
{"id": "6b3a7ba6-885a-43e6-b79b-169ef5eee676", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)  # Adjusted Line\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)  # Adjusted Line\n            adapt_factor = np.random.uniform(0.9, 1.1)  # New Line\n            new_pop_size = int(pop_size * adapt_factor)  # New Line\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.02  # Adjusted Line\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 5) == 0:\n                self.dim = min(self.dim + 2, self.max_layers)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='TNC', options={'maxiter': 100})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Optimize HybridMetaheuristic by enhancing trial solution evaluation with adaptive penalty based on solution variance.", "configspace": "", "generation": 17, "fitness": 0.7751043150254003, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.016. And the mean value of best solutions found was 0.161 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "54652c3b-b92e-404b-b194-f38352dfffe8", "metadata": {"aucs": [0.7592914427727951, 0.797501813787435, 0.7685196885159709], "final_y": [0.1595812039587805, 0.15556068664155298, 0.16709912349431444]}, "mutation_prompt": null}
{"id": "3eec2116-72f8-44d4-8c3d-1a4aa8de6469", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.8, 1.2)  # Modified Line\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.01  # Modified Line\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:  # Modified Line\n                self.dim = min(self.dim + 3, self.max_layers)  # Modified Line\n                \n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150})  # Modified Line\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhanced adaptive differential evolution by introducing a penalty for high variance solutions to guide convergence towards more stable regions.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"Cannot take a larger sample than population when 'replace=False'\").", "error": "ValueError(\"Cannot take a larger sample than population when 'replace=False'\")", "parent_id": "cb3ad52a-7fd7-4d16-84a6-a2b256ae2fe3", "metadata": {}, "mutation_prompt": null}
{"id": "3d47f77a-1f4b-4f03-aedf-aca1d22b319c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.3 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)  # Modified Line\n            CR = np.clip(0.7 + 0.3 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)  # Modified Line\n            adapt_factor = np.random.uniform(0.8, 1.2)\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)\n                \n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance exploration and robustness by dynamically adjusting adaptation factors in differential evolution.", "configspace": "", "generation": 18, "fitness": 0.7809500939218599, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.010. And the mean value of best solutions found was 0.162 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "cb3ad52a-7fd7-4d16-84a6-a2b256ae2fe3", "metadata": {"aucs": [0.7737358600426443, 0.7945901820562783, 0.7745242396666572], "final_y": [0.16197023951089595, 0.1601017669387199, 0.16481915582552575]}, "mutation_prompt": null}
{"id": "48b6163e-acf1-4eaa-9d46-bceb0b14a8c2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.8, 1.2)\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                F = 0.4 if func(pop[i]) < func(best) else F  # Modified Line\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)\n                \n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance convergence by dynamically adjusting the mutation factor based on solution quality.", "configspace": "", "generation": 18, "fitness": 0.7773770356427864, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.777 with standard deviation 0.032. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "cb3ad52a-7fd7-4d16-84a6-a2b256ae2fe3", "metadata": {"aucs": [0.735030488355529, 0.8128183981481024, 0.7842822204247273], "final_y": [0.14354769521082333, 0.1412484008456475, 0.15171727466860807]}, "mutation_prompt": null}
{"id": "3247b7e8-aebb-4eb2-8114-53e8260ca694", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.8, 1.2)  # Modified Line\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005  # Modified Line\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:  # Modified Line\n                self.dim = min(self.dim + 3, self.max_layers)  # Modified Line\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150})  # Modified Line\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introducing adaptive differential evolution with improved noise handling and controlled diversity-driven mutation for enhanced convergence.", "configspace": "", "generation": 18, "fitness": 0.7886053916298342, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.789 with standard deviation 0.015. And the mean value of best solutions found was 0.155 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "cb3ad52a-7fd7-4d16-84a6-a2b256ae2fe3", "metadata": {"aucs": [0.7962419216386118, 0.8020742777724722, 0.7674999754784182], "final_y": [0.14592147598855543, 0.15563489228589145, 0.1636697394066876]}, "mutation_prompt": null}
{"id": "141f70c8-c181-442d-9738-fdc93c8f3770", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        historical_best = best\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.8, 1.2)\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + 0.1 * (historical_best - a), bounds.lb, bounds.ub)  # Modified Line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                        historical_best = best\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)\n                \n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhancing convergence by modifying the mutation strategy to include weighted historical best solutions.", "configspace": "", "generation": 18, "fitness": 0.7937687250859793, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.001. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "cb3ad52a-7fd7-4d16-84a6-a2b256ae2fe3", "metadata": {"aucs": [0.7927543286002802, 0.7945901820562783, 0.7939616646013795], "final_y": [0.15596290982954197, 0.1601017669387199, 0.14727540506601167]}, "mutation_prompt": null}
{"id": "bf5b64e2-d6f7-4600-892f-c01f832ebd0b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        historical_best = best\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.8, 1.2)\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                improvement_rate = (func(historical_best) - func(best)) / func(historical_best)  # New Line\n                mutant = np.clip(a + F * (b - c) + 0.1 * improvement_rate * (historical_best - a), bounds.lb, bounds.ub)  # Modified Line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                        historical_best = best\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)\n                \n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance convergence by introducing adaptive mutation scaling based on historical improvement rates.", "configspace": "", "generation": 19, "fitness": 0.7620201313039009, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.762 with standard deviation 0.029. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "141f70c8-c181-442d-9738-fdc93c8f3770", "metadata": {"aucs": [0.7266358152767642, 0.7983507616871635, 0.7610738169477751], "final_y": [0.18475334089319084, 0.15606408473694777, 0.17296403364081692]}, "mutation_prompt": null}
{"id": "6db77016-1e72-47e3-9736-aee31c642268", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        historical_best = best\n        decay_factor = 0.95  # Added Line\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.8, 1.2)\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) * decay_factor + 0.1 * (historical_best - a), bounds.lb, bounds.ub)  # Modified Line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                        historical_best = best\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)\n                \n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Enhance adaptation by incorporating a decay factor into the selection of mutation vectors for better convergence.", "configspace": "", "generation": 19, "fitness": 0.7821985204263567, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.782 with standard deviation 0.027. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "141f70c8-c181-442d-9738-fdc93c8f3770", "metadata": {"aucs": [0.7459053427316771, 0.8093862769698059, 0.7913039415775872], "final_y": [0.17481970735241215, 0.1542751270986309, 0.15085669791114165]}, "mutation_prompt": null}
{"id": "ee4b298b-3497-4339-af87-66b10ef2a607", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        historical_best = best\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.8, 1.2)\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutation_weight = 0.2 if self.evaluations < self.budget / 2 else 0.1  # Modified Line\n                mutant = np.clip(a + F * (b - c) + mutation_weight * (historical_best - a), bounds.lb, bounds.ub)  # Modified Line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):  # Modified Line\n                        best = trial\n                        historical_best = best\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)\n                \n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Adaptive hybrid algorithm with dynamic layer increment and mutation strategy for improved convergence.", "configspace": "", "generation": 19, "fitness": 0.7800069088890326, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.012. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "141f70c8-c181-442d-9738-fdc93c8f3770", "metadata": {"aucs": [0.7660993165101434, 0.7945901820562783, 0.7793312281006759], "final_y": [0.15284524249201092, 0.1601017669387199, 0.1654537426337198]}, "mutation_prompt": null}
{"id": "d847a15a-7aa6-432b-a411-aef30fb42e09", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        historical_best = best\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.8, 1.2)\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + 0.1 * (historical_best - a), bounds.lb, bounds.ub)  # Modified Line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                        historical_best = best\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)\n                \n        return best\n\n    def local_search(self, func, x0, bounds):\n        adaptive_lr = 0.001 + 0.004 * (self.evaluations / self.budget)  # Line changed for adaptive learning rate\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150, 'learning_rate': adaptive_lr})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Incorporating an adaptive learning rate for local search to enhance convergence speed and precision.", "configspace": "", "generation": 19, "fitness": 0.7875579442602404, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.788 with standard deviation 0.012. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "141f70c8-c181-442d-9738-fdc93c8f3770", "metadata": {"aucs": [0.7789528977380863, 0.8041065283377397, 0.7796144067048953], "final_y": [0.16158002546530614, 0.15564980289597996, 0.15946214889239652]}, "mutation_prompt": null}
{"id": "aa30ca51-ee0e-45d8-9919-2b5e5586a82a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        historical_best = best\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.3 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)  # Modified Line\n            adapt_factor = np.random.uniform(0.8, 1.2)\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + 0.1 * (historical_best - a), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                        historical_best = best\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)\n                \n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introducing adaptive crossover rate adjustment based on population diversity for improved convergence.", "configspace": "", "generation": 19, "fitness": 0.7855119729879898, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.020. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "141f70c8-c181-442d-9738-fdc93c8f3770", "metadata": {"aucs": [0.7571299583559354, 0.7963835104259622, 0.8030224501820719], "final_y": [0.16720734438390883, 0.1585089997883229, 0.15415493548083392]}, "mutation_prompt": null}
{"id": "423df83f-6205-4950-9912-e25fb096389f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        historical_best = best\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.8, 1.2)\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + 0.1 * (historical_best - np.mean(pop, axis=0)), bounds.lb, bounds.ub)  # Modified Line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                        historical_best = best\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)\n                \n        return best\n\n    def local_search(self, func, x0, bounds):\n        adaptive_lr = 0.001 + 0.004 * (self.evaluations / self.budget)\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150, 'learning_rate': adaptive_lr})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Incorporating historical diversity in trial generation to enhance exploration.", "configspace": "", "generation": 20, "fitness": 0.7755725794968787, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.776 with standard deviation 0.014. And the mean value of best solutions found was 0.163 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "d847a15a-7aa6-432b-a411-aef30fb42e09", "metadata": {"aucs": [0.7695454444648893, 0.7945901820562783, 0.7625821119694685], "final_y": [0.1596183744400811, 0.1601017669387199, 0.17059053827944282]}, "mutation_prompt": null}
{"id": "d9b84508-6e6c-4246-8330-1abd03850218", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        historical_best = best\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.8, 1.2)\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + F * (historical_best - b), bounds.lb, bounds.ub)  # Modified Line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                        historical_best = best\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)\n                \n        return best\n\n    def local_search(self, func, x0, bounds):\n        adaptive_lr = 0.001 + 0.004 * (self.evaluations / self.budget)\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150, 'learning_rate': adaptive_lr})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improved mutation strategy in differential evolution for better global exploration.", "configspace": "", "generation": 20, "fitness": 0.7784693284968162, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.012. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "d847a15a-7aa6-432b-a411-aef30fb42e09", "metadata": {"aucs": [0.7660324924950094, 0.795237049901611, 0.7741384430938283], "final_y": [0.15225479073529025, 0.1491395726250323, 0.15914155760868642]}, "mutation_prompt": null}
{"id": "fd80428a-ace5-44e0-bdae-da2622fa457e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        historical_best = best\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.3 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 1.0)  # Changed Line\n            CR = np.clip(0.7 + 0.3 * (1 - diversity / (bounds.ub - bounds.lb).mean()), 0.6, 0.95)  # Changed Line\n            adapt_factor = np.random.uniform(0.8, 1.2)\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + 0.1 * (historical_best - a), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                        historical_best = best\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)\n                \n        return best\n\n    def local_search(self, func, x0, bounds):\n        adaptive_lr = 0.001 + 0.004 * (self.evaluations / self.budget)\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150, 'learning_rate': adaptive_lr})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Improved convergence by introducing a dynamic mutation factor and adaptive crossover probability in the differential evolution phase.", "configspace": "", "generation": 20, "fitness": 0.7793697054728801, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.779 with standard deviation 0.014. And the mean value of best solutions found was 0.162 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "d847a15a-7aa6-432b-a411-aef30fb42e09", "metadata": {"aucs": [0.7824451174145868, 0.7945901820562783, 0.7610738169477751], "final_y": [0.15279723285716973, 0.1601017669387199, 0.17296403364081692]}, "mutation_prompt": null}
{"id": "fab5ccbf-a6d5-436b-94c6-8b8c0e3b5da8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        historical_best = best\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.8, 1.2)\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + 0.1 * (historical_best - a), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.005\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                        historical_best = best\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)\n            if self.evaluations % (self.budget // 10) == 0:  # Added Line\n                best = self.local_search(func, best, bounds)\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        adaptive_lr = 0.001 + 0.004 * (self.evaluations / self.budget)\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150, 'learning_rate': adaptive_lr})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduce intermittent local search iterations during differential evolution to enhance convergence precision.", "configspace": "", "generation": 20, "fitness": 0.7874367898852696, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.787 with standard deviation 0.019. And the mean value of best solutions found was 0.155 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "d847a15a-7aa6-432b-a411-aef30fb42e09", "metadata": {"aucs": [0.7617934977526934, 0.7948644706248218, 0.8056524012782933], "final_y": [0.15806672760124751, 0.1598992580144042, 0.147026603851379]}, "mutation_prompt": null}
{"id": "58fa43f9-cf96-42a6-b0c9-3d31437dbb2a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 50\n        self.F = 0.5\n        self.CR = 0.9\n        self.evaluations = 0\n        self.max_layers = dim\n\n    def differential_evolution(self, func, bounds, pop_size=50, F=0.5, CR=0.9):\n        pop = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx]\n        historical_best = best\n        while self.evaluations < self.budget:\n            diversity = np.mean(np.std(pop, axis=0))\n            F = np.clip(0.5 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.4, 0.9)\n            CR = np.clip(0.7 + 0.2 * (diversity / (bounds.ub - bounds.lb).mean()), 0.6, 1.0)\n            adapt_factor = np.random.uniform(0.8, 1.2)\n            new_pop_size = int(pop_size * adapt_factor)\n            if new_pop_size > pop_size:\n                additional_pop = np.random.rand(new_pop_size - pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                pop = np.vstack((pop, additional_pop))\n            pop_size = new_pop_size\n\n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + 0.1 * np.random.rand(self.dim)*(historical_best - a), bounds.lb, bounds.ub)  # Modified Line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f = func(trial) + np.var(trial) * 0.002  # Modified Line\n                self.evaluations += 1\n                if f < func(pop[i]):\n                    pop[i] = trial\n                    if f < func(best):\n                        best = trial\n                        historical_best = best\n                if self.evaluations >= self.budget:\n                    break\n            if self.evaluations % (self.budget // 6) == 0:\n                self.dim = min(self.dim + 3, self.max_layers)\n                \n        return best\n\n    def local_search(self, func, x0, bounds):\n        adaptive_lr = 0.001 + 0.004 * (self.evaluations / self.budget)\n        res = minimize(func, x0, bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': 150, 'learning_rate': adaptive_lr})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds, self.pop_size, self.F, self.CR)\n        if self.evaluations < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "HybridMetaheuristic", "description": "Introduces layer randomization in DE mutation and reduces trial variance penalty to improve exploration and exploitation balance.", "configspace": "", "generation": 20, "fitness": 0.7928760836478551, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.793 with standard deviation 0.017. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "d847a15a-7aa6-432b-a411-aef30fb42e09", "metadata": {"aucs": [0.7732490889634501, 0.8154150476762693, 0.7899641143038459], "final_y": [0.15442353971772238, 0.1474188377050466, 0.15949027195930643]}, "mutation_prompt": null}
