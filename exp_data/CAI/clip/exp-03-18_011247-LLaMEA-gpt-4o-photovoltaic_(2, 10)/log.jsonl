{"id": "e9a64d65-3b46-42ff-a566-eb038f7f0f98", "solution": "import numpy as np\n\nclass HybridAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.7\n        self.population = np.random.rand(self.population_size, dim)\n        self.performance_memory = []\n        self.current_evaluations = 0\n\n    def adaptive_mutation_factor(self):\n        if len(self.performance_memory) < 2:\n            return self.mutation_factor\n        return 0.5 + 0.1 * np.sign(self.performance_memory[-1] - self.performance_memory[-2])\n\n    def select_parents(self, idx):\n        indices = list(range(self.population_size))\n        indices.remove(idx)\n        return np.random.choice(indices, 3, replace=False)\n\n    def mutate_and_crossover(self, target_idx, func):\n        a, b, c = self.select_parents(target_idx)\n        mutant_vector = self.population[a] + self.adaptive_mutation_factor() * (self.population[b] - self.population[c])\n        mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n        trial_vector = np.copy(self.population[target_idx])\n        crossover_points = np.random.rand(self.dim) < self.crossover_rate\n        trial_vector[crossover_points] = mutant_vector[crossover_points]\n        return trial_vector\n\n    def optimize(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        self.current_evaluations += self.population_size\n        for _ in range(self.budget - self.current_evaluations):\n            for i in range(self.population_size):\n                trial_vector = self.mutate_and_crossover(i, func)\n                trial_fitness = func(trial_vector)\n                self.current_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial_vector\n                    fitness[i] = trial_fitness\n            self.performance_memory.append(np.min(fitness))\n        return self.population[np.argmin(fitness)]\n\n    def __call__(self, func):\n        return self.optimize(func)\n\n# Example usage:\n# optimizer = HybridAdaptiveDifferentialEvolution(budget=1000, dim=10)\n# best_solution = optimizer(some_black_box_function)", "name": "HybridAdaptiveDifferentialEvolution", "description": "A novel hybrid algorithm combining adaptive differential evolution with a learning-based strategy to efficiently explore and exploit the search space within limited function evaluations.", "configspace": "", "generation": 0, "fitness": 0.660056797745749, "feedback": "The algorithm HybridAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.660 with standard deviation 0.000. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.660056797745749, 0.660056797745749, 0.660056797745749], "final_y": [0.21891901516198964, 0.21891901516198964, 0.21891901516198964]}, "mutation_prompt": null}
{"id": "bc6d9795-97d7-4a14-86a9-e9757aadc8c5", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.9  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 10\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "A hybrid metaheuristic algorithm combining particle swarm optimization (PSO) and simulated annealing (SA) for dynamic exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 0, "fitness": 0.8818696688040536, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.010. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": null, "metadata": {"aucs": [0.869268182557325, 0.8840015829849048, 0.8923392408699309], "final_y": [0.11549323219502639, 0.11416673593111526, 0.11621739898412964]}, "mutation_prompt": null}
{"id": "14cc247e-9af5-4162-b7df-dfb5cd3789c7", "solution": "import numpy as np\n\nclass HybridAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.7\n        self.population = np.random.rand(self.population_size, dim)\n        self.performance_memory = []\n        self.current_evaluations = 0\n\n    def adaptive_mutation_factor(self):\n        if len(self.performance_memory) < 2:\n            return self.mutation_factor\n        return 0.5 + 0.1 * np.sign(self.performance_memory[-1] - self.performance_memory[-2])\n\n    def adaptive_crossover_rate(self, success):\n        return min(1.0, self.crossover_rate + 0.1 * (1 if success else -1))  # Change 1\n\n    def select_parents(self, idx):\n        indices = list(range(self.population_size))\n        indices.remove(idx)\n        return np.random.choice(indices, 3, replace=False)\n\n    def mutate_and_crossover(self, target_idx, func):\n        a, b, c = self.select_parents(target_idx)\n        mutant_vector = self.population[a] + self.adaptive_mutation_factor() * (self.population[b] - self.population[c])\n        mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n        trial_vector = np.copy(self.population[target_idx])\n        crossover_points = np.random.rand(self.dim) < self.adaptive_crossover_rate(fitness[target_idx] > func(trial_vector))  # Change 2\n        trial_vector[crossover_points] = mutant_vector[crossover_points]\n        return trial_vector\n\n    def optimize(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        self.current_evaluations += self.population_size\n        for _ in range(self.budget - self.current_evaluations):\n            for i in range(self.population_size):\n                trial_vector = self.mutate_and_crossover(i, func)\n                trial_fitness = func(trial_vector)\n                self.current_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial_vector\n                    fitness[i] = trial_fitness\n            self.performance_memory.append(np.min(fitness))\n        return self.population[np.argmin(fitness)]\n\n    def __call__(self, func):\n        return self.optimize(func)\n\n# Example usage:\n# optimizer = HybridAdaptiveDifferentialEvolution(budget=1000, dim=10)\n# best_solution = optimizer(some_black_box_function)", "name": "HybridAdaptiveDifferentialEvolution", "description": "An enhanced hybrid adaptive differential evolution algorithm with improved mutation strategies and adaptive crossover rates for efficient optimization.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'fitness' is not defined\").", "error": "NameError(\"name 'fitness' is not defined\")", "parent_id": "e9a64d65-3b46-42ff-a566-eb038f7f0f98", "metadata": {}, "mutation_prompt": null}
{"id": "4012de5e-616e-4a5d-bb85-e7b9796c7516", "solution": "import numpy as np\n\nclass HybridAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.7\n        self.population = np.random.rand(self.population_size, dim)\n        self.performance_memory = []\n        self.current_evaluations = 0\n        self.learning_rate = 0.1  # New attribute for dynamic learning\n\n    def adaptive_mutation_factor(self):\n        if len(self.performance_memory) < 2:\n            return self.mutation_factor\n        improvement = self.performance_memory[-1] - self.performance_memory[-2]\n        return max(0.3, min(0.9, self.mutation_factor + self.learning_rate * np.sign(improvement)))\n\n    def select_parents(self, idx):\n        indices = list(range(self.population_size))\n        indices.remove(idx)\n        return np.random.choice(indices, 3, replace=False)\n\n    def mutate_and_crossover(self, target_idx, func):\n        a, b, c = self.select_parents(target_idx)\n        mutant_vector = self.population[a] + self.adaptive_mutation_factor() * (self.population[b] - self.population[c])\n        mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n        trial_vector = np.copy(self.population[target_idx])\n        crossover_points = np.random.rand(self.dim) < self.crossover_rate\n        trial_vector[crossover_points] = mutant_vector[crossover_points]\n        return trial_vector\n\n    def optimize(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        self.current_evaluations += self.population_size\n        for _ in range(self.budget - self.current_evaluations):\n            for i in range(self.population_size):\n                trial_vector = self.mutate_and_crossover(i, func)\n                trial_fitness = func(trial_vector)\n                self.current_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial_vector\n                    fitness[i] = trial_fitness\n            self.performance_memory.append(np.min(fitness))\n        return self.population[np.argmin(fitness)]\n\n    def __call__(self, func):\n        return self.optimize(func)", "name": "HybridAdaptiveDifferentialEvolution", "description": "An enhanced hybrid algorithm combining adaptive differential evolution with a dynamic learning-based strategy for better exploration and exploitation within limited evaluations.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'fitness' is not defined\").", "error": "NameError(\"name 'fitness' is not defined\")", "parent_id": "e9a64d65-3b46-42ff-a566-eb038f7f0f98", "metadata": {}, "mutation_prompt": null}
{"id": "69e477ca-29c0-4364-b63a-54debc615795", "solution": "import numpy as np\n\nclass HybridAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.7\n        self.population = np.random.rand(self.population_size, dim)\n        self.performance_memory = []\n        self.current_evaluations = 0\n\n    def adaptive_mutation_factor(self):\n        if len(self.performance_memory) < 2:\n            return self.mutation_factor\n        return 0.5 + 0.1 * np.tanh(self.performance_memory[-1] - self.performance_memory[-2])\n\n    def adaptive_crossover_rate(self):\n        if len(self.performance_memory) < 2:\n            return self.crossover_rate\n        return 0.7 + 0.2 * np.tanh(np.std(self.performance_memory[-5:]))\n\n    def select_parents(self, idx):\n        indices = list(range(self.population_size))\n        indices.remove(idx)\n        return np.random.choice(indices, 3, replace=False)\n\n    def mutate_and_crossover(self, target_idx, func):\n        a, b, c = self.select_parents(target_idx)\n        mutant_vector = self.population[a] + self.adaptive_mutation_factor() * (self.population[b] - self.population[c])\n        mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n        trial_vector = np.copy(self.population[target_idx])\n        crossover_points = np.random.rand(self.dim) < self.adaptive_crossover_rate()\n        trial_vector[crossover_points] = mutant_vector[crossover_points]\n        return trial_vector\n\n    def local_search(self, individual, func):\n        perturbation = np.random.normal(0, 0.1, size=self.dim)\n        new_individual = np.clip(individual + perturbation, func.bounds.lb, func.bounds.ub)\n        return new_individual if func(new_individual) < func(individual) else individual\n\n    def optimize(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        self.current_evaluations += self.population_size\n        for _ in range(self.budget - self.current_evaluations):\n            for i in range(self.population_size):\n                trial_vector = self.mutate_and_crossover(i, func)\n                trial_vector = self.local_search(trial_vector, func)\n                trial_fitness = func(trial_vector)\n                self.current_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial_vector\n                    fitness[i] = trial_fitness\n            self.performance_memory.append(np.min(fitness))\n        return self.population[np.argmin(fitness)]\n\n    def __call__(self, func):\n        return self.optimize(func)", "name": "HybridAdaptiveDifferentialEvolution", "description": "Improved Hybrid Adaptive Differential Evolution integrating local search and adaptive crossover strategy to enhance convergence speed and solution quality.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'fitness' is not defined\").", "error": "NameError(\"name 'fitness' is not defined\")", "parent_id": "e9a64d65-3b46-42ff-a566-eb038f7f0f98", "metadata": {}, "mutation_prompt": null}
{"id": "9ab12e8d-0b3f-4803-93f1-29d69b6d1f98", "solution": "import numpy as np\n\nclass HybridAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.7\n        self.population = np.random.rand(self.population_size, dim)\n        self.performance_memory = []\n        self.current_evaluations = 0\n\n    def adaptive_mutation_factor(self):\n        if len(self.performance_memory) < 2:\n            return self.mutation_factor\n        population_std = np.std(self.population, axis=0).mean()\n        return 0.5 + 0.1 * (1 - np.tanh(population_std))\n\n    def select_parents(self, idx):\n        indices = list(range(self.population_size))\n        indices.remove(idx)\n        return np.random.choice(indices, 3, replace=False)\n\n    def mutate_and_crossover(self, target_idx, func):\n        a, b, c = self.select_parents(target_idx)\n        mutant_vector = self.population[a] + self.adaptive_mutation_factor() * (self.population[b] - self.population[c])\n        mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n        trial_vector = np.copy(self.population[target_idx])\n        crossover_points = np.random.rand(self.dim) < self.crossover_rate\n        trial_vector[crossover_points] = mutant_vector[crossover_points]\n        return trial_vector\n\n    def optimize(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        self.current_evaluations += self.population_size\n        for _ in range(self.budget - self.current_evaluations):\n            for i in range(self.population_size):\n                trial_vector = self.mutate_and_crossover(i, func)\n                trial_fitness = func(trial_vector)\n                self.current_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial_vector\n                    fitness[i] = trial_fitness\n            self.performance_memory.append(np.min(fitness))\n        return self.population[np.argmin(fitness)]\n\n    def __call__(self, func):\n        return self.optimize(func)\n\n# Example usage:\n# optimizer = HybridAdaptiveDifferentialEvolution(budget=1000, dim=10)\n# best_solution = optimizer(some_black_box_function)", "name": "HybridAdaptiveDifferentialEvolution", "description": "Enhance exploration by updating the mutation factor based on diversity within the population.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'fitness' is not defined\").", "error": "NameError(\"name 'fitness' is not defined\")", "parent_id": "e9a64d65-3b46-42ff-a566-eb038f7f0f98", "metadata": {}, "mutation_prompt": null}
{"id": "dda42859-57d7-4dbe-82d3-96e91b02a51f", "solution": "import numpy as np\n\nclass HybridAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.7\n        self.population = np.random.rand(self.population_size, dim)\n        self.performance_memory = []\n        self.current_evaluations = 0\n\n    def adaptive_mutation_factor(self):\n        if len(self.performance_memory) < 2:\n            return self.mutation_factor\n        return 0.5 + 0.1 * np.sign(self.performance_memory[-1] - self.performance_memory[-2])\n\n    def adaptive_crossover_rate(self):\n        if len(self.performance_memory) < 2:\n            return self.crossover_rate\n        return 0.7 + 0.1 * np.sign(self.performance_memory[-1] - self.performance_memory[-2])\n\n    def select_parents(self, idx):\n        indices = list(range(self.population_size))\n        indices.remove(idx)\n        return np.random.choice(indices, 3, replace=False)\n\n    def mutate_and_crossover(self, target_idx, func):\n        a, b, c = self.select_parents(target_idx)\n        mutant_vector = self.population[a] + self.adaptive_mutation_factor() * (self.population[b] - self.population[c])\n        mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n        trial_vector = np.copy(self.population[target_idx])\n        crossover_points = np.random.rand(self.dim) < self.adaptive_crossover_rate()\n        trial_vector[crossover_points] = mutant_vector[crossover_points]\n        return trial_vector\n\n    def optimize(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        self.current_evaluations += self.population_size\n        for _ in range(self.budget - self.current_evaluations):\n            for i in range(self.population_size):\n                trial_vector = self.mutate_and_crossover(i, func)\n                trial_fitness = func(trial_vector)\n                self.current_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial_vector\n                    fitness[i] = trial_fitness\n            self.performance_memory.append(np.min(fitness))\n        return self.population[np.argmin(fitness)]\n\n    def __call__(self, func):\n        return self.optimize(func)\n\n# Example usage:\n# optimizer = HybridAdaptiveDifferentialEvolution(budget=1000, dim=10)\n# best_solution = optimizer(some_black_box_function)", "name": "HybridAdaptiveDifferentialEvolution", "description": "Enhanced exploration by incorporating a dynamic crossover rate based on recent performance trends in Differential Evolution.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'fitness' is not defined\").", "error": "NameError(\"name 'fitness' is not defined\")", "parent_id": "e9a64d65-3b46-42ff-a566-eb038f7f0f98", "metadata": {}, "mutation_prompt": null}
{"id": "44363e1c-a320-4e44-b9d5-754764c4cc7e", "solution": "import numpy as np\n\nclass HybridAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.7\n        self.population = np.random.rand(self.population_size, dim)\n        self.performance_memory = []\n        self.current_evaluations = 0\n\n    def adaptive_mutation_factor(self):\n        if len(self.performance_memory) < 2:\n            return self.mutation_factor\n        return 0.5 + 0.1 * np.sign(self.performance_memory[-1] - self.performance_memory[-2])\n\n    def adaptive_crossover_rate(self):\n        if len(self.performance_memory) < 2:\n            return self.crossover_rate\n        return max(0.1, min(0.9, self.crossover_rate + 0.1 * np.sign(self.performance_memory[-1] - self.performance_memory[-2])))\n\n    def select_parents(self, idx):\n        indices = list(range(self.population_size))\n        indices.remove(idx)\n        return np.random.choice(indices, 3, replace=False)\n\n    def mutate_and_crossover(self, target_idx, func):\n        a, b, c = self.select_parents(target_idx)\n        mutant_vector = self.population[a] + self.adaptive_mutation_factor() * (self.population[b] - self.population[c])\n        mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n        trial_vector = np.copy(self.population[target_idx])\n        crossover_points = np.random.rand(self.dim) < self.adaptive_crossover_rate()\n        trial_vector[crossover_points] = mutant_vector[crossover_points]\n        return trial_vector\n\n    def optimize(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        self.current_evaluations += self.population_size\n        for _ in range(self.budget - self.current_evaluations):\n            for i in range(self.population_size):\n                trial_vector = self.mutate_and_crossover(i, func)\n                trial_fitness = func(trial_vector)\n                self.current_evaluations += 1\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial_vector\n                    fitness[i] = trial_fitness\n            self.performance_memory.append(np.min(fitness))\n        return self.population[np.argmin(fitness)]\n\n    def __call__(self, func):\n        return self.optimize(func)", "name": "HybridAdaptiveDifferentialEvolution", "description": "Enhance exploration by dynamically adjusting the crossover rate based on recent performance trends.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'fitness' is not defined\").", "error": "NameError(\"name 'fitness' is not defined\")", "parent_id": "e9a64d65-3b46-42ff-a566-eb038f7f0f98", "metadata": {}, "mutation_prompt": null}
{"id": "f112554f-1289-4f0b-9b32-1604119bc4ee", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.w = 0.9  # Initial inertia weight, modified\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.9  # Cooling rate for SA\n        self.w_min = 0.4  # Minimum inertia weight, added\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = self.w_min + (0.9 - self.w_min) * (self.budget - eval_count) / self.budget  # Dynamic inertia weight\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 10\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhanced HybridPSO_SA by introducing a dynamic inertia weight strategy to improve exploration and convergence.", "configspace": "", "generation": 1, "fitness": 0.8429500170147873, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.031. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "bc6d9795-97d7-4a14-86a9-e9757aadc8c5", "metadata": {"aucs": [0.8286637671892929, 0.8146154474829818, 0.885570836372087], "final_y": [0.1322428084062851, 0.1425126786196571, 0.12055202166455103]}, "mutation_prompt": null}
{"id": "8045e128-81eb-4941-9864-a91c51b431a4", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.9  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = 0.9 - 0.5 * (eval_count / self.budget)  # Adaptive inertia weight\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 10\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "A refined hybrid algorithm enhancing PSO-SA by incorporating adaptive inertia weight for improved convergence.", "configspace": "", "generation": 1, "fitness": 0.840491318807858, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.024. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "bc6d9795-97d7-4a14-86a9-e9757aadc8c5", "metadata": {"aucs": [0.8091906805432179, 0.8669903143669846, 0.8452929615133714], "final_y": [0.14697698109909418, 0.11711169321739467, 0.11843052834477485]}, "mutation_prompt": null}
{"id": "9c2eb0b4-c225-4b14-8407-f490aaffcd80", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "An enhanced hybrid algorithm utilizing PSO and SA with adaptive velocity scaling and a refined temperature schedule for improved convergence in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": 0.8588723725367219, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.014. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "bc6d9795-97d7-4a14-86a9-e9757aadc8c5", "metadata": {"aucs": [0.8785435199992817, 0.8455928496085502, 0.8524807480023335], "final_y": [0.11770871754396506, 0.1300904105032763, 0.12650770113921572]}, "mutation_prompt": null}
{"id": "2d96099b-2aab-4faf-af34-dcc4cffeec00", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.w_max = 0.9  # Max inertia weight\n        self.w_min = 0.4  # Min inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-4  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            w = self.w_max - (self.w_max - self.w_min) * eval_count / self.budget  # Dynamic inertia\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 10\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhanced Hybrid Particle Swarm Optimization and Simulated Annealing with Dynamic Inertia and Adaptive Cooling.", "configspace": "", "generation": 1, "fitness": 0.8437522281936515, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.021. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "bc6d9795-97d7-4a14-86a9-e9757aadc8c5", "metadata": {"aucs": [0.8147173867514511, 0.8590601808284934, 0.8574791170010096], "final_y": [0.13343394027037148, 0.11809654209070752, 0.12040491172211787]}, "mutation_prompt": null}
{"id": "d636347a-080c-40ef-b151-9d4864b26532", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, dim)  # Adjust particle count based on dimension\n        self.w_max = 0.9  # Max inertia weight\n        self.w_min = 0.4  # Min inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-4  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            w = self.w_max - (self.w_max - self.w_min) * eval_count / self.budget  # Dynamic inertia\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 10\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Hybrid PSO-SA with Adaptive Particle Count dynamically adjusts the number of particles to optimize both exploration and exploitation.", "configspace": "", "generation": 2, "fitness": 0.8739831426465902, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.022. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "2d96099b-2aab-4faf-af34-dcc4cffeec00", "metadata": {"aucs": [0.8451355852059101, 0.8795846077831594, 0.8972292349507007], "final_y": [0.12850240955924863, 0.12816036626404914, 0.12143975746670976]}, "mutation_prompt": null}
{"id": "4bf597b5-9088-4ade-a258-d182405caad2", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.w_max = 0.9  # Max inertia weight\n        self.w_min = 0.4  # Min inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-4  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            w = self.w_max - (self.w_max - self.w_min) * eval_count / self.budget  # Dynamic inertia\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Opposite Learning Strategy\n            OX = lb + ub - X\n            OX = np.clip(OX, lb, ub)\n            OX_values = np.array([func(x) for x in OX])\n            for i in range(self.num_particles):\n                if OX_values[i] < P_best_values[i]:\n                    P_best[i] = OX[i]\n                    P_best_values[i] = OX_values[i]\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 10\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhanced Hybrid PSO and SA with Opposite Learning Strategy and Stochastic Tuning for Improved Exploration.", "configspace": "", "generation": 2, "fitness": 0.8652541559612029, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.865 with standard deviation 0.050. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2d96099b-2aab-4faf-af34-dcc4cffeec00", "metadata": {"aucs": [0.794485334922586, 0.9024312663334461, 0.8988458666275768], "final_y": [0.1356754699244016, 0.11868126170677995, 0.11833000004558791]}, "mutation_prompt": null}
{"id": "1d74c84a-2d0e-4314-a033-547ee1163f97", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.7  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Refinement of HybridPSO_SA by optimizing the cognitive coefficient for enhanced convergence.", "configspace": "", "generation": 2, "fitness": 0.8722959261842659, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.017. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "9c2eb0b4-c225-4b14-8407-f490aaffcd80", "metadata": {"aucs": [0.8540964157740225, 0.8677430493542322, 0.8950483134245428], "final_y": [0.13415441798105854, 0.1278082071535802, 0.11912101425450239]}, "mutation_prompt": null}
{"id": "d4397f6d-2570-47ba-9db7-58665e151bbf", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Dynamic inertia weight adjustment\n            self.w = 0.9 - 0.4 * (eval_count / self.budget)\n            \n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce a dynamic inertia weight adjustment based on the evaluation count to improve convergence speed and solution quality.", "configspace": "", "generation": 2, "fitness": 0.8940737177539487, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.894 with standard deviation 0.002. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "9c2eb0b4-c225-4b14-8407-f490aaffcd80", "metadata": {"aucs": [0.8925716294279502, 0.8925194042939991, 0.8971301195398971], "final_y": [0.11645419514049749, 0.12101454150619917, 0.11716795692829218]}, "mutation_prompt": null}
{"id": "8c225fe2-4a80-4991-aa97-c2253062d169", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhanced PSO-SA algorithm with dynamic particle count for improved convergence.", "configspace": "", "generation": 2, "fitness": 0.8946151937289449, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.895 with standard deviation 0.012. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "9c2eb0b4-c225-4b14-8407-f490aaffcd80", "metadata": {"aucs": [0.9058006323366573, 0.877241883848633, 0.9008030650015446], "final_y": [0.11622323649467137, 0.12195510588165182, 0.1144590853431352]}, "mutation_prompt": null}
{"id": "25f78514-fb40-4524-83bb-bcf9f2d7f436", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.w_max = 0.9  # Max inertia weight\n        self.w_min = 0.4  # Min inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-4  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            w = self.w_max - (self.w_max - self.w_min) * eval_count / self.budget  # Dynamic inertia\n            self.c1 = 2.0 - 1.5 * (eval_count / self.budget)  # Adaptive cognitive coefficient\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 10\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhanced hybrid optimization using PSO and SA with adaptive cognitive coefficient scaling for improved exploration-exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.8715363525282346, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.024. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "2d96099b-2aab-4faf-af34-dcc4cffeec00", "metadata": {"aucs": [0.8450473514759707, 0.8670419583389233, 0.9025197477698099], "final_y": [0.12565718412157012, 0.1294068187091768, 0.11650687127550163]}, "mutation_prompt": null}
{"id": "f971ed83-17a2-4427-9431-91b657435532", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-4\n        self.alpha = 0.95\n        self.min_particles = 5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            w = self.w_max - (self.w_max - self.w_min) * eval_count / self.budget\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, (ub-lb)/20, self.dim) * (ub - lb) / 10\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n            if eval_count % 100 == 0 and self.num_particles > self.min_particles:\n                self.num_particles -= 1\n                X = X[:self.num_particles]\n                V = V[:self.num_particles]\n                P_best = P_best[:self.num_particles]\n                P_best_values = P_best_values[:self.num_particles]\n\n        return G_best", "name": "HybridPSO_SA", "description": "A refined hybrid PSO and SA algorithm integrating adaptive swarm size and adaptive local search for enhanced convergence.", "configspace": "", "generation": 2, "fitness": 0.8494460207595846, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.058. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "2d96099b-2aab-4faf-af34-dcc4cffeec00", "metadata": {"aucs": [0.7669201711934035, 0.8876550122570321, 0.8937628788283182], "final_y": [0.1490427166338978, 0.1185812792945381, 0.11855677457294334]}, "mutation_prompt": null}
{"id": "3d7346f4-1564-4ecf-84b8-d61fcad0a388", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(20, 5 + dim)  # Adaptive based on dimensionality\n        self.w_max = 0.9  # Max inertia weight\n        self.w_min = 0.4  # Min inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-4  # Minimum temperature for SA\n        self.alpha = 0.9  # Cooling rate for SA, adjusted for exploration\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            w = self.w_max - (self.w_max - self.w_min) * eval_count / self.budget  # Dynamic inertia\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 10\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Improved Hybrid PSO-SA with adaptive particle count and enhanced exploration-exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.8786990624161978, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.879 with standard deviation 0.036. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "2d96099b-2aab-4faf-af34-dcc4cffeec00", "metadata": {"aucs": [0.8299901661226661, 0.8890671224819271, 0.9170398986440002], "final_y": [0.1236021579437594, 0.12216993670277987, 0.11604307266874148]}, "mutation_prompt": null}
{"id": "a730e643-1939-4df7-a3b7-dd073e6e3015", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.w_max = 0.9  # Max inertia weight\n        self.w_min = 0.4  # Min inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-4  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            w = self.w_max - (self.w_max - self.w_min) * eval_count / self.budget  # Dynamic inertia\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 10\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha * (1 + np.sin(eval_count/self.budget*np.pi)))\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introducing a dynamic adaptive cooling rate to enhance exploration and exploitation balance in hybrid PSO-SA.", "configspace": "", "generation": 2, "fitness": 0.8616146800283556, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.862 with standard deviation 0.033. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "2d96099b-2aab-4faf-af34-dcc4cffeec00", "metadata": {"aucs": [0.8224721135100449, 0.8598560104825894, 0.9025159160924324], "final_y": [0.13715332655698398, 0.1194899664249589, 0.11684334360463966]}, "mutation_prompt": null}
{"id": "f1fdf4a8-5ae0-402e-b2bd-f1070a32c130", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(10, dim * 2)  # Adaptive swarm size\n        self.w_max = 0.9  # Max inertia weight\n        self.w_min = 0.4  # Min inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-4  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            w = self.w_max - (self.w_max - self.w_min) * eval_count / self.budget\n            r1 = np.random.rand(self.num_particles, self.dim)\n            r2 = np.random.rand(self.num_particles, self.dim)\n            # Adaptive cognitive and social coefficients\n            c1_t = self.c1 * (T / self.T_init)\n            c2_t = self.c2 * (1 - T / self.T_init)\n            V = w * V + c1_t * r1 * (P_best - X) + c2_t * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 10\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhanced Hybrid PSO and SA with Adaptive Swarm Size and Temperature-Dependent Coefficients for Improved Local Search.", "configspace": "", "generation": 2, "fitness": 0.882946313801621, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.883 with standard deviation 0.021. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "2d96099b-2aab-4faf-af34-dcc4cffeec00", "metadata": {"aucs": [0.8543248700653295, 0.8911397320042438, 0.9033743393352897], "final_y": [0.11893000075255855, 0.12085355077808091, 0.11866570942404031]}, "mutation_prompt": null}
{"id": "31bc3016-f8ff-4e45-b17b-b71b6846c0c2", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)  # Add chaotic map for diversity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))  # Adaptive velocity scaling\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity  # Apply adaptive scaling\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i]  # Use chaotic map factor\n                new_value = func(new_pos * chaotic_factor)  # Apply chaotic influence\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce adaptive velocity scaling and chaotic maps to enhance exploration and convergence efficiency.", "configspace": "", "generation": 3, "fitness": 0.8761709189570507, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.017. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "8c225fe2-4a80-4991-aa97-c2253062d169", "metadata": {"aucs": [0.8530529118110987, 0.8914580947558228, 0.8840017503042307], "final_y": [0.1261829846950574, 0.12316579198622635, 0.12062999554872333]}, "mutation_prompt": null}
{"id": "0a44a4b2-be9a-46dd-a045-e12e55cecb4c", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.99  # Increased cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Adaptive learning coefficients\n            self.c1 = 1.5 + 0.5 * (eval_count / self.budget)\n            self.c2 = 1.5 - 0.5 * (eval_count / self.budget)\n            \n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhanced hybrid PSO-SA algorithm with adaptive learning coefficients and cooling schedule for improved performance.", "configspace": "", "generation": 3, "fitness": 0.8456052859231346, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.846 with standard deviation 0.013. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "8c225fe2-4a80-4991-aa97-c2253062d169", "metadata": {"aucs": [0.836283192385107, 0.8644067786254035, 0.8361258867588932], "final_y": [0.1361211163355952, 0.12696613528397072, 0.1266023682142523]}, "mutation_prompt": null}
{"id": "6e2d1298-cb2a-4a5e-a552-40f9da45a196", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = 0.9 - 0.5 * (eval_count / self.budget)  # Adjust inertia weight dynamically\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Optimized inertia weight adjustment based on budget utilization for enhanced convergence speed.", "configspace": "", "generation": 3, "fitness": 0.8642588338668071, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.014. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "8c225fe2-4a80-4991-aa97-c2253062d169", "metadata": {"aucs": [0.8454461273051084, 0.8685189074194113, 0.878811466875902], "final_y": [0.13372075050232335, 0.12269337179847184, 0.1215147373920763]}, "mutation_prompt": null}
{"id": "cc447a8d-bda2-49e9-8615-2dd50d7563f0", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * (self.alpha + 0.05))  # Increased alpha by 0.05\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhanced PSO-SA algorithm with temperature-adaptive cooling for improved exploratory capabilities.", "configspace": "", "generation": 3, "fitness": 0.8599230831473044, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.009. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "8c225fe2-4a80-4991-aa97-c2253062d169", "metadata": {"aucs": [0.8524313092143528, 0.8718230619060863, 0.8555148783214743], "final_y": [0.1316155139375692, 0.1244762119496523, 0.12637061751073775]}, "mutation_prompt": null}
{"id": "6f685202-4038-4016-8a44-45a491a630fd", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.c1 = 1.5 * (1 - eval_count / self.budget)  # Adaptive cognitive coefficient\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introducing adaptive cognitive and social coefficients in PSO for improved convergence.", "configspace": "", "generation": 3, "fitness": 0.8554271757188706, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.001. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "8c225fe2-4a80-4991-aa97-c2253062d169", "metadata": {"aucs": [0.8562388919202948, 0.8536882007645374, 0.8563544344717797], "final_y": [0.1315430280637998, 0.1295927696853557, 0.1219200622932155]}, "mutation_prompt": null}
{"id": "4d0ed727-351b-4021-93b2-227b96e4cfaf", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 20  # Changed from 15 to 20 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            dynamic_w = self.w * (1 - eval_count / self.budget)  # Added dynamic inertia weight\n            V = dynamic_w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Improved exploration capability using adaptive velocity control and dynamic particle count adjustment.", "configspace": "", "generation": 3, "fitness": 0.8600128317624365, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.023. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "8c225fe2-4a80-4991-aa97-c2253062d169", "metadata": {"aucs": [0.8290068985997592, 0.8654738725108605, 0.8855577241766898], "final_y": [0.13825096271725523, 0.11858121379306186, 0.1240348960126274]}, "mutation_prompt": null}
{"id": "64e62290-8c09-4a0e-b8bb-02e6d5fd9f81", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Adaptive inertia weight adjustment\n            self.w = 0.9 - 0.5 * (eval_count / self.budget)\n\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introducing adaptive inertia weight adjustment based on the evaluation count to enhance convergence dynamics in PSO.", "configspace": "", "generation": 3, "fitness": 0.8709449422533723, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.021. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "8c225fe2-4a80-4991-aa97-c2253062d169", "metadata": {"aucs": [0.8419857456070192, 0.8881224000362816, 0.8827266811168156], "final_y": [0.13869922740684482, 0.12047319441723392, 0.117919348117412]}, "mutation_prompt": null}
{"id": "20d589f8-59b0-4e12-9581-22ac7195a908", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Dynamic inertia weight adjustment\n            self.w = 0.9 - 0.4 * (eval_count / self.budget)\n            # Dynamic social coefficient adjustment\n            self.c2 = 1.5 + 0.5 * (1 - eval_count / self.budget)\n            \n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce a dynamic social coefficient to enhance swarm adaptability and solution quality.", "configspace": "", "generation": 3, "fitness": 0.8703964764298614, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.020. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "d4397f6d-2570-47ba-9db7-58665e151bbf", "metadata": {"aucs": [0.8520722016474866, 0.8610889540738342, 0.8980282735682634], "final_y": [0.12484236288799688, 0.12722418862018314, 0.11806096444420988]}, "mutation_prompt": null}
{"id": "b578048c-8e5e-4c66-be77-e13c4fbf368d", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2_initial = 1.0  # Social coefficient initial value\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Dynamic adjustment of social coefficient\n            self.c2 = self.c2_initial + (1.5 - self.c2_initial) * (1 - eval_count / self.budget)\n\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduced a dynamic adjustment of the social coefficient to balance exploration and exploitation.", "configspace": "", "generation": 3, "fitness": 0.8474660838716349, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.847 with standard deviation 0.012. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "8c225fe2-4a80-4991-aa97-c2253062d169", "metadata": {"aucs": [0.8635969695381209, 0.8437001089271128, 0.8351011731496712], "final_y": [0.1325644371802177, 0.12206151351763217, 0.13452218240755498]}, "mutation_prompt": null}
{"id": "a794deb4-c835-4963-8884-99affa82e4ca", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.97  # Cooling rate for SA changed from 0.95 to 0.97\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Improve convergence by adapting the cooling rate of Simulated Annealing.", "configspace": "", "generation": 3, "fitness": 0.8637375945594693, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.018. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "8c225fe2-4a80-4991-aa97-c2253062d169", "metadata": {"aucs": [0.838602135700936, 0.8803668809454678, 0.872243767032004], "final_y": [0.13738408572573935, 0.12114898371774596, 0.12018185258152381]}, "mutation_prompt": null}
{"id": "1d7de8b9-1c1d-44dd-8541-97f3788dadd0", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Adaptive inertia weight adjustment\n            self.w = 0.9 - 0.5 * (eval_count / self.budget)\n\n            # Calculate diversity\n            diversity = np.mean(np.std(X, axis=0))\n            # Dynamic cognitive and social coefficients\n            self.c1 = 1.5 + 0.5 * diversity\n            self.c2 = 1.5 - 0.5 * diversity\n\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Incorporate dynamic learning rate adaptation based on particle diversity to enhance exploration and exploitation balance in HybridPSO_SA.", "configspace": "", "generation": 4, "fitness": 0.8087570714807178, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.090. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.043.", "error": "", "parent_id": "64e62290-8c09-4a0e-b8bb-02e6d5fd9f81", "metadata": {"aucs": [0.6821785166397535, 0.8846905002134341, 0.8594021975889661], "final_y": [0.2070058463964779, 0.11732729850706325, 0.11591013143445883]}, "mutation_prompt": null}
{"id": "6e761edf-77bc-47e5-965f-2246cb35aece", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)  # Add chaotic map for diversity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))  # Adaptive velocity scaling\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity  # Apply adaptive scaling\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] * (1 + 0.1 * np.sin(np.pi * eval_count / self.budget))  # Dynamic chaotic factor\n                new_value = func(new_pos * chaotic_factor)  # Apply chaotic influence\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhance exploration by incorporating a dynamic chaotic factor scaling based on the iteration count to improve solution diversity.", "configspace": "", "generation": 4, "fitness": 0.847509241054648, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.055. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "31bc3016-f8ff-4e45-b17b-b71b6846c0c2", "metadata": {"aucs": [0.7740215945619604, 0.8607441746771208, 0.907761953924863], "final_y": [0.1505831865823638, 0.12775148101013323, 0.11300298405833331]}, "mutation_prompt": null}
{"id": "ea67d0e2-9337-4656-b630-b1098851c903", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n\n    def levy_flight(self, lam=1.5):\n        u = np.random.normal(0, 0.1, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / np.abs(v)**(1/lam)\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Adaptive inertia weight adjustment\n            self.w = 0.9 - 0.5 * (eval_count / self.budget)\n\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.c1 = 2.5 - 1.5 * (eval_count / self.budget)  # Adaptive adjustment\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n            \n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + self.levy_flight() * (ub - lb) / 20  # Integrated Levy flight\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Integrate Levy flights and adaptive learning rates into PSO for enhanced exploration and exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.8668589968209063, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.867 with standard deviation 0.017. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "64e62290-8c09-4a0e-b8bb-02e6d5fd9f81", "metadata": {"aucs": [0.8847443727001846, 0.8434840221007349, 0.8723485956617992], "final_y": [0.11474225812463423, 0.12422452466085177, 0.11436216403082433]}, "mutation_prompt": null}
{"id": "3cef4ce6-3550-4455-9c26-24963f602a3e", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.sin(np.arange(self.num_particles))  # Use sinusoidal chaotic map\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))  # Adaptive velocity scaling\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity  # Apply adaptive scaling\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i]  # Use chaotic map factor\n                new_value = func(new_pos * chaotic_factor)  # Apply chaotic influence\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Leverage a sinusoidal chaotic map to enhance diversity and convergence stability.", "configspace": "", "generation": 4, "fitness": 0.8602163186953523, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.032. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "31bc3016-f8ff-4e45-b17b-b71b6846c0c2", "metadata": {"aucs": [0.8155181198206064, 0.8751604280593142, 0.8899704082061364], "final_y": [0.1511455691091248, 0.1230424381419628, 0.11517282474381252]}, "mutation_prompt": null}
{"id": "029c39a5-b2a0-4dd5-85d7-42d62bebcb9f", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Adaptive inertia weight adjustment\n            self.w = 0.9 - 0.5 * (eval_count / self.budget)\n\n            # Adaptive cognitive and social coefficients\n            self.c1 = 1.0 + 0.5 * (eval_count / self.budget)\n            self.c2 = 2.0 - 0.5 * (eval_count / self.budget)\n\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhance PSO dynamic range by introducing adaptive cognitive and social coefficients based on the evaluation count.", "configspace": "", "generation": 4, "fitness": 0.8618108226162305, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.862 with standard deviation 0.014. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "64e62290-8c09-4a0e-b8bb-02e6d5fd9f81", "metadata": {"aucs": [0.8521316706037187, 0.8516259117247549, 0.881674885520218], "final_y": [0.12411752653337971, 0.12493605307478006, 0.11972569271367361]}, "mutation_prompt": null}
{"id": "8bb61dc4-2e61-488f-a902-1a134e918767", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1.0, 1.0, (self.num_particles, self.dim))  # Change made here\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Adaptive inertia weight adjustment\n            self.w = 0.9 - 0.5 * (eval_count / self.budget)\n\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhance exploration by increasing the velocity update range in PSO for better convergence.", "configspace": "", "generation": 4, "fitness": 0.8692817498175852, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.869 with standard deviation 0.026. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "64e62290-8c09-4a0e-b8bb-02e6d5fd9f81", "metadata": {"aucs": [0.8363913582849969, 0.8991183125105728, 0.8723355786571856], "final_y": [0.1352937831565486, 0.11816368902442298, 0.1146752331321147]}, "mutation_prompt": null}
{"id": "786136e9-3737-48e1-a7e8-3e1346912d71", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            self.w = 0.9 - 0.5 * (eval_count / self.budget)\n\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_pos += np.random.normal(0, 0.1, self.dim) * (ub - lb)  # Hybrid mutation strategy\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Integrate a hybrid mutation strategy in PSO to enhance exploration and escape local optima.", "configspace": "", "generation": 4, "fitness": 0.8530630540679148, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.853 with standard deviation 0.032. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "64e62290-8c09-4a0e-b8bb-02e6d5fd9f81", "metadata": {"aucs": [0.8102353881685143, 0.8873210201114652, 0.8616327539237651], "final_y": [0.14364812120759152, 0.11683482272611301, 0.12300973920861369]}, "mutation_prompt": null}
{"id": "ad8bf308-b549-48af-9977-1c50f754f9b5", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)  # Add chaotic map for diversity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))  # Adaptive velocity scaling\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity  # Apply adaptive scaling\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))  # Adaptive chaotic factor\n                new_value = func(new_pos * chaotic_factor)  # Apply chaotic influence\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce adaptive chaotic factor scaling to enhance exploration dynamics and convergence efficiency.", "configspace": "", "generation": 4, "fitness": 0.8712506936098574, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.029. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "31bc3016-f8ff-4e45-b17b-b71b6846c0c2", "metadata": {"aucs": [0.8408146238266939, 0.8631445021131603, 0.909792954889718], "final_y": [0.1348682331848552, 0.12249900444539152, 0.11400985025344401]}, "mutation_prompt": null}
{"id": "12e05b95-f7e7-4bfe-821b-c119741ee619", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n        chaotic_seq = np.random.rand(self.num_particles)  # Chaotic sequence initialization\n\n        while eval_count < self.budget:\n            # Adaptive inertia weight adjustment\n            self.w = 0.9 - 0.5 * (eval_count / self.budget)\n\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                chaotic_seq[i] = 4 * chaotic_seq[i] * (1 - chaotic_seq[i])  # Logistic map for chaos\n                new_pos = X[i] + chaotic_seq[i] * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce a chaotic search mechanism to escape local optima and enhance exploration during optimization.", "configspace": "", "generation": 4, "fitness": 0.8636015540751302, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.024. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "64e62290-8c09-4a0e-b8bb-02e6d5fd9f81", "metadata": {"aucs": [0.8305507555051047, 0.8719130679417519, 0.8883408387785339], "final_y": [0.13085145950720767, 0.11591769453928746, 0.11731025326233002]}, "mutation_prompt": null}
{"id": "0ca02355-c26a-47be-87da-2ce5e38ba141", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Adaptive inertia weight adjustment\n            self.w = 0.9 - 0.5 * (eval_count / self.budget)\n\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Differential Evolution Mutation\n            for i in range(self.num_particles):\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n                mutant_vector = P_best[indices[0]] + 0.5 * (P_best[indices[1]] - P_best[indices[2]])\n                X[i] = np.clip(mutant_vector, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhanced exploitation by incorporating differential evolution mutation in PSO.", "configspace": "", "generation": 4, "fitness": 0.8556223879955023, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.028. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "64e62290-8c09-4a0e-b8bb-02e6d5fd9f81", "metadata": {"aucs": [0.8156037612281988, 0.8784892092795555, 0.8727741934787526], "final_y": [0.13500435144362288, 0.11823972081682343, 0.11914500793856553]}, "mutation_prompt": null}
{"id": "48433926-71e0-41f7-b97e-09f730653e5c", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w_max = 0.9  # Changed fixed inertia to dynamic\n        self.w_min = 0.4  # New minimum inertia weight\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            w = self.w_max - (self.w_max - self.w_min) * (eval_count / self.budget)  # Dynamic inertia weight\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce a dynamically adjusted inertia weight to balance exploration and exploitation more effectively in PSO.", "configspace": "", "generation": 5, "fitness": 0.8187670572264004, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.819 with standard deviation 0.017. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "ad8bf308-b549-48af-9977-1c50f754f9b5", "metadata": {"aucs": [0.8032561161103594, 0.8104787686434703, 0.8425662869253716], "final_y": [0.14196676750288306, 0.13875154449775318, 0.1225306110075074]}, "mutation_prompt": null}
{"id": "3d3e84b4-899e-41bc-bd6f-f8608ae397fb", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1.0, 1.0, (self.num_particles, self.dim))  # Change made here\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Adaptive inertia weight adjustment\n            self.w = 0.9 - 0.5 * (eval_count / self.budget)\n\n            # Dynamic neighborhood topology\n            neighbors = np.random.choice(range(self.num_particles), (self.num_particles, 3), replace=True)\n\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            for i in range(self.num_particles):\n                best_neighbor_value = min(P_best_values[neighbors[i]])\n                best_neighbor = P_best[neighbors[i][np.argmin(P_best_values[neighbors[i]])]]\n                V[i] = self.w * V[i] + self.c1 * r1[i] * (P_best[i] - X[i]) + self.c2 * r2[i] * (best_neighbor - X[i])\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Integrate dynamic neighborhood topology in PSO to enhance information sharing and convergence efficiency.", "configspace": "", "generation": 5, "fitness": 0.8291304320687747, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.829 with standard deviation 0.004. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "8bb61dc4-2e61-488f-a902-1a134e918767", "metadata": {"aucs": [0.8298276222182723, 0.8235122125182751, 0.8340514614697769], "final_y": [0.13085628883303857, 0.1385676690988532, 0.13990348010436682]}, "mutation_prompt": null}
{"id": "af9ca57b-134b-4978-9b88-5329d81f482b", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            w_decay = self.w * (1 - eval_count / self.budget)  # Line 1 change: Introduce weight decay\n            V = w_decay * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)  # Line 2 change: Use w_decay\n            V *= adaptive_velocity\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce adaptive weight decay combined with chaotic factor scaling to enhance exploration and exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.8336072887908689, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.834 with standard deviation 0.013. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "ad8bf308-b549-48af-9977-1c50f754f9b5", "metadata": {"aucs": [0.8199641583060597, 0.8504865631250416, 0.8303711449415055], "final_y": [0.14098840008097324, 0.1329424541127191, 0.13459309574026623]}, "mutation_prompt": null}
{"id": "57b72d22-b71b-442e-9cdb-45d28a0601d1", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = 0.9 - eval_count / self.budget * 0.5  # Self-adaptive inertia weight\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce self-adaptive inertia and adaptive mutation with chaotic factor to enhance convergence and diversity.", "configspace": "", "generation": 5, "fitness": 0.8547318534493428, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.025. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "ad8bf308-b549-48af-9977-1c50f754f9b5", "metadata": {"aucs": [0.8223590504809604, 0.8576051265672158, 0.8842313832998522], "final_y": [0.13265039659114963, 0.12795823253460747, 0.12334637787993896]}, "mutation_prompt": null}
{"id": "ca31a55a-2dcf-40e6-91ca-ab125e617a9c", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)  # Add chaotic map for diversity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = 0.4 + 0.5 * np.random.rand()  # Stochastic inertia weight\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.cos(np.pi * eval_count / self.budget))  # Adjust chaotic factor\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Improve the balance between exploration and exploitation by introducing stochastic inertia weight and adaptive chaotic factor update.", "configspace": "", "generation": 5, "fitness": 0.8401374499391557, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.009. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "ad8bf308-b549-48af-9977-1c50f754f9b5", "metadata": {"aucs": [0.8418286371262819, 0.8278158594001181, 0.8507678532910674], "final_y": [0.1354571846538073, 0.14125801103561275, 0.12721595910806394]}, "mutation_prompt": null}
{"id": "4125d909-3f31-4c71-8256-6d9db980a184", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))  # Dynamic inertia\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity \n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos += np.random.standard_cauchy(self.dim) * (ub - lb) / 100  # Lvy flight\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce a dynamic inertia weight and Lvy flight for improved exploration and convergence.", "configspace": "", "generation": 5, "fitness": 0.8515379646505151, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.008. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "ad8bf308-b549-48af-9977-1c50f754f9b5", "metadata": {"aucs": [0.8609504830545033, 0.8421989709679913, 0.8514644399290512], "final_y": [0.13101368507122824, 0.125244123932993, 0.1259387767501644]}, "mutation_prompt": null}
{"id": "5e0a8310-f4b3-4c5c-ba97-cbb46ececa6e", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)  # Add chaotic map for diversity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))  # Adaptive velocity scaling\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity  # Apply adaptive scaling\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))  # Adaptive chaotic factor\n                new_value = func(new_pos * chaotic_factor)  # Apply chaotic influence\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * (self.alpha + 0.005))  # Slightly adjust cooling rate\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhance the algorithm by refining the velocity update for improved convergence.", "configspace": "", "generation": 5, "fitness": 0.8456702091938638, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.846 with standard deviation 0.023. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "ad8bf308-b549-48af-9977-1c50f754f9b5", "metadata": {"aucs": [0.8150930800641898, 0.8689074587292884, 0.8530100887881134], "final_y": [0.1441898781778873, 0.12651337119591133, 0.1340877479530922]}, "mutation_prompt": null}
{"id": "b904b18a-04dc-490d-9f1b-838e4d8aeb96", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-2.0, 2.0, (self.num_particles, self.dim))  # Change made here\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Adaptive inertia weight adjustment\n            self.w = 0.9 - 0.5 * (eval_count / self.budget)\n\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Improve exploration by modifying the velocity boundary initialization to increase variance.", "configspace": "", "generation": 5, "fitness": 0.8418993973995604, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.842 with standard deviation 0.011. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "8bb61dc4-2e61-488f-a902-1a134e918767", "metadata": {"aucs": [0.847131572865983, 0.8262854300777239, 0.8522811892549742], "final_y": [0.12413783426484115, 0.13691412919989943, 0.12401301424488498]}, "mutation_prompt": null}
{"id": "abcfcbf9-d1a8-4d66-b2d2-8ec331ad5666", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1.0, 1.0, (self.num_particles, self.dim))  # Change made here\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Adaptive inertia weight adjustment\n            self.w = 0.9 - 0.5 * (eval_count / self.budget)\n\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.c2 = 2.0 - 1.5 * (eval_count / self.budget)  # Change made here\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V = np.clip(V, -0.5, 0.5)  # Change made here\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce velocity clamping and a dynamic social coefficient to enhance convergence and stability.", "configspace": "", "generation": 5, "fitness": 0.806148641421017, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.038. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "8bb61dc4-2e61-488f-a902-1a134e918767", "metadata": {"aucs": [0.7532853017145209, 0.8367407344201758, 0.8284198881283544], "final_y": [0.16854462350431654, 0.1354788249335961, 0.13699110659446367]}, "mutation_prompt": null}
{"id": "c9093a52-2d3b-40b3-94c1-7daff895d27f", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15  # Changed from 10 to 15 particles\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.T_init = 1.0  # Initial temperature for SA\n        self.T_min = 1e-3  # Minimum temperature for SA\n        self.alpha = 0.95  # Cooling rate for SA\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-1.0, 1.0, (self.num_particles, self.dim))  # Change made here\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n        \n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            # Adaptive inertia weight adjustment\n            self.w = 0.9 - 0.5 * (eval_count / self.budget)\n\n            # Particle Swarm Optimization (PSO) step\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            V = self.w * 0.99 * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)  # Modified line\n            X = np.clip(X + V, lb, ub)\n\n            # Evaluate new positions\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            # Update personal and global bests\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            # Simulated Annealing (SA) step\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                new_value = func(new_pos)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            # Cooling schedule for SA\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Integrate an inertia weight decay factor for improved convergence dynamics.", "configspace": "", "generation": 5, "fitness": 0.8462148161110195, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.846 with standard deviation 0.008. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "8bb61dc4-2e61-488f-a902-1a134e918767", "metadata": {"aucs": [0.8457774914737635, 0.8360569439709538, 0.8568100128883412], "final_y": [0.1306007569315233, 0.1305778883740748, 0.12786620012513872]}, "mutation_prompt": null}
{"id": "6e6e8e06-9b15-4d0a-a173-0a9e6f48d837", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = 0.9 - eval_count / self.budget * 0.5  # Self-adaptive inertia weight\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            dynamic_c1 = self.c1 * (1 - eval_count / self.budget)  # Dynamic learning factor\n            V = self.w * V + dynamic_c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                self.chaotic_map[i] = 4 * chaotic_factor * (1 - chaotic_factor)  # Stochastic update\n                new_value = func(new_pos * self.chaotic_map[i])\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce a dynamic learning factor and stochastic chaotic map update to balance exploration and exploitation.", "configspace": "", "generation": 6, "fitness": 0.8268711753341629, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.025. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "57b72d22-b71b-442e-9cdb-45d28a0601d1", "metadata": {"aucs": [0.8566677444098936, 0.796599226205425, 0.82734655538717], "final_y": [0.1370903536311633, 0.15308861102462923, 0.13718486697419563]}, "mutation_prompt": null}
{"id": "e5e95c9c-9552-4ffa-961b-afa6218294b9", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))  # Dynamic inertia\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity \n            V += 0.1 * V**2  # Quadratic velocity term\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos += np.random.standard_cauchy(self.dim) * (ub - lb) / 100  # Lvy flight\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = 1 / (1 + np.exp(-self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))))  # Sigmoid-based chaotic factor\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Integrate a quadratic velocity term and a sigmoid-based chaotic factor for enhanced convergence.", "configspace": "", "generation": 6, "fitness": 0.8082052322828123, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.016. And the mean value of best solutions found was 0.150 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "4125d909-3f31-4c71-8256-6d9db980a184", "metadata": {"aucs": [0.7990046364976957, 0.8301394544279213, 0.7954716059228201], "final_y": [0.1580696287185014, 0.14683141103429997, 0.1455671504626097]}, "mutation_prompt": null}
{"id": "bea8ce7f-ce12-47d0-8b2b-c494ac3bac0d", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))  # Dynamic inertia\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity \n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i] or (current_values[i] == P_best_values[i] and np.random.rand() > 0.5):  # Stochastic tiebreaker\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos += np.random.standard_cauchy(self.dim) * (ub - lb) / 100  # Lvy flight\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce a stochastic tiebreaker to enhance diversification and escape local optima more effectively.", "configspace": "", "generation": 6, "fitness": 0.8221827461333332, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.007. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "4125d909-3f31-4c71-8256-6d9db980a184", "metadata": {"aucs": [0.8295438943385534, 0.8248827730126508, 0.8121215710487955], "final_y": [0.13205882446096262, 0.14830297918459912, 0.13197195554311414]}, "mutation_prompt": null}
{"id": "6702f5b8-53f2-4fe9-bbb9-e216b1458ba5", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.chaotic_map = 3.9 * self.chaotic_map * (1 - self.chaotic_map)  # Logistic map for chaos\n            self.w = 0.9 - eval_count / self.budget * 0.5 * self.chaotic_map  # Chaotic inertia weight\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = self.w[:, None] * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce a chaotic inertia weight based on logistic map to enhance exploration and convergence.", "configspace": "", "generation": 6, "fitness": 0.8180993221442976, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.030. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "57b72d22-b71b-442e-9cdb-45d28a0601d1", "metadata": {"aucs": [0.7894792390707714, 0.804613951856745, 0.8602047755053764], "final_y": [0.1399207792764391, 0.1560504279266024, 0.13020355774655135]}, "mutation_prompt": null}
{"id": "247362dc-1c3b-492f-802f-af752ea79481", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = 0.9 - eval_count / self.budget * 0.5  # Self-adaptive inertia weight\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, (1 - np.linalg.norm(G_best - X[i]) / np.linalg.norm(ub - lb)), self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce adaptive local search intensity by scaling mutation based on proximity to global best.", "configspace": "", "generation": 6, "fitness": 0.8276043054885239, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.005. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "57b72d22-b71b-442e-9cdb-45d28a0601d1", "metadata": {"aucs": [0.8294375816970139, 0.8324326270619182, 0.8209427077066396], "final_y": [0.13595331630421792, 0.14536245231780842, 0.1358838102720712]}, "mutation_prompt": null}
{"id": "ab9bf893-c11d-4ba5-aa9a-17cf0043ab62", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles) * 4 + 0.5  # Adjusted chaotic map initialization\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = 0.9 - eval_count / self.budget * 0.5  # Self-adaptive inertia weight\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                scale_factor = (1 - eval_count / self.budget) * (ub - lb) / 15\n                new_pos = X[i] + np.random.normal(0, scale_factor, self.dim)\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] * np.sin(np.pi * eval_count / self.budget)\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhance global exploration with adaptive chaotic maps and a time-varying Gaussian mutation strategy.", "configspace": "", "generation": 6, "fitness": 0.8030779313227092, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.023. And the mean value of best solutions found was 0.150 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "57b72d22-b71b-442e-9cdb-45d28a0601d1", "metadata": {"aucs": [0.776259304372269, 0.7996102312711894, 0.8333642583246693], "final_y": [0.15808631905617043, 0.15797283571439957, 0.1339768200364596]}, "mutation_prompt": null}
{"id": "fc7c94b0-ca71-4a6e-ad43-bc2f4274a76d", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))  # Dynamic inertia\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos += np.random.standard_cauchy(self.dim) * (ub - lb) / 100  # Lvy flight\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.2 * np.sin(np.pi * eval_count / self.budget))  # Updated chaotic factor\n                new_value = func(new_pos * (0.5 + 0.5 * chaotic_factor))  # Weighted local search \n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhance exploration with time-varying chaotic factor and weighted local search for improved convergence.", "configspace": "", "generation": 6, "fitness": 0.8023466416153181, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.028. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "4125d909-3f31-4c71-8256-6d9db980a184", "metadata": {"aucs": [0.7719429415353488, 0.8389156231763883, 0.7961813601342169], "final_y": [0.1563179013919751, 0.1441148069674304, 0.1404891194288571]}, "mutation_prompt": null}
{"id": "2f3a9691-6075-4540-aa40-28d4946838c4", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))  # Dynamic inertia\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget)) * (1 + 0.5 * np.sin(2 * np.pi * eval_count / self.budget))  # Time-varying chaotic factor\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity \n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos += np.random.standard_cauchy(self.dim) * (ub - lb) / 100  # Lvy flight\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce a time-varying chaotic factor to enhance the adaptive velocity for improved exploration and convergence.", "configspace": "", "generation": 6, "fitness": 0.8398623215030657, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.024. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "4125d909-3f31-4c71-8256-6d9db980a184", "metadata": {"aucs": [0.8734462881557581, 0.8234572674354976, 0.8226834089179417], "final_y": [0.12916843828666214, 0.14866619737026143, 0.13242917776464624]}, "mutation_prompt": null}
{"id": "f504072b-127f-48ef-9e00-c75c1311ef0c", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))  # Dynamic inertia\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity \n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos += np.random.standard_cauchy(self.dim) * (ub - lb) / 100  # Lvy flight\n                new_pos = np.clip(new_pos, lb, ub)\n                # Changed line below\n                self.chaotic_map[i] = np.mod(4 * self.chaotic_map[i] * (1 - self.chaotic_map[i]), 1)  # Logistic map\n                new_value = func(new_pos * self.chaotic_map[i]) \n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce a dynamic chaotic factor to enhance search space exploration and local optima avoidance.", "configspace": "", "generation": 6, "fitness": 0.8043805832152963, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.021. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4125d909-3f31-4c71-8256-6d9db980a184", "metadata": {"aucs": [0.7957672463525077, 0.8334228504932181, 0.7839516528001633], "final_y": [0.1455030293513061, 0.14601424612557146, 0.14661003284322538]}, "mutation_prompt": null}
{"id": "6dd528da-53f7-4d99-ad0d-588f90f7c04e", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_max = 2.5  # Adaptive learning rates\n        self.c1_min = 1.0\n        self.c2_max = 2.5\n        self.c2_min = 1.0\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))  # Dynamic inertia\n            c1 = self.c1_max - ((self.c1_max - self.c1_min) * (eval_count / self.budget))  # Adaptive c1\n            c2 = self.c2_max - ((self.c2_max - self.c2_min) * (eval_count / self.budget))  # Adaptive c2\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = w * V + c1 * r1 * (P_best - X) + c2 * r2 * (G_best - X)\n            V *= adaptive_velocity \n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos += np.random.standard_cauchy(self.dim) * (ub - lb) / 100  # Lvy flight\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = np.tanh(self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget)))  # Dynamic chaotic map\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhance convergence using adaptive learning rates for cognitive and social components, and incorporate dynamic chaotic maps for diversity.", "configspace": "", "generation": 6, "fitness": 0.7955056539855953, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.796 with standard deviation 0.014. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "4125d909-3f31-4c71-8256-6d9db980a184", "metadata": {"aucs": [0.8119102810352357, 0.795939082411226, 0.778667598510324], "final_y": [0.13740645568535348, 0.14794230991773827, 0.14645100271793698]}, "mutation_prompt": null}
{"id": "4742edc6-7fe4-4003-ad16-f7db0dae02a0", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))  # Dynamic inertia\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget)) * (1 + 0.5 * np.sin(2 * np.pi * eval_count / self.budget))  # Time-varying chaotic factor\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity \n            V += np.random.normal(0, 0.1, V.shape)  # Introduce stochastic disturbance\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos += np.random.standard_cauchy(self.dim) * (ub - lb) / 100  # Lvy flight\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce a small stochastic disturbance in particle velocity to escape local optima.", "configspace": "", "generation": 7, "fitness": 0.8055219746432746, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.014. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "2f3a9691-6075-4540-aa40-28d4946838c4", "metadata": {"aucs": [0.824754810050861, 0.7987085668808455, 0.7931025469981173], "final_y": [0.14226680338597175, 0.15160716171470567, 0.14491873249397158]}, "mutation_prompt": null}
{"id": "8d8241b7-0c31-4951-a58e-ceda4bbd0fdf", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n        self.learning_rate = 0.01  # New learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))  # Dynamic inertia\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget)) * (1 + 0.5 * np.sin(2 * np.pi * eval_count / self.budget))  # Time-varying chaotic factor\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity * self.learning_rate  # Apply learning rate\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                new_pos += np.random.standard_cauchy(self.dim) * (ub - lb) / 100  # Lvy flight\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            if np.random.rand() < 0.1:  # Stochastic tournament selection\n                candidate_index = np.random.choice(self.num_particles)\n                if current_values[candidate_index] < G_best_value:\n                    G_best = X[candidate_index]\n                    G_best_value = current_values[candidate_index]\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Utilize adaptive learning rates and a stochastic tournament selection to enhance exploration while maintaining convergence stability.", "configspace": "", "generation": 7, "fitness": 0.8062366067498964, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.004. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "2f3a9691-6075-4540-aa40-28d4946838c4", "metadata": {"aucs": [0.805391054219864, 0.8118073248844109, 0.8015114411454145], "final_y": [0.15088044623114283, 0.1484506753669267, 0.13793829829788962]}, "mutation_prompt": null}
{"id": "c2b130a4-9bbd-433d-9a5f-942c9db482b1", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = 0.9 - eval_count / self.budget * 0.5  # Self-adaptive inertia weight\n            adaptive_velocity = 1 + 0.5 * np.sin(np.pi * eval_count / self.budget)  # Adjusted scaling factor\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, (1 - np.linalg.norm(G_best - X[i]) / np.linalg.norm(ub - lb)), self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce adaptive velocity scaling factor based on chaotic dynamics to enhance exploration and convergence in particle swarm.", "configspace": "", "generation": 7, "fitness": 0.8436136652963885, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.030. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "247362dc-1c3b-492f-802f-af752ea79481", "metadata": {"aucs": [0.8060915697594863, 0.8792646451735544, 0.8454847809561248], "final_y": [0.13800821138295194, 0.12985699952410412, 0.12791596022642104]}, "mutation_prompt": null}
{"id": "4586215d-5647-44d4-bf2b-27d675fda772", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = 0.9 - eval_count / self.budget * 0.5  # Self-adaptive inertia weight\n            dynamic_social_learning = np.cos(np.pi * eval_count / self.budget)\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X) * dynamic_social_learning\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, (1 - np.linalg.norm(G_best - X[i]) / np.linalg.norm(ub - lb)), self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.cos(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhance exploration and convergence by introducing a dynamic social learning factor and adaptive chaotic mapping.", "configspace": "", "generation": 7, "fitness": 0.8355696055045195, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.836 with standard deviation 0.011. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "247362dc-1c3b-492f-802f-af752ea79481", "metadata": {"aucs": [0.8261911350297415, 0.8514688734857674, 0.8290488079980498], "final_y": [0.14118758880027638, 0.1397754364562448, 0.14172003099633723]}, "mutation_prompt": null}
{"id": "d8dd8787-ab4a-4fcf-9532-065b8f771ca4", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = 0.9 - eval_count / self.budget * 0.5  # Self-adaptive inertia weight\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, (1 - np.linalg.norm(G_best - X[i]) / np.linalg.norm(ub - lb)), self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(2 * np.pi * eval_count / self.budget))  # Modified chaotic factor\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce a dynamic chaotic factor that evolves non-linearly over time to enhance exploration and exploitation balance.", "configspace": "", "generation": 7, "fitness": 0.8285709604750898, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.829 with standard deviation 0.026. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "247362dc-1c3b-492f-802f-af752ea79481", "metadata": {"aucs": [0.8149835859947646, 0.8059832897127437, 0.8647460057177611], "final_y": [0.14626186345070247, 0.1556638774009178, 0.13229981788054657]}, "mutation_prompt": null}
{"id": "f3ea14f6-b476-4328-b18c-c94750a68599", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = 0.9 - eval_count / self.budget * 0.5  # Self-adaptive inertia weight\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            self.c1 = 1.5 + 0.5 * (eval_count / self.budget)  # Adaptive learning rate\n            self.c2 = 1.5 + 0.5 * (1 - eval_count / self.budget)  # Adaptive learning rate\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity\n            V += 0.01 * np.sin(2 * np.pi * eval_count / self.budget)  # Chaotic velocity update\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, (1 - np.linalg.norm(G_best - X[i]) / np.linalg.norm(ub - lb)), self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Enhance convergence by introducing adaptive learning rates and adding a new chaotic update mechanism to particle velocities.", "configspace": "", "generation": 7, "fitness": 0.8142649930690794, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.814 with standard deviation 0.024. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "247362dc-1c3b-492f-802f-af752ea79481", "metadata": {"aucs": [0.8474033241661495, 0.7940834636388622, 0.8013081914022266], "final_y": [0.13990533681973572, 0.16012373284543724, 0.1412450855784837]}, "mutation_prompt": null}
{"id": "b02713e5-fe51-49a1-acc1-0e0d7903b515", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))  # Dynamic inertia\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget)) * (1 + 0.5 * np.sin(2 * np.pi * eval_count / self.budget))  # Time-varying chaotic factor\n            V = w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity \n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, 1, self.dim) * (ub - lb) / 20\n                dynamic_levy_factor = 1 + np.sin(np.pi * eval_count / self.budget)  # Dynamic scaling factor\n                new_pos += np.random.standard_cauchy(self.dim) * (ub - lb) / 100 * dynamic_levy_factor  # Lvy flight\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce a dynamic scaling factor to the Lvy flight step to enhance diverse search capabilities.", "configspace": "", "generation": 7, "fitness": 0.808001888506725, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.013. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "2f3a9691-6075-4540-aa40-28d4946838c4", "metadata": {"aucs": [0.8219462355986316, 0.8119914284628794, 0.7900680014586638], "final_y": [0.1350729157403764, 0.1503746098465374, 0.14147496607718024]}, "mutation_prompt": null}
{"id": "24bc7ff8-f7de-46f3-a912-94d5d613f56b", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = 0.9 - eval_count / self.budget * 0.5  # Self-adaptive inertia weight\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, (1 - np.linalg.norm(G_best - X[i]) / np.linalg.norm(ub - lb)), self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] * (1 + 0.1 * np.cos(np.pi * eval_count / self.budget)) # Dynamic scaling\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce dynamic scaling of chaotic factors for enhanced global search capabilities.", "configspace": "", "generation": 7, "fitness": 0.8336206508935055, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.834 with standard deviation 0.004. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "247362dc-1c3b-492f-802f-af752ea79481", "metadata": {"aucs": [0.8338631681342237, 0.8384000470581775, 0.8285987374881152], "final_y": [0.13754324114630256, 0.14084123268386117, 0.13617850461233938]}, "mutation_prompt": null}
{"id": "9bf14faa-3b15-4876-9c34-8b7997b63880", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = 0.9 - eval_count / self.budget * 0.5  # Self-adaptive inertia weight\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, (1 - np.linalg.norm(G_best - X[i]) / np.linalg.norm(ub - lb)), self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                dynamic_factor = 1 + (0.5 * (P_best_values[i] - G_best_value) / (max(P_best_values) - min(P_best_values)))\n                new_value = func(new_pos * chaotic_factor * dynamic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Incorporate an adaptive exploration mechanism using a dynamic attraction-repulsion factor based on particle performance.", "configspace": "", "generation": 7, "fitness": 0.8310668561975495, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.831 with standard deviation 0.025. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "247362dc-1c3b-492f-802f-af752ea79481", "metadata": {"aucs": [0.8496008553277637, 0.7963189971386189, 0.8472807161262657], "final_y": [0.13278888265206013, 0.1410657202917006, 0.13388531840780282]}, "mutation_prompt": null}
{"id": "5058ac09-107c-4e60-97a6-5057f8e855e5", "solution": "import numpy as np\n\nclass HybridPSO_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = 15\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.T_init = 1.0\n        self.T_min = 1e-3\n        self.alpha = 0.95\n        self.chaotic_map = np.random.rand(self.num_particles)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        X = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        V = np.random.uniform(-0.5, 0.5, (self.num_particles, self.dim))\n        P_best = X.copy()\n        P_best_values = np.array([func(x) for x in P_best])\n        G_best = P_best[np.argmin(P_best_values)]\n        G_best_value = min(P_best_values)\n\n        eval_count = self.num_particles\n        T = self.T_init\n\n        while eval_count < self.budget:\n            r1, r2 = np.random.rand(self.num_particles, self.dim), np.random.rand(self.num_particles, self.dim)\n            self.w = 0.9 - eval_count / self.budget * 0.5  # Self-adaptive inertia weight\n            adaptive_velocity = 1 + (np.sin(np.pi * eval_count / self.budget))\n            V = self.w * V + self.c1 * r1 * (P_best - X) + self.c2 * r2 * (G_best - X)\n            V *= adaptive_velocity\n            X = np.clip(X + V, lb, ub)\n\n            current_values = np.array([func(x) for x in X])\n            eval_count += self.num_particles\n\n            for i in range(self.num_particles):\n                if current_values[i] < P_best_values[i]:\n                    P_best[i] = X[i]\n                    P_best_values[i] = current_values[i]\n                    if P_best_values[i] < G_best_value:\n                        G_best = P_best[i]\n                        G_best_value = P_best_values[i]\n\n            for i in range(self.num_particles):\n                new_pos = X[i] + np.random.normal(0, (1 - np.linalg.norm(G_best - X[i]) / np.linalg.norm(ub - lb)), self.dim) * (ub - lb) / 20\n                new_pos = np.clip(new_pos, lb, ub)\n                chaotic_factor = self.chaotic_map[i] + (0.1 * np.sin(np.pi * eval_count / self.budget))\n                new_value = func(new_pos * chaotic_factor)\n                eval_count += 1\n                if new_value < current_values[i] or np.random.rand() < np.exp((current_values[i] - new_value) / T):\n                    X[i] = new_pos\n                    current_values[i] = new_value\n                    if new_value < P_best_values[i]:\n                        P_best[i] = new_pos\n                        P_best_values[i] = new_value\n                        if new_value < G_best_value:\n                            G_best = new_pos\n                            G_best_value = new_value\n\n            T = max(self.T_min, T * self.alpha)\n\n        return G_best", "name": "HybridPSO_SA", "description": "Introduce a dynamic particle count adjustment mechanism to improve exploration and exploitation balance.", "configspace": "", "generation": 7, "fitness": 0.8374647122320514, "feedback": "The algorithm HybridPSO_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.024. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "247362dc-1c3b-492f-802f-af752ea79481", "metadata": {"aucs": [0.8552875160304564, 0.8535263964812585, 0.8035802241844398], "final_y": [0.13161140950305317, 0.13905106896118125, 0.15393955688737027]}, "mutation_prompt": null}
