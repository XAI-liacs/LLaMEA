{"id": "817ec622-6752-4957-9a42-495225e8c04f", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.5   # Inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic combining differential evolution and a swarm-based strategy for diverse exploration and exploitation in black box optimization problems.", "configspace": "", "generation": 0, "fitness": 0.807939505663703, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.017. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": null, "metadata": {"aucs": [0.7843981372874468, 0.8188228860279289, 0.8205974936757332], "final_y": [0.14854888364773733, 0.1383770052937483, 0.13773485275207953]}, "mutation_prompt": null}
{"id": "e6d19b01-8b12-4e54-8028-1c35ac05b0ab", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9   # Inertia weight (changed from 0.5 to 0.9 for better exploration)\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Refined hybrid metaheuristic with enhanced exploration using adaptive inertia weight in PSO component.", "configspace": "", "generation": 1, "fitness": 0.800201941678408, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.021. And the mean value of best solutions found was 0.150 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "817ec622-6752-4957-9a42-495225e8c04f", "metadata": {"aucs": [0.7709561772806419, 0.8098829983963345, 0.8197666493582476], "final_y": [0.16077055289026576, 0.15188136038248456, 0.13837358140277745]}, "mutation_prompt": null}
{"id": "e4f3eeed-d02f-419f-8c23-c27b1b179dd3", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9   # Inertia weight, changed for adaptive strategy\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive inertia weight to improve convergence in black box optimization.", "configspace": "", "generation": 1, "fitness": 0.8191532866670133, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.819 with standard deviation 0.019. And the mean value of best solutions found was 0.143 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "817ec622-6752-4957-9a42-495225e8c04f", "metadata": {"aucs": [0.8410819405292691, 0.8211025250952038, 0.7952753943765668], "final_y": [0.13680523610720952, 0.13804956441631377, 0.1540101028751294]}, "mutation_prompt": null}
{"id": "c070bbda-d0be-4cca-a120-e8d4becd851f", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.5   # Inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced exploration via adaptive differential weight adjustment in hybrid metaheuristics.", "configspace": "", "generation": 1, "fitness": 0.821989202771562, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.017. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "817ec622-6752-4957-9a42-495225e8c04f", "metadata": {"aucs": [0.841219884729548, 0.8002412670566968, 0.824506456528441], "final_y": [0.122078367919642, 0.1460660834764328, 0.14215079645910023]}, "mutation_prompt": null}
{"id": "78dafeca-74ef-46af-ade4-67015c374f0b", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.5   # Inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update with adaptive inertia weight\n            w = 0.9 - 0.5 * (evaluations / self.budget)  # Adjusting inertia weight dynamically\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic that incorporates adaptive parameters for dynamic balance between exploration and exploitation in black box optimization.", "configspace": "", "generation": 1, "fitness": 0.8133295403161095, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.008. And the mean value of best solutions found was 0.143 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "817ec622-6752-4957-9a42-495225e8c04f", "metadata": {"aucs": [0.8107197562881628, 0.804994373962305, 0.8242744906978605], "final_y": [0.1393951266637311, 0.14683040627083366, 0.14153109282871468]}, "mutation_prompt": null}
{"id": "a002f071-777f-49f3-aa49-f8c1a38d37ef", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w_initial = 0.9  # Initial inertia weight\n        w_final = 0.4    # Final inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            # Dynamically adjust the inertia weight\n            w = w_initial - (w_initial - w_final) * (evaluations / self.budget)\n            \n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced exploration of the search space by adjusting the inertia weight dynamically for better convergence.", "configspace": "", "generation": 1, "fitness": 0.8085583279822931, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.022. And the mean value of best solutions found was 0.143 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "817ec622-6752-4957-9a42-495225e8c04f", "metadata": {"aucs": [0.7936817486594956, 0.8399821192645464, 0.7920111160228374], "final_y": [0.13883311236548046, 0.1376839769902748, 0.15215168935716816]}, "mutation_prompt": null}
{"id": "92a98997-7469-47ec-95cc-d7a14da2a5a4", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.5   # Inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced adaptive crossover probability to enhance solution diversity and convergence speed in hybrid metaheuristics.", "configspace": "", "generation": 2, "fitness": 0.8326009286118902, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.833 with standard deviation 0.012. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "c070bbda-d0be-4cca-a120-e8d4becd851f", "metadata": {"aucs": [0.8410921247982955, 0.8417064668575927, 0.8150041941797822], "final_y": [0.12190515372417765, 0.12729675859528244, 0.13744045826861873]}, "mutation_prompt": null}
{"id": "51e62b0a-03ff-4814-b43b-57ba924aaa49", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w_max = 0.9   # Max inertia weight\n        w_min = 0.4   # Min inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = w_max - ((w_max - w_min) * evaluations / self.budget)  # Adaptive inertia weight\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Improved adaptive control of the inertia weight in the PSO component for better balance between exploration and exploitation.", "configspace": "", "generation": 2, "fitness": 0.8145006904241705, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.006. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "c070bbda-d0be-4cca-a120-e8d4becd851f", "metadata": {"aucs": [0.805727036285366, 0.8173019720125035, 0.8204730629746421], "final_y": [0.1403032093268991, 0.12724666817708252, 0.12360246194115487]}, "mutation_prompt": null}
{"id": "7776493a-6075-43a9-a898-7538edcd1323", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w_initial = 0.9\n        w_final = 0.4\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Dynamic inertia weight calculation\n            w = w_final + (w_initial - w_final) * ((self.budget - evaluations) / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduce dynamic inertia weight to improve convergence and exploration balance in PSO.", "configspace": "", "generation": 2, "fitness": 0.817406880164094, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.006. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "c070bbda-d0be-4cca-a120-e8d4becd851f", "metadata": {"aucs": [0.8093869421410244, 0.8226198108081264, 0.8202138875431314], "final_y": [0.13093881262411045, 0.1363515040764297, 0.13075208172657204]}, "mutation_prompt": null}
{"id": "b80f06db-2e01-4d91-af35-60c06a47cec3", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.5   # Inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = np.std(population) / (func.bounds.ub - func.bounds.lb)  # Change 1 line, dynamic CR\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Fine-tune the crossover rate dynamically based on population diversity to improve convergence in hybrid metaheuristics.", "configspace": "", "generation": 2, "fitness": 0.7983737288591053, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.020. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "c070bbda-d0be-4cca-a120-e8d4becd851f", "metadata": {"aucs": [0.7704032927531161, 0.8136056724827356, 0.8111122213414643], "final_y": [0.15825940978819553, 0.13728862715411505, 0.13558883663552357]}, "mutation_prompt": null}
{"id": "7b4799ea-a974-412d-8861-f1198c5374c4", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.5   # Inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update with local search integration\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population + np.random.normal(0, 0.05, population.shape), func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhance exploration via adaptive strategy mixing and local search integration in hybrid metaheuristics.", "configspace": "", "generation": 2, "fitness": 0.8145321650099566, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.023. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "c070bbda-d0be-4cca-a120-e8d4becd851f", "metadata": {"aucs": [0.8329004408150948, 0.829244533853804, 0.7814515203609707], "final_y": [0.12576120662118684, 0.13521118962650625, 0.14504547478576446]}, "mutation_prompt": null}
{"id": "3aafef99-b0a8-407f-9e2b-ee202ba7b4f4", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9  # Adaptive inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced adaptive inertia weight and dynamic population sizing to enhance exploration and convergence balance.", "configspace": "", "generation": 3, "fitness": 0.8263978245283691, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.030. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "92a98997-7469-47ec-95cc-d7a14da2a5a4", "metadata": {"aucs": [0.7836645776301939, 0.8511509351061399, 0.8443779608487733], "final_y": [0.1554869631011454, 0.125016780979044, 0.13327046192793035]}, "mutation_prompt": null}
{"id": "ceaae020-b364-4b05-9b82-9c734387dc82", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w_max = 0.9  # Changed: Maximum inertia weight\n        w_min = 0.4  # Changed: Minimum inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            w = w_max - ((w_max - w_min) * (evaluations / self.budget))  # Changed: Dynamic adaptive inertia weight\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced dynamic adaptive inertia weight in Particle Swarm Optimization for improved balance between exploration and exploitation.", "configspace": "", "generation": 3, "fitness": 0.8080260492878525, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.013. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "92a98997-7469-47ec-95cc-d7a14da2a5a4", "metadata": {"aucs": [0.7930197864067356, 0.8070727931386678, 0.8239855683181542], "final_y": [0.14056647717383286, 0.13894793649577775, 0.14302765308968368]}, "mutation_prompt": null}
{"id": "dc5479c4-b829-4385-923c-43686726f8c5", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9   # Adjusted inertia weight for convergence\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                F = 0.5 + (0.9 - 0.5) * (self.budget - evaluations) / self.budget  # Dynamic mutation factor\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced the adaptive strategy by dynamically adjusting the inertia weight and mutation factor for better convergence.  ", "configspace": "", "generation": 3, "fitness": 0.8234296454251995, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.823 with standard deviation 0.024. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "92a98997-7469-47ec-95cc-d7a14da2a5a4", "metadata": {"aucs": [0.7903151562737034, 0.8433848562415801, 0.8365889237603148], "final_y": [0.1529245903769847, 0.12584828263856718, 0.1343922517269278]}, "mutation_prompt": null}
{"id": "5334e6b7-c216-4ca2-889e-9a4b1dd088e8", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.5   # Inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                delta = np.random.uniform(-0.1, 0.1, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < p_best_fitness[i]:\n                        p_best[i] = new_solution\n                        p_best_fitness[i] = new_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced local search step after PSO update to refine individual solutions and enhance convergence precision.", "configspace": "", "generation": 3, "fitness": 0.8425691779753013, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.032. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "92a98997-7469-47ec-95cc-d7a14da2a5a4", "metadata": {"aucs": [0.8847013931763789, 0.8077753572326052, 0.8352307835169199], "final_y": [0.11827600109714242, 0.13875395288449166, 0.1346056382593389]}, "mutation_prompt": null}
{"id": "ccdc0448-4b4f-455d-b1f5-511f4d063079", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w_max = 0.9  # Max inertia weight\n        w_min = 0.4  # Min inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            w = w_max - (w_max - w_min) * (evaluations / self.budget)  # Dynamic inertia weight\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Implemented dynamic adjustment of the inertia weight w in PSO to enhance exploration and exploitation balance.", "configspace": "", "generation": 3, "fitness": 0.8301780941920006, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.017. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "92a98997-7469-47ec-95cc-d7a14da2a5a4", "metadata": {"aucs": [0.8431072561747135, 0.8067906030582799, 0.8406364233430085], "final_y": [0.12256903354382831, 0.15111124446752966, 0.13024867188607803]}, "mutation_prompt": null}
{"id": "29688b62-1272-4d09-a7f2-5fb783602060", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.5   # Inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step with gradient-based refinement\n            for i in range(pop_size):\n                gradient = np.gradient(func(population[i]))\n                new_solution = np.clip(population[i] - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < p_best_fitness[i]:\n                        p_best[i] = new_solution\n                        p_best_fitness[i] = new_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced local search by incorporating a gradient-based approach for more efficient exploitation of the search space.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"can't multiply sequence by non-int of type 'float'\").", "error": "TypeError(\"can't multiply sequence by non-int of type 'float'\")", "parent_id": "5334e6b7-c216-4ca2-889e-9a4b1dd088e8", "metadata": {}, "mutation_prompt": null}
{"id": "21637aeb-54eb-4c74-83f1-55c7155cff96", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.5   # Inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                delta = np.random.uniform(-0.05, 0.05, self.dim)  # Reduced range for delta\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < p_best_fitness[i]:\n                        p_best[i] = new_solution\n                        p_best_fitness[i] = new_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced local search precision by reducing delta range for finer exploration.", "configspace": "", "generation": 4, "fitness": 0.8202374093388479, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.820 with standard deviation 0.038. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "5334e6b7-c216-4ca2-889e-9a4b1dd088e8", "metadata": {"aucs": [0.7670077021361263, 0.850875687561817, 0.8428288383186008], "final_y": [0.16838033114331152, 0.12905481276592512, 0.13677035632814605]}, "mutation_prompt": null}
{"id": "4dd1184d-ed86-4e76-85c5-203843efaf76", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9   # Initial inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = 0.4 + 0.5 * (self.budget - evaluations) / self.budget  # Adaptive inertia weight\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                delta = np.random.uniform(-0.1, 0.1, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < p_best_fitness[i]:\n                        p_best[i] = new_solution\n                        p_best_fitness[i] = new_fitness\n                        \n                if evaluations >= self.budget:\n                    break\n\n            # Elitism: Retain best solution\n            if g_best_fitness < np.min(fitness):\n                worst_idx = np.argmax(fitness)\n                population[worst_idx] = g_best\n                fitness[worst_idx] = g_best_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced adaptive inertia weight update and employed elitism to retain the best solutions across generations.", "configspace": "", "generation": 4, "fitness": 0.8129276810615927, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.010. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "5334e6b7-c216-4ca2-889e-9a4b1dd088e8", "metadata": {"aucs": [0.800581472533856, 0.8131524794454568, 0.8250490912054658], "final_y": [0.15173893001508276, 0.12929480573284624, 0.14261064600284978]}, "mutation_prompt": null}
{"id": "52d696e2-430e-4533-9c61-8d93fb908ada", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.5   # Inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                delta = np.random.uniform(-0.05, 0.05, self.dim)  # Modified delta range\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < p_best_fitness[i]:\n                        p_best[i] = new_solution\n                        p_best_fitness[i] = new_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Improved the local search step by refining the delta range to increase precision in the exploration of the solution space.", "configspace": "", "generation": 4, "fitness": 0.8269714714334571, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.013. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "5334e6b7-c216-4ca2-889e-9a4b1dd088e8", "metadata": {"aucs": [0.8089071504910613, 0.8377771718577709, 0.8342300919515391], "final_y": [0.14297005336971225, 0.12890321829023144, 0.13247355132003835]}, "mutation_prompt": null}
{"id": "c41b49f6-2e4c-4fb3-acb8-5f92f52acc30", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.5   # Inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced the local search step by introducing adaptive step size and integrating a gradient-based refinement to improve convergence.", "configspace": "", "generation": 4, "fitness": 0.8412954816096828, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.841 with standard deviation 0.029. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "5334e6b7-c216-4ca2-889e-9a4b1dd088e8", "metadata": {"aucs": [0.8799357406835789, 0.8117260281318401, 0.8322246760136294], "final_y": [0.11777205359472132, 0.13411970387756633, 0.1344525233354179]}, "mutation_prompt": null}
{"id": "4bf2297e-f008-45cf-a597-430debc8293a", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.5   # Inertia weight\n        momentum = 0.1  # Momentum factor for velocity update\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = momentum * velocity + w * (velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population))\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced a momentum factor in the velocity update of PSO to enhance exploration capabilities.", "configspace": "", "generation": 5, "fitness": 0.8113387981584612, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.014. And the mean value of best solutions found was 0.143 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "c41b49f6-2e4c-4fb3-acb8-5f92f52acc30", "metadata": {"aucs": [0.7951186506528141, 0.80874081075768, 0.8301569330648897], "final_y": [0.15302513112698546, 0.1393808273587155, 0.13514770176119684]}, "mutation_prompt": null}
{"id": "3b227dcf-873f-4660-9502-8602f66e6430", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.5   # Inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = 0.9 - 0.5 * (evaluations / self.budget)  # Dynamic inertia weight\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Implement a dynamic inertia weight update in the PSO component for improved convergence control.", "configspace": "", "generation": 5, "fitness": 0.8062420882526063, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.014. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "c41b49f6-2e4c-4fb3-acb8-5f92f52acc30", "metadata": {"aucs": [0.7961874727976417, 0.7960227229092862, 0.8265160690508914], "final_y": [0.1469640761553187, 0.13030501431249497, 0.13926596726846519]}, "mutation_prompt": null}
{"id": "30e815b4-8be7-44c0-9915-93673b47b74d", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w_max = 0.9  # Maximum inertia weight\n        w_min = 0.4  # Minimum inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            w = w_max - (w_max - w_min) * (evaluations / self.budget)  # Dynamic inertia weight\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Improved balance of exploration and exploitation by dynamically adjusting inertia in the PSO component.", "configspace": "", "generation": 5, "fitness": 0.797278165526995, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.017. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "c41b49f6-2e4c-4fb3-acb8-5f92f52acc30", "metadata": {"aucs": [0.7942096685608792, 0.777987350888237, 0.8196374771318689], "final_y": [0.1386539007472689, 0.14416688784477782, 0.1430962783270876]}, "mutation_prompt": null}
{"id": "a1516582-ef58-4dc7-8a79-89bb75318b11", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9   # Adaptive inertia weight (changed line)\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced adaptive inertia weight in PSO update to improve exploration-exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.8106000889793931, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.011. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "c41b49f6-2e4c-4fb3-acb8-5f92f52acc30", "metadata": {"aucs": [0.8068914107013484, 0.7999585181523887, 0.8249503380844421], "final_y": [0.14631112760379483, 0.14832153648093316, 0.1459343669642963]}, "mutation_prompt": null}
{"id": "9892d6ca-74c4-4c8b-80a2-86fe649631de", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced a dynamic adjustment for the inertia weight `w` in PSO to enhance convergence speed and solution quality.", "configspace": "", "generation": 5, "fitness": 0.8178482758181774, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.015. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "c41b49f6-2e4c-4fb3-acb8-5f92f52acc30", "metadata": {"aucs": [0.798210912478105, 0.8199349515229714, 0.835398963453456], "final_y": [0.1531690911950392, 0.12890438722329356, 0.1408993848803014]}, "mutation_prompt": null}
{"id": "012cc5df-1735-4518-8dda-11fa5e8b796e", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (evaluations / self.budget)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Tweaked the dynamic inertia weight formula in PSO for better exploration-exploitation balance. ", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"cannot access local variable 'evaluations' where it is not associated with a value\").", "error": "UnboundLocalError(\"cannot access local variable 'evaluations' where it is not associated with a value\")", "parent_id": "9892d6ca-74c4-4c8b-80a2-86fe649631de", "metadata": {}, "mutation_prompt": null}
{"id": "7b5e3259-f38f-493a-ad65-ad6508471895", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced global best selection and adaptive component tuning for improved convergence and solution quality.", "configspace": "", "generation": 6, "fitness": 0.8173385812783541, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.013. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "9892d6ca-74c4-4c8b-80a2-86fe649631de", "metadata": {"aucs": [0.8032874364750278, 0.8140384791852022, 0.8346898281748325], "final_y": [0.14848894775767063, 0.1458436902398158, 0.14067961296539266]}, "mutation_prompt": null}
{"id": "14321dc1-c984-49fb-aa23-ba62121697d7", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (pop_size / self.budget)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced the adaptive strategy by modifying dynamic inertia weight adjustment for improved convergence.", "configspace": "", "generation": 6, "fitness": 0.8130260148925498, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.010. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "9892d6ca-74c4-4c8b-80a2-86fe649631de", "metadata": {"aucs": [0.8014915669092186, 0.8112122950291696, 0.8263741827392612], "final_y": [0.15085657263900132, 0.14705554178360503, 0.1442338875531154]}, "mutation_prompt": null}
{"id": "a5fbe2b4-0f80-43c8-9bfe-3536d81bdc22", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget ** 0.5 / pop_size)  # Non-linear dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced a non-linear adaptive scheme for the inertia weight `w` to further enhance convergence stability and solution quality.", "configspace": "", "generation": 6, "fitness": 0.8071199031225443, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.807 with standard deviation 0.010. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "9892d6ca-74c4-4c8b-80a2-86fe649631de", "metadata": {"aucs": [0.793145146658871, 0.813190160573374, 0.8150244021353876], "final_y": [0.15478154414385192, 0.15011473985288282, 0.1498122918974003]}, "mutation_prompt": null}
{"id": "952ac63d-ad08-410c-8c55-cf5e50727746", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 1.0)  # Slightly increased adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced a slight randomness in crossover probability `CR` to enhance exploration.", "configspace": "", "generation": 6, "fitness": 0.7831965434839793, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.027. And the mean value of best solutions found was 0.162 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "9892d6ca-74c4-4c8b-80a2-86fe649631de", "metadata": {"aucs": [0.7621487467349742, 0.765552182466437, 0.8218887012505263], "final_y": [0.1699939837695177, 0.16829293789121647, 0.14681686465611143]}, "mutation_prompt": null}
{"id": "e72cf7a8-6167-40f1-9db1-acc5285be4f5", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * (evaluations / self.budget)  # Dynamic crossover probability\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced a dynamic crossover probability that adapts based on evaluations to enhance the balance between exploration and exploitation.", "configspace": "", "generation": 7, "fitness": 0.8210113958116271, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.821 with standard deviation 0.010. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "7b5e3259-f38f-493a-ad65-ad6508471895", "metadata": {"aucs": [0.8338077256747771, 0.8091394811134727, 0.8200869806466313], "final_y": [0.13829311400604738, 0.14828643578170586, 0.14714040126323769]}, "mutation_prompt": null}
{"id": "68e37b90-c630-4ca7-adbe-edcea9219e11", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-0.5, 0.5, (pop_size, self.dim))  # Refined velocity initialization\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 * (1.0 - evaluations / self.budget)  # Adaptive mutation scaling\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Improved local search with adaptive mutation scaling and refined velocity initialization for enhanced solution accuracy.", "configspace": "", "generation": 7, "fitness": 0.8069172523250697, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.807 with standard deviation 0.013. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "7b5e3259-f38f-493a-ad65-ad6508471895", "metadata": {"aucs": [0.8241808684496469, 0.7921212637369043, 0.8044496247886581], "final_y": [0.14094367611738123, 0.15604985130151294, 0.154953440476705]}, "mutation_prompt": null}
{"id": "4d20583c-0c6a-4f8e-98e5-89b9783099a5", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F_base = 0.5\n        CR_base = 0.9\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        chaotic_sequence = np.random.rand(pop_size)\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_rate = np.abs(np.sin(np.pi * evaluations / self.budget))\n\n                F = F_base * adaptive_rate\n                CR = CR_base * adaptive_rate\n\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                \n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            chaotic_sequence = 4.0 * chaotic_sequence * (1.0 - chaotic_sequence)\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta * chaotic_sequence[i], func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Incorporates adaptive learning rates and chaotic search mechanisms to enhance diversity and convergence in hybrid metaheuristics.", "configspace": "", "generation": 7, "fitness": 0.8180944163678201, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.009. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "7b5e3259-f38f-493a-ad65-ad6508471895", "metadata": {"aucs": [0.806176958608469, 0.8286377514873611, 0.8194685390076301], "final_y": [0.1413965732279382, 0.1406518781935231, 0.14857034217140042]}, "mutation_prompt": null}
{"id": "906bcae2-0122-49f1-8012-eb0e997641c8", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.5 - 0.4) * (self.budget / pop_size)  # Refined dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Refined inertia weight adjustment in Particle Swarm Optimization to enhance convergence performance.", "configspace": "", "generation": 7, "fitness": 0.8024612463584543, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.018. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "7b5e3259-f38f-493a-ad65-ad6508471895", "metadata": {"aucs": [0.8046185282359166, 0.7795222216970179, 0.8232429891424285], "final_y": [0.13623844429111132, 0.16065291847333696, 0.14293156166840826]}, "mutation_prompt": null}
{"id": "ad896e61-1541-4d18-9746-278265d8698b", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.6, 0.95)  # Slightly increased range for differential weight\n        CR = np.random.uniform(0.6, 0.95)  # Slightly increased range for crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.7  # Increased cognitive component\n        c2 = 1.3  # Decreased social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Periodic reinitialization for diversity\n            if evaluations % (0.1 * self.budget) < pop_size:\n                population[:int(0.1 * pop_size)] = np.random.uniform(func.bounds.lb, func.bounds.ub, (int(0.1 * pop_size), self.dim))\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "A refined hybrid metaheuristic with periodic reinitialization and diversified mutation for improved exploration and convergence.", "configspace": "", "generation": 7, "fitness": 0.8204971941909284, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.820 with standard deviation 0.007. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "7b5e3259-f38f-493a-ad65-ad6508471895", "metadata": {"aucs": [0.8105739484623448, 0.8273328724252488, 0.8235847616851918], "final_y": [0.13957328342565245, 0.14128795066511524, 0.14238581978875486]}, "mutation_prompt": null}
{"id": "eab84b2c-f0ed-4f84-b8e6-edcd94a0664d", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n        \n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * (evaluations / self.budget)  \n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                adaptive_step = 0.1 / (1 + 0.05 * evaluations)\n                delta = np.random.uniform(-adaptive_step, adaptive_step, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    refined_solution = np.clip(new_solution - 0.01 * (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8), func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Hybrid algorithm with adaptive mutation strategy and enhanced local search for better convergence.", "configspace": "", "generation": 8, "fitness": 0.7740123648896501, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.022. And the mean value of best solutions found was 0.163 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "e72cf7a8-6167-40f1-9db1-acc5285be4f5", "metadata": {"aucs": [0.8044895960007338, 0.7662606930030458, 0.7512868056651709], "final_y": [0.14608246495532662, 0.16793719107425276, 0.1753941292704606]}, "mutation_prompt": null}
{"id": "a1d759eb-03e2-454f-8c68-c9cb246392f5", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 1.0)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * (evaluations / self.budget)  # Dynamic crossover probability\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced an adaptive mutation factor in the Differential Evolution phase to enhance exploration capabilities.", "configspace": "", "generation": 8, "fitness": 0.8025793118549333, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.026. And the mean value of best solutions found was 0.150 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "e72cf7a8-6167-40f1-9db1-acc5285be4f5", "metadata": {"aucs": [0.7785874260335306, 0.7904604451683144, 0.8386900643629548], "final_y": [0.16011904785542586, 0.15643428891207545, 0.13486404441459487]}, "mutation_prompt": null}
{"id": "ae40a0ea-d304-47d2-ac0b-956a3f800a1a", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * (evaluations / self.budget)  # Dynamic crossover probability\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 1.0) * (evaluations / self.budget)  # Changed from 0.5 to 1.0\n            c2 = 2.5 - (2.5 - 1.5) * (evaluations / self.budget)  # Changed from 1.0 to 1.5\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Adjusted the cognitive and social components' adaptation formula to enhance convergence speed while maintaining a 2.1% change rate.", "configspace": "", "generation": 8, "fitness": 0.756325945417097, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.756 with standard deviation 0.016. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "e72cf7a8-6167-40f1-9db1-acc5285be4f5", "metadata": {"aucs": [0.7372344646409325, 0.7768943945933597, 0.754848977016999], "final_y": [0.17274528858370708, 0.15929579472366096, 0.17324130764582046]}, "mutation_prompt": null}
{"id": "469422cc-7ca3-4f1d-bf99-f64243637e00", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (1 - (self.budget / pop_size))  # Modified dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * (evaluations / self.budget)  # Dynamic crossover probability\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Modified the inertia weight calculation to dynamically adjust based on the evaluation ratio for improved convergence.", "configspace": "", "generation": 8, "fitness": 0.7869673623481229, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.787 with standard deviation 0.007. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "e72cf7a8-6167-40f1-9db1-acc5285be4f5", "metadata": {"aucs": [0.7924013682343, 0.776944757456663, 0.791555961353406], "final_y": [0.145505654930978, 0.15806003244991584, 0.15609032290780733]}, "mutation_prompt": null}
{"id": "54372d90-e473-4a2e-8712-a4aea7fcbccb", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7  # Initial value for chaotic mapping\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)  # Logistic map for chaos\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z  # Dynamic crossover probability using chaos\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced exploration by introducing chaotic logistic mapping for dynamic parameter tuning.", "configspace": "", "generation": 8, "fitness": 0.8130656700249143, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.017. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "e72cf7a8-6167-40f1-9db1-acc5285be4f5", "metadata": {"aucs": [0.8030295573708445, 0.7988071716850165, 0.837360281018882], "final_y": [0.14984254710331646, 0.14337971598524424, 0.1305459556142481]}, "mutation_prompt": null}
{"id": "ee7aa854-656c-46dc-a75c-753f57adb64f", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7  # Initial value for chaotic mapping\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)  # Logistic map for chaos\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z  # Dynamic crossover probability using chaos\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions using Lévy flight\n            for i in range(pop_size):\n                levy_lambda = 1.5\n                step_size = 0.1 * np.random.standard_cauchy(size=self.dim) ** levy_lambda  # Lévy flight step\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced Lévy flight-based exploration and adaptive local search for improved convergence.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: OverflowError('Range exceeds valid bounds').", "error": "OverflowError('Range exceeds valid bounds')", "parent_id": "54372d90-e473-4a2e-8712-a4aea7fcbccb", "metadata": {}, "mutation_prompt": null}
{"id": "8f3cc1e8-8d6c-4315-ab6a-65055018c819", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9  # Initial inertia weight\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7  # Initial value for chaotic mapping\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)  # Logistic map for chaos\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z  # Dynamic crossover probability using chaos\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = 0.9 - (0.5 * z)  # Dynamic inertia weight using chaos\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced a chaotic logistic map to dynamically adjust the inertia weight in PSO for enhanced convergence.", "configspace": "", "generation": 9, "fitness": 0.7836958229258976, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.007. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "54372d90-e473-4a2e-8712-a4aea7fcbccb", "metadata": {"aucs": [0.7918854821640552, 0.7742717606808314, 0.7849302259328064], "final_y": [0.14005122422386085, 0.16209704502064348, 0.1596997275409039]}, "mutation_prompt": null}
{"id": "f8350fdf-4894-4832-8ec7-04f00ff802d3", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7  # Initial value for chaotic mapping\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)  # Logistic map for chaos\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z  # Dynamic crossover probability using chaos\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update with chaotic influence\n            r1, r2 = np.random.rand(2)\n            velocity = w * (1 + z * 0.1) * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced local exploration by refining velocity update with chaotic mapping.", "configspace": "", "generation": 9, "fitness": 0.7782581957467224, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.014. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "54372d90-e473-4a2e-8712-a4aea7fcbccb", "metadata": {"aucs": [0.7579714837108034, 0.7868302851387219, 0.7899728183906417], "final_y": [0.16521722563453145, 0.15721032557919523, 0.15596898827401395]}, "mutation_prompt": null}
{"id": "66c0bf2b-a9fa-4dd2-9433-be2fe85c4ad0", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7  # Initial value for chaotic mapping\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)  # Logistic map for chaos\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                F = 0.5 + 0.4 * z  # Dynamic differential weight using chaos\n                CR = 0.5 + 0.4 * z  # Dynamic crossover probability using chaos\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduced adaptive chaotic mapping for dynamically tuning both differential weight and crossover probability.", "configspace": "", "generation": 9, "fitness": 0.7735341013037038, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.013. And the mean value of best solutions found was 0.161 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "54372d90-e473-4a2e-8712-a4aea7fcbccb", "metadata": {"aucs": [0.7580436396602692, 0.7716496617558668, 0.7909090024949753], "final_y": [0.1695677938277882, 0.16505474568933454, 0.14827034003990813]}, "mutation_prompt": null}
{"id": "f0c4124b-6852-4c72-a75b-5c54da310b53", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7  # Initial value for chaotic mapping\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)  # Logistic map for chaos\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z  # Dynamic crossover probability using chaos\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget))  # Adaptive chaotic inertia weight\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Improved balance between exploration and exploitation using adaptive chaotic inertia weight tuning.", "configspace": "", "generation": 9, "fitness": 0.7906716430595176, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.791 with standard deviation 0.017. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "54372d90-e473-4a2e-8712-a4aea7fcbccb", "metadata": {"aucs": [0.7665549660470854, 0.7986823247401115, 0.8067776383913562], "final_y": [0.1483147087035157, 0.14727294980813854, 0.14972746574002116]}, "mutation_prompt": null}
{"id": "d5f0e23a-95f4-40e9-8e7b-9c7119d43bc6", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7  # Initial value for chaotic mapping\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)  # Logistic map for chaos\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z  # Dynamic crossover probability using chaos\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget))  # Adaptive chaotic inertia weight\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced exploration via chaos-based adaptive weight tuning for improved optimization.", "configspace": "", "generation": 10, "fitness": 0.8256836883568934, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.013. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "f0c4124b-6852-4c72-a75b-5c54da310b53", "metadata": {"aucs": [0.8306050500137995, 0.8387919123072098, 0.807654102749671], "final_y": [0.13549434928733517, 0.13001871799402875, 0.13464402534584408]}, "mutation_prompt": null}
{"id": "6ae5f461-b37e-4025-a146-8a6c9245c440", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  \n        CR = 0.5  \n        \n        c1 = 1.5  \n        c2 = 1.5  \n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  \n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7  \n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)  \n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                F = 0.5 + 0.4 * z  # Adaptive F using chaos\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z  \n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget))  \n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  \n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  \n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced exploration-exploitation balance using adaptive chaotic mapping in mutation and crossover.", "configspace": "", "generation": 10, "fitness": 0.8024543572337518, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.029. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "f0c4124b-6852-4c72-a75b-5c54da310b53", "metadata": {"aucs": [0.763626037282946, 0.8106463813771553, 0.833090653041154], "final_y": [0.14857661934321986, 0.1350264612556642, 0.1372071780536085]}, "mutation_prompt": null}
{"id": "b05e4aed-12be-4ea2-a495-abf3c7a2c312", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.6, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.8 - (0.8 - 0.3) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7  # Initial value for chaotic mapping\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)  # Logistic map for chaos\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z  # Dynamic crossover probability using chaos\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.8 - (0.8 - 0.3) * (evaluations / self.budget))  # Adaptive chaotic inertia weight\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced exploration by incorporating a dynamic mutation factor and altered inertia weight.", "configspace": "", "generation": 10, "fitness": 0.802584475184636, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.001. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "f0c4124b-6852-4c72-a75b-5c54da310b53", "metadata": {"aucs": [0.8018017760989009, 0.8017298245258893, 0.8042218249291178], "final_y": [0.1439511159842003, 0.13701400422319454, 0.1346735711688889]}, "mutation_prompt": null}
{"id": "6ae067ec-0db5-49e3-ae85-ce9e296ef584", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7  # Chaotic mapping initial value\n        levy_alpha = 1.5  # Levy flight exponent\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)  # Logistic map for chaos\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z  # Dynamic crossover probability using chaos\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning with Levy flights\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget))  # Adaptive chaotic inertia weight\n            levy_step = np.random.standard_cauchy(self.dim)  # Levy flight step\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population) + levy_step / np.power(np.abs(levy_step), 1.0 / levy_alpha)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced exploration and exploitation using chaotic Levy flights and dynamic parameter tuning for improved convergence.", "configspace": "", "generation": 10, "fitness": 0.7922035141392754, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.792 with standard deviation 0.008. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "f0c4124b-6852-4c72-a75b-5c54da310b53", "metadata": {"aucs": [0.7909065205912534, 0.8026285007715402, 0.7830755210550325], "final_y": [0.13544076116676196, 0.14163450308092673, 0.1349326090517361]}, "mutation_prompt": null}
{"id": "c5b6e33e-5734-4204-a5eb-39d859da81c4", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7  # Initial value for chaotic mapping\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)  # Logistic map for chaos\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z  # Dynamic crossover probability using chaos\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget))  # Adaptive chaotic inertia weight\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            \n            levy_flights = np.random.normal(0, 1, (pop_size, self.dim)) * ((0.01 / (np.abs(np.random.normal(0, 1, self.dim)) ** (1/1.5))))\n            population = population + velocity + levy_flights\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced exploration with Levy flights and improved convergence through adaptive parameter tuning.", "configspace": "", "generation": 10, "fitness": 0.7994260050466909, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.799 with standard deviation 0.016. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "f0c4124b-6852-4c72-a75b-5c54da310b53", "metadata": {"aucs": [0.7797335051438524, 0.8190820398381709, 0.7994624701580493], "final_y": [0.15842101577492684, 0.13906852307992623, 0.1439552572011743]}, "mutation_prompt": null}
{"id": "0216df73-9169-4d39-8b33-77cec59cca5b", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n\n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n\n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7  # Initial value for chaotic mapping\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)  # Logistic map for chaos\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z  # Dynamic crossover probability using chaos\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n\n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n\n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget))  # Adaptive chaotic inertia weight\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n            # Chaotic Lévy flights for enhanced exploration\n            step = np.random.standard_cauchy(self.dim) * (0.01 * (evaluations / self.budget))\n            population += step\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Improved hybrid exploration through chaotic Lévy flights and adaptive memory for enhanced optimization convergence.", "configspace": "", "generation": 11, "fitness": 0.8126985204913995, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.013. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "d5f0e23a-95f4-40e9-8e7b-9c7119d43bc6", "metadata": {"aucs": [0.7994977688630605, 0.8307918205029193, 0.8078059721082183], "final_y": [0.1400163485246363, 0.12409434743616432, 0.14437356543169733]}, "mutation_prompt": null}
{"id": "9f6b8241-2686-46d4-87b8-ea9459614f0f", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7  # Initial value for chaotic mapping\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)  # Logistic map for chaos\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z  # Dynamic crossover probability using chaos\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget))  # Adaptive chaotic inertia weight\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Enhanced gradient\n                    refined_solution = np.clip(new_solution - 0.02 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced local search refinement via adaptive gradient approximation for improved convergence.", "configspace": "", "generation": 11, "fitness": 0.815008673474848, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.012. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "d5f0e23a-95f4-40e9-8e7b-9c7119d43bc6", "metadata": {"aucs": [0.7980032590672183, 0.8232230838441515, 0.823799677513174], "final_y": [0.13427401163035246, 0.1420480545867734, 0.12102762992250204]}, "mutation_prompt": null}
{"id": "7266eda9-a01e-44a3-8f2e-95faab708e49", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.35 * z  # Changed from 0.4 to 0.35\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget))\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta + np.random.normal(0, 0.01, self.dim), func.bounds.lb, func.bounds.ub)  # Added stochastic perturbation\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Integrating stochastic perturbation and adaptive crossover for enhanced local exploitation and diversity.", "configspace": "", "generation": 11, "fitness": 0.8151387283665898, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.024. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "d5f0e23a-95f4-40e9-8e7b-9c7119d43bc6", "metadata": {"aucs": [0.7810978622869675, 0.8319884615981041, 0.8323298612146978], "final_y": [0.1473490784538637, 0.13799775932111635, 0.123313054889436]}, "mutation_prompt": null}
{"id": "81a3bec5-4690-48ba-a8ae-1ab6f84d702c", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)  # New adaptive scaling factor\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0  # New combined strategy\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget))\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introducing a multi-phase mutation strategy and dynamic component scaling to improve adaptation and exploration capabilities.", "configspace": "", "generation": 11, "fitness": 0.8290529711651073, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.829 with standard deviation 0.024. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "d5f0e23a-95f4-40e9-8e7b-9c7119d43bc6", "metadata": {"aucs": [0.8560023514202988, 0.7976233895488186, 0.8335331725262044], "final_y": [0.12985720352620733, 0.1367736566772415, 0.12438610512444936]}, "mutation_prompt": null}
{"id": "238bb8cd-9c5c-4f0a-8e0c-cf53fe63405c", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)  # Adaptive differential weight\n        CR = 0.5  # Static crossover probability\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5  # Cognitive component\n        c2 = 1.5  # Social component\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)  # Dynamic inertia weight adjustment\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7  # Initial value for chaotic mapping\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)  # Logistic map for chaos\n            if evaluations > (self.budget / 2): pop_size = int(pop_size * 0.95)  # Dynamic reduction of pop_size\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z  # Dynamic crossover probability using chaos\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget))  # Adaptive chaotic inertia weight\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adaptive step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)  # Gradient approximation\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with dynamic population size for improved convergence.", "configspace": "", "generation": 11, "fitness": 0.8077427593666124, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.012. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "d5f0e23a-95f4-40e9-8e7b-9c7119d43bc6", "metadata": {"aucs": [0.7916163746826272, 0.8104156542655987, 0.8211962491516114], "final_y": [0.14082836912378094, 0.1375425945206762, 0.1347369200065901]}, "mutation_prompt": null}
{"id": "aa8f93f2-296a-4884-84d2-8a696dea1e02", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)  # New adaptive scaling factor\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0  # New combined strategy\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget))\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            momentum = 0.9  # New momentum term\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.01 * gradient + momentum * delta, func.bounds.lb, func.bounds.ub)  # Incorporate momentum\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Refine the local search step by introducing a momentum term to enhance convergence speed and stability.", "configspace": "", "generation": 12, "fitness": 0.794161187846798, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.013. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "81a3bec5-4690-48ba-a8ae-1ab6f84d702c", "metadata": {"aucs": [0.7782437419369316, 0.8104201621358894, 0.7938196594675729], "final_y": [0.1502726310442415, 0.13597986168763643, 0.15055239271353393]}, "mutation_prompt": null}
{"id": "8fabb035-05a4-4835-9d60-640be92be42f", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = np.random.uniform(0.5, 0.9)  # Adaptive z initialization\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)  # New adaptive scaling factor\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0  # New combined strategy\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget))\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced global search by adjusting scaling factor for improved exploration; now with adaptive z initialization.", "configspace": "", "generation": 12, "fitness": 0.7947544526441227, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.795 with standard deviation 0.006. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "81a3bec5-4690-48ba-a8ae-1ab6f84d702c", "metadata": {"aucs": [0.786547910503014, 0.7981488789482883, 0.7995665684810657], "final_y": [0.1405512156383788, 0.14726161716495134, 0.1385967693499749]}, "mutation_prompt": null}
{"id": "9e69bc29-ceec-4782-aeee-37b3d83672a3", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)  # New adaptive scaling factor\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0  # New combined strategy\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = 0.9 - (0.5 * evaluations / self.budget)  # Adaptive inertia weight strategy\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Incorporating an adaptive inertia weight strategy to enhance convergence speed and exploration capabilities in the hybrid optimization process.", "configspace": "", "generation": 12, "fitness": 0.8094559008264502, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.024. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "81a3bec5-4690-48ba-a8ae-1ab6f84d702c", "metadata": {"aucs": [0.8006937986478087, 0.7856761596210909, 0.8419977442104511], "final_y": [0.13307295498691207, 0.14355923304138218, 0.13271074077806566]}, "mutation_prompt": null}
{"id": "f2cf4072-55f1-4b38-b00d-b854a4326b88", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)  # New adaptive scaling factor\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0  # New combined strategy\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim  # Adaptive inertia\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introducing an adaptive inertia weight strategy for enhanced exploration and exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.8103019543473028, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.033. And the mean value of best solutions found was 0.143 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "81a3bec5-4690-48ba-a8ae-1ab6f84d702c", "metadata": {"aucs": [0.7914443781836824, 0.7834191158282539, 0.8560423690299718], "final_y": [0.15138615998565041, 0.15367413322032542, 0.12347265809538299]}, "mutation_prompt": null}
{"id": "ec871f2c-0146-4f4a-845a-b9ff3e9e79e1", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5\n        c2 = 1.5\n        w_max = 0.9\n        w_min = 0.4\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.6 * z)  # Modified adaptive scaling factor\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + 0.3*mutant2 + 0.7*population[i]) / 2.0  # Blended strategy\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = w_max - (w_max - w_min) * ((evaluations / self.budget)**2)  # Non-linear inertia weight decay\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhancing the selection mechanism with a blended mutation strategy and introducing non-linear inertia weight decay for improved convergence stability.", "configspace": "", "generation": 12, "fitness": 0.7973422469800839, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.007. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "81a3bec5-4690-48ba-a8ae-1ab6f84d702c", "metadata": {"aucs": [0.788624695981659, 0.8062684235389487, 0.7971336214196441], "final_y": [0.14516050477779985, 0.14853870932149105, 0.1438469354517664]}, "mutation_prompt": null}
{"id": "8cdc4380-873c-4b2f-aaa9-a059386e422c", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)  # New adaptive scaling factor\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + 0.8 * mutant2) / 1.8  # Enhanced scaling mechanism\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim  # Adaptive inertia\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Refining mutation strategy by applying an enhanced scaling mechanism for improved convergence.", "configspace": "", "generation": 13, "fitness": 0.7768488731116058, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.777 with standard deviation 0.019. And the mean value of best solutions found was 0.162 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "f2cf4072-55f1-4b38-b00d-b854a4326b88", "metadata": {"aucs": [0.7994848345435484, 0.7779495023396157, 0.7531122824516532], "final_y": [0.15039535191435827, 0.16133245756640235, 0.17438986118545063]}, "mutation_prompt": null}
{"id": "1456cc8a-f057-4179-bbf1-818409484895", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)  # New adaptive scaling factor\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0  # New combined strategy\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.6 + 0.3 * z    # Adjusted crossover rate\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim  # Adaptive inertia\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            velocity *= (0.9 + 0.1 * np.random.rand())  # Introduced random velocity scaling for diversity\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced diversity with adaptive velocity scaling and crossover rate strategies.", "configspace": "", "generation": 13, "fitness": 0.7797971267924092, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.026. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "f2cf4072-55f1-4b38-b00d-b854a4326b88", "metadata": {"aucs": [0.7464538546191348, 0.7818117677430603, 0.8111257580150323], "final_y": [0.16787938647835132, 0.16062901731745227, 0.1471425060122844]}, "mutation_prompt": null}
{"id": "03642e93-21f1-462a-b061-2122a2033ff7", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)  # New adaptive scaling factor\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0  # New combined strategy\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim  # Adaptive inertia\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.05 / (1 + 0.01 * evaluations)  # Adjusted step size for local search\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Refining convergence through dynamic scaling and inertia adjustments in the hybrid metaheuristic framework.", "configspace": "", "generation": 13, "fitness": 0.7941146981269318, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.022. And the mean value of best solutions found was 0.155 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "f2cf4072-55f1-4b38-b00d-b854a4326b88", "metadata": {"aucs": [0.771860526025821, 0.8239762058530233, 0.7865073625019512], "final_y": [0.16483359641760242, 0.1417860811961581, 0.15855736996351932]}, "mutation_prompt": null}
{"id": "a6617c44-5f59-46a0-bfbe-944d533e1778", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)  # New chaotic map\n        while evaluations < self.budget:\n            z = chaotic_map(z)  # Use chaotic map\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.05 / (1 + 0.01 * evaluations)  # Adjusted step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)  # Adjusted gradient step\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introducing a chaotic map for enhanced exploration and a dynamic local search strategy for improved convergence.", "configspace": "", "generation": 13, "fitness": 0.833560694708377, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.834 with standard deviation 0.031. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "f2cf4072-55f1-4b38-b00d-b854a4326b88", "metadata": {"aucs": [0.8671308115851633, 0.7925840608136986, 0.8409672117262692], "final_y": [0.1268410549363862, 0.15443737588599982, 0.1344233539462384]}, "mutation_prompt": null}
{"id": "05f9b5a2-c511-4364-8f99-5260b67127f4", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution (DE)\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        # Initialize parameters for Particle Swarm Optimization (PSO)\n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        # Initialize personal best and global best\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        while evaluations < self.budget:\n            z = 4.0 * z * (1 - z)\n            for i in range(pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)  # New adaptive scaling factor\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0  # New combined strategy\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Select between trial and original\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                # Update global best\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Adaptive component tuning\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            # Particle Swarm Update\n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim  # Adaptive inertia\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Local search step to refine solutions\n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.01 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Fine-tune differential mutation rates for enhanced exploration-exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.8162753033779403, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.816 with standard deviation 0.041. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "f2cf4072-55f1-4b38-b00d-b854a4326b88", "metadata": {"aucs": [0.764814133904114, 0.8196354475958065, 0.8643763286339008], "final_y": [0.1661342423343951, 0.14412181942509272, 0.12577429922576122]}, "mutation_prompt": null}
{"id": "80bd57a0-b28d-406a-9ed1-364dab51851b", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)  # New chaotic map\n        while evaluations < self.budget:\n            z = chaotic_map(z)  # Use chaotic map\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.05 / (1 + 0.01 * evaluations)  # Adjusted step size\n                alpha = 0.01 * (1.0 + 0.1 * (evaluations / self.budget))\n                delta = alpha * np.random.standard_normal(self.dim)  # Lévy flight step\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)  # Adjusted gradient step\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introducing a Lévy flight mechanism for enhanced exploration and adaptive learning rates for refined convergence.", "configspace": "", "generation": 14, "fitness": 0.7867654140740475, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.787 with standard deviation 0.024. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "a6617c44-5f59-46a0-bfbe-944d533e1778", "metadata": {"aucs": [0.8061224488150243, 0.8008206083296128, 0.7533531850775057], "final_y": [0.15083806749929585, 0.15159324124028184, 0.17425465797861517]}, "mutation_prompt": null}
{"id": "5833211d-b41e-4fcc-8de3-705fe9b5a9a4", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)  # New chaotic map\n        while evaluations < self.budget:\n            z = chaotic_map(z)  # Use chaotic map\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = 0.4 + (0.5 * z) * (func.bounds.ub - func.bounds.lb) / self.dim  # Adaptive inertia weight\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.07 / (1 + 0.01 * evaluations)  # Diversified step size\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)  # Adjusted gradient step\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Leveraging adaptive inertia weight and a diversified local search for enhanced exploration and exploitation balance.", "configspace": "", "generation": 14, "fitness": 0.7634494367319835, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.763 with standard deviation 0.004. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "a6617c44-5f59-46a0-bfbe-944d533e1778", "metadata": {"aucs": [0.7632334754103988, 0.7679108149954519, 0.7592040197900999], "final_y": [0.16434168001315885, 0.16753346717015327, 0.16846151791822073]}, "mutation_prompt": null}
{"id": "b0e0fb07-b4c7-4724-83d6-417f9ea5f609", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)\n        while evaluations < self.budget:\n            z = chaotic_map(z)\n            fitness_diversity = np.std(fitness) / (np.mean(fitness) + 1e-8)  # Calculate fitness diversity\n            CR = 0.5 + 0.4 * fitness_diversity  # Adaptive crossover rate based on diversity\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.05 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introducing an adaptive crossover rate based on fitness diversity to enhance exploration and stability.", "configspace": "", "generation": 14, "fitness": 0.7852460475318465, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.785 with standard deviation 0.013. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "a6617c44-5f59-46a0-bfbe-944d533e1778", "metadata": {"aucs": [0.8014076297543022, 0.768452492610863, 0.785878020230374], "final_y": [0.14991977025398628, 0.16705334042523967, 0.15808718546128464]}, "mutation_prompt": null}
{"id": "56f0ff38-0f0b-47e1-ba05-b7bcc69b7b0f", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)\n        while evaluations < self.budget:\n            z = chaotic_map(z)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                # Changed: Adaptive mutation strategy\n                F_adaptive = 0.5 + 0.4 * np.sin(np.pi * z)\n                mutant1 = population[a] + F_adaptive * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                # Changed: Fine-tuned crossover rate\n                CR = 0.6 + 0.3 * np.cos(np.pi * z)\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.05 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introducing a new adaptive mutation strategy and fine-tuning crossover for enhanced exploration and convergence.", "configspace": "", "generation": 14, "fitness": 0.7742219905562249, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.010. And the mean value of best solutions found was 0.162 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "a6617c44-5f59-46a0-bfbe-944d533e1778", "metadata": {"aucs": [0.7606598154191078, 0.7845281788928526, 0.7774779773567142], "final_y": [0.1656484175906089, 0.1592925399979389, 0.16114092949564995]}, "mutation_prompt": null}
{"id": "9c39490f-3a28-4d00-93de-b0c0f5054a59", "solution": "import numpy as np\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)\n        \n        def levy_flight(Lambda):\n            sigma_u = (np.math.gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2) / (np.math.gamma((1 + Lambda) / 2) * Lambda * 2 ** ((Lambda - 1) / 2))) ** (1 / Lambda)\n            u = np.random.normal(0, sigma_u, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            step = u / np.abs(v) ** (1 / Lambda)\n            return 0.01 * step\n\n        while evaluations < self.budget:\n            z = chaotic_map(z)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                if np.random.rand() < 0.8:  # Apply Lévy flight with 80% probability\n                    mutant += levy_flight(1.5)\n\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n            \n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.05 / (1 + 0.01 * evaluations)\n                delta = np.random.uniform(-step_size, step_size, self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "EnhancedHybridMetaheuristic", "description": "Enhanced exploration with Lévy flights and adaptive parameter tuning for improved convergence efficiency.", "configspace": "", "generation": 14, "fitness": 0.7798094701893117, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.014. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "a6617c44-5f59-46a0-bfbe-944d533e1778", "metadata": {"aucs": [0.7608790692772543, 0.7959540631526976, 0.7825952781379828], "final_y": [0.16522978740478778, 0.15125058114913015, 0.15919519933224258]}, "mutation_prompt": null}
{"id": "c58d89b8-bea7-4283-94e8-87a2f807b70e", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)  # New chaotic map\n        while evaluations < self.budget:\n            z = chaotic_map(z)  # Use chaotic map\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.05 / (1 + 0.01 * evaluations)  # Adjusted step size\n                alpha = 0.01 * (1.0 + 0.1 * (evaluations / self.budget))\n                delta = alpha * np.random.standard_normal(self.dim)  # Lévy flight step\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)  # Adjusted gradient step\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introducing a Lévy flight mechanism for enhanced exploration and adaptive learning rates for refined convergence.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "80bd57a0-b28d-406a-9ed1-364dab51851b", "metadata": {"aucs": [0.8061224488150243, 0.8008206083296128, 0.7533531850775057], "final_y": [0.15083806749929585, 0.15159324124028184, 0.17425465797861517]}, "mutation_prompt": null}
{"id": "94450d5c-c01d-4438-8bfe-97e687cccaba", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n\n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n\n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)\n        while evaluations < self.budget:\n            z = chaotic_map(z)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                F_dynamic = F * (1 + 0.1 * np.sin(evaluations / self.budget * np.pi))\n                mutant1 = population[a] + F_dynamic * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim\n            velocity_damping = np.exp(-0.01 * evaluations / self.budget)\n            velocity = w * velocity * velocity_damping + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.05 / (1 + 0.01 * evaluations)\n                alpha = 0.01 * (1.0 + 0.1 * (evaluations / self.budget))\n                delta = alpha * np.random.standard_normal(self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Integrate a dynamic mutation strategy and stochastic damping factor for improved adaptability and convergence precision.", "configspace": "", "generation": 15, "fitness": 0.790037738020099, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.790 with standard deviation 0.034. And the mean value of best solutions found was 0.157 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "80bd57a0-b28d-406a-9ed1-364dab51851b", "metadata": {"aucs": [0.7413820251438779, 0.8156013526855006, 0.8131298362309185], "final_y": [0.17718244190953913, 0.14545482074823357, 0.14739470200556126]}, "mutation_prompt": null}
{"id": "7214344a-40b0-44a7-bcf2-d0ffd709288f", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)  # New chaotic map\n        while evaluations < self.budget:\n            z = chaotic_map(z)  # Use chaotic map\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adjusted step size\n                alpha = 0.02 * (1.0 + 0.1 * (evaluations / self.budget))  # Adjusted alpha\n                delta = alpha * np.random.standard_normal(self.dim)  # Lévy flight step\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n                if evaluations % (0.1 * self.budget) == 0:  # Reinitialize stagnated particles\n                    stagnated = fitness > np.percentile(fitness, 75)\n                    population[stagnated] = np.random.uniform(func.bounds.lb, func.bounds.ub, (np.sum(stagnated), self.dim))\n                    fitness[stagnated] = np.apply_along_axis(func, 1, population[stagnated])\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhancing exploitation by refining Lévy flight step size and introducing diversity through random reinitialization of stagnated particles.", "configspace": "", "generation": 15, "fitness": 0.7901920410826487, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.790 with standard deviation 0.020. And the mean value of best solutions found was 0.156 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "80bd57a0-b28d-406a-9ed1-364dab51851b", "metadata": {"aucs": [0.8183553068120324, 0.7762535028983344, 0.7759673135375794], "final_y": [0.14045352594269467, 0.16267914031997288, 0.16343974185759702]}, "mutation_prompt": null}
{"id": "d237491b-597d-4ca2-b306-09fa6d784bbf", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)  # New chaotic map\n        while evaluations < self.budget:\n            z = chaotic_map(z)  # Use chaotic map\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.05 / (1 + 0.01 * evaluations)  # Adjusted step size\n                alpha = 0.01 * (1.0 + 0.1 * (evaluations / self.budget))\n                delta = alpha * np.random.standard_normal(self.dim)  # Lévy flight step\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i] or np.random.rand() < 0.1:  # Introduced stochastic selection\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)  # Adjusted gradient step\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introducing stochastic selection pressure to balance exploration and exploitation more effectively.", "configspace": "", "generation": 15, "fitness": 0.7838534962727809, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.038. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "80bd57a0-b28d-406a-9ed1-364dab51851b", "metadata": {"aucs": [0.7472817504525309, 0.7682756044767423, 0.8360031338890697], "final_y": [0.17351619900400173, 0.16718350779989255, 0.14022807225541867]}, "mutation_prompt": null}
{"id": "7954b7c5-9537-46a3-9b12-cb0379e4645b", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)  # New chaotic map\n        while evaluations < self.budget:\n            z = chaotic_map(z)  # Use chaotic map\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.05 / (1 + 0.01 * evaluations)  # Adjusted step size\n                alpha = 0.01 * (1.0 + 0.1 * (evaluations / self.budget))\n                delta = alpha * np.random.standard_normal(self.dim)  # Lévy flight step\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.045 * gradient, func.bounds.lb, func.bounds.ub)  # Adjusted gradient step\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Enhanced adaptive control factor for improved convergence in HybridMetaheuristic.", "configspace": "", "generation": 15, "fitness": 0.7809801207209994, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.008. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "80bd57a0-b28d-406a-9ed1-364dab51851b", "metadata": {"aucs": [0.7739617193760294, 0.7774960700183736, 0.7914825727685951], "final_y": [0.16384972583847923, 0.16154363149907713, 0.14931468168285078]}, "mutation_prompt": null}
{"id": "f5c125bd-3426-4214-8139-8a6ffb4cd965", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)  # New chaotic map\n        while evaluations < self.budget:\n            z = chaotic_map(z)  # Use chaotic map\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adjusted step size\n                alpha = 0.02 * (1.0 + 0.1 * (evaluations / self.budget))  # Adjusted alpha\n                delta = alpha * np.random.standard_normal(self.dim)  # Lévy flight step\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n                if evaluations % (0.1 * self.budget) == 0:  # Reinitialize stagnated particles\n                    diversity = np.std(population, axis=0).mean()  # Compute diversity\n                    reinit_threshold = 75 + 25 * (1 - diversity)  # Adaptive threshold\n                    stagnated = fitness > np.percentile(fitness, reinit_threshold)\n                    population[stagnated] = np.random.uniform(func.bounds.lb, func.bounds.ub, (np.sum(stagnated), self.dim))\n                    fitness[stagnated] = np.apply_along_axis(func, 1, population[stagnated])\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introduce an adaptive random reinitialization threshold for stagnated particles based on current diversity.", "configspace": "", "generation": 16, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('Percentiles must be in the range [0, 100]').", "error": "ValueError('Percentiles must be in the range [0, 100]')", "parent_id": "7214344a-40b0-44a7-bcf2-d0ffd709288f", "metadata": {}, "mutation_prompt": null}
{"id": "d2a5f924-161e-4460-8b9f-fd75f5f44ded", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9\n\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)  # New chaotic map\n        while evaluations < self.budget:\n            z = chaotic_map(z)  # Use chaotic map\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = 0.9 - 0.5 * (np.std(fitness) / np.mean(fitness))  # Dynamic adjustment of w\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adjusted step size\n                alpha = 0.02 * (1.0 + 0.1 * (evaluations / self.budget))  # Adjusted alpha\n                delta = alpha * np.random.standard_normal(self.dim)  # Lévy flight step\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n                if evaluations % (0.1 * self.budget) == 0:  # Reinitialize stagnated particles\n                    stagnated = fitness > np.percentile(fitness, 75)\n                    population[stagnated] = np.random.uniform(func.bounds.lb, func.bounds.ub, (np.sum(stagnated), self.dim))\n                    fitness[stagnated] = np.apply_along_axis(func, 1, population[stagnated])\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Improve convergence by dynamically adjusting the weighting factor 'w' based on fitness diversity.", "configspace": "", "generation": 16, "fitness": 0.7917949107029058, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.792 with standard deviation 0.006. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "7214344a-40b0-44a7-bcf2-d0ffd709288f", "metadata": {"aucs": [0.7977944379677326, 0.7939217410911554, 0.7836685530498291], "final_y": [0.14273170183772632, 0.15455830342537435, 0.16034284083392314]}, "mutation_prompt": null}
{"id": "5dc9dcea-8a4a-4c82-97a7-5cd884216ef1", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)  # New chaotic map\n        while evaluations < self.budget:\n            z = chaotic_map(z)  # Use chaotic map\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)  # Adaptive crossover rate adjustment\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adjusted step size\n                alpha = 0.02 * (1.0 + 0.1 * (evaluations / self.budget))  # Adjusted alpha\n                delta = alpha * np.random.standard_normal(self.dim)  # Lévy flight step\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n                if evaluations % (0.1 * self.budget) == 0:  # Reinitialize stagnated particles\n                    stagnated = fitness > np.percentile(fitness, 75)\n                    population[stagnated] = np.random.uniform(func.bounds.lb, func.bounds.ub, (np.sum(stagnated), self.dim))\n                    fitness[stagnated] = np.apply_along_axis(func, 1, population[stagnated])\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Integrating adaptive crossover rate adjustment to enhance exploration and convergence efficiency.", "configspace": "", "generation": 16, "fitness": 0.7921158244542911, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.792 with standard deviation 0.029. And the mean value of best solutions found was 0.155 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "7214344a-40b0-44a7-bcf2-d0ffd709288f", "metadata": {"aucs": [0.8011239044090488, 0.8218307084714821, 0.7533928604823423], "final_y": [0.14852023706026818, 0.14287685443277087, 0.17424539926430505]}, "mutation_prompt": null}
{"id": "fc967449-b339-4991-82f6-4a9e1813b91b", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)  # New chaotic map\n        while evaluations < self.budget:\n            z = chaotic_map(z)  # Use chaotic map\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant1 = population[a] + F * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = z * (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)  # Adjusted step size\n                alpha = 0.02 * (1.0 + 0.1 * (evaluations / self.budget))  # Adjusted alpha\n                delta = alpha * np.random.standard_normal(self.dim)  # Lévy flight step\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n                if evaluations % (0.1 * self.budget) == 0:  # Reinitialize stagnated particles\n                    stagnated = fitness > np.percentile(fitness, 75)\n                    population[stagnated] = np.random.uniform(func.bounds.lb, func.bounds.ub, (np.sum(stagnated), self.dim))\n                    fitness[stagnated] = np.apply_along_axis(func, 1, population[stagnated])\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Introducing adaptive chaotic factor to enhance exploration.", "configspace": "", "generation": 16, "fitness": 0.7873877937796805, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.787 with standard deviation 0.021. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "7214344a-40b0-44a7-bcf2-d0ffd709288f", "metadata": {"aucs": [0.7672734985707339, 0.8161068954638097, 0.7787829873044979], "final_y": [0.16587534797785586, 0.1443591263479841, 0.16333742870618717]}, "mutation_prompt": null}
{"id": "91f9ee25-290c-4368-a04c-ec7191e370da", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        pop_size = 10 * self.dim\n        F = np.random.uniform(0.5, 0.9)\n        CR = 0.5\n        \n        c1 = 1.5\n        c2 = 1.5\n        w = 0.9 - (0.9 - 0.4) * (self.budget / pop_size)\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        p_best = np.copy(population)\n        p_best_fitness = np.copy(fitness)\n        g_best = population[np.argmin(fitness)]\n        g_best_fitness = np.min(fitness)\n\n        evaluations = pop_size\n\n        z = 0.7\n        chaotic_map = lambda x: np.sin(np.pi * x)\n        while evaluations < self.budget:\n            z = chaotic_map(z)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                F_adaptive = 0.5 + 0.5 * np.random.rand()  # Adaptive factor\n                mutant1 = population[a] + F_adaptive * (population[b] - population[c])\n                F2 = F * (1.2 - 0.4 * z)\n                mutant2 = population[a] + F2 * (population[b] - population[c])\n                mutant = (mutant1 + mutant2) / 2.0\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = np.copy(population[i])\n                CR = 0.5 + 0.4 * z\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                \n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < p_best_fitness[i]:\n                        p_best[i] = trial\n                        p_best_fitness[i] = trial_fitness\n\n                if trial_fitness < g_best_fitness:\n                    g_best = trial\n                    g_best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            c1 = 2.0 - (2.0 - 0.5) * (evaluations / self.budget)\n            c2 = 2.5 - (2.5 - 1.0) * (evaluations / self.budget)\n            \n            r1, r2 = np.random.rand(2)\n            w = (0.9 - (0.9 - 0.4) * (evaluations / self.budget)) * (func.bounds.ub - func.bounds.lb) / self.dim\n            velocity = w * velocity + c1 * r1 * (p_best - population) + c2 * r2 * (g_best - population)\n            velocity *= 0.5 + 0.5 * np.random.rand(pop_size, self.dim)  # Dynamic scaling\n            population = population + velocity\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            for i in range(pop_size):\n                step_size = 0.1 / (1 + 0.01 * evaluations)\n                alpha = 0.02 * (1.0 + 0.1 * (evaluations / self.budget))\n                delta = alpha * np.random.standard_normal(self.dim)\n                new_solution = np.clip(population[i] + delta, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    gradient = (new_fitness - fitness[i]) / (np.linalg.norm(delta) + 1e-8)\n                    refined_solution = np.clip(new_solution - 0.05 * gradient, func.bounds.lb, func.bounds.ub)\n                    refined_fitness = func(refined_solution)\n                    if refined_fitness < fitness[i]:\n                        population[i] = refined_solution\n                        fitness[i] = refined_fitness\n                        if refined_fitness < p_best_fitness[i]:\n                            p_best[i] = refined_solution\n                            p_best_fitness[i] = refined_fitness\n\n                if evaluations % (0.1 * self.budget) == 0:\n                    stagnated = fitness > np.percentile(fitness, 75)\n                    population[stagnated] = np.random.uniform(func.bounds.lb, func.bounds.ub, (np.sum(stagnated), self.dim))\n                    fitness[stagnated] = np.apply_along_axis(func, 1, population[stagnated])\n\n        return g_best", "name": "HybridMetaheuristic", "description": "Integrate adaptive mutation strategy and dynamic velocity scaling to enhance exploration and exploitation balance.", "configspace": "", "generation": 16, "fitness": 0.781820483888676, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.782 with standard deviation 0.007. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "7214344a-40b0-44a7-bcf2-d0ffd709288f", "metadata": {"aucs": [0.7747236135348085, 0.7798464161288602, 0.7908914220023593], "final_y": [0.16084931760149046, 0.16196452426802754, 0.15591944659907886]}, "mutation_prompt": null}
