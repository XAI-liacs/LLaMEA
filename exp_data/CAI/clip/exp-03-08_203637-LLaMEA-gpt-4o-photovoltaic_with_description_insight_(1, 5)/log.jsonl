{"id": "db959279-9afa-409d-bbef-04737007fc91", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        # Initialize a population of solutions\n        pop = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            pop[:, i] = bounds.lb[i] + pop[:, i] * (bounds.ub[i] - bounds.lb[i])\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            for j in range(pop_size):\n                # Mutation\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                # Crossover\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                # Selection\n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        # Gradually increase the complexity (number of layers)\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([bounds.lb[:new_dim], bounds.ub[:new_dim]])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm that combines Differential Evolution for global exploration with a local search strategy, using iterative layer addition and robustness enhancement for optimizing multilayer photovoltaic structures.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\n  File \"<string>\", line 51, in local_search\nAttributeError: 'numpy.ndarray' object has no attribute 'lb'\n.", "error": "AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\n  File \"<string>\", line 51, in local_search\nAttributeError: 'numpy.ndarray' object has no attribute 'lb'\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "620bde5d-06d0-4f28-abfe-7509cd3f1e8c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        # Initialize a population of solutions with normalized bounds\n        lb, ub = bounds.lb, bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            # Adaptive mutation factor\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([bounds.lb[:new_dim], bounds.ub[:new_dim]])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "An improved hybrid metaheuristic algorithm using normalized bounds and adaptive mutating rates to enhance optimization of multilayer photovoltaic structures.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\").", "error": "AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\")", "parent_id": "db959279-9afa-409d-bbef-04737007fc91", "metadata": {}, "mutation_prompt": null}
{"id": "0ffa46ae-e045-493d-97ce-1394f0d04d6a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        # Initialize a population of solutions\n        pop = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            pop[:, i] = bounds.lb[i] + pop[:, i] * (bounds.ub[i] - bounds.lb[i])\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            for j in range(pop_size):\n                # Mutation\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                # Crossover\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                # Selection\n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        # Gradually increase the complexity (number of layers)\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([bounds.lb[:new_dim], bounds.ub[:new_dim]])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "A refined hybrid metaheuristic algorithm enhancing robustness by fixing bounds-related issues during local search and optimizing for photovoltaic structures.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The number of bounds is not compatible with the length of `x0`.').", "error": "ValueError('The number of bounds is not compatible with the length of `x0`.')", "parent_id": "db959279-9afa-409d-bbef-04737007fc91", "metadata": {}, "mutation_prompt": null}
{"id": "cd9e3ac1-5e14-411e-ad3c-54bf2e61665b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        # Initialize a population of solutions\n        pop = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            pop[:, i] = bounds.lb[i] + pop[:, i] * (bounds.ub[i] - bounds.lb[i])\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            for j in range(pop_size):\n                # Mutation\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                # Crossover\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                # Selection\n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        # Gradually increase the complexity (number of layers)\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = type(bounds)(lb=bounds.lb[:new_dim], ub=bounds.ub[:new_dim])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced the local search strategy by correctly handling bounds to avoid errors during optimization of multilayer photovoltaic structures.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The number of bounds is not compatible with the length of `x0`.').", "error": "ValueError('The number of bounds is not compatible with the length of `x0`.')", "parent_id": "db959279-9afa-409d-bbef-04737007fc91", "metadata": {}, "mutation_prompt": null}
{"id": "30414d9d-7874-4bd5-8bfc-b75cfddfe143", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        # Initialize a population of solutions\n        pop = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            pop[:, i] = bounds.lb[i] + pop[:, i] * (bounds.ub[i] - bounds.lb[i])\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            for j in range(pop_size):\n                # Mutation\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                # Crossover\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                # Selection\n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                pop_size = max(10, pop_size // 2)  # Adaptive population size\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        # Gradually increase the complexity (number of layers)\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([bounds.lb[:new_dim], bounds.ub[:new_dim]])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "An improved hybrid metaheuristic algorithm using Differential Evolution and local search, with robust error handling for function bounds and enhanced exploration by adaptive population size.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\").", "error": "AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\")", "parent_id": "db959279-9afa-409d-bbef-04737007fc91", "metadata": {}, "mutation_prompt": null}
{"id": "5b76c2a6-4974-4e15-bf78-48cc236f1153", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        # Initialize a population of solutions\n        pop = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            pop[:, i] = bounds.lb[i] + pop[:, i] * (bounds.ub[i] - bounds.lb[i])\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            for j in range(pop_size):\n                # Mutation\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                # Crossover\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                # Selection\n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(len(x0))], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        # Gradually increase the complexity (number of layers)\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([bounds.lb[:new_dim], bounds.ub[:new_dim]])\n            best_solution = self.local_search(func, best_solution[:new_dim], bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced HybridMetaheuristic algorithm with fixed bounds handling in local search for optimizing multilayer photovoltaic structures.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\").", "error": "AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\")", "parent_id": "db959279-9afa-409d-bbef-04737007fc91", "metadata": {}, "mutation_prompt": null}
{"id": "9920d612-870e-4a22-a46e-33b940b37d80", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        # Initialize a population of solutions with normalized bounds\n        lb, ub = bounds[0], bounds[1]  # Corrected from bounds.lb and bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            # Adaptive mutation factor\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([bounds[0][:new_dim], bounds[1][:new_dim]])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic algorithm with corrected bounds handling for photovoltaic structure optimization.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'ioh.iohcpp.RealBounds' object is not subscriptable\").", "error": "TypeError(\"'ioh.iohcpp.RealBounds' object is not subscriptable\")", "parent_id": "620bde5d-06d0-4f28-abfe-7509cd3f1e8c", "metadata": {}, "mutation_prompt": null}
{"id": "38f918c2-63a1-4ade-af4c-641dba138365", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ImprovedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def adaptive_differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = np.array(bounds['lb']), np.array(bounds['ub'])\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n\n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_refinement(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds['lb'], bounds['ub'])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = {'lb': bounds['lb'][:new_dim], 'ub': bounds['ub'][:new_dim]}\n            best_solution = self.local_refinement(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "ImprovedHybridMetaheuristic", "description": "An enhanced hybrid metaheuristic algorithm that combines adaptive differential evolution with local search while efficiently managing dimensionality and ensuring functional robustness.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'ioh.iohcpp.RealBounds' object is not subscriptable\").", "error": "TypeError(\"'ioh.iohcpp.RealBounds' object is not subscriptable\")", "parent_id": "620bde5d-06d0-4f28-abfe-7509cd3f1e8c", "metadata": {}, "mutation_prompt": null}
{"id": "7c9a86e2-7358-4d23-a75e-4c316c9ededd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = bounds[0], bounds[1]\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = (bounds[0][:new_dim], bounds[1][:new_dim])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced metaheuristic integrating adaptive DE with local refinement for robust optimization of multilayer photovoltaics.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The number of bounds is not compatible with the length of `x0`.').", "error": "ValueError('The number of bounds is not compatible with the length of `x0`.')", "parent_id": "620bde5d-06d0-4f28-abfe-7509cd3f1e8c", "metadata": {}, "mutation_prompt": null}
{"id": "98f9ab84-3389-4c7c-87c7-1949093445c5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = bounds['lb'], bounds['ub']  # Fixed bounds dictionary\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n\n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n\n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n\n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds['lb'], bounds['ub'])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = {'lb': func.bounds.lb, 'ub': func.bounds.ub}  # Fixed bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = {'lb': bounds['lb'][:new_dim], 'ub': bounds['ub'][:new_dim]}  # Adjusted bounds\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "A refined hybrid metaheuristic algorithm using fixed bounds, population diversity, and improved local search for optimizing multilayer photovoltaic structures.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The number of bounds is not compatible with the length of `x0`.').", "error": "ValueError('The number of bounds is not compatible with the length of `x0`.')", "parent_id": "620bde5d-06d0-4f28-abfe-7509cd3f1e8c", "metadata": {}, "mutation_prompt": null}
{"id": "a661517a-711a-4c0a-a063-bc5a353b3bed", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        # Initialize a population of solutions with normalized bounds\n        lb, ub = bounds['lb'], bounds['ub']\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            # Adaptive mutation factor\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds['lb'], bounds['ub'])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = {'lb': func.bounds.lb, 'ub': func.bounds.ub}\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([bounds['lb'][:new_dim], bounds['ub'][:new_dim]])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "An improved hybrid metaheuristic algorithm using adaptive mutating rates and correct bounds handling to enhance optimization of multilayer photovoltaic structures.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices').", "error": "IndexError('only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices')", "parent_id": "620bde5d-06d0-4f28-abfe-7509cd3f1e8c", "metadata": {}, "mutation_prompt": null}
{"id": "172bd5df-8001-42ee-833a-827782d38d4a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub  # Fixed bounds handling\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(func.bounds.lb, func.bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb[:new_dim], func.bounds.ub[:new_dim]])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic that combines differential evolution with adaptive mutation and local optimization, incorporating bounds handling and noise resilience.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The number of bounds is not compatible with the length of `x0`.').", "error": "ValueError('The number of bounds is not compatible with the length of `x0`.')", "parent_id": "9920d612-870e-4a22-a46e-33b940b37d80", "metadata": {}, "mutation_prompt": null}
{"id": "0d2b7ac8-e8b4-498b-a081-c88adb44c1fb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)  # Corrected bounds handling\n        pop_size = min(pop_size, self.budget - self.evaluations)  # Dynamic population size\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([bounds.lb[:new_dim], bounds.ub[:new_dim]])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improved hybrid metaheuristic algorithm with adaptive bounds handling and dynamic population size.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The number of bounds is not compatible with the length of `x0`.').", "error": "ValueError('The number of bounds is not compatible with the length of `x0`.')", "parent_id": "9920d612-870e-4a22-a46e-33b940b37d80", "metadata": {}, "mutation_prompt": null}
{"id": "8eb71dce-d0e6-49ea-a9fb-b42972f49b68", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = np.array(bounds.lb), np.array(bounds.ub)  # Fixed error in bounds handling\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n\n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial) + np.random.normal(0, 0.1)  # Added noise handling\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')  # Corrected bounds handling\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([bounds.lb[:new_dim], bounds.ub[:new_dim]])  # Corrected bounds handling\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Refined hybrid metaheuristic incorporating adaptive population size and noise handling for photovoltaic optimization.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\").", "error": "AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\")", "parent_id": "9920d612-870e-4a22-a46e-33b940b37d80", "metadata": {}, "mutation_prompt": null}
{"id": "83a98218-c487-4e5e-8fac-a015b2449d56", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        # Initialize a population of solutions with normalized bounds\n        lb, ub = bounds.lb, bounds.ub  # Corrected the usage of bounds\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            # Adaptive mutation factor\n            F = 0.5 + 0.3 * np.random.rand()  # Added randomness to F for exploration\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=zip(bounds.lb, bounds.ub), method='L-BFGS-B')  # Adjusted bounds usage\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([bounds.lb[:new_dim], bounds.ub[:new_dim]])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improved hybrid metaheuristic with adaptive DE and corrected bounds handling for photovoltaic optimization.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\").", "error": "AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\")", "parent_id": "9920d612-870e-4a22-a46e-33b940b37d80", "metadata": {}, "mutation_prompt": null}
{"id": "aa961a0a-34f0-4224-ba19-1e6af21f6e9b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        # Initialize a population of solutions with normalized bounds\n        lb, ub = bounds.lb, bounds.ub  # Corrected from bounds[0] and bounds[1]\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            # Adaptive mutation factor\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            # Adaptive crossover rate\n            CR = 0.5 + 0.5 * (self.evaluations / self.budget)\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([bounds.lb[:new_dim], bounds.ub[:new_dim]])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improved hybrid metaheuristic algorithm with adaptive crossover for photovoltaic structure optimization.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\").", "error": "AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\")", "parent_id": "9920d612-870e-4a22-a46e-33b940b37d80", "metadata": {}, "mutation_prompt": null}
{"id": "c3c61801-e8c2-45b9-95df-db183b225e01", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub  # Fixed bounds handling\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb[:new_dim], func.bounds.ub[:new_dim]])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improved hybrid metaheuristic with correct dimensional handling during local search.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The number of bounds is not compatible with the length of `x0`.').", "error": "ValueError('The number of bounds is not compatible with the length of `x0`.')", "parent_id": "172bd5df-8001-42ee-833a-827782d38d4a", "metadata": {}, "mutation_prompt": null}
{"id": "1f3a6418-e15f-46f3-b1e9-b122676956c6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub  # Fixed bounds handling\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(func.bounds.lb, func.bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb[:new_dim], func.bounds.ub[:new_dim]])\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)  # Fix\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with fixed bound checks for local search to improve convergence.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The number of bounds is not compatible with the length of `x0`.').", "error": "ValueError('The number of bounds is not compatible with the length of `x0`.')", "parent_id": "172bd5df-8001-42ee-833a-827782d38d4a", "metadata": {}, "mutation_prompt": null}
{"id": "efb6b410-6094-4138-ac43-746808c96220", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(func.bounds.lb, func.bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb[:new_dim], func.bounds.ub[:new_dim]]).T  # Fixed bounds issue\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)  # Updated sub_bounds usage\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "A refined hybrid metaheuristic using differential evolution with adaptive parameters and local optimization, enhancing convergence and noise handling.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The number of bounds is not compatible with the length of `x0`.').", "error": "ValueError('The number of bounds is not compatible with the length of `x0`.')", "parent_id": "172bd5df-8001-42ee-833a-827782d38d4a", "metadata": {}, "mutation_prompt": null}
{"id": "1100cd5b-8131-424d-a85f-6002f9f3d003", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub  # Fixed bounds handling\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb[:new_dim], func.bounds.ub[:new_dim]])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic combining differential evolution with adaptive mutation and local optimization, improved with dynamic dimensional growth and robust bounds handling.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The number of bounds is not compatible with the length of `x0`.').", "error": "ValueError('The number of bounds is not compatible with the length of `x0`.')", "parent_id": "172bd5df-8001-42ee-833a-827782d38d4a", "metadata": {}, "mutation_prompt": null}
{"id": "fadf8484-7b33-41d8-9974-5b665676d98a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub  # Fixed bounds handling\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb[:self.dim], func.bounds.ub[:self.dim]])\n            best_solution = self.local_search(func, best_solution, sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "A refined hybrid metaheuristic with fixed dimensionality for accurate bounds handling in local search.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The number of bounds is not compatible with the length of `x0`.').", "error": "ValueError('The number of bounds is not compatible with the length of `x0`.')", "parent_id": "172bd5df-8001-42ee-833a-827782d38d4a", "metadata": {}, "mutation_prompt": null}
{"id": "0112f770-ccfc-49e9-ac3e-bc4f2efbcb6f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub  # Fixed bounds handling\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        for _ in range(self.budget // 2):  # Use half budget for DE\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        full_bounds = [(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)]  # Correct bounds\n        res = minimize(func, x0, bounds=full_bounds, method='L-BFGS-B')  # Update bounds\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb[:new_dim], func.bounds.ub[:new_dim]])\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], func.bounds)  # Updated bounds\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic integrating adaptive dimensional exploration and local refinement with fixed bounds handling.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The number of bounds is not compatible with the length of `x0`.').", "error": "ValueError('The number of bounds is not compatible with the length of `x0`.')", "parent_id": "c3c61801-e8c2-45b9-95df-db183b225e01", "metadata": {}, "mutation_prompt": null}
{"id": "2e04359d-fa8d-4811-b9b8-784a3792b9c6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = [bounds.lb[:new_dim], bounds.ub[:new_dim]]  # Fixed bounds handling\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "An optimized hybrid metaheuristic algorithm that combines adaptive differential evolution with dimensional-aware local search for effective exploration and exploitation.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The number of bounds is not compatible with the length of `x0`.').", "error": "ValueError('The number of bounds is not compatible with the length of `x0`.')", "parent_id": "c3c61801-e8c2-45b9-95df-db183b225e01", "metadata": {}, "mutation_prompt": null}
{"id": "451f0378-c745-46fc-a9f3-ddf57f3e5180", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb[:new_dim], func.bounds.ub[:new_dim]])\n            best_solution = np.pad(best_solution[:new_dim], (0, self.dim - new_dim), constant_values=(best_solution[new_dim-1],))  # Padding for compatibility\n            best_solution = self.local_search(func, best_solution, (func.bounds.lb, func.bounds.ub))\n            \n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Refined hybrid metaheuristic with improved dimensional handling during local search.", "configspace": "", "generation": 5, "fitness": 0.8106624207816715, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.008. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "c3c61801-e8c2-45b9-95df-db183b225e01", "metadata": {"aucs": [0.8100981214166267, 0.8008981304786591, 0.8209910104497288], "final_y": [0.14058425547945974, 0.1405068478010686, 0.13399092792203615]}, "mutation_prompt": null}
{"id": "6fc4b756-d2ee-4d68-9070-f826042f589f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub  # Fixed bounds handling\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0][:len(x0)], bounds[1][:len(x0)])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb[:new_dim], func.bounds.ub[:new_dim]])\n            best_solution = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced dimensional handling in local search to ensure compatibility with x0 and bounds.", "configspace": "", "generation": 5, "fitness": 0.7972970226754909, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.029. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "c3c61801-e8c2-45b9-95df-db183b225e01", "metadata": {"aucs": [0.8382336765214403, 0.7811762229266884, 0.7724811685783441], "final_y": [0.12820451857982862, 0.15789231576132956, 0.15735232602188975]}, "mutation_prompt": null}
{"id": "aef1874a-338a-453b-9acb-9aa255d55522", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub  # Fixed bounds handling\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]  # Corrected sub-bounds handling\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)  # Corrected assignment\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Refined hybrid metaheuristic algorithm with corrected dimensional handling for local search.", "configspace": "", "generation": 5, "fitness": 0.8141498776688468, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.814 with standard deviation 0.009. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "c3c61801-e8c2-45b9-95df-db183b225e01", "metadata": {"aucs": [0.8258370654788452, 0.8027380329332134, 0.8138745345944816], "final_y": [0.13641569873824666, 0.14173949633471217, 0.14163742103051358]}, "mutation_prompt": null}
{"id": "d7ac361d-edcf-464a-8e10-f96ddb120d67", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub  # Fixed bounds handling\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            CR = 0.5 + 0.4 * np.random.rand()  # Adaptive crossover rate scaling\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]  # Corrected sub-bounds handling\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)  # Corrected assignment\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introducing adaptive crossover rate scaling in differential evolution for enhanced exploration-exploitation balance.", "configspace": "", "generation": 6, "fitness": 0.7825945655604388, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.021. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.019.", "error": "", "parent_id": "aef1874a-338a-453b-9acb-9aa255d55522", "metadata": {"aucs": [0.8000966753896885, 0.7946238333350429, 0.7530631879565849], "final_y": [0.12572506742249479, 0.16019875421321128, 0.1695618667476202]}, "mutation_prompt": null}
{"id": "075fa0e4-4162-4c26-aac8-9b3ed7d35cb1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub  # Fixed bounds handling\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial) + 0.01 * np.std(trial)  # Added robustness metric\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim] \n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive mutation, scaling factors, and perturbation robustness.", "configspace": "", "generation": 6, "fitness": 0.762803696439415, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.763 with standard deviation 0.005. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "aef1874a-338a-453b-9acb-9aa255d55522", "metadata": {"aucs": [0.7568718803180365, 0.7688268437993733, 0.7627123652008354], "final_y": [0.16495985208452868, 0.1680008142434094, 0.1479518799856132]}, "mutation_prompt": null}
{"id": "66ac2259-500c-4a2b-b532-58900cb53bef", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            \n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            # Adaptive update frequency for local search\n            if self.evaluations >= self.budget * 0.75:  \n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic with adaptive local search for improved exploration-exploitation balance.", "configspace": "", "generation": 6, "fitness": 0.7827941725683983, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.013. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "aef1874a-338a-453b-9acb-9aa255d55522", "metadata": {"aucs": [0.8009731267187488, 0.7733917602939012, 0.7740176306925453], "final_y": [0.14778297749053826, 0.16668420928882455, 0.16497773145012729]}, "mutation_prompt": null}
{"id": "2c6a3ea8-aada-4ee2-879a-80e87cd035b9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced local refinement by utilizing a dynamic step size adjustment in Differential Evolution.", "configspace": "", "generation": 6, "fitness": 0.795017767664473, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.795 with standard deviation 0.007. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "aef1874a-338a-453b-9acb-9aa255d55522", "metadata": {"aucs": [0.8003614986146126, 0.798851260529212, 0.7858405438495943], "final_y": [0.12950254703973363, 0.1572963916009924, 0.1506982917324281]}, "mutation_prompt": null}
{"id": "4257422a-77be-45d2-987c-66ada44538e8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=25, F=0.7, CR=0.9):  # Adjusted pop_size and F for better exploration\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n\n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n\n        return pop[best_idx]\n\n    def adaptive_local_search(self, func, x0, bounds, epsilon=1e-6):  # Improved local search with adaptive stopping criteria\n        options = {'gtol': epsilon, 'disp': False}\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', options=options)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 8)  # Adjusted step for gradual complexity\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.adaptive_local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic algorithm with adaptive local search and better convergence strategies.", "configspace": "", "generation": 6, "fitness": 0.7791184827168646, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.779 with standard deviation 0.016. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "aef1874a-338a-453b-9acb-9aa255d55522", "metadata": {"aucs": [0.8014090901269827, 0.7730591540331264, 0.7628872039904847], "final_y": [0.14088995003097438, 0.1670251507209779, 0.1453515519130848]}, "mutation_prompt": null}
{"id": "60bb80df-dc87-4394-9473-cda895a46874", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            # Gradually increase population size\n            pop_size = 20 + (self.evaluations // (self.budget // 5))\n\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a gradual increase in population size to enhance exploration in Differential Evolution.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "2c6a3ea8-aada-4ee2-879a-80e87cd035b9", "metadata": {}, "mutation_prompt": null}
{"id": "3ffabc3d-04e2-49d1-934a-c6b78e9b766c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive mutation strategy and refine stopping criteria based on fitness improvement rate.", "configspace": "", "generation": 7, "fitness": 0.833930870938222, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.834 with standard deviation 0.042. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "2c6a3ea8-aada-4ee2-879a-80e87cd035b9", "metadata": {"aucs": [0.8936630451897606, 0.7994067982844808, 0.8087227693404246], "final_y": [0.1217013630760494, 0.1316522879148465, 0.1369718805950858]}, "mutation_prompt": null}
{"id": "d6127937-26a1-403a-ae1b-983d67db3a60", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.4 * (self.budget - self.evaluations) / self.budget  # Adjust F scaling factor\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            CR = 0.8 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improved hybrid metaheuristic by refining initialization and dynamic step size adjustment.", "configspace": "", "generation": 7, "fitness": 0.8044948240534616, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.009. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "2c6a3ea8-aada-4ee2-879a-80e87cd035b9", "metadata": {"aucs": [0.7925257051088543, 0.8116318308220188, 0.8093269362295116], "final_y": [0.13392465192462155, 0.13186582916971568, 0.13945237037930525]}, "mutation_prompt": null}
{"id": "10c2fb35-3b2c-4d1d-90cb-6c0f6e57caa8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n            pop_size = max(10, int(pop_size * 0.99))  # Adaptive population size reduction\n\n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Dynamic step size and adaptive population scaling enhance exploration and local refinement in Differential Evolution. ", "configspace": "", "generation": 7, "fitness": 0.8097774912927053, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.002. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2c6a3ea8-aada-4ee2-879a-80e87cd035b9", "metadata": {"aucs": [0.8093512249501132, 0.8127450339730529, 0.8072362149549501], "final_y": [0.12951511310815733, 0.12944289054944114, 0.13046570238428168]}, "mutation_prompt": null}
{"id": "df7e13c7-42d4-40ed-8f9a-0a473e1e607a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            # Adaptively adjust population size\n            pop_size = max(5, int(20 * (self.budget - self.evaluations) / self.budget))\n            new_pop = np.empty((pop_size, self.dim))\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = np.copy(new_pop)\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Incorporate adaptive population size in Differential Evolution to enhance exploration and convergence.", "configspace": "", "generation": 7, "fitness": 0.8103649923458661, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.017. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "2c6a3ea8-aada-4ee2-879a-80e87cd035b9", "metadata": {"aucs": [0.7859157115491537, 0.8242294472037321, 0.8209498182847123], "final_y": [0.1479228816054332, 0.12588403776602064, 0.1392919141887924]}, "mutation_prompt": null}
{"id": "32aa8bc1-b695-4a74-abc2-0f4799518f1c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))  # Dynamic population size\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance search efficiency by adapting population size dynamically in response to fitness improvement rates.", "configspace": "", "generation": 8, "fitness": 0.9009836773663169, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.901 with standard deviation 0.023. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "3ffabc3d-04e2-49d1-934a-c6b78e9b766c", "metadata": {"aucs": [0.8721015920058546, 0.9284088355978575, 0.9024406044952387], "final_y": [0.12896189380515022, 0.11441274513710575, 0.11967222108355502]}, "mutation_prompt": null}
{"id": "77c25be8-0919-4caa-963c-b888a6ddfe18", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n            pop_size = max(10, int(20 - 10 * (self.evaluations / self.budget)))  # Dynamically adjust population size\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance adaptive mutation by dynamically adjusting population size alongside crossover rate.", "configspace": "", "generation": 8, "fitness": 0.81972970611836, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.820 with standard deviation 0.065. And the mean value of best solutions found was 0.149 (0. is the best) with standard deviation 0.025.", "error": "", "parent_id": "3ffabc3d-04e2-49d1-934a-c6b78e9b766c", "metadata": {"aucs": [0.7323451233780615, 0.8892163349656502, 0.8376276600113686], "final_y": [0.1824235982322039, 0.12303425715673211, 0.14156719155774822]}, "mutation_prompt": null}
{"id": "3c4980c2-f0c2-4b00-b074-7491b5db304d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            current_pop_size = pop_size + int(5 * (self.budget - self.evaluations) / self.budget)\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial) + np.random.normal(0, 0.01)  # Handle noise\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration using adaptive population size and introduce a noise-handling mechanism for better robustness under realistic conditions.", "configspace": "", "generation": 8, "fitness": 0.8700493170325023, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.034. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "3ffabc3d-04e2-49d1-934a-c6b78e9b766c", "metadata": {"aucs": [0.8850451148838229, 0.8226622317184453, 0.9024406044952387], "final_y": [0.1202582575495944, 0.1479807139709185, 0.11967222108355502]}, "mutation_prompt": null}
{"id": "ce64ad1a-3014-42d2-b50e-937309584dc0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            pop_size = int(max(10, 20 * (self.budget - self.evaluations) / self.budget))  # Adjust pop size based on remaining budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration phase by varying population size based on remaining budget.", "configspace": "", "generation": 8, "fitness": 0.8857028557733782, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.043. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3ffabc3d-04e2-49d1-934a-c6b78e9b766c", "metadata": {"aucs": [0.8262591272270383, 0.9284088355978575, 0.9024406044952387], "final_y": [0.13435548959620458, 0.11441274513710575, 0.11967222108355502]}, "mutation_prompt": null}
{"id": "7220eb14-e0f3-4c12-8c05-7289e0b73a18", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            pop_size = max(10, int(pop_size * (1 - 0.5 * self.evaluations / self.budget)))  # Dynamic adjustment\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce dynamic population size adjustment based on fitness improvement.", "configspace": "", "generation": 8, "fitness": 0.8764047671799989, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.029. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "3ffabc3d-04e2-49d1-934a-c6b78e9b766c", "metadata": {"aucs": [0.891092688341729, 0.8356810087030291, 0.9024406044952387], "final_y": [0.12514549852991097, 0.13328878527573285, 0.11967222108355502]}, "mutation_prompt": null}
{"id": "bd9e1680-75a0-49b8-bffb-3d4c3fe3c54e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))  # Dynamic population size\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive mutation within DE to enhance exploration and convergence.", "configspace": "", "generation": 9, "fitness": 0.9059658705185646, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.906 with standard deviation 0.018. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "32aa8bc1-b695-4a74-abc2-0f4799518f1c", "metadata": {"aucs": [0.8859611855872688, 0.9296951313571258, 0.9022412946112993], "final_y": [0.1233091431592015, 0.11440437270952741, 0.11967222108355502]}, "mutation_prompt": null}
{"id": "909ff0a2-9c45-436f-aa18-f682decbd7db", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))  # Dynamic population size\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n            F = F + 0.1 * np.random.randn()  # Dynamic mutation strategy\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Combine adaptive differential evolution with a dynamic mutation strategy to enhance exploration and prevent premature convergence.", "configspace": "", "generation": 9, "fitness": 0.8800496513451983, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.023. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "32aa8bc1-b695-4a74-abc2-0f4799518f1c", "metadata": {"aucs": [0.848832368534933, 0.8890752908893627, 0.9022412946112993], "final_y": [0.13200058683828042, 0.12303425715673211, 0.11967222108355502]}, "mutation_prompt": null}
{"id": "22de6f38-f9a6-4c9a-af07-a22cd5e56874", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))  # Dynamic population size\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n            F = 0.5 + 0.5 * (best_score / (best_score + 0.1))  # Adjust mutation step size\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance population diversity by adjusting mutation strategy dynamically based on search progress.", "configspace": "", "generation": 9, "fitness": 0.8730429391489759, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.047. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "32aa8bc1-b695-4a74-abc2-0f4799518f1c", "metadata": {"aucs": [0.8067375710732194, 0.9101499517624092, 0.9022412946112993], "final_y": [0.15161879790947075, 0.11759908158200982, 0.11967222108355502]}, "mutation_prompt": null}
{"id": "ac7721c1-285d-4f21-90be-cfce256f1f3f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))  # Dynamic population size\n            CR = 0.9 - 0.6 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve search efficiency by dynamically adjusting both population size and crossover rate based on fitness improvement rates and runtime progress.", "configspace": "", "generation": 9, "fitness": 0.8814143959328095, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.881 with standard deviation 0.021. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "32aa8bc1-b695-4a74-abc2-0f4799518f1c", "metadata": {"aucs": [0.8529266022977664, 0.8890752908893627, 0.9022412946112993], "final_y": [0.13435739264851854, 0.12303425715673211, 0.11967222108355502]}, "mutation_prompt": null}
{"id": "ed433d65-beff-488a-8834-f83170bc6831", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.4 * (self.budget - self.evaluations) / self.budget  # Adjusted mutation factor\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))  # Dynamic population size\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve search efficiency by dynamically adjusting both the mutation factor and crossover rate based on evaluations and fitness improvements.", "configspace": "", "generation": 9, "fitness": 0.8971642578039738, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.897 with standard deviation 0.006. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "32aa8bc1-b695-4a74-abc2-0f4799518f1c", "metadata": {"aucs": [0.9001761879112595, 0.8890752908893627, 0.9022412946112993], "final_y": [0.11466007103965759, 0.12303425715673211, 0.11967222108355502]}, "mutation_prompt": null}
{"id": "749d5e13-be67-4a4a-b678-f769f19bbaaa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mean_dist = np.mean(np.linalg.norm(pop - np.mean(pop, axis=0), axis=1)) # Diversity measurement\n                adaptive_F = F * (1 + np.random.normal(0, 0.1)) * (1 + 0.1 * mean_dist)  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))  # Dynamic population size\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by introducing a diversity-based mutation scaling factor in DE.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "bd9e1680-75a0-49b8-bffb-3d4c3fe3c54e", "metadata": {}, "mutation_prompt": null}
{"id": "11fbc9fe-7a62-459e-bb9a-bdc08779f55f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j]) + np.random.normal(0, 0.01, self.dim)  # Added Gaussian noise\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))  # Dynamic population size\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration of Differential Evolution by adding Gaussian noise to the trial vector.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "bd9e1680-75a0-49b8-bffb-3d4c3fe3c54e", "metadata": {}, "mutation_prompt": null}
{"id": "d8869791-8442-48f6-8936-4ca33e103ec7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))  # Dynamic population size\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive mutation within DE and dynamically adjust strategy parameters to enhance exploration and convergence.", "configspace": "", "generation": 10, "fitness": 0.7791893625832046, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.779 with standard deviation 0.045. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.019.", "error": "", "parent_id": "bd9e1680-75a0-49b8-bffb-3d4c3fe3c54e", "metadata": {"aucs": [0.8258255608538296, 0.7929694781633276, 0.7187730487324568], "final_y": [0.14731858399375053, 0.15816158606213115, 0.19103396660959382]}, "mutation_prompt": null}
{"id": "0c9809c6-85bc-4b12-b2d9-d18b60c0aa86", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            # Slight adjustment to population size calculation for improved convergence\n            pop_size = max(10, int(pop_size * (1 + (best_score - trial_score) / best_score)))  # Dynamic population size\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance dynamic population size adjustment in DE for improved convergence rate.", "configspace": "", "generation": 10, "fitness": 0.7354643587253539, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.735 with standard deviation 0.023. And the mean value of best solutions found was 0.183 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "bd9e1680-75a0-49b8-bffb-3d4c3fe3c54e", "metadata": {"aucs": [0.7193151072105217, 0.7683049202330825, 0.7187730487324575], "final_y": [0.18700471314634548, 0.1702433386818767, 0.19103396660959382]}, "mutation_prompt": null}
{"id": "21df7625-8506-49af-a06b-e36f19869ae1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            decay_factor = 1 - (self.evaluations / self.budget)  # New line added\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1) * decay_factor)  # Modified line\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))  # Dynamic population size\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a decay factor to modulate the adaptive mutation factor in DE to improve exploration and convergence balance.", "configspace": "", "generation": 10, "fitness": 0.7784996552771298, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.044. And the mean value of best solutions found was 0.162 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "bd9e1680-75a0-49b8-bffb-3d4c3fe3c54e", "metadata": {"aucs": [0.8237564389356049, 0.7929694781633276, 0.7187730487324568], "final_y": [0.13802445372212235, 0.15816158606213115, 0.19103396660959382]}, "mutation_prompt": null}
{"id": "7503552a-1cbb-45d0-9ce5-2514526f3584", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        kernel = C(1.0, (1e-3, 1e3)) * RBF(self.dim, (1e-2, 1e2))\n        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            X_sample, y_sample = pop, np.array([func(ind) for ind in pop])\n            gp.fit(X_sample, y_sample)\n\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            CR = 0.9 - 0.4 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover and integrate a Bayesian surrogate model for enhanced exploration and exploitation balance.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "d8869791-8442-48f6-8936-4ca33e103ec7", "metadata": {}, "mutation_prompt": null}
{"id": "c7f3e20c-496f-4738-ac9f-3a589f107981", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))  # Adaptive mutation factor\n                perturbation = np.random.normal(0, 0.05, self.dim)  # Added perturbation term\n                mutant = np.clip(a + adaptive_F * (b - c) + perturbation, lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))  # Dynamic population size\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve exploration by dynamically adjusting the mutation factor and introducing a perturbation term.", "configspace": "", "generation": 11, "fitness": 0.8569662990732624, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.075. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "d8869791-8442-48f6-8936-4ca33e103ec7", "metadata": {"aucs": [0.7510259744813577, 0.9139248139092924, 0.9059481088291369], "final_y": [0.17289264707558472, 0.11767803717437775, 0.11827082670590416]}, "mutation_prompt": null}
{"id": "24e38be2-e399-44b0-945a-bd233abe6cc2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget  # Dynamic weighting factor\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic weighting factor for the trial solution to balance exploration and exploitation.", "configspace": "", "generation": 11, "fitness": 0.8641210621490484, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.007. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "d8869791-8442-48f6-8936-4ca33e103ec7", "metadata": {"aucs": [0.8568470370384088, 0.8736516383344164, 0.8618645110743199], "final_y": [0.13580780952856997, 0.12835362605630374, 0.1286604486570846]}, "mutation_prompt": null}
{"id": "5f5073e7-a21a-415f-a64e-2c1c334f208c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))  # Dynamic population size\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', options={'maxiter': 50})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive scaling into the local search to improve convergence towards optimal solutions.", "configspace": "", "generation": 11, "fitness": 0.8342715880525559, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.834 with standard deviation 0.020. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "d8869791-8442-48f6-8936-4ca33e103ec7", "metadata": {"aucs": [0.8258255608538296, 0.8151246922295183, 0.8618645110743199], "final_y": [0.14731858399375053, 0.14830884668523447, 0.1286604486570846]}, "mutation_prompt": null}
{"id": "6f7786f6-f1a2-4e89-9d91-493a4ae2b4e4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))  # Adaptive mutation factor\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                trial = np.where(crossover, mutant, pop[j])\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))  # Dynamic population size\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)  # Adjust crossover rate based on progress\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:  # Stop if no significant improvement\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', options={'ftol': 1e-9})\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance HybridMetaheuristic by incorporating adaptive selection pressure and refined local search.", "configspace": "", "generation": 11, "fitness": 0.8533718678919803, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.853 with standard deviation 0.038. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "d8869791-8442-48f6-8936-4ca33e103ec7", "metadata": {"aucs": [0.8948141209151252, 0.8034369716864956, 0.8618645110743199], "final_y": [0.11897047909077452, 0.15176828839447665, 0.1286604486570846]}, "mutation_prompt": null}
{"id": "66588058-ef5a-4d2b-9050-714b8af001b8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            inertia = 0.4 + 0.2 * (self.evaluations / self.budget)  # Added inertia term\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, (pop[j] * weight + mutant * (1 - weight)) * inertia)\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance adaptive differential evolution with a novel inertia term for better convergence.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 23 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 23 is out of bounds for axis 0 with size 20')", "parent_id": "24e38be2-e399-44b0-945a-bd233abe6cc2", "metadata": {}, "mutation_prompt": null}
{"id": "7939eebf-df67-4446-92df-231d0ef082c7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget  # Dynamic weighting factor\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)\n            F = 0.5 + 0.3 * (np.std(pop) / (ub - lb))  # Adjust mutation factor based on diversity\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by adjusting the mutation factor dynamically based on population diversity.", "configspace": "", "generation": 12, "fitness": 0.8415058752853607, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.842 with standard deviation 0.072. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.025.", "error": "", "parent_id": "24e38be2-e399-44b0-945a-bd233abe6cc2", "metadata": {"aucs": [0.9367819657969468, 0.8257812323911164, 0.7619544276680188], "final_y": [0.11059852594539665, 0.14522421411878506, 0.17110862100519142]}, "mutation_prompt": null}
{"id": "cb2bb232-73d7-4a11-a8dd-3b77183ddc86", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR + 0.05 * (func(pop[j]) / best_score)\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget  # Dynamic weighting factor\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability and weighted average for trial solutions based on fitness to improve convergence.", "configspace": "", "generation": 12, "fitness": 0.839436450941362, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.839 with standard deviation 0.071. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "24e38be2-e399-44b0-945a-bd233abe6cc2", "metadata": {"aucs": [0.7406473424488806, 0.8717139015460682, 0.9059481088291369], "final_y": [0.17815703004186734, 0.12687551224116334, 0.11827082670590416]}, "mutation_prompt": null}
{"id": "617d7e74-9e57-4269-8552-1869158e11d7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget  # Dynamic weighting factor\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce dynamic CR and step size adaptation for improved exploration-exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.8680696341524788, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.004. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "24e38be2-e399-44b0-945a-bd233abe6cc2", "metadata": {"aucs": [0.8706304898370485, 0.8717139015460682, 0.8618645110743199], "final_y": [0.1299086490165986, 0.12687551224116334, 0.1286604486570846]}, "mutation_prompt": null}
{"id": "6d0be7c9-c324-40c7-a5bc-fc99696ff348", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR * (1 + np.random.normal(0, 0.1)))\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget  # Dynamic weighting factor\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the exploration capability by adjusting the crossover rate dynamically based on the evaluations.", "configspace": "", "generation": 12, "fitness": 0.85453855233876, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.025. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "24e38be2-e399-44b0-945a-bd233abe6cc2", "metadata": {"aucs": [0.8210859367359569, 0.8806652092060032, 0.8618645110743199], "final_y": [0.14731965055725926, 0.12416517787843007, 0.1286604486570846]}, "mutation_prompt": null}
{"id": "cf2a9dab-6e80-44f7-93f9-eecf2e9ee2ae", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                \n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(5, int(pop_size * (1 + 0.5 * (best_score - func(pop[best_idx])) / best_score))) # Adjust population size\n\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1) * (1 - (best_score - func(pop[best_idx])) / best_score) # Adaptive CR\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by dynamically adjusting population size and introduce adaptive CR based on performance improvement.", "configspace": "", "generation": 13, "fitness": 0.8179229225766941, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.031. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "617d7e74-9e57-4269-8552-1869158e11d7", "metadata": {"aucs": [0.7902317081904907, 0.8018442139971824, 0.8616928455424093], "final_y": [0.15900452607995708, 0.15612295991425473, 0.1286604486570846]}, "mutation_prompt": null}
{"id": "18c2ea57-8a0a-4bb8-bd12-0a1efac16566", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR + 0.1 * np.sin(2 * np.pi * self.evaluations / self.budget))\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget  # Dynamic weighting factor\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + np.random.normal(0.1, 0.05) * ((best_score - func(pop[best_idx])) / best_score))))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce time-varying crossover rates and adaptive population resizing for enhanced convergence.", "configspace": "", "generation": 13, "fitness": 0.8576610562985977, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.858 with standard deviation 0.013. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "617d7e74-9e57-4269-8552-1869158e11d7", "metadata": {"aucs": [0.8397041141264039, 0.8715862092269799, 0.8616928455424093], "final_y": [0.13886469264458812, 0.12687551224116334, 0.1286604486570846]}, "mutation_prompt": null}
{"id": "ceb6928a-0664-488c-bb45-10c089e509ac", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR + 0.05 * np.random.normal(0, 1))  # Adjusted line\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget  # Dynamic weighting factor\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance dynamic adaptation by incorporating Gaussian noise into crossover probability for improved search diversity.", "configspace": "", "generation": 13, "fitness": 0.823043778114199, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.823 with standard deviation 0.062. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "617d7e74-9e57-4269-8552-1869158e11d7", "metadata": {"aucs": [0.7358522795732075, 0.8715862092269799, 0.8616928455424093], "final_y": [0.17775119071341117, 0.12687551224116334, 0.1286604486570846]}, "mutation_prompt": null}
{"id": "ec16e474-b800-45e4-a308-52d7c3933658", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n\n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n\n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget  \n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n\n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n\n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / max(best_score, 1e-8))))  # Added noise-handling\n\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Integrate adaptive population size adjustment with noise-handling measures to enhance optimization efficacy.", "configspace": "", "generation": 13, "fitness": 0.865248639121235, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.865 with standard deviation 0.034. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "617d7e74-9e57-4269-8552-1869158e11d7", "metadata": {"aucs": [0.8258255608538296, 0.9082275109674658, 0.8616928455424093], "final_y": [0.14731858399375053, 0.1133895226264886, 0.1286604486570846]}, "mutation_prompt": null}
{"id": "67e7e441-434d-4166-aa5f-eb4f6250b572", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget  # Dynamic weighting factor\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce an adaptive population size strategy to enhance exploration and exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.8679417630346412, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.010. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "617d7e74-9e57-4269-8552-1869158e11d7", "metadata": {"aucs": [0.8543456079101462, 0.8715862092269799, 0.8778934719667976], "final_y": [0.1331696937194614, 0.12687551224116334, 0.12306682790991319]}, "mutation_prompt": null}
{"id": "11958aee-5629-43f6-a147-e768977bc82a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget  # Dynamic weighting factor\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            stochastic_factor = np.random.uniform(0.9, 1.1)  # New stochastic component\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score) * stochastic_factor))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Implement a stochastic component in adaptive population size strategy for better exploration.", "configspace": "", "generation": 14, "fitness": 0.8354569658838328, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.835 with standard deviation 0.071. And the mean value of best solutions found was 0.143 (0. is the best) with standard deviation 0.027.", "error": "", "parent_id": "67e7e441-434d-4166-aa5f-eb4f6250b572", "metadata": {"aucs": [0.7386304550406995, 0.9058759315364792, 0.8618645110743199], "final_y": [0.18019448304053765, 0.11967430990635797, 0.1286604486570846]}, "mutation_prompt": null}
{"id": "443c8a3c-affb-4cda-9917-a85d5c1864f9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance local search by adjusting the convergence tolerance dynamically based on remaining budget.", "configspace": "", "generation": 14, "fitness": 0.8818449123569975, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.018. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "67e7e441-434d-4166-aa5f-eb4f6250b572", "metadata": {"aucs": [0.877794294460193, 0.9058759315364792, 0.8618645110743199], "final_y": [0.12699552343965625, 0.11967430990635797, 0.1286604486570846]}, "mutation_prompt": null}
{"id": "eed88ad5-f2f2-4a80-8a74-7027ecc6690b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget  # Dynamic weighting factor\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by incorporating richer diversity in mutation strategies.", "configspace": "", "generation": 14, "fitness": 0.836623661500055, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.025. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "67e7e441-434d-4166-aa5f-eb4f6250b572", "metadata": {"aucs": [0.8258255608538296, 0.8717139015460682, 0.8123315221002669], "final_y": [0.14731858399375053, 0.12687551224116334, 0.1503148061743269]}, "mutation_prompt": null}
{"id": "1a316e50-9f31-4050-9109-dd4a9bb89fdf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget  # Dynamic weighting factor\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.8 - 0.5 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)  # Adjusted crossover rate\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Optimize exploration and exploitation by refining mutation and crossover dynamics in the differential evolution strategy.", "configspace": "", "generation": 14, "fitness": 0.8748833686363956, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.875 with standard deviation 0.025. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "67e7e441-434d-4166-aa5f-eb4f6250b572", "metadata": {"aucs": [0.8408458548330311, 0.9015970470845929, 0.8822072039915627], "final_y": [0.1392576340467535, 0.12078619815397573, 0.12069507121996459]}, "mutation_prompt": null}
{"id": "37c31b46-3d83-4d7f-a7df-440189ee628e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR + 0.1 * (self.evaluations / self.budget)\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget  # Dynamic weighting factor\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B')\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve exploration by dynamically adjusting population size and crossover rate based on the evaluation budget.", "configspace": "", "generation": 14, "fitness": 0.8588803336035037, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.034. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "67e7e441-434d-4166-aa5f-eb4f6250b572", "metadata": {"aucs": [0.8155813438947519, 0.8991951458414394, 0.8618645110743199], "final_y": [0.147352369796654, 0.12078288810107962, 0.1286604486570846]}, "mutation_prompt": null}
{"id": "5204073d-da31-4a21-9031-28c68e0f94ca", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1)) * (best_score / func(pop[best_idx]))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, b * weight + pop[j] * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve adaptive mutation scaling by making it dependent on the best score and enhance trial vector strategy.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[150.73897086, 187.3416606 , 162.60794274, 149.87430026,\\n         123.20405585, 172.09670487, 126.26918648, 226.19006017,\\n         242.00580731, 114.35713414],\\n        [204.17950838, 146.35688235, 154.96980344, 233.63126042,\\n          45.6279328 ,  49.16844593,  34.44804744, 213.17636602,\\n         201.19448521, 221.40267261],\\n        [245.29603529, 205.81488413, 131.5254597 , 201.71641878,\\n          56.02037369, 170.78262469,  61.53772323, 237.82716175,\\n         144.80663079, 121.2256268 ],\\n        [ 88.20223466, 200.33141168, 130.35307309, 155.05546875,\\n          34.1337561 , 165.87980936, 164.661059  , 165.72547931,\\n         237.62457727, 180.0004658 ],\\n        [109.09173813, 126.14702984, 183.4788631 ,  43.24960376,\\n         176.6886774 , 177.54033132,  76.28416344,  58.36378548,\\n          99.3942372 , 110.01636961],\\n        [155.44328949, 126.49233296, 247.44224437,  52.44985836,\\n          75.95288634,  65.48809393, 173.6838316 ,  85.72415256,\\n         132.58837003,  83.77363024],\\n        [ 64.9733084 ,  54.28253106, 174.39250968,  60.4002493 ,\\n          73.24811957, 111.11953755, 210.61851057,  51.36228067,\\n         214.34787965,  51.14164974],\\n        [244.8210823 , 133.10326436, 244.8874394 , 163.06601434,\\n         192.63798747,  38.6213143 ,  92.21753177,  56.44324347,\\n          95.15084345,  56.12009817],\\n        [ 99.95629947, 121.13785879,  44.1124492 , 182.34386626,\\n         154.65231993,  88.38568801, 145.11457176,  50.66691237,\\n         156.70822902, 234.44516347],\\n        [100.08516954, 176.83028359,  58.99552973, 187.59198491,\\n          93.66934045,  70.30209964, 159.03284566,  34.42366016,\\n         212.36680643,  31.03300476],\\n        [179.1196381 ,  89.4017541 , 191.74268487, 241.68147993,\\n          84.72569157, 156.75461357, 160.24922488, 155.89541927,\\n          79.07795918, 239.60478253],\\n        [128.3675833 , 216.20990794, 183.88544057,  95.43612919,\\n         209.03552033, 117.23126299, 223.84270336, 157.88003198,\\n         223.98177961, 182.35694982],\\n        [189.55594156, 140.29136402, 240.33839964, 171.67784383,\\n         123.24811068, 163.40650711,  34.22250363,  96.34645967,\\n         175.23817825,  93.81707359],\\n        [165.96339438, 124.32911421,  59.80429413,  95.62211171,\\n         155.39228035, 159.99200747, 156.35155475, 173.70418037,\\n         173.4627194 , 124.9120558 ],\\n        [227.24025109, 110.86361141, 125.89028356, 226.2231381 ,\\n         207.36267759, 184.85548838,  52.04991521, 232.28617502,\\n         187.1330859 , 249.74634144],\\n        [ 62.87862702, 220.98773262,  65.74844563, 165.42310414,\\n          57.24039623, 216.56181045, 207.61017092, 155.2021625 ,\\n         119.58032539,  45.216739  ],\\n        [183.43433009, 129.77939019, 188.85223188, 220.6041117 ,\\n         244.6147311 , 218.27673533,  32.57709852, 109.19517419,\\n         190.59792373,  67.758529  ],\\n        [144.62805336,  41.95435743,  73.99923548,  34.07479478,\\n         204.61349474,  79.26343137, 105.97736975, 234.17788456,\\n         184.97116842,  37.0045645 ],\\n        [ 66.23271443, 166.72524833, 156.99028949,  82.3364207 ,\\n         235.52707954, 165.07251031, 147.83921667, 159.7801948 ,\\n         190.62684649,  98.62789901],\\n        [117.60863369,  76.16562477,  70.96246129, 237.7619258 ,\\n         192.70117491, 137.9009379 ,  80.03121815,  85.95842599,\\n          42.76641527, 125.57165762]]])').", "error": "TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[150.73897086, 187.3416606 , 162.60794274, 149.87430026,\\n         123.20405585, 172.09670487, 126.26918648, 226.19006017,\\n         242.00580731, 114.35713414],\\n        [204.17950838, 146.35688235, 154.96980344, 233.63126042,\\n          45.6279328 ,  49.16844593,  34.44804744, 213.17636602,\\n         201.19448521, 221.40267261],\\n        [245.29603529, 205.81488413, 131.5254597 , 201.71641878,\\n          56.02037369, 170.78262469,  61.53772323, 237.82716175,\\n         144.80663079, 121.2256268 ],\\n        [ 88.20223466, 200.33141168, 130.35307309, 155.05546875,\\n          34.1337561 , 165.87980936, 164.661059  , 165.72547931,\\n         237.62457727, 180.0004658 ],\\n        [109.09173813, 126.14702984, 183.4788631 ,  43.24960376,\\n         176.6886774 , 177.54033132,  76.28416344,  58.36378548,\\n          99.3942372 , 110.01636961],\\n        [155.44328949, 126.49233296, 247.44224437,  52.44985836,\\n          75.95288634,  65.48809393, 173.6838316 ,  85.72415256,\\n         132.58837003,  83.77363024],\\n        [ 64.9733084 ,  54.28253106, 174.39250968,  60.4002493 ,\\n          73.24811957, 111.11953755, 210.61851057,  51.36228067,\\n         214.34787965,  51.14164974],\\n        [244.8210823 , 133.10326436, 244.8874394 , 163.06601434,\\n         192.63798747,  38.6213143 ,  92.21753177,  56.44324347,\\n          95.15084345,  56.12009817],\\n        [ 99.95629947, 121.13785879,  44.1124492 , 182.34386626,\\n         154.65231993,  88.38568801, 145.11457176,  50.66691237,\\n         156.70822902, 234.44516347],\\n        [100.08516954, 176.83028359,  58.99552973, 187.59198491,\\n          93.66934045,  70.30209964, 159.03284566,  34.42366016,\\n         212.36680643,  31.03300476],\\n        [179.1196381 ,  89.4017541 , 191.74268487, 241.68147993,\\n          84.72569157, 156.75461357, 160.24922488, 155.89541927,\\n          79.07795918, 239.60478253],\\n        [128.3675833 , 216.20990794, 183.88544057,  95.43612919,\\n         209.03552033, 117.23126299, 223.84270336, 157.88003198,\\n         223.98177961, 182.35694982],\\n        [189.55594156, 140.29136402, 240.33839964, 171.67784383,\\n         123.24811068, 163.40650711,  34.22250363,  96.34645967,\\n         175.23817825,  93.81707359],\\n        [165.96339438, 124.32911421,  59.80429413,  95.62211171,\\n         155.39228035, 159.99200747, 156.35155475, 173.70418037,\\n         173.4627194 , 124.9120558 ],\\n        [227.24025109, 110.86361141, 125.89028356, 226.2231381 ,\\n         207.36267759, 184.85548838,  52.04991521, 232.28617502,\\n         187.1330859 , 249.74634144],\\n        [ 62.87862702, 220.98773262,  65.74844563, 165.42310414,\\n          57.24039623, 216.56181045, 207.61017092, 155.2021625 ,\\n         119.58032539,  45.216739  ],\\n        [183.43433009, 129.77939019, 188.85223188, 220.6041117 ,\\n         244.6147311 , 218.27673533,  32.57709852, 109.19517419,\\n         190.59792373,  67.758529  ],\\n        [144.62805336,  41.95435743,  73.99923548,  34.07479478,\\n         204.61349474,  79.26343137, 105.97736975, 234.17788456,\\n         184.97116842,  37.0045645 ],\\n        [ 66.23271443, 166.72524833, 156.99028949,  82.3364207 ,\\n         235.52707954, 165.07251031, 147.83921667, 159.7801948 ,\\n         190.62684649,  98.62789901],\\n        [117.60863369,  76.16562477,  70.96246129, 237.7619258 ,\\n         192.70117491, 137.9009379 ,  80.03121815,  85.95842599,\\n          42.76641527, 125.57165762]]])')", "parent_id": "443c8a3c-affb-4cda-9917-a85d5c1864f9", "metadata": {}, "mutation_prompt": null}
{"id": "3460b098-5a66-4488-adf2-59b8165e4205", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                perturbation = np.random.normal(0, 0.02, self.dim)  # New line\n                trial = np.where(crossover, mutant + perturbation, pop[j])  # Modified line\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance solution diversity by introducing stochastic perturbation in crossover and adaptive mutation scaling.", "configspace": "", "generation": 15, "fitness": 0.7965844765876193, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.085. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.035.", "error": "", "parent_id": "443c8a3c-affb-4cda-9917-a85d5c1864f9", "metadata": {"aucs": [0.6898017776116833, 0.8966340900031828, 0.8033175621479918], "final_y": [0.20394359379771732, 0.11875635962126474, 0.15660691969614204]}, "mutation_prompt": null}
{"id": "1fcfc035-014a-4479-8fe1-6aa84269d45e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n\n                # Add Gaussian noise to trial to improve diversity\n                trial += np.random.normal(0, 0.05, size=self.dim)\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve diversity of the population by adding Gaussian noise to offspring before evaluation.", "configspace": "", "generation": 15, "fitness": 0.8599517808300101, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.041. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "443c8a3c-affb-4cda-9917-a85d5c1864f9", "metadata": {"aucs": [0.8799036903388555, 0.8966340900031828, 0.8033175621479918], "final_y": [0.12605146662684596, 0.11875635962126474, 0.15660691969614204]}, "mutation_prompt": null}
{"id": "a7f8558e-37b6-4d73-937d-6ff0d62bdf15", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR + 0.1 * np.random.normal())\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce an adaptive crossover rate in differential evolution to improve exploration and convergence.", "configspace": "", "generation": 15, "fitness": 0.8676277018714424, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.046. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "443c8a3c-affb-4cda-9917-a85d5c1864f9", "metadata": {"aucs": [0.9029314534631527, 0.8966340900031828, 0.8033175621479918], "final_y": [0.11795494940924, 0.11875635962126474, 0.15660691969614204]}, "mutation_prompt": null}
{"id": "d863f60b-c1e2-4a8b-a18b-ff88000d852c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.35 * (self.budget - self.evaluations) / self.budget  # Changed from 0.3 to 0.35\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.8 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)  # Changed from 0.9 to 0.8\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Fine-tune mutation factor and crossover probability dynamically to optimize balance between exploration and exploitation.", "configspace": "", "generation": 15, "fitness": 0.8448209740508976, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.039. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "443c8a3c-affb-4cda-9917-a85d5c1864f9", "metadata": {"aucs": [0.8345112700015183, 0.8966340900031828, 0.8033175621479918], "final_y": [0.13802450643034658, 0.11875635962126474, 0.15660691969614204]}, "mutation_prompt": null}
{"id": "141ad4c0-cb7a-4009-945c-1d9f745321cf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * np.exp(-0.005 * self.evaluations) # Change here\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (0.9 + 0.1 * (best_score - func(pop[best_idx])) / best_score))) # Change here\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx] # Change here\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive population reduction and dynamic weighting in trial vector selection to enhance convergence.", "configspace": "", "generation": 16, "fitness": 0.845974063026547, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.846 with standard deviation 0.062. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "a7f8558e-37b6-4d73-937d-6ff0d62bdf15", "metadata": {"aucs": [0.8582920531885718, 0.914901142993108, 0.7647289928979609], "final_y": [0.13266540396620785, 0.11760942379446115, 0.1716196373900648]}, "mutation_prompt": null}
{"id": "f5ce3c9c-9e37-411f-8d9c-17e0c98ef004", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR + 0.1 * np.random.normal())\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            # Modified line for stopping criterion\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-7:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Modify the stopping criterion to improve convergence efficiency by lowering the threshold for early stopping.", "configspace": "", "generation": 16, "fitness": 0.8095545726044598, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.037. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "a7f8558e-37b6-4d73-937d-6ff0d62bdf15", "metadata": {"aucs": [0.8554482691689265, 0.8084864557464921, 0.7647289928979609], "final_y": [0.1339151205650474, 0.15226257606522575, 0.1716196373900648]}, "mutation_prompt": null}
{"id": "46479382-e3a9-4be7-9712-bbd03d10a2df", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR + 0.1 * np.random.normal())\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by dynamically updating the population size based on improvement rate.", "configspace": "", "generation": 16, "fitness": 0.8543386213611445, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.044. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "a7f8558e-37b6-4d73-937d-6ff0d62bdf15", "metadata": {"aucs": [0.8531649403939028, 0.908683740989638, 0.8011671826998927], "final_y": [0.13391186220124018, 0.11936350506307813, 0.15308916296286135]}, "mutation_prompt": null}
{"id": "fb50f581-31c8-4217-a680-e59dc1cf7e19", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR + 0.1 * np.random.normal())\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget  # Changed line\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Adjust the local search termination tolerance adaptively based on the evaluations remaining, enhancing balance between exploration and exploitation.", "configspace": "", "generation": 16, "fitness": 0.8001609536384741, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.066. And the mean value of best solutions found was 0.155 (0. is the best) with standard deviation 0.027.", "error": "", "parent_id": "a7f8558e-37b6-4d73-937d-6ff0d62bdf15", "metadata": {"aucs": [0.8016545002805932, 0.880084434000971, 0.718743926633858], "final_y": [0.14878822521647017, 0.1262969216143951, 0.19103396660959382]}, "mutation_prompt": null}
{"id": "992c9cba-263a-4872-b193-228569821f7c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.uniform(-0.2, 0.2))  # Changed line for adaptive exploration\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR + 0.1 * np.random.normal())\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Adjust the mutation strategy to adaptively enhance exploration near promising areas.", "configspace": "", "generation": 16, "fitness": 0.783700510817225, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.059. And the mean value of best solutions found was 0.163 (0. is the best) with standard deviation 0.025.", "error": "", "parent_id": "a7f8558e-37b6-4d73-937d-6ff0d62bdf15", "metadata": {"aucs": [0.7710132890845853, 0.8613443167332316, 0.718743926633858], "final_y": [0.16787298697446507, 0.1310854420785299, 0.19103396660959382]}, "mutation_prompt": null}
{"id": "e994d9c7-1188-4d16-ba84-01a1811556c2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR + 0.1 * np.random.normal())\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive mutation scaling to enhance exploration and local search flexibility.", "configspace": "", "generation": 17, "fitness": 0.7744505679152192, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.100. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.038.", "error": "", "parent_id": "46479382-e3a9-4be7-9712-bbd03d10a2df", "metadata": {"aucs": [0.6896311310040558, 0.9149766461077437, 0.718743926633858], "final_y": [0.20411712450787967, 0.11760942379446115, 0.19103396660959382]}, "mutation_prompt": null}
{"id": "135d90c7-43e7-4def-8d8e-375d24d3bfed", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)  # Adaptive crossover\n                crossover = np.random.rand(self.dim) < CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover and leverage historical data to guide local search initiation.", "configspace": "", "generation": 17, "fitness": 0.761583877454778, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.762 with standard deviation 0.055. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "46479382-e3a9-4be7-9712-bbd03d10a2df", "metadata": {"aucs": [0.6925768460911166, 0.8274457933752561, 0.7647289928979609], "final_y": [0.2024552058733773, 0.14424801205832227, 0.1716196373900648]}, "mutation_prompt": null}
{"id": "94d59989-af1e-4a9c-8ebe-3597dade34c2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR + 0.1 * np.random.normal())\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8)  # Modification: added scaling to tol\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Refine convergence by incorporating a dynamically adapted scaling factor in the local search phase.", "configspace": "", "generation": 17, "fitness": 0.8550763226393747, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.082. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.031.", "error": "", "parent_id": "46479382-e3a9-4be7-9712-bbd03d10a2df", "metadata": {"aucs": [0.7391146101778513, 0.9107974258927112, 0.9153169318475617], "final_y": [0.18157309017015755, 0.11762730826251921, 0.11480778774200562]}, "mutation_prompt": null}
{"id": "4190db4e-a85a-46b4-abfe-3bbfff9054b8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n\n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                dynamic_CR = CR * (0.5 + 0.5 * np.random.rand())  # Dynamic crossover rate\n                crossover = np.random.rand(self.dim) < dynamic_CR\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.5 * (self.evaluations / self.budget)  # Reduce decrement in CR\n\n            if self.evaluations >= self.budget // 2 or best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n    \n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 5)  # Adjust step size for adaptive layer addition\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduces adaptive layer addition and dynamic crossover to enhance diversity and convergence efficiency.", "configspace": "", "generation": 17, "fitness": 0.8095254279964209, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.040. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "46479382-e3a9-4be7-9712-bbd03d10a2df", "metadata": {"aucs": [0.80250297435807, 0.8613443167332316, 0.7647289928979609], "final_y": [0.15345859309550025, 0.1310854420785299, 0.1716196373900648]}, "mutation_prompt": null}
{"id": "dcf9945c-f612-45ed-9891-941bd9fcb1c1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR + 0.1 * np.random.normal())\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6 or \\\n               (self.evaluations > self.budget / 4 and np.std(new_pop) < 1e-6):  # Change made here\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a convergence threshold to terminate early if no improvement in recent iterations.", "configspace": "", "generation": 17, "fitness": 0.8092426033039285, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.040. And the mean value of best solutions found was 0.150 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "46479382-e3a9-4be7-9712-bbd03d10a2df", "metadata": {"aucs": [0.8016545002805932, 0.8613443167332316, 0.7647289928979609], "final_y": [0.14878822521647017, 0.1310854420785299, 0.1716196373900648]}, "mutation_prompt": null}
{"id": "48897f3e-c38d-4aa5-a83a-42bdd20f0916", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR + 0.1 * np.random.normal())\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.85)  # Modification: adjusted factor to 0.85\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve local search convergence by slightly adjusting the gradient tolerance scaling factor.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "94d59989-af1e-4a9c-8ebe-3597dade34c2", "metadata": {}, "mutation_prompt": null}
{"id": "21beb0a2-4cb3-4d31-8b52-94497b1b7327", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                perturbation = np.random.uniform(-0.1, 0.1) * (self.budget / (self.budget - self.evaluations))  # Added line\n                mutant = np.clip(a + adaptive_F * (b - c) + perturbation, lb, ub)  # Modified line\n                \n                crossover = np.random.rand(self.dim) < (CR + 0.1 * np.random.normal())\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8)  # Modification: added scaling to tol\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance solution diversity by integrating a dynamic mutation strategy that adjusts the mutation variability based on convergence rates.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "94d59989-af1e-4a9c-8ebe-3597dade34c2", "metadata": {}, "mutation_prompt": null}
{"id": "a4015075-bd94-4956-b3b5-c837d0df7e3f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                # Modified line below (change CR dynamically for diversity)\n                crossover = np.random.rand(self.dim) < (CR * (1 - self.evaluations/self.budget))\n\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8)  # Modification: added scaling to tol\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a diversity-enhancing mechanism by modifying the crossover probability dynamically to improve exploration. ", "configspace": "", "generation": 18, "fitness": 0.8686327732342182, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.869 with standard deviation 0.079. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.028.", "error": "", "parent_id": "94d59989-af1e-4a9c-8ebe-3597dade34c2", "metadata": {"aucs": [0.9370750730606031, 0.9112797360028544, 0.7575435106391968], "final_y": [0.11059817256459847, 0.11936359726444246, 0.1745059689468419]}, "mutation_prompt": null}
{"id": "9a12e759-f665-4ff6-8704-762ef8ebd11c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * np.sin(np.pi * self.evaluations / self.budget)  # Modified adaptive F with sine function\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR + 0.1 * np.random.normal())\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8)  # Modification: added scaling to tol\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance adaptive scaling factor variation for improved solution space exploration.", "configspace": "", "generation": 18, "fitness": 0.7831492311186811, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.018. And the mean value of best solutions found was 0.161 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "94d59989-af1e-4a9c-8ebe-3597dade34c2", "metadata": {"aucs": [0.8004777295898571, 0.7914264531269896, 0.7575435106391968], "final_y": [0.15264944292114324, 0.1571152822863202, 0.1745059689468419]}, "mutation_prompt": null}
{"id": "f36d5bb9-a846-47e4-b2a7-eb1698dc4778", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            \n            # Calculate diversity measure for mutation scaling\n            diversity = np.var(pop, axis=0).mean()\n            \n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1)) * (1 + diversity)  # Changed line\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR + 0.1 * np.random.normal())\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance diversity by adding a mutation scaling factor based on population diversity.", "configspace": "", "generation": 18, "fitness": 0.7662263406277586, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.766 with standard deviation 0.018. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "94d59989-af1e-4a9c-8ebe-3597dade34c2", "metadata": {"aucs": [0.7497090581170897, 0.7914264531269896, 0.7575435106391968], "final_y": [0.16930850949558474, 0.1571152822863202, 0.1745059689468419]}, "mutation_prompt": null}
{"id": "d36684e0-fd72-454b-a4aa-c86330d8ce9a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                # Changed line below: adapt F more dynamically based on improvement\n                adaptive_F = F * (1 + np.random.normal(0, 0.1) * (best_score - func(pop[j])) / best_score)\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR * (1 - self.evaluations/self.budget))\n\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8)  # Modification: added scaling to tol\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by adapting the mutation strategy based on current best solution's improvement rate.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[150.73897086, 187.3416606 , 162.60794274, 149.87430026,\\n         123.20405585, 172.09670487, 126.26918648, 226.19006017,\\n         242.00580731, 114.35713414],\\n        [204.17950838, 146.35688235, 154.96980344, 233.63126042,\\n          45.6279328 ,  49.16844593,  34.44804744, 213.17636602,\\n         201.19448521, 221.40267261],\\n        [245.29603529, 205.81488413, 131.5254597 , 201.71641878,\\n          56.02037369, 170.78262469,  61.53772323, 237.82716175,\\n         144.80663079, 121.2256268 ],\\n        [ 88.20223466, 200.33141168, 130.35307309, 155.05546875,\\n          34.1337561 , 165.87980936, 164.661059  , 165.72547931,\\n         237.62457727, 180.0004658 ],\\n        [109.09173813, 126.14702984, 183.4788631 ,  43.24960376,\\n         176.6886774 , 177.54033132,  76.28416344,  58.36378548,\\n          99.3942372 , 110.01636961],\\n        [155.44328949, 126.49233296, 247.44224437,  52.44985836,\\n          75.95288634,  65.48809393, 173.6838316 ,  85.72415256,\\n         132.58837003,  83.77363024],\\n        [ 64.9733084 ,  54.28253106, 174.39250968,  60.4002493 ,\\n          73.24811957, 111.11953755, 210.61851057,  51.36228067,\\n         214.34787965,  51.14164974],\\n        [244.8210823 , 133.10326436, 244.8874394 , 163.06601434,\\n         192.63798747,  38.6213143 ,  92.21753177,  56.44324347,\\n          95.15084345,  56.12009817],\\n        [ 99.95629947, 121.13785879,  44.1124492 , 182.34386626,\\n         154.65231993,  88.38568801, 145.11457176,  50.66691237,\\n         156.70822902, 234.44516347],\\n        [100.08516954, 176.83028359,  58.99552973, 187.59198491,\\n          93.66934045,  70.30209964, 159.03284566,  34.42366016,\\n         212.36680643,  31.03300476],\\n        [179.1196381 ,  89.4017541 , 191.74268487, 241.68147993,\\n          84.72569157, 156.75461357, 160.24922488, 155.89541927,\\n          79.07795918, 239.60478253],\\n        [128.3675833 , 216.20990794, 183.88544057,  95.43612919,\\n         209.03552033, 117.23126299, 223.84270336, 157.88003198,\\n         223.98177961, 182.35694982],\\n        [189.55594156, 140.29136402, 240.33839964, 171.67784383,\\n         123.24811068, 163.40650711,  34.22250363,  96.34645967,\\n         175.23817825,  93.81707359],\\n        [165.96339438, 124.32911421,  59.80429413,  95.62211171,\\n         155.39228035, 159.99200747, 156.35155475, 173.70418037,\\n         173.4627194 , 124.9120558 ],\\n        [227.24025109, 110.86361141, 125.89028356, 226.2231381 ,\\n         207.36267759, 184.85548838,  52.04991521, 232.28617502,\\n         187.1330859 , 249.74634144],\\n        [ 62.87862702, 220.98773262,  65.74844563, 165.42310414,\\n          57.24039623, 216.56181045, 207.61017092, 155.2021625 ,\\n         119.58032539,  45.216739  ],\\n        [183.43433009, 129.77939019, 188.85223188, 220.6041117 ,\\n         244.6147311 , 218.27673533,  32.57709852, 109.19517419,\\n         190.59792373,  67.758529  ],\\n        [144.62805336,  41.95435743,  73.99923548,  34.07479478,\\n         204.61349474,  79.26343137, 105.97736975, 234.17788456,\\n         184.97116842,  37.0045645 ],\\n        [ 66.23271443, 166.72524833, 156.99028949,  82.3364207 ,\\n         235.52707954, 165.07251031, 147.83921667, 159.7801948 ,\\n         190.62684649,  98.62789901],\\n        [117.60863369,  76.16562477,  70.96246129, 237.7619258 ,\\n         192.70117491, 137.9009379 ,  80.03121815,  85.95842599,\\n          42.76641527, 125.57165762]]])').", "error": "TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[150.73897086, 187.3416606 , 162.60794274, 149.87430026,\\n         123.20405585, 172.09670487, 126.26918648, 226.19006017,\\n         242.00580731, 114.35713414],\\n        [204.17950838, 146.35688235, 154.96980344, 233.63126042,\\n          45.6279328 ,  49.16844593,  34.44804744, 213.17636602,\\n         201.19448521, 221.40267261],\\n        [245.29603529, 205.81488413, 131.5254597 , 201.71641878,\\n          56.02037369, 170.78262469,  61.53772323, 237.82716175,\\n         144.80663079, 121.2256268 ],\\n        [ 88.20223466, 200.33141168, 130.35307309, 155.05546875,\\n          34.1337561 , 165.87980936, 164.661059  , 165.72547931,\\n         237.62457727, 180.0004658 ],\\n        [109.09173813, 126.14702984, 183.4788631 ,  43.24960376,\\n         176.6886774 , 177.54033132,  76.28416344,  58.36378548,\\n          99.3942372 , 110.01636961],\\n        [155.44328949, 126.49233296, 247.44224437,  52.44985836,\\n          75.95288634,  65.48809393, 173.6838316 ,  85.72415256,\\n         132.58837003,  83.77363024],\\n        [ 64.9733084 ,  54.28253106, 174.39250968,  60.4002493 ,\\n          73.24811957, 111.11953755, 210.61851057,  51.36228067,\\n         214.34787965,  51.14164974],\\n        [244.8210823 , 133.10326436, 244.8874394 , 163.06601434,\\n         192.63798747,  38.6213143 ,  92.21753177,  56.44324347,\\n          95.15084345,  56.12009817],\\n        [ 99.95629947, 121.13785879,  44.1124492 , 182.34386626,\\n         154.65231993,  88.38568801, 145.11457176,  50.66691237,\\n         156.70822902, 234.44516347],\\n        [100.08516954, 176.83028359,  58.99552973, 187.59198491,\\n          93.66934045,  70.30209964, 159.03284566,  34.42366016,\\n         212.36680643,  31.03300476],\\n        [179.1196381 ,  89.4017541 , 191.74268487, 241.68147993,\\n          84.72569157, 156.75461357, 160.24922488, 155.89541927,\\n          79.07795918, 239.60478253],\\n        [128.3675833 , 216.20990794, 183.88544057,  95.43612919,\\n         209.03552033, 117.23126299, 223.84270336, 157.88003198,\\n         223.98177961, 182.35694982],\\n        [189.55594156, 140.29136402, 240.33839964, 171.67784383,\\n         123.24811068, 163.40650711,  34.22250363,  96.34645967,\\n         175.23817825,  93.81707359],\\n        [165.96339438, 124.32911421,  59.80429413,  95.62211171,\\n         155.39228035, 159.99200747, 156.35155475, 173.70418037,\\n         173.4627194 , 124.9120558 ],\\n        [227.24025109, 110.86361141, 125.89028356, 226.2231381 ,\\n         207.36267759, 184.85548838,  52.04991521, 232.28617502,\\n         187.1330859 , 249.74634144],\\n        [ 62.87862702, 220.98773262,  65.74844563, 165.42310414,\\n          57.24039623, 216.56181045, 207.61017092, 155.2021625 ,\\n         119.58032539,  45.216739  ],\\n        [183.43433009, 129.77939019, 188.85223188, 220.6041117 ,\\n         244.6147311 , 218.27673533,  32.57709852, 109.19517419,\\n         190.59792373,  67.758529  ],\\n        [144.62805336,  41.95435743,  73.99923548,  34.07479478,\\n         204.61349474,  79.26343137, 105.97736975, 234.17788456,\\n         184.97116842,  37.0045645 ],\\n        [ 66.23271443, 166.72524833, 156.99028949,  82.3364207 ,\\n         235.52707954, 165.07251031, 147.83921667, 159.7801948 ,\\n         190.62684649,  98.62789901],\\n        [117.60863369,  76.16562477,  70.96246129, 237.7619258 ,\\n         192.70117491, 137.9009379 ,  80.03121815,  85.95842599,\\n          42.76641527, 125.57165762]]])')", "parent_id": "a4015075-bd94-4956-b3b5-c837d0df7e3f", "metadata": {}, "mutation_prompt": null}
{"id": "46378020-b0c1-46c9-bb22-0714241b2820", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                # Modified line below (change CR dynamically for diversity)\n                crossover = np.random.rand(self.dim) < (CR * (1 - self.evaluations/self.budget))\n\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            if np.random.rand() < 0.01:  # Restart mechanism with a 1% chance\n                norm_pop = np.random.rand(pop_size, self.dim)\n                pop = lb + norm_pop * (ub - lb)\n                \n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8)  # Modification: added scaling to tol\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introducing a restart mechanism in the differential evolution to escape local optima and enhance global search.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[121.74484103, 188.47138856,  30.02516246,  96.51316598,\\n          62.28629598,  50.31449085,  70.9772465 , 106.02335995,\\n         117.28884433, 148.53968148],\\n        [122.22279317, 180.74829009,  74.97949494, 223.18583601,\\n          36.0252705 , 177.50285224, 121.80705652, 152.91176226,\\n          60.88512649,  73.5823276 ],\\n        [206.16380511, 243.01754666,  98.9533192 , 182.31097545,\\n         222.80561351, 226.81346597,  48.7097265 ,  38.59205231,\\n          67.3626923 , 223.19135075],\\n        [ 51.63630344, 122.6436775 , 240.73569663, 147.29636269,\\n         182.21296507,  99.41343882, 181.03020409, 213.61764782,\\n          34.02342102, 195.03174929],\\n        [247.54943956, 194.59644396,  91.69767825, 203.64145226,\\n          52.70972145, 128.53657576, 229.89101068,  94.59511264,\\n          93.31057449,  58.60628587],\\n        [ 34.26073073, 179.34381725,  76.55818552,  88.42026506,\\n         138.14609504,  41.73975993, 156.30587321,  62.28028648,\\n         159.64721812, 183.9468392 ],\\n        [ 52.51357434, 121.09231732, 182.7680347 , 121.1194393 ,\\n          40.98976097, 147.8972093 , 176.03482195, 143.27560465,\\n         237.81084632, 159.04210891],\\n        [228.74842136,  60.24443491,  60.6407964 , 207.62608352,\\n         117.48890414,  66.37792337, 234.05188769, 106.50848914,\\n         195.17866269, 189.71955678],\\n        [224.32734007, 167.20788555, 195.20733549, 106.75763524,\\n          89.38413619, 227.094968  , 124.18006177, 242.26481037,\\n         175.95712952, 166.77305845],\\n        [ 55.24411405, 238.88763692, 128.98066937, 157.24571517,\\n         119.79009661,  82.14593565, 228.74349452, 156.20948707,\\n          30.63147195, 165.771881  ],\\n        [101.86187839, 145.9527825 , 224.90726185, 108.5993472 ,\\n         229.8777332 , 167.13922547,  33.48067343, 234.47619142,\\n         181.99732185, 249.4110271 ],\\n        [ 67.91491184,  60.16986492, 235.17100187, 183.29999553,\\n          44.520038  , 196.20187157, 195.85276146, 233.06539782,\\n         186.5354469 ,  57.33961163],\\n        [ 34.37362944,  35.76641711,  36.22742736,  84.16643487,\\n         219.20614871, 148.54283416, 151.62083531, 215.24679632,\\n          57.31812933,  91.42040938],\\n        [158.86703972, 243.31106463, 153.42664824,  34.10240366,\\n         206.13918799,  81.25434025, 207.56314304, 115.32934169,\\n         219.979208  , 194.3667614 ],\\n        [152.37285148,  60.02014965,  43.18189169,  56.69556026,\\n          39.80141328,  53.6487084 ,  79.65605449, 186.85757568,\\n         153.13773605,  32.76231564],\\n        [ 45.83434153, 242.8007926 , 154.98210162,  74.72451163,\\n          85.51166381, 193.6416879 ,  72.99448584, 157.898964  ,\\n         243.4043976 , 216.30233633],\\n        [ 82.76650701, 138.62933714, 166.39025804, 212.3757979 ,\\n          64.49410682,  34.08676448,  45.40487162, 136.99592441,\\n         163.39248156, 155.14731616],\\n        [ 99.81973005, 247.49555397, 157.54394823, 113.63105798,\\n         151.20860821, 193.9735748 , 177.23123656,  88.28230269,\\n          44.59366357, 111.41852354],\\n        [168.53785154,  76.23828218, 195.60622182,  44.6380259 ,\\n          87.26932169, 207.04600402,  72.55554218, 170.68139379,\\n         145.42746801, 233.45775349],\\n        [ 87.92528951,  44.51143995, 191.71451192, 199.8791665 ,\\n         229.71948755, 235.03385522,  33.06934605,  81.55965895,\\n         165.69123854, 238.78359055]]])').", "error": "TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[121.74484103, 188.47138856,  30.02516246,  96.51316598,\\n          62.28629598,  50.31449085,  70.9772465 , 106.02335995,\\n         117.28884433, 148.53968148],\\n        [122.22279317, 180.74829009,  74.97949494, 223.18583601,\\n          36.0252705 , 177.50285224, 121.80705652, 152.91176226,\\n          60.88512649,  73.5823276 ],\\n        [206.16380511, 243.01754666,  98.9533192 , 182.31097545,\\n         222.80561351, 226.81346597,  48.7097265 ,  38.59205231,\\n          67.3626923 , 223.19135075],\\n        [ 51.63630344, 122.6436775 , 240.73569663, 147.29636269,\\n         182.21296507,  99.41343882, 181.03020409, 213.61764782,\\n          34.02342102, 195.03174929],\\n        [247.54943956, 194.59644396,  91.69767825, 203.64145226,\\n          52.70972145, 128.53657576, 229.89101068,  94.59511264,\\n          93.31057449,  58.60628587],\\n        [ 34.26073073, 179.34381725,  76.55818552,  88.42026506,\\n         138.14609504,  41.73975993, 156.30587321,  62.28028648,\\n         159.64721812, 183.9468392 ],\\n        [ 52.51357434, 121.09231732, 182.7680347 , 121.1194393 ,\\n          40.98976097, 147.8972093 , 176.03482195, 143.27560465,\\n         237.81084632, 159.04210891],\\n        [228.74842136,  60.24443491,  60.6407964 , 207.62608352,\\n         117.48890414,  66.37792337, 234.05188769, 106.50848914,\\n         195.17866269, 189.71955678],\\n        [224.32734007, 167.20788555, 195.20733549, 106.75763524,\\n          89.38413619, 227.094968  , 124.18006177, 242.26481037,\\n         175.95712952, 166.77305845],\\n        [ 55.24411405, 238.88763692, 128.98066937, 157.24571517,\\n         119.79009661,  82.14593565, 228.74349452, 156.20948707,\\n          30.63147195, 165.771881  ],\\n        [101.86187839, 145.9527825 , 224.90726185, 108.5993472 ,\\n         229.8777332 , 167.13922547,  33.48067343, 234.47619142,\\n         181.99732185, 249.4110271 ],\\n        [ 67.91491184,  60.16986492, 235.17100187, 183.29999553,\\n          44.520038  , 196.20187157, 195.85276146, 233.06539782,\\n         186.5354469 ,  57.33961163],\\n        [ 34.37362944,  35.76641711,  36.22742736,  84.16643487,\\n         219.20614871, 148.54283416, 151.62083531, 215.24679632,\\n          57.31812933,  91.42040938],\\n        [158.86703972, 243.31106463, 153.42664824,  34.10240366,\\n         206.13918799,  81.25434025, 207.56314304, 115.32934169,\\n         219.979208  , 194.3667614 ],\\n        [152.37285148,  60.02014965,  43.18189169,  56.69556026,\\n          39.80141328,  53.6487084 ,  79.65605449, 186.85757568,\\n         153.13773605,  32.76231564],\\n        [ 45.83434153, 242.8007926 , 154.98210162,  74.72451163,\\n          85.51166381, 193.6416879 ,  72.99448584, 157.898964  ,\\n         243.4043976 , 216.30233633],\\n        [ 82.76650701, 138.62933714, 166.39025804, 212.3757979 ,\\n          64.49410682,  34.08676448,  45.40487162, 136.99592441,\\n         163.39248156, 155.14731616],\\n        [ 99.81973005, 247.49555397, 157.54394823, 113.63105798,\\n         151.20860821, 193.9735748 , 177.23123656,  88.28230269,\\n          44.59366357, 111.41852354],\\n        [168.53785154,  76.23828218, 195.60622182,  44.6380259 ,\\n          87.26932169, 207.04600402,  72.55554218, 170.68139379,\\n         145.42746801, 233.45775349],\\n        [ 87.92528951,  44.51143995, 191.71451192, 199.8791665 ,\\n         229.71948755, 235.03385522,  33.06934605,  81.55965895,\\n         165.69123854, 238.78359055]]])')", "parent_id": "a4015075-bd94-4956-b3b5-c837d0df7e3f", "metadata": {}, "mutation_prompt": null}
{"id": "f801a6be-305e-4d59-89dc-b43488dacfc3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                # Modified line below (change CR dynamically for diversity)\n                crossover = np.random.rand(self.dim) < (CR * (1 - self.evaluations/self.budget))\n\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n              \n            # New line for enhancing population diversity based on best_score\n            if best_score < 0.01: pop_size = int(pop_size * 1.2)\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8)  # Modification: added scaling to tol\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by incorporating a mechanism to increase population diversity adaptively based on the current best score.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[121.74484103, 188.47138856,  30.02516246,  96.51316598,\\n          62.28629598,  50.31449085,  70.9772465 , 106.02335995,\\n         117.28884433, 148.53968148],\\n        [122.22279317, 180.74829009,  74.97949494, 223.18583601,\\n          36.0252705 , 177.50285224, 121.80705652, 152.91176226,\\n          60.88512649,  73.5823276 ],\\n        [206.16380511, 243.01754666,  98.9533192 , 182.31097545,\\n         222.80561351, 226.81346597,  48.7097265 ,  38.59205231,\\n          67.3626923 , 223.19135075],\\n        [ 51.63630344, 122.6436775 , 240.73569663, 147.29636269,\\n         182.21296507,  99.41343882, 181.03020409, 213.61764782,\\n          34.02342102, 195.03174929],\\n        [247.54943956, 194.59644396,  91.69767825, 203.64145226,\\n          52.70972145, 128.53657576, 229.89101068,  94.59511264,\\n          93.31057449,  58.60628587],\\n        [ 34.26073073, 179.34381725,  76.55818552,  88.42026506,\\n         138.14609504,  41.73975993, 156.30587321,  62.28028648,\\n         159.64721812, 183.9468392 ],\\n        [ 52.51357434, 121.09231732, 182.7680347 , 121.1194393 ,\\n          40.98976097, 147.8972093 , 176.03482195, 143.27560465,\\n         237.81084632, 159.04210891],\\n        [228.74842136,  60.24443491,  60.6407964 , 207.62608352,\\n         117.48890414,  66.37792337, 234.05188769, 106.50848914,\\n         195.17866269, 189.71955678],\\n        [224.32734007, 167.20788555, 195.20733549, 106.75763524,\\n          89.38413619, 227.094968  , 124.18006177, 242.26481037,\\n         175.95712952, 166.77305845],\\n        [ 55.24411405, 238.88763692, 128.98066937, 157.24571517,\\n         119.79009661,  82.14593565, 228.74349452, 156.20948707,\\n          30.63147195, 165.771881  ],\\n        [101.86187839, 145.9527825 , 224.90726185, 108.5993472 ,\\n         229.8777332 , 167.13922547,  33.48067343, 234.47619142,\\n         181.99732185, 249.4110271 ],\\n        [ 67.91491184,  60.16986492, 235.17100187, 183.29999553,\\n          44.520038  , 196.20187157, 195.85276146, 233.06539782,\\n         186.5354469 ,  57.33961163],\\n        [ 34.37362944,  35.76641711,  36.22742736,  84.16643487,\\n         219.20614871, 148.54283416, 151.62083531, 215.24679632,\\n          57.31812933,  91.42040938],\\n        [158.86703972, 243.31106463, 153.42664824,  34.10240366,\\n         206.13918799,  81.25434025, 207.56314304, 115.32934169,\\n         219.979208  , 194.3667614 ],\\n        [152.37285148,  60.02014965,  43.18189169,  56.69556026,\\n          39.80141328,  53.6487084 ,  79.65605449, 186.85757568,\\n         153.13773605,  32.76231564],\\n        [ 45.83434153, 242.8007926 , 154.98210162,  74.72451163,\\n          85.51166381, 193.6416879 ,  72.99448584, 157.898964  ,\\n         243.4043976 , 216.30233633],\\n        [ 82.76650701, 138.62933714, 166.39025804, 212.3757979 ,\\n          64.49410682,  34.08676448,  45.40487162, 136.99592441,\\n         163.39248156, 155.14731616],\\n        [ 99.81973005, 247.49555397, 157.54394823, 113.63105798,\\n         151.20860821, 193.9735748 , 177.23123656,  88.28230269,\\n          44.59366357, 111.41852354],\\n        [168.53785154,  76.23828218, 195.60622182,  44.6380259 ,\\n          87.26932169, 207.04600402,  72.55554218, 170.68139379,\\n         145.42746801, 233.45775349],\\n        [ 87.92528951,  44.51143995, 191.71451192, 199.8791665 ,\\n         229.71948755, 235.03385522,  33.06934605,  81.55965895,\\n         165.69123854, 238.78359055]]])').", "error": "TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[121.74484103, 188.47138856,  30.02516246,  96.51316598,\\n          62.28629598,  50.31449085,  70.9772465 , 106.02335995,\\n         117.28884433, 148.53968148],\\n        [122.22279317, 180.74829009,  74.97949494, 223.18583601,\\n          36.0252705 , 177.50285224, 121.80705652, 152.91176226,\\n          60.88512649,  73.5823276 ],\\n        [206.16380511, 243.01754666,  98.9533192 , 182.31097545,\\n         222.80561351, 226.81346597,  48.7097265 ,  38.59205231,\\n          67.3626923 , 223.19135075],\\n        [ 51.63630344, 122.6436775 , 240.73569663, 147.29636269,\\n         182.21296507,  99.41343882, 181.03020409, 213.61764782,\\n          34.02342102, 195.03174929],\\n        [247.54943956, 194.59644396,  91.69767825, 203.64145226,\\n          52.70972145, 128.53657576, 229.89101068,  94.59511264,\\n          93.31057449,  58.60628587],\\n        [ 34.26073073, 179.34381725,  76.55818552,  88.42026506,\\n         138.14609504,  41.73975993, 156.30587321,  62.28028648,\\n         159.64721812, 183.9468392 ],\\n        [ 52.51357434, 121.09231732, 182.7680347 , 121.1194393 ,\\n          40.98976097, 147.8972093 , 176.03482195, 143.27560465,\\n         237.81084632, 159.04210891],\\n        [228.74842136,  60.24443491,  60.6407964 , 207.62608352,\\n         117.48890414,  66.37792337, 234.05188769, 106.50848914,\\n         195.17866269, 189.71955678],\\n        [224.32734007, 167.20788555, 195.20733549, 106.75763524,\\n          89.38413619, 227.094968  , 124.18006177, 242.26481037,\\n         175.95712952, 166.77305845],\\n        [ 55.24411405, 238.88763692, 128.98066937, 157.24571517,\\n         119.79009661,  82.14593565, 228.74349452, 156.20948707,\\n          30.63147195, 165.771881  ],\\n        [101.86187839, 145.9527825 , 224.90726185, 108.5993472 ,\\n         229.8777332 , 167.13922547,  33.48067343, 234.47619142,\\n         181.99732185, 249.4110271 ],\\n        [ 67.91491184,  60.16986492, 235.17100187, 183.29999553,\\n          44.520038  , 196.20187157, 195.85276146, 233.06539782,\\n         186.5354469 ,  57.33961163],\\n        [ 34.37362944,  35.76641711,  36.22742736,  84.16643487,\\n         219.20614871, 148.54283416, 151.62083531, 215.24679632,\\n          57.31812933,  91.42040938],\\n        [158.86703972, 243.31106463, 153.42664824,  34.10240366,\\n         206.13918799,  81.25434025, 207.56314304, 115.32934169,\\n         219.979208  , 194.3667614 ],\\n        [152.37285148,  60.02014965,  43.18189169,  56.69556026,\\n          39.80141328,  53.6487084 ,  79.65605449, 186.85757568,\\n         153.13773605,  32.76231564],\\n        [ 45.83434153, 242.8007926 , 154.98210162,  74.72451163,\\n          85.51166381, 193.6416879 ,  72.99448584, 157.898964  ,\\n         243.4043976 , 216.30233633],\\n        [ 82.76650701, 138.62933714, 166.39025804, 212.3757979 ,\\n          64.49410682,  34.08676448,  45.40487162, 136.99592441,\\n         163.39248156, 155.14731616],\\n        [ 99.81973005, 247.49555397, 157.54394823, 113.63105798,\\n         151.20860821, 193.9735748 , 177.23123656,  88.28230269,\\n          44.59366357, 111.41852354],\\n        [168.53785154,  76.23828218, 195.60622182,  44.6380259 ,\\n          87.26932169, 207.04600402,  72.55554218, 170.68139379,\\n         145.42746801, 233.45775349],\\n        [ 87.92528951,  44.51143995, 191.71451192, 199.8791665 ,\\n         229.71948755, 235.03385522,  33.06934605,  81.55965895,\\n         165.69123854, 238.78359055]]])')", "parent_id": "a4015075-bd94-4956-b3b5-c837d0df7e3f", "metadata": {}, "mutation_prompt": null}
{"id": "f402134c-90bf-45d7-bb2f-d51d32b9c66b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                # Modified line below (change CR dynamically for diversity)\n                crossover = np.random.rand(self.dim) < (CR * (1 - self.evaluations/self.budget))\n\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            # Change 1: Improved population adaptation strategy\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / (best_score + 1e-9))))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        # Change 2: Adjust search space for better refinement\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8) \n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            # Change 3: Enhanced local search integration by refining sub-range\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by refining the mutation strategy and local search integration.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[121.74484103, 188.47138856,  30.02516246,  96.51316598,\\n          62.28629598,  50.31449085,  70.9772465 , 106.02335995,\\n         117.28884433, 148.53968148],\\n        [122.22279317, 180.74829009,  74.97949494, 223.18583601,\\n          36.0252705 , 177.50285224, 121.80705652, 152.91176226,\\n          60.88512649,  73.5823276 ],\\n        [206.16380511, 243.01754666,  98.9533192 , 182.31097545,\\n         222.80561351, 226.81346597,  48.7097265 ,  38.59205231,\\n          67.3626923 , 223.19135075],\\n        [ 51.63630344, 122.6436775 , 240.73569663, 147.29636269,\\n         182.21296507,  99.41343882, 181.03020409, 213.61764782,\\n          34.02342102, 195.03174929],\\n        [247.54943956, 194.59644396,  91.69767825, 203.64145226,\\n          52.70972145, 128.53657576, 229.89101068,  94.59511264,\\n          93.31057449,  58.60628587],\\n        [ 34.26073073, 179.34381725,  76.55818552,  88.42026506,\\n         138.14609504,  41.73975993, 156.30587321,  62.28028648,\\n         159.64721812, 183.9468392 ],\\n        [ 52.51357434, 121.09231732, 182.7680347 , 121.1194393 ,\\n          40.98976097, 147.8972093 , 176.03482195, 143.27560465,\\n         237.81084632, 159.04210891],\\n        [228.74842136,  60.24443491,  60.6407964 , 207.62608352,\\n         117.48890414,  66.37792337, 234.05188769, 106.50848914,\\n         195.17866269, 189.71955678],\\n        [224.32734007, 167.20788555, 195.20733549, 106.75763524,\\n          89.38413619, 227.094968  , 124.18006177, 242.26481037,\\n         175.95712952, 166.77305845],\\n        [ 55.24411405, 238.88763692, 128.98066937, 157.24571517,\\n         119.79009661,  82.14593565, 228.74349452, 156.20948707,\\n          30.63147195, 165.771881  ],\\n        [101.86187839, 145.9527825 , 224.90726185, 108.5993472 ,\\n         229.8777332 , 167.13922547,  33.48067343, 234.47619142,\\n         181.99732185, 249.4110271 ],\\n        [ 67.91491184,  60.16986492, 235.17100187, 183.29999553,\\n          44.520038  , 196.20187157, 195.85276146, 233.06539782,\\n         186.5354469 ,  57.33961163],\\n        [ 34.37362944,  35.76641711,  36.22742736,  84.16643487,\\n         219.20614871, 148.54283416, 151.62083531, 215.24679632,\\n          57.31812933,  91.42040938],\\n        [158.86703972, 243.31106463, 153.42664824,  34.10240366,\\n         206.13918799,  81.25434025, 207.56314304, 115.32934169,\\n         219.979208  , 194.3667614 ],\\n        [152.37285148,  60.02014965,  43.18189169,  56.69556026,\\n          39.80141328,  53.6487084 ,  79.65605449, 186.85757568,\\n         153.13773605,  32.76231564],\\n        [ 45.83434153, 242.8007926 , 154.98210162,  74.72451163,\\n          85.51166381, 193.6416879 ,  72.99448584, 157.898964  ,\\n         243.4043976 , 216.30233633],\\n        [ 82.76650701, 138.62933714, 166.39025804, 212.3757979 ,\\n          64.49410682,  34.08676448,  45.40487162, 136.99592441,\\n         163.39248156, 155.14731616],\\n        [ 99.81973005, 247.49555397, 157.54394823, 113.63105798,\\n         151.20860821, 193.9735748 , 177.23123656,  88.28230269,\\n          44.59366357, 111.41852354],\\n        [168.53785154,  76.23828218, 195.60622182,  44.6380259 ,\\n          87.26932169, 207.04600402,  72.55554218, 170.68139379,\\n         145.42746801, 233.45775349],\\n        [ 87.92528951,  44.51143995, 191.71451192, 199.8791665 ,\\n         229.71948755, 235.03385522,  33.06934605,  81.55965895,\\n         165.69123854, 238.78359055]]])').", "error": "TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[121.74484103, 188.47138856,  30.02516246,  96.51316598,\\n          62.28629598,  50.31449085,  70.9772465 , 106.02335995,\\n         117.28884433, 148.53968148],\\n        [122.22279317, 180.74829009,  74.97949494, 223.18583601,\\n          36.0252705 , 177.50285224, 121.80705652, 152.91176226,\\n          60.88512649,  73.5823276 ],\\n        [206.16380511, 243.01754666,  98.9533192 , 182.31097545,\\n         222.80561351, 226.81346597,  48.7097265 ,  38.59205231,\\n          67.3626923 , 223.19135075],\\n        [ 51.63630344, 122.6436775 , 240.73569663, 147.29636269,\\n         182.21296507,  99.41343882, 181.03020409, 213.61764782,\\n          34.02342102, 195.03174929],\\n        [247.54943956, 194.59644396,  91.69767825, 203.64145226,\\n          52.70972145, 128.53657576, 229.89101068,  94.59511264,\\n          93.31057449,  58.60628587],\\n        [ 34.26073073, 179.34381725,  76.55818552,  88.42026506,\\n         138.14609504,  41.73975993, 156.30587321,  62.28028648,\\n         159.64721812, 183.9468392 ],\\n        [ 52.51357434, 121.09231732, 182.7680347 , 121.1194393 ,\\n          40.98976097, 147.8972093 , 176.03482195, 143.27560465,\\n         237.81084632, 159.04210891],\\n        [228.74842136,  60.24443491,  60.6407964 , 207.62608352,\\n         117.48890414,  66.37792337, 234.05188769, 106.50848914,\\n         195.17866269, 189.71955678],\\n        [224.32734007, 167.20788555, 195.20733549, 106.75763524,\\n          89.38413619, 227.094968  , 124.18006177, 242.26481037,\\n         175.95712952, 166.77305845],\\n        [ 55.24411405, 238.88763692, 128.98066937, 157.24571517,\\n         119.79009661,  82.14593565, 228.74349452, 156.20948707,\\n          30.63147195, 165.771881  ],\\n        [101.86187839, 145.9527825 , 224.90726185, 108.5993472 ,\\n         229.8777332 , 167.13922547,  33.48067343, 234.47619142,\\n         181.99732185, 249.4110271 ],\\n        [ 67.91491184,  60.16986492, 235.17100187, 183.29999553,\\n          44.520038  , 196.20187157, 195.85276146, 233.06539782,\\n         186.5354469 ,  57.33961163],\\n        [ 34.37362944,  35.76641711,  36.22742736,  84.16643487,\\n         219.20614871, 148.54283416, 151.62083531, 215.24679632,\\n          57.31812933,  91.42040938],\\n        [158.86703972, 243.31106463, 153.42664824,  34.10240366,\\n         206.13918799,  81.25434025, 207.56314304, 115.32934169,\\n         219.979208  , 194.3667614 ],\\n        [152.37285148,  60.02014965,  43.18189169,  56.69556026,\\n          39.80141328,  53.6487084 ,  79.65605449, 186.85757568,\\n         153.13773605,  32.76231564],\\n        [ 45.83434153, 242.8007926 , 154.98210162,  74.72451163,\\n          85.51166381, 193.6416879 ,  72.99448584, 157.898964  ,\\n         243.4043976 , 216.30233633],\\n        [ 82.76650701, 138.62933714, 166.39025804, 212.3757979 ,\\n          64.49410682,  34.08676448,  45.40487162, 136.99592441,\\n         163.39248156, 155.14731616],\\n        [ 99.81973005, 247.49555397, 157.54394823, 113.63105798,\\n         151.20860821, 193.9735748 , 177.23123656,  88.28230269,\\n          44.59366357, 111.41852354],\\n        [168.53785154,  76.23828218, 195.60622182,  44.6380259 ,\\n          87.26932169, 207.04600402,  72.55554218, 170.68139379,\\n         145.42746801, 233.45775349],\\n        [ 87.92528951,  44.51143995, 191.71451192, 199.8791665 ,\\n         229.71948755, 235.03385522,  33.06934605,  81.55965895,\\n         165.69123854, 238.78359055]]])')", "parent_id": "a4015075-bd94-4956-b3b5-c837d0df7e3f", "metadata": {}, "mutation_prompt": null}
{"id": "638030c0-e805-4801-9709-bb971b65e751", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * ((self.budget - self.evaluations) / self.budget) * 0.9  # Updated line\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR * (1 - self.evaluations/self.budget))\n\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8)  \n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce iteration-based adaptive scaling on mutation factor to enhance convergence.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[121.74484103, 188.47138856,  30.02516246,  96.51316598,\\n          62.28629598,  50.31449085,  70.9772465 , 106.02335995,\\n         117.28884433, 148.53968148],\\n        [122.22279317, 180.74829009,  74.97949494, 223.18583601,\\n          36.0252705 , 177.50285224, 121.80705652, 152.91176226,\\n          60.88512649,  73.5823276 ],\\n        [206.16380511, 243.01754666,  98.9533192 , 182.31097545,\\n         222.80561351, 226.81346597,  48.7097265 ,  38.59205231,\\n          67.3626923 , 223.19135075],\\n        [ 51.63630344, 122.6436775 , 240.73569663, 147.29636269,\\n         182.21296507,  99.41343882, 181.03020409, 213.61764782,\\n          34.02342102, 195.03174929],\\n        [247.54943956, 194.59644396,  91.69767825, 203.64145226,\\n          52.70972145, 128.53657576, 229.89101068,  94.59511264,\\n          93.31057449,  58.60628587],\\n        [ 34.26073073, 179.34381725,  76.55818552,  88.42026506,\\n         138.14609504,  41.73975993, 156.30587321,  62.28028648,\\n         159.64721812, 183.9468392 ],\\n        [ 52.51357434, 121.09231732, 182.7680347 , 121.1194393 ,\\n          40.98976097, 147.8972093 , 176.03482195, 143.27560465,\\n         237.81084632, 159.04210891],\\n        [228.74842136,  60.24443491,  60.6407964 , 207.62608352,\\n         117.48890414,  66.37792337, 234.05188769, 106.50848914,\\n         195.17866269, 189.71955678],\\n        [224.32734007, 167.20788555, 195.20733549, 106.75763524,\\n          89.38413619, 227.094968  , 124.18006177, 242.26481037,\\n         175.95712952, 166.77305845],\\n        [ 55.24411405, 238.88763692, 128.98066937, 157.24571517,\\n         119.79009661,  82.14593565, 228.74349452, 156.20948707,\\n          30.63147195, 165.771881  ],\\n        [101.86187839, 145.9527825 , 224.90726185, 108.5993472 ,\\n         229.8777332 , 167.13922547,  33.48067343, 234.47619142,\\n         181.99732185, 249.4110271 ],\\n        [ 67.91491184,  60.16986492, 235.17100187, 183.29999553,\\n          44.520038  , 196.20187157, 195.85276146, 233.06539782,\\n         186.5354469 ,  57.33961163],\\n        [ 34.37362944,  35.76641711,  36.22742736,  84.16643487,\\n         219.20614871, 148.54283416, 151.62083531, 215.24679632,\\n          57.31812933,  91.42040938],\\n        [158.86703972, 243.31106463, 153.42664824,  34.10240366,\\n         206.13918799,  81.25434025, 207.56314304, 115.32934169,\\n         219.979208  , 194.3667614 ],\\n        [152.37285148,  60.02014965,  43.18189169,  56.69556026,\\n          39.80141328,  53.6487084 ,  79.65605449, 186.85757568,\\n         153.13773605,  32.76231564],\\n        [ 45.83434153, 242.8007926 , 154.98210162,  74.72451163,\\n          85.51166381, 193.6416879 ,  72.99448584, 157.898964  ,\\n         243.4043976 , 216.30233633],\\n        [ 82.76650701, 138.62933714, 166.39025804, 212.3757979 ,\\n          64.49410682,  34.08676448,  45.40487162, 136.99592441,\\n         163.39248156, 155.14731616],\\n        [ 99.81973005, 247.49555397, 157.54394823, 113.63105798,\\n         151.20860821, 193.9735748 , 177.23123656,  88.28230269,\\n          44.59366357, 111.41852354],\\n        [168.53785154,  76.23828218, 195.60622182,  44.6380259 ,\\n          87.26932169, 207.04600402,  72.55554218, 170.68139379,\\n         145.42746801, 233.45775349],\\n        [ 87.92528951,  44.51143995, 191.71451192, 199.8791665 ,\\n         229.71948755, 235.03385522,  33.06934605,  81.55965895,\\n         165.69123854, 238.78359055]]])').", "error": "TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[121.74484103, 188.47138856,  30.02516246,  96.51316598,\\n          62.28629598,  50.31449085,  70.9772465 , 106.02335995,\\n         117.28884433, 148.53968148],\\n        [122.22279317, 180.74829009,  74.97949494, 223.18583601,\\n          36.0252705 , 177.50285224, 121.80705652, 152.91176226,\\n          60.88512649,  73.5823276 ],\\n        [206.16380511, 243.01754666,  98.9533192 , 182.31097545,\\n         222.80561351, 226.81346597,  48.7097265 ,  38.59205231,\\n          67.3626923 , 223.19135075],\\n        [ 51.63630344, 122.6436775 , 240.73569663, 147.29636269,\\n         182.21296507,  99.41343882, 181.03020409, 213.61764782,\\n          34.02342102, 195.03174929],\\n        [247.54943956, 194.59644396,  91.69767825, 203.64145226,\\n          52.70972145, 128.53657576, 229.89101068,  94.59511264,\\n          93.31057449,  58.60628587],\\n        [ 34.26073073, 179.34381725,  76.55818552,  88.42026506,\\n         138.14609504,  41.73975993, 156.30587321,  62.28028648,\\n         159.64721812, 183.9468392 ],\\n        [ 52.51357434, 121.09231732, 182.7680347 , 121.1194393 ,\\n          40.98976097, 147.8972093 , 176.03482195, 143.27560465,\\n         237.81084632, 159.04210891],\\n        [228.74842136,  60.24443491,  60.6407964 , 207.62608352,\\n         117.48890414,  66.37792337, 234.05188769, 106.50848914,\\n         195.17866269, 189.71955678],\\n        [224.32734007, 167.20788555, 195.20733549, 106.75763524,\\n          89.38413619, 227.094968  , 124.18006177, 242.26481037,\\n         175.95712952, 166.77305845],\\n        [ 55.24411405, 238.88763692, 128.98066937, 157.24571517,\\n         119.79009661,  82.14593565, 228.74349452, 156.20948707,\\n          30.63147195, 165.771881  ],\\n        [101.86187839, 145.9527825 , 224.90726185, 108.5993472 ,\\n         229.8777332 , 167.13922547,  33.48067343, 234.47619142,\\n         181.99732185, 249.4110271 ],\\n        [ 67.91491184,  60.16986492, 235.17100187, 183.29999553,\\n          44.520038  , 196.20187157, 195.85276146, 233.06539782,\\n         186.5354469 ,  57.33961163],\\n        [ 34.37362944,  35.76641711,  36.22742736,  84.16643487,\\n         219.20614871, 148.54283416, 151.62083531, 215.24679632,\\n          57.31812933,  91.42040938],\\n        [158.86703972, 243.31106463, 153.42664824,  34.10240366,\\n         206.13918799,  81.25434025, 207.56314304, 115.32934169,\\n         219.979208  , 194.3667614 ],\\n        [152.37285148,  60.02014965,  43.18189169,  56.69556026,\\n          39.80141328,  53.6487084 ,  79.65605449, 186.85757568,\\n         153.13773605,  32.76231564],\\n        [ 45.83434153, 242.8007926 , 154.98210162,  74.72451163,\\n          85.51166381, 193.6416879 ,  72.99448584, 157.898964  ,\\n         243.4043976 , 216.30233633],\\n        [ 82.76650701, 138.62933714, 166.39025804, 212.3757979 ,\\n          64.49410682,  34.08676448,  45.40487162, 136.99592441,\\n         163.39248156, 155.14731616],\\n        [ 99.81973005, 247.49555397, 157.54394823, 113.63105798,\\n         151.20860821, 193.9735748 , 177.23123656,  88.28230269,\\n          44.59366357, 111.41852354],\\n        [168.53785154,  76.23828218, 195.60622182,  44.6380259 ,\\n          87.26932169, 207.04600402,  72.55554218, 170.68139379,\\n         145.42746801, 233.45775349],\\n        [ 87.92528951,  44.51143995, 191.71451192, 199.8791665 ,\\n         229.71948755, 235.03385522,  33.06934605,  81.55965895,\\n         165.69123854, 238.78359055]]])')", "parent_id": "a4015075-bd94-4956-b3b5-c837d0df7e3f", "metadata": {}, "mutation_prompt": null}
{"id": "b196fb6d-1ca2-4775-9886-673a076ee201", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1) * (best_score - func(pop[j])) / max(1e-6, best_score))\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                crossover = np.random.rand(self.dim) < (CR * (1 - self.evaluations/self.budget))\n                trial = np.where(crossover, mutant, pop[j])\n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / max(1e-6, best_score))))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n            if self.evaluations >= self.budget // 2 or best_score - func(pop[best_idx]) < 1e-6:\n                break\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n        step = max(1, self.dim // 5)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb[:new_dim], func.bounds.ub[:new_dim]])\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n            if self.evaluations >= self.budget:\n                break\n        return best_solution", "name": "HybridMetaheuristic", "description": "Integrate progressive dimensionality increase and adaptive global-local strategy to enhance exploration while maintaining solution robustness.", "configspace": "", "generation": 20, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[150.73897086, 187.3416606 , 162.60794274, 149.87430026,\\n         123.20405585, 172.09670487, 126.26918648, 226.19006017,\\n         242.00580731, 114.35713414],\\n        [204.17950838, 146.35688235, 154.96980344, 233.63126042,\\n          45.6279328 ,  49.16844593,  34.44804744, 213.17636602,\\n         201.19448521, 221.40267261],\\n        [245.29603529, 205.81488413, 131.5254597 , 201.71641878,\\n          56.02037369, 170.78262469,  61.53772323, 237.82716175,\\n         144.80663079, 121.2256268 ],\\n        [ 88.20223466, 200.33141168, 130.35307309, 155.05546875,\\n          34.1337561 , 165.87980936, 164.661059  , 165.72547931,\\n         237.62457727, 180.0004658 ],\\n        [109.09173813, 126.14702984, 183.4788631 ,  43.24960376,\\n         176.6886774 , 177.54033132,  76.28416344,  58.36378548,\\n          99.3942372 , 110.01636961],\\n        [155.44328949, 126.49233296, 247.44224437,  52.44985836,\\n          75.95288634,  65.48809393, 173.6838316 ,  85.72415256,\\n         132.58837003,  83.77363024],\\n        [ 64.9733084 ,  54.28253106, 174.39250968,  60.4002493 ,\\n          73.24811957, 111.11953755, 210.61851057,  51.36228067,\\n         214.34787965,  51.14164974],\\n        [244.8210823 , 133.10326436, 244.8874394 , 163.06601434,\\n         192.63798747,  38.6213143 ,  92.21753177,  56.44324347,\\n          95.15084345,  56.12009817],\\n        [ 99.95629947, 121.13785879,  44.1124492 , 182.34386626,\\n         154.65231993,  88.38568801, 145.11457176,  50.66691237,\\n         156.70822902, 234.44516347],\\n        [100.08516954, 176.83028359,  58.99552973, 187.59198491,\\n          93.66934045,  70.30209964, 159.03284566,  34.42366016,\\n         212.36680643,  31.03300476],\\n        [179.1196381 ,  89.4017541 , 191.74268487, 241.68147993,\\n          84.72569157, 156.75461357, 160.24922488, 155.89541927,\\n          79.07795918, 239.60478253],\\n        [128.3675833 , 216.20990794, 183.88544057,  95.43612919,\\n         209.03552033, 117.23126299, 223.84270336, 157.88003198,\\n         223.98177961, 182.35694982],\\n        [189.55594156, 140.29136402, 240.33839964, 171.67784383,\\n         123.24811068, 163.40650711,  34.22250363,  96.34645967,\\n         175.23817825,  93.81707359],\\n        [165.96339438, 124.32911421,  59.80429413,  95.62211171,\\n         155.39228035, 159.99200747, 156.35155475, 173.70418037,\\n         173.4627194 , 124.9120558 ],\\n        [227.24025109, 110.86361141, 125.89028356, 226.2231381 ,\\n         207.36267759, 184.85548838,  52.04991521, 232.28617502,\\n         187.1330859 , 249.74634144],\\n        [ 62.87862702, 220.98773262,  65.74844563, 165.42310414,\\n          57.24039623, 216.56181045, 207.61017092, 155.2021625 ,\\n         119.58032539,  45.216739  ],\\n        [183.43433009, 129.77939019, 188.85223188, 220.6041117 ,\\n         244.6147311 , 218.27673533,  32.57709852, 109.19517419,\\n         190.59792373,  67.758529  ],\\n        [144.62805336,  41.95435743,  73.99923548,  34.07479478,\\n         204.61349474,  79.26343137, 105.97736975, 234.17788456,\\n         184.97116842,  37.0045645 ],\\n        [ 66.23271443, 166.72524833, 156.99028949,  82.3364207 ,\\n         235.52707954, 165.07251031, 147.83921667, 159.7801948 ,\\n         190.62684649,  98.62789901],\\n        [117.60863369,  76.16562477,  70.96246129, 237.7619258 ,\\n         192.70117491, 137.9009379 ,  80.03121815,  85.95842599,\\n          42.76641527, 125.57165762]]])').", "error": "TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[150.73897086, 187.3416606 , 162.60794274, 149.87430026,\\n         123.20405585, 172.09670487, 126.26918648, 226.19006017,\\n         242.00580731, 114.35713414],\\n        [204.17950838, 146.35688235, 154.96980344, 233.63126042,\\n          45.6279328 ,  49.16844593,  34.44804744, 213.17636602,\\n         201.19448521, 221.40267261],\\n        [245.29603529, 205.81488413, 131.5254597 , 201.71641878,\\n          56.02037369, 170.78262469,  61.53772323, 237.82716175,\\n         144.80663079, 121.2256268 ],\\n        [ 88.20223466, 200.33141168, 130.35307309, 155.05546875,\\n          34.1337561 , 165.87980936, 164.661059  , 165.72547931,\\n         237.62457727, 180.0004658 ],\\n        [109.09173813, 126.14702984, 183.4788631 ,  43.24960376,\\n         176.6886774 , 177.54033132,  76.28416344,  58.36378548,\\n          99.3942372 , 110.01636961],\\n        [155.44328949, 126.49233296, 247.44224437,  52.44985836,\\n          75.95288634,  65.48809393, 173.6838316 ,  85.72415256,\\n         132.58837003,  83.77363024],\\n        [ 64.9733084 ,  54.28253106, 174.39250968,  60.4002493 ,\\n          73.24811957, 111.11953755, 210.61851057,  51.36228067,\\n         214.34787965,  51.14164974],\\n        [244.8210823 , 133.10326436, 244.8874394 , 163.06601434,\\n         192.63798747,  38.6213143 ,  92.21753177,  56.44324347,\\n          95.15084345,  56.12009817],\\n        [ 99.95629947, 121.13785879,  44.1124492 , 182.34386626,\\n         154.65231993,  88.38568801, 145.11457176,  50.66691237,\\n         156.70822902, 234.44516347],\\n        [100.08516954, 176.83028359,  58.99552973, 187.59198491,\\n          93.66934045,  70.30209964, 159.03284566,  34.42366016,\\n         212.36680643,  31.03300476],\\n        [179.1196381 ,  89.4017541 , 191.74268487, 241.68147993,\\n          84.72569157, 156.75461357, 160.24922488, 155.89541927,\\n          79.07795918, 239.60478253],\\n        [128.3675833 , 216.20990794, 183.88544057,  95.43612919,\\n         209.03552033, 117.23126299, 223.84270336, 157.88003198,\\n         223.98177961, 182.35694982],\\n        [189.55594156, 140.29136402, 240.33839964, 171.67784383,\\n         123.24811068, 163.40650711,  34.22250363,  96.34645967,\\n         175.23817825,  93.81707359],\\n        [165.96339438, 124.32911421,  59.80429413,  95.62211171,\\n         155.39228035, 159.99200747, 156.35155475, 173.70418037,\\n         173.4627194 , 124.9120558 ],\\n        [227.24025109, 110.86361141, 125.89028356, 226.2231381 ,\\n         207.36267759, 184.85548838,  52.04991521, 232.28617502,\\n         187.1330859 , 249.74634144],\\n        [ 62.87862702, 220.98773262,  65.74844563, 165.42310414,\\n          57.24039623, 216.56181045, 207.61017092, 155.2021625 ,\\n         119.58032539,  45.216739  ],\\n        [183.43433009, 129.77939019, 188.85223188, 220.6041117 ,\\n         244.6147311 , 218.27673533,  32.57709852, 109.19517419,\\n         190.59792373,  67.758529  ],\\n        [144.62805336,  41.95435743,  73.99923548,  34.07479478,\\n         204.61349474,  79.26343137, 105.97736975, 234.17788456,\\n         184.97116842,  37.0045645 ],\\n        [ 66.23271443, 166.72524833, 156.99028949,  82.3364207 ,\\n         235.52707954, 165.07251031, 147.83921667, 159.7801948 ,\\n         190.62684649,  98.62789901],\\n        [117.60863369,  76.16562477,  70.96246129, 237.7619258 ,\\n         192.70117491, 137.9009379 ,  80.03121815,  85.95842599,\\n          42.76641527, 125.57165762]]])')", "parent_id": "d36684e0-fd72-454b-a4aa-c86330d8ce9a", "metadata": {}, "mutation_prompt": null}
{"id": "8ec8c5dc-25c7-4606-8fb5-cd589ce38287", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1) * (best_score - func(pop[j])) / best_score)\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                # Line changed to enhance robustness during exploration\n                crossover = np.random.rand(self.dim) < (CR * (1 - (self.evaluations/self.budget)**2))\n\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Implement a minor adjustment to the crossover strategy for a more robust exploration phase in Differential Evolution.", "configspace": "", "generation": 20, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[150.73897086, 187.3416606 , 162.60794274, 149.87430026,\\n         123.20405585, 172.09670487, 126.26918648, 226.19006017,\\n         242.00580731, 114.35713414],\\n        [204.17950838, 146.35688235, 154.96980344, 233.63126042,\\n          45.6279328 ,  49.16844593,  34.44804744, 213.17636602,\\n         201.19448521, 221.40267261],\\n        [245.29603529, 205.81488413, 131.5254597 , 201.71641878,\\n          56.02037369, 170.78262469,  61.53772323, 237.82716175,\\n         144.80663079, 121.2256268 ],\\n        [ 88.20223466, 200.33141168, 130.35307309, 155.05546875,\\n          34.1337561 , 165.87980936, 164.661059  , 165.72547931,\\n         237.62457727, 180.0004658 ],\\n        [109.09173813, 126.14702984, 183.4788631 ,  43.24960376,\\n         176.6886774 , 177.54033132,  76.28416344,  58.36378548,\\n          99.3942372 , 110.01636961],\\n        [155.44328949, 126.49233296, 247.44224437,  52.44985836,\\n          75.95288634,  65.48809393, 173.6838316 ,  85.72415256,\\n         132.58837003,  83.77363024],\\n        [ 64.9733084 ,  54.28253106, 174.39250968,  60.4002493 ,\\n          73.24811957, 111.11953755, 210.61851057,  51.36228067,\\n         214.34787965,  51.14164974],\\n        [244.8210823 , 133.10326436, 244.8874394 , 163.06601434,\\n         192.63798747,  38.6213143 ,  92.21753177,  56.44324347,\\n          95.15084345,  56.12009817],\\n        [ 99.95629947, 121.13785879,  44.1124492 , 182.34386626,\\n         154.65231993,  88.38568801, 145.11457176,  50.66691237,\\n         156.70822902, 234.44516347],\\n        [100.08516954, 176.83028359,  58.99552973, 187.59198491,\\n          93.66934045,  70.30209964, 159.03284566,  34.42366016,\\n         212.36680643,  31.03300476],\\n        [179.1196381 ,  89.4017541 , 191.74268487, 241.68147993,\\n          84.72569157, 156.75461357, 160.24922488, 155.89541927,\\n          79.07795918, 239.60478253],\\n        [128.3675833 , 216.20990794, 183.88544057,  95.43612919,\\n         209.03552033, 117.23126299, 223.84270336, 157.88003198,\\n         223.98177961, 182.35694982],\\n        [189.55594156, 140.29136402, 240.33839964, 171.67784383,\\n         123.24811068, 163.40650711,  34.22250363,  96.34645967,\\n         175.23817825,  93.81707359],\\n        [165.96339438, 124.32911421,  59.80429413,  95.62211171,\\n         155.39228035, 159.99200747, 156.35155475, 173.70418037,\\n         173.4627194 , 124.9120558 ],\\n        [227.24025109, 110.86361141, 125.89028356, 226.2231381 ,\\n         207.36267759, 184.85548838,  52.04991521, 232.28617502,\\n         187.1330859 , 249.74634144],\\n        [ 62.87862702, 220.98773262,  65.74844563, 165.42310414,\\n          57.24039623, 216.56181045, 207.61017092, 155.2021625 ,\\n         119.58032539,  45.216739  ],\\n        [183.43433009, 129.77939019, 188.85223188, 220.6041117 ,\\n         244.6147311 , 218.27673533,  32.57709852, 109.19517419,\\n         190.59792373,  67.758529  ],\\n        [144.62805336,  41.95435743,  73.99923548,  34.07479478,\\n         204.61349474,  79.26343137, 105.97736975, 234.17788456,\\n         184.97116842,  37.0045645 ],\\n        [ 66.23271443, 166.72524833, 156.99028949,  82.3364207 ,\\n         235.52707954, 165.07251031, 147.83921667, 159.7801948 ,\\n         190.62684649,  98.62789901],\\n        [117.60863369,  76.16562477,  70.96246129, 237.7619258 ,\\n         192.70117491, 137.9009379 ,  80.03121815,  85.95842599,\\n          42.76641527, 125.57165762]]])').", "error": "TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[150.73897086, 187.3416606 , 162.60794274, 149.87430026,\\n         123.20405585, 172.09670487, 126.26918648, 226.19006017,\\n         242.00580731, 114.35713414],\\n        [204.17950838, 146.35688235, 154.96980344, 233.63126042,\\n          45.6279328 ,  49.16844593,  34.44804744, 213.17636602,\\n         201.19448521, 221.40267261],\\n        [245.29603529, 205.81488413, 131.5254597 , 201.71641878,\\n          56.02037369, 170.78262469,  61.53772323, 237.82716175,\\n         144.80663079, 121.2256268 ],\\n        [ 88.20223466, 200.33141168, 130.35307309, 155.05546875,\\n          34.1337561 , 165.87980936, 164.661059  , 165.72547931,\\n         237.62457727, 180.0004658 ],\\n        [109.09173813, 126.14702984, 183.4788631 ,  43.24960376,\\n         176.6886774 , 177.54033132,  76.28416344,  58.36378548,\\n          99.3942372 , 110.01636961],\\n        [155.44328949, 126.49233296, 247.44224437,  52.44985836,\\n          75.95288634,  65.48809393, 173.6838316 ,  85.72415256,\\n         132.58837003,  83.77363024],\\n        [ 64.9733084 ,  54.28253106, 174.39250968,  60.4002493 ,\\n          73.24811957, 111.11953755, 210.61851057,  51.36228067,\\n         214.34787965,  51.14164974],\\n        [244.8210823 , 133.10326436, 244.8874394 , 163.06601434,\\n         192.63798747,  38.6213143 ,  92.21753177,  56.44324347,\\n          95.15084345,  56.12009817],\\n        [ 99.95629947, 121.13785879,  44.1124492 , 182.34386626,\\n         154.65231993,  88.38568801, 145.11457176,  50.66691237,\\n         156.70822902, 234.44516347],\\n        [100.08516954, 176.83028359,  58.99552973, 187.59198491,\\n          93.66934045,  70.30209964, 159.03284566,  34.42366016,\\n         212.36680643,  31.03300476],\\n        [179.1196381 ,  89.4017541 , 191.74268487, 241.68147993,\\n          84.72569157, 156.75461357, 160.24922488, 155.89541927,\\n          79.07795918, 239.60478253],\\n        [128.3675833 , 216.20990794, 183.88544057,  95.43612919,\\n         209.03552033, 117.23126299, 223.84270336, 157.88003198,\\n         223.98177961, 182.35694982],\\n        [189.55594156, 140.29136402, 240.33839964, 171.67784383,\\n         123.24811068, 163.40650711,  34.22250363,  96.34645967,\\n         175.23817825,  93.81707359],\\n        [165.96339438, 124.32911421,  59.80429413,  95.62211171,\\n         155.39228035, 159.99200747, 156.35155475, 173.70418037,\\n         173.4627194 , 124.9120558 ],\\n        [227.24025109, 110.86361141, 125.89028356, 226.2231381 ,\\n         207.36267759, 184.85548838,  52.04991521, 232.28617502,\\n         187.1330859 , 249.74634144],\\n        [ 62.87862702, 220.98773262,  65.74844563, 165.42310414,\\n          57.24039623, 216.56181045, 207.61017092, 155.2021625 ,\\n         119.58032539,  45.216739  ],\\n        [183.43433009, 129.77939019, 188.85223188, 220.6041117 ,\\n         244.6147311 , 218.27673533,  32.57709852, 109.19517419,\\n         190.59792373,  67.758529  ],\\n        [144.62805336,  41.95435743,  73.99923548,  34.07479478,\\n         204.61349474,  79.26343137, 105.97736975, 234.17788456,\\n         184.97116842,  37.0045645 ],\\n        [ 66.23271443, 166.72524833, 156.99028949,  82.3364207 ,\\n         235.52707954, 165.07251031, 147.83921667, 159.7801948 ,\\n         190.62684649,  98.62789901],\\n        [117.60863369,  76.16562477,  70.96246129, 237.7619258 ,\\n         192.70117491, 137.9009379 ,  80.03121815,  85.95842599,\\n          42.76641527, 125.57165762]]])')", "parent_id": "d36684e0-fd72-454b-a4aa-c86330d8ce9a", "metadata": {}, "mutation_prompt": null}
{"id": "f1784542-d14a-41dd-b7b2-71f74e5e9bc6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                # Changed line below: adapt F more dynamically based on improvement\n                adaptive_F = F * (1 + np.random.normal(0, 0.1) * (best_score - func(pop[j])) / best_score)\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR * (1 - self.evaluations/self.budget))\n\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * self.evaluations / self.budget  # Modified line: refine local search's tolerance dynamically\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by refining local search's tolerance dynamically based on remaining budget.", "configspace": "", "generation": 20, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[150.73897086, 187.3416606 , 162.60794274, 149.87430026,\\n         123.20405585, 172.09670487, 126.26918648, 226.19006017,\\n         242.00580731, 114.35713414],\\n        [204.17950838, 146.35688235, 154.96980344, 233.63126042,\\n          45.6279328 ,  49.16844593,  34.44804744, 213.17636602,\\n         201.19448521, 221.40267261],\\n        [245.29603529, 205.81488413, 131.5254597 , 201.71641878,\\n          56.02037369, 170.78262469,  61.53772323, 237.82716175,\\n         144.80663079, 121.2256268 ],\\n        [ 88.20223466, 200.33141168, 130.35307309, 155.05546875,\\n          34.1337561 , 165.87980936, 164.661059  , 165.72547931,\\n         237.62457727, 180.0004658 ],\\n        [109.09173813, 126.14702984, 183.4788631 ,  43.24960376,\\n         176.6886774 , 177.54033132,  76.28416344,  58.36378548,\\n          99.3942372 , 110.01636961],\\n        [155.44328949, 126.49233296, 247.44224437,  52.44985836,\\n          75.95288634,  65.48809393, 173.6838316 ,  85.72415256,\\n         132.58837003,  83.77363024],\\n        [ 64.9733084 ,  54.28253106, 174.39250968,  60.4002493 ,\\n          73.24811957, 111.11953755, 210.61851057,  51.36228067,\\n         214.34787965,  51.14164974],\\n        [244.8210823 , 133.10326436, 244.8874394 , 163.06601434,\\n         192.63798747,  38.6213143 ,  92.21753177,  56.44324347,\\n          95.15084345,  56.12009817],\\n        [ 99.95629947, 121.13785879,  44.1124492 , 182.34386626,\\n         154.65231993,  88.38568801, 145.11457176,  50.66691237,\\n         156.70822902, 234.44516347],\\n        [100.08516954, 176.83028359,  58.99552973, 187.59198491,\\n          93.66934045,  70.30209964, 159.03284566,  34.42366016,\\n         212.36680643,  31.03300476],\\n        [179.1196381 ,  89.4017541 , 191.74268487, 241.68147993,\\n          84.72569157, 156.75461357, 160.24922488, 155.89541927,\\n          79.07795918, 239.60478253],\\n        [128.3675833 , 216.20990794, 183.88544057,  95.43612919,\\n         209.03552033, 117.23126299, 223.84270336, 157.88003198,\\n         223.98177961, 182.35694982],\\n        [189.55594156, 140.29136402, 240.33839964, 171.67784383,\\n         123.24811068, 163.40650711,  34.22250363,  96.34645967,\\n         175.23817825,  93.81707359],\\n        [165.96339438, 124.32911421,  59.80429413,  95.62211171,\\n         155.39228035, 159.99200747, 156.35155475, 173.70418037,\\n         173.4627194 , 124.9120558 ],\\n        [227.24025109, 110.86361141, 125.89028356, 226.2231381 ,\\n         207.36267759, 184.85548838,  52.04991521, 232.28617502,\\n         187.1330859 , 249.74634144],\\n        [ 62.87862702, 220.98773262,  65.74844563, 165.42310414,\\n          57.24039623, 216.56181045, 207.61017092, 155.2021625 ,\\n         119.58032539,  45.216739  ],\\n        [183.43433009, 129.77939019, 188.85223188, 220.6041117 ,\\n         244.6147311 , 218.27673533,  32.57709852, 109.19517419,\\n         190.59792373,  67.758529  ],\\n        [144.62805336,  41.95435743,  73.99923548,  34.07479478,\\n         204.61349474,  79.26343137, 105.97736975, 234.17788456,\\n         184.97116842,  37.0045645 ],\\n        [ 66.23271443, 166.72524833, 156.99028949,  82.3364207 ,\\n         235.52707954, 165.07251031, 147.83921667, 159.7801948 ,\\n         190.62684649,  98.62789901],\\n        [117.60863369,  76.16562477,  70.96246129, 237.7619258 ,\\n         192.70117491, 137.9009379 ,  80.03121815,  85.95842599,\\n          42.76641527, 125.57165762]]])').", "error": "TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[150.73897086, 187.3416606 , 162.60794274, 149.87430026,\\n         123.20405585, 172.09670487, 126.26918648, 226.19006017,\\n         242.00580731, 114.35713414],\\n        [204.17950838, 146.35688235, 154.96980344, 233.63126042,\\n          45.6279328 ,  49.16844593,  34.44804744, 213.17636602,\\n         201.19448521, 221.40267261],\\n        [245.29603529, 205.81488413, 131.5254597 , 201.71641878,\\n          56.02037369, 170.78262469,  61.53772323, 237.82716175,\\n         144.80663079, 121.2256268 ],\\n        [ 88.20223466, 200.33141168, 130.35307309, 155.05546875,\\n          34.1337561 , 165.87980936, 164.661059  , 165.72547931,\\n         237.62457727, 180.0004658 ],\\n        [109.09173813, 126.14702984, 183.4788631 ,  43.24960376,\\n         176.6886774 , 177.54033132,  76.28416344,  58.36378548,\\n          99.3942372 , 110.01636961],\\n        [155.44328949, 126.49233296, 247.44224437,  52.44985836,\\n          75.95288634,  65.48809393, 173.6838316 ,  85.72415256,\\n         132.58837003,  83.77363024],\\n        [ 64.9733084 ,  54.28253106, 174.39250968,  60.4002493 ,\\n          73.24811957, 111.11953755, 210.61851057,  51.36228067,\\n         214.34787965,  51.14164974],\\n        [244.8210823 , 133.10326436, 244.8874394 , 163.06601434,\\n         192.63798747,  38.6213143 ,  92.21753177,  56.44324347,\\n          95.15084345,  56.12009817],\\n        [ 99.95629947, 121.13785879,  44.1124492 , 182.34386626,\\n         154.65231993,  88.38568801, 145.11457176,  50.66691237,\\n         156.70822902, 234.44516347],\\n        [100.08516954, 176.83028359,  58.99552973, 187.59198491,\\n          93.66934045,  70.30209964, 159.03284566,  34.42366016,\\n         212.36680643,  31.03300476],\\n        [179.1196381 ,  89.4017541 , 191.74268487, 241.68147993,\\n          84.72569157, 156.75461357, 160.24922488, 155.89541927,\\n          79.07795918, 239.60478253],\\n        [128.3675833 , 216.20990794, 183.88544057,  95.43612919,\\n         209.03552033, 117.23126299, 223.84270336, 157.88003198,\\n         223.98177961, 182.35694982],\\n        [189.55594156, 140.29136402, 240.33839964, 171.67784383,\\n         123.24811068, 163.40650711,  34.22250363,  96.34645967,\\n         175.23817825,  93.81707359],\\n        [165.96339438, 124.32911421,  59.80429413,  95.62211171,\\n         155.39228035, 159.99200747, 156.35155475, 173.70418037,\\n         173.4627194 , 124.9120558 ],\\n        [227.24025109, 110.86361141, 125.89028356, 226.2231381 ,\\n         207.36267759, 184.85548838,  52.04991521, 232.28617502,\\n         187.1330859 , 249.74634144],\\n        [ 62.87862702, 220.98773262,  65.74844563, 165.42310414,\\n          57.24039623, 216.56181045, 207.61017092, 155.2021625 ,\\n         119.58032539,  45.216739  ],\\n        [183.43433009, 129.77939019, 188.85223188, 220.6041117 ,\\n         244.6147311 , 218.27673533,  32.57709852, 109.19517419,\\n         190.59792373,  67.758529  ],\\n        [144.62805336,  41.95435743,  73.99923548,  34.07479478,\\n         204.61349474,  79.26343137, 105.97736975, 234.17788456,\\n         184.97116842,  37.0045645 ],\\n        [ 66.23271443, 166.72524833, 156.99028949,  82.3364207 ,\\n         235.52707954, 165.07251031, 147.83921667, 159.7801948 ,\\n         190.62684649,  98.62789901],\\n        [117.60863369,  76.16562477,  70.96246129, 237.7619258 ,\\n         192.70117491, 137.9009379 ,  80.03121815,  85.95842599,\\n          42.76641527, 125.57165762]]])')", "parent_id": "d36684e0-fd72-454b-a4aa-c86330d8ce9a", "metadata": {}, "mutation_prompt": null}
{"id": "63319e8d-fa2c-4369-91b2-65d2b5dec664", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                adaptive_F = F * (1 + np.random.normal(0, 0.1) * (best_score - func(pop[j])) / best_score)\n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                # Modified line below: dynamically adjust CR based on progress\n                crossover = np.random.rand(self.dim) < (CR * (1 - (best_score - func(pop[j])) / best_score))\n\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8)\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by introducing a dynamic adjustment of the crossover rate based on convergence progress while ensuring robust exploration.", "configspace": "", "generation": 20, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[150.73897086, 187.3416606 , 162.60794274, 149.87430026,\\n         123.20405585, 172.09670487, 126.26918648, 226.19006017,\\n         242.00580731, 114.35713414],\\n        [204.17950838, 146.35688235, 154.96980344, 233.63126042,\\n          45.6279328 ,  49.16844593,  34.44804744, 213.17636602,\\n         201.19448521, 221.40267261],\\n        [245.29603529, 205.81488413, 131.5254597 , 201.71641878,\\n          56.02037369, 170.78262469,  61.53772323, 237.82716175,\\n         144.80663079, 121.2256268 ],\\n        [ 88.20223466, 200.33141168, 130.35307309, 155.05546875,\\n          34.1337561 , 165.87980936, 164.661059  , 165.72547931,\\n         237.62457727, 180.0004658 ],\\n        [109.09173813, 126.14702984, 183.4788631 ,  43.24960376,\\n         176.6886774 , 177.54033132,  76.28416344,  58.36378548,\\n          99.3942372 , 110.01636961],\\n        [155.44328949, 126.49233296, 247.44224437,  52.44985836,\\n          75.95288634,  65.48809393, 173.6838316 ,  85.72415256,\\n         132.58837003,  83.77363024],\\n        [ 64.9733084 ,  54.28253106, 174.39250968,  60.4002493 ,\\n          73.24811957, 111.11953755, 210.61851057,  51.36228067,\\n         214.34787965,  51.14164974],\\n        [244.8210823 , 133.10326436, 244.8874394 , 163.06601434,\\n         192.63798747,  38.6213143 ,  92.21753177,  56.44324347,\\n          95.15084345,  56.12009817],\\n        [ 99.95629947, 121.13785879,  44.1124492 , 182.34386626,\\n         154.65231993,  88.38568801, 145.11457176,  50.66691237,\\n         156.70822902, 234.44516347],\\n        [100.08516954, 176.83028359,  58.99552973, 187.59198491,\\n          93.66934045,  70.30209964, 159.03284566,  34.42366016,\\n         212.36680643,  31.03300476],\\n        [179.1196381 ,  89.4017541 , 191.74268487, 241.68147993,\\n          84.72569157, 156.75461357, 160.24922488, 155.89541927,\\n          79.07795918, 239.60478253],\\n        [128.3675833 , 216.20990794, 183.88544057,  95.43612919,\\n         209.03552033, 117.23126299, 223.84270336, 157.88003198,\\n         223.98177961, 182.35694982],\\n        [189.55594156, 140.29136402, 240.33839964, 171.67784383,\\n         123.24811068, 163.40650711,  34.22250363,  96.34645967,\\n         175.23817825,  93.81707359],\\n        [165.96339438, 124.32911421,  59.80429413,  95.62211171,\\n         155.39228035, 159.99200747, 156.35155475, 173.70418037,\\n         173.4627194 , 124.9120558 ],\\n        [227.24025109, 110.86361141, 125.89028356, 226.2231381 ,\\n         207.36267759, 184.85548838,  52.04991521, 232.28617502,\\n         187.1330859 , 249.74634144],\\n        [ 62.87862702, 220.98773262,  65.74844563, 165.42310414,\\n          57.24039623, 216.56181045, 207.61017092, 155.2021625 ,\\n         119.58032539,  45.216739  ],\\n        [183.43433009, 129.77939019, 188.85223188, 220.6041117 ,\\n         244.6147311 , 218.27673533,  32.57709852, 109.19517419,\\n         190.59792373,  67.758529  ],\\n        [144.62805336,  41.95435743,  73.99923548,  34.07479478,\\n         204.61349474,  79.26343137, 105.97736975, 234.17788456,\\n         184.97116842,  37.0045645 ],\\n        [ 66.23271443, 166.72524833, 156.99028949,  82.3364207 ,\\n         235.52707954, 165.07251031, 147.83921667, 159.7801948 ,\\n         190.62684649,  98.62789901],\\n        [117.60863369,  76.16562477,  70.96246129, 237.7619258 ,\\n         192.70117491, 137.9009379 ,  80.03121815,  85.95842599,\\n          42.76641527, 125.57165762]]])').", "error": "TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. sophisticated_antireflection_design (iid=1 dim=10)>, array([[[150.73897086, 187.3416606 , 162.60794274, 149.87430026,\\n         123.20405585, 172.09670487, 126.26918648, 226.19006017,\\n         242.00580731, 114.35713414],\\n        [204.17950838, 146.35688235, 154.96980344, 233.63126042,\\n          45.6279328 ,  49.16844593,  34.44804744, 213.17636602,\\n         201.19448521, 221.40267261],\\n        [245.29603529, 205.81488413, 131.5254597 , 201.71641878,\\n          56.02037369, 170.78262469,  61.53772323, 237.82716175,\\n         144.80663079, 121.2256268 ],\\n        [ 88.20223466, 200.33141168, 130.35307309, 155.05546875,\\n          34.1337561 , 165.87980936, 164.661059  , 165.72547931,\\n         237.62457727, 180.0004658 ],\\n        [109.09173813, 126.14702984, 183.4788631 ,  43.24960376,\\n         176.6886774 , 177.54033132,  76.28416344,  58.36378548,\\n          99.3942372 , 110.01636961],\\n        [155.44328949, 126.49233296, 247.44224437,  52.44985836,\\n          75.95288634,  65.48809393, 173.6838316 ,  85.72415256,\\n         132.58837003,  83.77363024],\\n        [ 64.9733084 ,  54.28253106, 174.39250968,  60.4002493 ,\\n          73.24811957, 111.11953755, 210.61851057,  51.36228067,\\n         214.34787965,  51.14164974],\\n        [244.8210823 , 133.10326436, 244.8874394 , 163.06601434,\\n         192.63798747,  38.6213143 ,  92.21753177,  56.44324347,\\n          95.15084345,  56.12009817],\\n        [ 99.95629947, 121.13785879,  44.1124492 , 182.34386626,\\n         154.65231993,  88.38568801, 145.11457176,  50.66691237,\\n         156.70822902, 234.44516347],\\n        [100.08516954, 176.83028359,  58.99552973, 187.59198491,\\n          93.66934045,  70.30209964, 159.03284566,  34.42366016,\\n         212.36680643,  31.03300476],\\n        [179.1196381 ,  89.4017541 , 191.74268487, 241.68147993,\\n          84.72569157, 156.75461357, 160.24922488, 155.89541927,\\n          79.07795918, 239.60478253],\\n        [128.3675833 , 216.20990794, 183.88544057,  95.43612919,\\n         209.03552033, 117.23126299, 223.84270336, 157.88003198,\\n         223.98177961, 182.35694982],\\n        [189.55594156, 140.29136402, 240.33839964, 171.67784383,\\n         123.24811068, 163.40650711,  34.22250363,  96.34645967,\\n         175.23817825,  93.81707359],\\n        [165.96339438, 124.32911421,  59.80429413,  95.62211171,\\n         155.39228035, 159.99200747, 156.35155475, 173.70418037,\\n         173.4627194 , 124.9120558 ],\\n        [227.24025109, 110.86361141, 125.89028356, 226.2231381 ,\\n         207.36267759, 184.85548838,  52.04991521, 232.28617502,\\n         187.1330859 , 249.74634144],\\n        [ 62.87862702, 220.98773262,  65.74844563, 165.42310414,\\n          57.24039623, 216.56181045, 207.61017092, 155.2021625 ,\\n         119.58032539,  45.216739  ],\\n        [183.43433009, 129.77939019, 188.85223188, 220.6041117 ,\\n         244.6147311 , 218.27673533,  32.57709852, 109.19517419,\\n         190.59792373,  67.758529  ],\\n        [144.62805336,  41.95435743,  73.99923548,  34.07479478,\\n         204.61349474,  79.26343137, 105.97736975, 234.17788456,\\n         184.97116842,  37.0045645 ],\\n        [ 66.23271443, 166.72524833, 156.99028949,  82.3364207 ,\\n         235.52707954, 165.07251031, 147.83921667, 159.7801948 ,\\n         190.62684649,  98.62789901],\\n        [117.60863369,  76.16562477,  70.96246129, 237.7619258 ,\\n         192.70117491, 137.9009379 ,  80.03121815,  85.95842599,\\n          42.76641527, 125.57165762]]])')", "parent_id": "d36684e0-fd72-454b-a4aa-c86330d8ce9a", "metadata": {}, "mutation_prompt": null}
{"id": "2f2e4b44-9da8-42e5-a919-5c7c0993fe6b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        norm_pop = np.random.rand(pop_size, self.dim)\n        pop = lb + norm_pop * (ub - lb)\n        \n        best_idx = None\n        best_score = float('inf')\n\n        while self.evaluations < self.budget:\n            new_pop = np.empty_like(pop)\n            F = 0.5 + 0.3 * (self.budget - self.evaluations) / self.budget\n            for j in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != j]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                # Changed line below: adapt F based on relative improvement\n                adaptive_F = F * (1 + np.random.normal(0, 0.1) * (np.std(pop) / np.std(new_pop)))  \n                mutant = np.clip(a + adaptive_F * (b - c), lb, ub)\n                \n                crossover = np.random.rand(self.dim) < (CR * (1 - self.evaluations/self.budget))\n\n                weight = 0.5 * (self.budget - self.evaluations) / self.budget\n                trial = np.where(crossover, mutant, pop[j] * weight + mutant * (1 - weight))\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                if trial_score < func(pop[j]):\n                    new_pop[j] = trial\n                else:\n                    new_pop[j] = pop[j]\n                \n                if trial_score < best_score:\n                    best_score = trial_score\n                    best_idx = j\n\n            pop = new_pop\n            pop_size = max(10, int(pop_size * (1 + (best_score - func(pop[best_idx])) / best_score)))\n            CR = 0.9 - 0.7 * (self.evaluations / self.budget) + 0.1 * np.random.normal(0, 0.1)\n\n            if self.evaluations >= self.budget // 2 or \\\n               best_score - func(pop[best_idx]) < 1e-6:\n                break\n\n        return pop[best_idx]\n\n    def local_search(self, func, x0, bounds):\n        tol = 1e-6 * (self.budget - self.evaluations) / self.budget\n        res = minimize(func, x0, bounds=[(b, ub) for b, ub in zip(bounds[0], bounds[1])], method='L-BFGS-B', tol=tol * 0.8)  # Modification: added scaling to tol\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n\n        step = max(1, self.dim // 10)\n        for new_dim in range(step, self.dim + 1, step):\n            sub_bounds = np.array([func.bounds.lb, func.bounds.ub])[:, :new_dim]\n            best_solution[:new_dim] = self.local_search(func, best_solution[:new_dim], sub_bounds)\n\n            if self.evaluations >= self.budget:\n                break\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by dynamically updating the mutation factor based on the relative improvement in population diversity.", "configspace": "", "generation": 20, "fitness": 0.8546702819550932, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.019. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "d36684e0-fd72-454b-a4aa-c86330d8ce9a", "metadata": {"aucs": [0.8287374112064817, 0.8733986424441198, 0.8618747922146779], "final_y": [0.13952945328982513, 0.1292933283987978, 0.1286602940026068]}, "mutation_prompt": null}
